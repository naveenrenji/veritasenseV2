Address, the specific issue, keeping in mind that each time you go up one level you will get more calls, because at level one can be automated. Level 2, you have people that are not the software engineer. So software developers. But they have a functional knowledge. When you go to the software developers, they cost more to reasons why. Not? Because that's probably the M. Salad is higher, but also because you are disacting them from developing new software, making a updates. How they access insult. So those are a TV guidelines. 3. Level on the customers. Some other issues, recruiting, hiding, software personnel. That's always a not easy things to do. So there are several characteristics that you need to match. Obviously you need to match technical skills. So you need to be sure that the people you are hiring is a able to do the things that he would be. Hoshi would be higher for so and that's kind of a easy point. But then you have other skills so we'll be talking a moment about that. The project team has a sort of a life cycle, so it doesn't stop with the hiding so you need to hire the right people. Give you an example. It's an example that they use all the I was working in a technology company. I was a co-o and a CTO for the company, and we hire the A team leader. So one who was in charge to manager A, the technical team. So the company was doing a ticketing I mean our our largest client for the Disney. We were in Orlando, Florida, and we managed the the tickets issued by Disney to people like going to their team. Parks. It's there are several conditions. So there is a time, condition. That is a man issue. There are customer federalization issues. So there are many issues seems to be an easy problem ticketing command is a ticket not so easy. So I I mean, we interviewed quite a lot of candidates. We ended up with a candidate that was interviewed by several of us, so I hired him, and then, after he had all the boxes checked, he had the right scale. So apparently the right motivations, and so on. After a few days it started saying, I wear a car keys the rest of the team is wearing shorts. I don't feel comfortable. How can we do that? I don't want to give up my targets, and I said, You know what it's Florida. I mean people sometimes. So they come to the office. In short, what can you do? And that's the first one and the second one was I I come to the office very early, 6, 37 tops, and I want to live early. I want to live after 8 h, but most of the team is saying longer. I mean, if you are the team leader, you need to be the first to come in the last to leave. So those are things that are really difficult to detect when you are hiding the person. So for sure there was a a probation period and a we didn't keep the the person, and I don't know what is doing now about. We hire the L later on, another person. So this is just to say that there are technical issues, technical great, so that you can like easily test. But there are software issues that are more difficult to detect. So again, when you create a team, there is the formation of the team, the development, the maintenance is kind of a a, a life cycle. When you build a team again, you really need to keep in mind. The people will work in a team. So sometimes is more relevant to have a a good person doing okay from the technical sent point. Okay. But the real team player then, having a genius who is a zo social skill and not a team player, can you mute yourself, please? So those are things that you really need to keep in mind Let me skip some of those And we already mentioned that developing the team. So once you have the team, you need to be sure that you keep the team motivated. You need, to be sure that you provide the the right reasons to be engaged, and there is no single way to do it, because each team is different. Each person in the team is different, and you need to understand what people want. If you really want to keep the the team sometimes can be just just money sometimes can be challenges, sometimes can be the environment, the quarrel leaving. And each time you need to compromise, because you cannot give unlimited money. You cannot say you stay home all the time, and you work 3 h a day, so I mean, you need to compromise. But the it is like in everything in life. You need to keep the attention, as it is not too much not to little. So, if the attention is to match, it will break, meaning the teamm member will leave if it's too little, they will underperform, and you will never have your job. So let me stop for a second the sharing. Yes, and let me be sure that everyone is muted because I still have some noises Okay. Now, everyone should be muted Alright, so let me Go back. Sorry for the interruption. So developing a team, creating new challenges, make sure that everybody is on board and stay on board When I was managing just to continue with the same example. So I was a cause. You and CTO. All of the company, but the vast majority of my time was a being the terrorist dealing with with personal problems. I mean you human beings. We have issues, and sometimes, if you are the boss or the one in charge for the team, at whatever level it is, you need to make sure that you create the the right within the the team and the trusting you as a manager, or whatever is the role that that you Have. And then you need to deal with the problems that they may have. I mean, sometimes our relational problems. I don't like the people in my team, and then that you need to negotiate because you cannot just decide that you are fired. But you say on board the sometimes he's a mother of the type of a TV that they are doing so. And then again, you need to compromise some. Being a manager. It's a lot of compromises it's it's very rare that the solutions are black and white. So you need to work with that you need to work with people when you work with people you have a is like a vector or a, if you have one, vector you have one direction. If you have m, vectors, you have n directions, and then you need to do a sort of resulting vectors all over all of it. Knowing that you will use something. So sometimes you cannot motivate people. You cannot do what some people may ask, and then, at a point, there is no way to come back. So anyway, I just wanted to give you a little bit of a highlight on what can be behind the scenes? What can be in a hiring, managing teams, maintaining the teams and growing things. So I would be happy to share other experiences if you like, but that's something that, in my opinion, can be useful. I mean, we are engineering management down the road, or right now you are managing teams, and when you have taken your people like you will probably do, considering your engineering , you may have personality. So you have a technical issues, and you have a soft issues, even issues that could be. I mean more on the personal, the relational aspects is interesting. That when you work in a consulting company, so he wasn't consulting a good portion of my career, and when you assemble teams and you assemble teams quite often when you are in a consulting because each time you have a new Job, new new plans, or the new Pvd. With with an existing client you need to assemble the team, so you need to send people there to do the job physically or lost easily in a remote work. Times, how you you create a team. So so you are a consulting company. You are a for profit organization, meaning you have. You will want to create the team that will deliver the best results at the lowest cost possible. So you may not want to have your superstars involved the all the time. So you want to have a probably one super Sarah, and a bunch of supporters. But then how you mix a match. So you need to keep in mind who has the competencies on one side, but on the other side you also need to work on the compatibility. A few years ago, Price Waterhouse created the an AI to do the matching. So they, I mean, improve the the existing apps on a line, database, with the additional information I'm not decides technical up the soft skills. So the compatibility, the history of companies. And then they had this system doing the match. So that seems to be a trivial, but when you have, like like a And it's or 1,000 or projects, and you need to have a 100. So 1,000 a teams project can last longer or short time. So if it's longer time, it's kind of more manageable, actually different problems. But if you have a shorter projects, I mean, you have a continuous need of creating a team. S, and having a sort of an expert system. An AI, that will help you. It's really a big deal Alright! So that was on something not related to Python. Let's go back to Python and let's talk about input outputs and dictionaries and tubes. So it's really difficult to to think about any program. Any code that you can write without having any form of interaction with the outside world. So far the form of interaction that we used was taking an input from the user. But most of the time you have files that you read you process, you do something, and eventually you write the results. So input output. He's a an essential portion of writing code in any language. Most of the time we will use files so that are up text like. So it can be a play in a T Xc. File can be a Csv. Comma, but I did value there is pretty much a text, but they have the advantage that they can be read as a tables in excel where the comma is what is telling excel to that one character is in one column, and the character After the comma is in the following column, and so on. So we primarily use a txt and Csv files. But keep in mind that that that I don't can handle. Look pretty much any type of file, let's say, all up the open source type of files can be handled by python. Then, if you have a probably the reformata , then you may not have the library call for reading it, but then is a different problem. When you open a a file, you will use the def function. Open, so open. Now we'll make the file available for the type of operation that you are going to do. So a text file or a Csv file, it's typically a sequence of lines. So each line can be a record, or can be an item to be processed The open function is called the file handle, and is what is used to again start the process of either reading or writing files. So it's like, in a wordprocessor your file open, and then you specify the name of the. They. Syntax is a whatever is the name of the variable open by name, mode. The mode or modality is either a read writer or a read writer, so those are the options you have when you open a file for reading, you do not need to specify the mode, so the default is our then if you want to write, then it will be w so. By name. That can be a variable with the name of the file, or can be a string with the name like in this case, is inbox dot txt so, but it could be. I can create a value called infile, and then over here I would have open in file if I want to do read only that could be the end. I would be a close parenthesis. So file open. That's the case. It's reading it because it's the default. If you print it, you don't print the actual file, but you print the pointer to to the file, and that's what you have. Then that yeah, I mean, again, file is not the result word you can use it to name the file. Not that you need to, but you can. So again, when you print the the variable, or it is the handle on the file you will get only the point that to define, not the real content. Let's do an example. So you have this file is a txt file. You have one line, then you have a a blank line and another line with other text. So you open the file in a readinger Then, for example, you initialize account for the number of lines, and you start the loop. So the barrier bowl that will be in the Loopo. It's called text line, and it it's it will loop into the file. So, line count. That will be was 0 will become one, and then it will print the line counter. That would be one, and that that the text line and it is the phone line of the file. And then we go an app, and it will be to the second line, or is a blank. Third, and then the end. So, and that's the outcome. So R is reading. You don't need to have it. The handle is the essential to point the file that you are going to use Again. You can do loops That's another example. You can have a a counter You can get the number of records so you can read the file with file. Read oops, and then, with the Len over the read the the this point. You have the entire file in this value, and then that you can print the number of records keeping in mind that that blanks will count as one like in this example define as 3 lines just like you can see here And then, because, as we know, strings and lists can be partitioned somehow, you can take a piece of it Could you please mute yourself? Thank you. So let's say you have a some tweets. So is that text file with one tweet each line. So, this is a a via case. I don'tloaded the those suites a while ago. Oh! You have again a file with the all the texts on the of tweets, and then that's an example. How to do it, how how to do something. I will explain what is the same thing. So you open the file again. You don't need the to have Ara, because going to be in a read mode, then, that I, initializing a list containing a the words with the hashtag, and then I stop the loop into the file, so the first Hi Duration would be the first one. This one on the top Then what I'm doing is basically the split common will create a list out of the string. So each line. For example, this one will be read as a string. You play to separate the the words I need to split the string, creating a list where each element is a ward of the initial string. So. And that's what I'm doing here. So split will split the string if nothing is specified in the parentheses will be blank. So if there is a blanket, there is a space that could be the end of the world. Then once I had the list of all the words, I'm looping into this list, checking if the world is starting with hashtag, if the word is starting with dash, then I will append the word to the list of hashtags, if not we'll continue, so at the end of The loop, for I will analyze everything that is, you know, the first line that I mean at the first round will be the first line. So in this case, at the end of the loop there will be nothing, because there is no ward. The starting with, I will, so it will end this looper. We'll go back next line. Same thing at the very end. It will, in the variable in the list, hashtag. I will have all the wards starting with dash, tag, and then I'm printing the first 10, and that's what I got. So again, that's an example just to practice how to use the open and and how to create a a case. Somehow you can use. Continue if you don't see like a I mean in this case you knew wouldn't make sense, it's intrinsically true, but you can have it so. If not, if it is not starting with from continue meaning. You just keep. I mean, we can. We could have done that to the previous example. That's I would skip that again, drawing your attention to the split statement. So you have a this. Variable here. If you split it with the exclamation marker, you will have a the first, I mean, you will get a list. The list will have a Hello. Then you have a the exclamation market. That will be the separation item. Then you have the second one that will start with the space and will continue till the d. Over cold, then the next one will be space, and then you have all the rest again. The default is the blank is the white space, and if you do not with exclamation Mark! But with the space you will will basically have every single word keeping in mind that is, not removing the special characters. So I mean, the first word is Hello! With exclamation Mark! And so on, so the punctuation would not be removed. Let's say I have a this file here. So is a Csv meaning it's a commercial separated value. You, but you can open it with the regular text. Edit, or with excel. So you have, John American, one Mexican. Yuan is Greek, Jean, that is French. So you open the file. Then you print nationalities, and then you start looping in it. So you you take the first row. You split it by the comma, meaning the list. That you will generate will have 2 elements, John and America, and then you will preint the the second element. That will be the nationality meaning you have a American, Mexican, Greek, and French. You don't see it. But at the end of each character there is a which line there is a special character that is new line. So when you have new line, you have this blank current is the new line, the skip line that you have it to remove it. You want to use strip strip is striping the the the string by all the spaces and all the special characters that are either left or right, so if you don't specify anything that means left and right, you can specify our or L for left or right and at that point, we didn't move only from the right or from the left. If you do that, you are basically removing the new line. That is a deal the over each line, and you will not have the space that you have here. So, and then you have the nationalities just the way they probably supposed to be writing a file. Is not that much different? Is Mmm. I mean, instead of r is W. So you write the file with the name of the handle right? And then you write whatever you want to do. You may want to have a new line eventually, and that's what you have in Python 2 point something you had to close the file with the close statement after writing it with python 3 is not required that I generally do it just because It's giving me the end, the beginning, Internet. Okay. I will keep that. Those are some of the links. So let's talk now about dictionaries and 2 balls. So we mentioned a strings integer. So floating lists, there are 2 more types of variables that you will use one of the 2 at least, that we will use a lot or a significant number of times to build so not that much dictionary. Yes, so so when you have a Elisa, lists are ordered meaning, you create a list. In a certain way they are mute. A bowl, but unless you change either the order or the content, you will say the same for the duration of your program. So, if you have a Bcd, that's the way you created, and you printed, you will end time, you an end time. You. You will get a Bcd. If you go for the third element, meaning alphabet 2, you will get see? So that's what is A is going to say. This is not the case. With the addition of this. So list our collections. This is kind of a a recap, but this are mutable. You can change it. You can get the length of a string. You can generate ranges as a list of ranges of numbers. As a list you can do a little bit of operations. You can do again. Then we already mentioned Max mean some. Those are all things that you can do. Once you are sure that the content is nomadical, otherwise you would get an error. You can split the. So we already saw that last class With tuples. Things are a little bit different from lists. They are kind of similar meaning. You have elements inside the the party, Ebola. They? They are separated by comma inside can be pretty much anything. But there are 2 main differences, one instead of square brackets. You have round the brackets regular parentheses. And they are a non mutable meaning. You cannot change this from A to the so if you try to do it, you will get an app. So, who both are immutable lists are mutable, and that's the main difference. There are not many cases when you use a tuple, so sometime you use 2 pulse when you want to be sure that the content would not be changed by your program. Sometimes we do think so, that are changing content or variables. We may or may not want to do that. If we want to be sure that is not going to happen, instead of a list you created to go, I probably use the Tuples in less than 5%. Or of my code Again. You can assign it to you can create it, but you cannot modify it. Dictionaries. So we mentioned that lists and tables are all that dictionaries are an order, meaning that when you create a addiction, it's a dictionary. So are kind of different from lists, instead of regular parentheses, they have a con brackets like in this case, and that with each element, as 2 portions, one is the key, and one is the value. So in this case is about population. The population, or U.S.A. is a 318. Either is 59. Japan is a 1 27. So the key is you. The keys are U.S.A. Italy and Japan. The values are 3, 1859, 127. So when you create a a dictionary, you don't really need to keep the order, because you will. Recall the single element by the key they have. So meeting, keeping the order, wouldn't you benefit you at all? So in a day, right infinite wisdom they creators uhied on this side that that if it's not needed that will not so. Dictionaries are not order. Meaning that that you can call an element by calling the key, and you will get it. You can ask for all the keys or the values If you create a list and you printed, you will not necessarily get the same order, because again, they are an order, so they will print the I mean, you will print the content, but if you created the with the U.S.A. Italy Japan and you print. You can get Japan, Italy, U.S.A., or any combination. So again. Don don't expect the same order, because it is not going to be there. You can access by the key, by the position, by by, by, by the key or a combination of key value, you can add the elements. So they're mute. Ebola, you can add the, for example, Uk equals 64, and when you print it you will get an additional element Obviously the key cannot be mute. The ball line, because otherwise you will lose somehow the relationship key value You can like. In this example you have a Charlie one, the 42, Jan 100. Whatever that would mean. You do a looper, you print the the key, and you print the value for the key. Then you have a John Andred, Charlie one, and Fred 42, You can get a list of keys, values, or items meaning both from additionally. So you can transform a dictionary into a list. So in this case, I transform the I mean only the keys, the like. In this case the values or the entire items Hello! Skip! That Dictionaries are sometimes used as a form of a data representation, because you have a key and a value. And this is a sort of a basic data structure. It's very basic. Meaning. If you have something that is more complex, there are a better data structures to serve you. 1: But is a possibility. So that's another example. So you have a amendment. 1: It's based on that. This file. So you have a work counter. 1: You initialize the variable? What counter as a an empty dictionary, and then you start reading the file. 1: Then you split data meaning, you are creating a a, A, a list with all the words. 1: And then you start out the looper into each line 1: First line, second line, and so on, and then what you want to do. 1: You want to count the number of times. Each word is in the 5. 1: So initially? What counter as nothing so utterly work, counter work? 1: Ewa. What counter would the last one so and the first round that you will get an error because it is empty, so there is no element that that can be addressed at that point? 1: You will create a work counter world, e. One. If at this second round or end round the world, is there, then you will add the one to the account. 1: So at the end you will print this case the first you will create a variable with the first 10, and you will print each line at the time, and that's what you would. 1: So keep in mind that you will use this one in the in class exercise that you will do. 1: Show. 1: Counter again. It's something that we use all the time 1: And those are the usual links to the website for more. 1: The Python website for more information. So let me stop sharing for a second questions 1: Okay, so is a 7, 34. And let me introduce the in class assignment 1: And 1: Share the screen again, now 1: Let me go here. So this in class exercise as a part one and part 2. 1: So you will read the a fine name, a fine name the names that don't txt that I will show you what is inside that 1: That's the file 1: So there are a few names, and they are, I mean, unique names, and they are repeated the multiple times 1: So you read the file, you count the how many unique names so that are in the file and print the the result to the screen. 1: So the name a record. The T. Times the name 1: Then you will print the name that is used the the most in the file 1: So let me make sure 1: That the file is there 1: Yeah, you have it. Let me stop sharing 1: About that 1: And let let me create the breakout rooms 1: So I'm creating a 9 breakout rooms. 1: Each one will have 3, 4 participants. Once I create the rules, you would join the rooms, you will start working with your team and you will have about 25 min to work on it. 1: Then I will close the room, so we will discuss the solution, and then I will introduce the aside. 1: Then the next assignment, and that will be the end of the class 1: Alright! So all the rooms are closed. Let me resume the recording. 1: How was it? 1: Volunteers who is going to share what you did 1: Again. There is no judgment, and is really useful to share the issues. 1: You had the solutions you created 1: Come on! 1: I have one to pick names. Are they good 1: Okay. Let's say, Rahula 1: What do you do? 1: Okay to shy 1: Nicolas 1: Okay. That was not particularly successful. So no volunteer. Alright. 1: Okay, so let me share the screen and let me go. 1: Yeah, right. 1: So again, that's an example. This case I created the I mean, it's kind of similar to what was in one of the slides. 1: So I initialized account a counter, a a dictionary as empty. 1: Then I initialized 0. The A. Counter for the frequency, and I initialized to blank that not necessary, but to blank the name with the highest frequency. 1: Then I started the loop. I open the file, reading a one line at the time 1: And then within the line I'm removing that the blanks and special characters. 1: So let them write. Then I'm asking if line the name is an account dictionary. 1: If yes, I add one to the value. 1: Or that key, and and then I'm asking if the value is greater than the Max frequency, and initially, it will. 1: If it is, then I will update the content of the maximum frequency, and then I will also use that name as the name that will get the the maximum frequency. 1: If not, the value will be one meaning is the first time is reading that name, and and I'm moving to the next line. 1: Once everything is finished it will print the the dictionary, I mean. 1: The list of names, unique names, and then the number recording the most is the name and the end of the quarter. 1: So if I run it 1: That's what you'll have. I'm in printing as it is a dictionary is not nice looking. 1: I could have done better, but it is getting the results. 1: So you have a dot? 31 Luke, 15, Leah, 54, the name, according to Moss, is Leah, with 54 occurrences. 1: So you have probably an idea of what those names are about. 1: Okay. So I will publish this 1: And I will also P. Publish. 1: The exercise. 1: And let me talk a little bit about the exercise for next week. 1: This exercise as one level or complexity that we didn't have before. 1: So before going there, let me explain what I mean. 1: With the concept of creating a story, because what you would do would be to read the 2 files. 1: So the files will look pretty much like this. One is about a New Year, Cdcd bike. 1: So you had the duration the day stuff. Then stop, stop! 1: Stop time and things like that, and you will do some calculation on each one of the files, creating a account for the number of lines, a counter for lines with customer, the counter for the subscriber as user type, and then you calculate the percentage of users that 1: Are customers so, and you do pretty much the same for the other side. 1: So the 2 files are not similar. You did the calculation, and then there is a part 3 where you have all those numbers, so you have the default, and from N. 1: 0 to N. 5. You have a Z. One z. 2, and you can compare those. 1: So you want to check if there are more, I mean, one is related to winter. 1: One is very easy to spring. You want to compare it. 1: There are more riders in winter or in spring, and then you want to check if there are more customers than subscribers in one of the 2, and you will print the results in a proper way, and but you will also write the one-page interpretation the interpretation would be and not narrative a 1: Description. So this is something that we will do a a lot. 1: During the discourse, the the courts is, is not about writing a codes, but it's about using code to extract the value meaning from the data. 1: So let me share the screen again, and this time I will do shared sound 1: And let me go here, and I will share 2 videos with you. 1: On that data storytelling. So that's the first one 1: As I'm in the midst of applying for Residency right now, which we look grammarly, has again been a lifesaver, with my application 1: There's a very significant soft scale gap and data is showing that the gap is greatest in communication skills that's oral skills and written skills. 1: So it's kind of funny that data has proven that we're not great communicators yet. 1: The roles that operate with data are the ones that have the greatest gap in expectations to be a good communicator is greater when you're working with data, it's kind of meta. 1: So what happens when you learn to communicate well is your data moves from making sense to creating meaning. And everybody that works in data should learn how to communicate it. Well. 1: Good, when I use the word story. I'm not talking about taking your data and turn it into a fairy tale or into fiction. 1: What I'm talking about is these fundamental frameworks that work every time and those frameworks come from story. 1: So now that we can hook up Fmri machines to the brain, we're starting to see the science of what's going on in people's heads. 1: When they're listening to stories. And it's profound. 1: So if you take that brain science and wrap data with these story frameworks and apply the brain is going to respond to you the same way. 1: The brain responds when a story is being told, and that's a powerful way to communicate 1: Almost every role today is impacted by data. In fact, a Pwc study states that 67% of roles are enabled by data. 1: So that means every career in every career, progression is going to be impacted by how you handle that data. 1: So picture that when you start you learn to explore the data. 1: You're just digging around in the data, and you're an explorer at the data. 1: Well, once you become an explainer of the data that's a really big career leap. 1: You go from being an individual contributor as an explorer of the data, and then you become a strategic advisor. 1: As you learn how to explain it. Well, and sure enough, when you explain your ideas in the data well, and you take a point of view and it's true. 1: You move from strategic advisor even to leader. So it will impact your career. 1: It will impact your promotion. Criteria, and you will soon one day be a great leader. 1: Through the data that you use every day 1: And I have a second one. Another 2 min or so 1: These have your skills. You plead the up of the town. 1: But you also need to know very specifically if you want to create a data story, your first and most important step is to come up with a proper question or hypothesis. 1: You want to prove or disprove that is sort of the most crucial part, where also your specific knowledge as a journalist comes in. 1: If you cover a specific beat, it will be much easier for you to understand what an interesting question of hypothesis is 1: Next step is, is there data to answer that question? That's when most of the ideas you might have had in the first step are going to be shut down because there is simply no data that could help you analyze it. 1: But if you're lucky and you have a good question, and you find the data, then you can sort of move on to the next part, which is, prepare the data to be analyzed 1: I understand the data and see what variables there are, whether there are missing values. 1: Make sure all the spelling of names is correct and homogeneous, and once you're all set with that, then you can move on to the next step, which is the actual analysis part 1: When we, as a team work on data stories, we usually come up with, like, let's say, 8 to 20 questions we want to ask to our data set. 1: And we actually really document the question. And then we do the coding to answer that question. 1: After that we have a have an overview of our main hypotheses, and all the sub questions with the possible answers. 1: The data can give us 1: And then that's the point where we meet with a reporter and sit down. 1: Talk through what we found, and see what is most interesting from their perspective, to go into an article, to talk to experts about auto politicians, and to then talk to her in the whole thing into an entire story, that maybe features some graphics along with that the 1: Okay, so. 1: I hope I gave you an idea through those 2 videos. 1: What I mean with the you need to tell a story, so I don't want to see just called the in. 1: The reporter, I mean in this case is one page down the road. 1: It would be a longer record. So. But I want to see your interpretation. 1: So you use the the code that you will write to create the metrics, the elements that you will use to draw conclusions to create a story around the the data that that you have. 1: You don't know. You don't need to become a Jo data journalist or things like that. 1: But giving just the visuals, giving just the tables, giving just samples of code, or the description of the process. 1: That's the most common error that I see. When I ask for a sorry, most of the students, instead of giving me a story they're giving me the the process that they use. 1: I don't want the documentation on the software. I want to have a what does software gave you? 1: Which kind of additional information through the software you extract it from the data. 1: So that's data storytelling. Yes, I co-created the courts in data storytelling. 1: That is a graduate, undergraduate with the College of Arts, and that's I was doing the data portion, and they did the the storytelling. 1: But I mean, besides that, the there is a a, a real need for getting a sense out of the data, and that's one of the skills that I really want you to get in this courts alright so is a 8 21 if you don't have a 1: Any question. That's the end of the class. I really thank you for being with me. 1: So long. And if you have questions, so down the road the send me or see you an email, and I will get back to you as soon as possible.

ft STEVENS

lw INSTITUTE of TECHNOLOGY

Introduction to
EM624 & Python

a



clipizzi@stevens.edu

SSE

 

/(~ =
Users .vs. Programmers Y

Users see Computers as a set of tools - word processor,
soreadsheet, map, to-do list, etc.

Programmers learn the computer “ways” and the
computer language

Programmers have some tools that allow them to build
new tools

Programmers sometimes write tools for lots of users and
sometimes programmers write little “helpers” for
themselves to automate a task

STEVENS INSTITUTE of TECHNOLOGY | 2

 

of,
What is Code? Software? A Program? &

 

¢ A sequence of stored instructions
— It is a little piece of our intelligence in the computer

— It is a little piece of our intelligence we can give to
others - we figure something out and then we
encode if and then give It fo someone else to save
them the time and energy of figuring If out

¢ A piece of creative art - particularly when we do a
good job on user experience

STEVENS INSTITUTE of TECHNOLOGY | 3

A computer schema e


 

Central Processing Unit: Runs the
Program - The CPU is

always wondering “what to do
next’ 2

Input Devices: Keyboard, Mouse,
Touch Screen

Output Devices: Screen, Soeakers,
Printer, DVD Burner

Main Memory: Fast small temporary
storage - lost on reboot - aka RAM
Secondary Memory: Slower large
permanent storage - lasts until
deleted - disk drive / flash drive

STEVENS INSTITUTE of TECHNOLOGY | 4

 

 

Interpreted vs Compiled Y
Why is Python “slow”?

Machine
Source Executable 
source codecode 

 

preprocessing processing

Compilation

~ Interpreter
Source code
Intermediate code
 

preprocessing processing

Interpretation

STEVENS INSTITUTE of TECHNOLOGY | 5

 

cf
What is Python? &

¢ Python is a multi-paradigm, high-level, interpreted, programming
language.

¢ Multi-paradigm: supports various programming paradigm such as
procedural, object-oriented, and functional programming.

¢ High-level: easy to write, closer to human language than to
machine language.

¢ Interpreted: the program is executed directly by the interpreter,
instead of being translated in machine language.

¢ These characteristics granted if popularity in text-mining, data
analysis, scientific simulations, web-scraping, and many other
scripting tasks.

¢ lis flexibility comes with the cost of a lower performance with
resoect to other languages (C, C++, Java). For this reason it
cannot be used for high-frequency trading.

_
STEVENS INSTITUTE of TECHNOLOGY | 6

 

History of Python

Guido Van Rossum created the. first
version of Python in 1989.

The current version is Python 3, but for this
class we are going to consider Python 2.

The language is named after Monty
Python. This reflects in the use of example
variable and function names such as
spam, eggs, bacon, and sausage.

At the end of this course you will be a
Pythonista, which is someone who speaks
the Python language.

 

 

 

 

STEVENS INSTITUTE of TECHNOLOGY | 7

Installing Python e

Go to: hitps://www.python.orgq/downloads/release thon-2712

Select the version corresponding to your operating system, download it, and
install it.

Python can be used either through:
¢ The official Python Integrated DeveLopment Environment (IDLE)

¢ Third part IDEs such as Canopy or PyCharm

STEVENS INSTITUTE of TECHNOLOGY | g

 

 

First Examples we
">>>" is the ‘orompt’ for the user to type >>> myname = "Francisco"
commands >>> print myname.upper()
FRANCISCO
>>> print myname. Llower()
francisco

>>> x = 5

>>> y = 3

>>> print x * y
15

 >>> example_list = [1, 2, 3, -10, 2]
 >>> print sum(example_list)
 -2


Three simple properties of variables: Name, Type, Value
Name Type Value
X integer 5
y integer 3
myname string “Francisco”
example_list list [1, 2, 3, -10, 2]

MM
STEVENS INSTITUTE of TECHNOLOGY | 9

Simple Operations

>>> 2+ 2


>>> 3 * (10 - 5)
15

>>> 10 / 2
5

>>> 10 / 3 #Integer division
3

>>> 10 / 3.0 #Float division
3, 3333333333

>>> 10.0 / 3 #Float division
3. 3333333333

>>> 3 + 4.0

7.0

>>> 2**3 #Power

8

>>> 9 % 7 #Modulo

2

STEVENS INSTITUTE of TECHNOLOGY | 10

 

Variable Assignment Y


eggs = 3 #Assigning an integer

meat = ‘spam’ #Assigning a string

breakfast = [eggs, meat, ‘coffee’ ] #Assigning a list
print breakfast

[3,‘spam’, ‘coffee? |

breakfast = [] #Assigning an empty list

meat = ‘sausage’ #Re-assigning a variable
print breakfast

[3,‘sausage’, ‘coffee? ]

STEVENS INSTITUTE of TECHNOLOGY | 11

 

Variable Names lw

You can name variables whatever you want as long as the names do not
start with a number and only use letters, numbers, and underscores. (Names
are case sensitive):

myName 10_best_things _temporary_id

%)#&@ five_people Savings_2016

STEVENS INSTITUTE of TECHNOLOGY | 12

 

atts
Variable Names we

It is wise To Use descriptive variable names rather than short ones — you won't
remember what ‘a’ Is after 100 lines of code!

Good Bad
>>> height = 1.87 #m >>> a = 1.78
>>> weight = 90 #kg >>> b = 90

>>> BMI = weight / height ** 2 >>> c = b / a**2
>>> print “Your BMI is”, BMI >>> print “Your BMI is”, c

Your BMI is 28.40550435551067 Your BMI is 28.40550435551067

# Is used for comments (they are for humans to read, not the computer)
** is the power operator (like “ in Excel)

_
STEVENS INSTITUTE of TECHNOLOGY | 13

 

 

atts
Mnemonic Variable Names we

¢ Since we can choose our variable names, there is
a bit of “best practice”

¢ We name variables to help us remember what
we intend fo store in them (“mnemonic” =
“memory aid”)

¢ This can confuse beginning students because
well named variables often “sound” so good that
they must be keywords

http://en.wikipedia.org/wiki/Mnemonic

_
STEVENS INSTITUTE of TECHNOLOGY | 14

 

Interactive versus Script Y

« Interactive

— You type directly to Python one line at a time
and It resoonds

¢ Script
— You enter a sequence of statemenis (lines)

into a file using a text editor and fell Python to
execute the statements in the Tile

STEVENS INSTITUTE of TECHNOLOGY | 15

 

Python Scripts (Programs) Y

¢ Interactive Python Is good for experiments and
programs of 3-4 lines long

¢ But most programs are much longer so we type
them into a file and tell python to run the
commands in the file

¢ As convention, we add “.py” as the suffix on the
end of these files to indicate they contain Python

STEVENS INSTITUTE of TECHNOLOGY | 14

 

Useful Links lw

hitp://learnpythonthehardway.org/book/ex33.html
http://www.learnpython.org/en/Loops

https://www.codecademy.com/courses/python-beginner-en-
CXxMGf/0/1?¢cCurricuUlum_id=4f89dab3d788890003000096

https://en.wikibooks.org/wiki/Python_Programming/Conditional_Statements
hitp://www.peachpit.com/articles/article.asox? p=1312792&SeqNum=3

STEVENS INSTITUTE of TECHNOLOGY | 17



ft, STEVENS

lw INSTITUTE of TECHNOLOGY
Ls

Introduction to Python

meme



clipizzi@stevens.edu

SSE

 

Python Components Y

Python has many components similar to other languages:

Variables
Assignment operator
Functions
Conditionals
Comparators

Loops

Mathematical operators

STEVENS INSTITUTE of TECHNOLOGY | 2

 

 

offs
Most Common Built-in Types Y

¢ Numerics
¢ int to represent integer numbers
¢ float to represent real numbers
« Sequences
¢ str to represent strings of text such as characters or words
¢ list to represent lists of elements of any type
¢ tuple similar to lists but immutable
¢ Mappings

¢ dict to store values that can be accessed fast by using a certain key. The keys
in a dictionary have to be immutable types, while values can be of any type.

STEVENS INSTITUTE of TECHNOLOGY | 3

Variable Assignment Y


eggs = 3 #Assigning an integer

meat = ‘spam’ #Assigning a string

breakfast = [eggs, meat, ‘coffee’ ] #Assigning a list
print (breakfast)

[3,‘spam’, ‘coffee? |

breakfast = [] #Assigning an empty list

meat = ‘sausage’ #Re-assigning a variable
print (breakfast)

[3,‘sausage’, ‘coffee? |

STEVENS INSTITUTE of TECHNOLOGY | 4

 

 

Variable Names we

You can name variables whatever you want as long as the names do not
start with Gd number and only use letters, numbers, and underscores. (Names
are case sensitive):

myName 10_best_things _temporary_id
%)#&@ five_people Savings_016

There are ‘reserved words’ that you also cannot use:

and del from not while
as elif global or with
assert else if pass yield
break except import print

class exec in raise

continue finally is return

def ror lambda try

https://docs.python.org/2/reference/lexical_analysis.html#keywords

_
STEVENS INSTITUTE of TECHNOLOGY | 5

 

tts
Mnemonic Variable Names we

¢ Since we can choose our variable names, there is
a bit of “best practice”

¢ We name variables to help us remember what
we intend fo store in them (“mnemonic” =
“memory aid”)

¢ This can confuse beginning students because
well named variables often “sound” so good that
they must be keywords

http://en.wikipedia.org/wiki/Mnemonic

_
STEVENS INSTITUTE of TECHNOLOGY | ¢

[~~ Sl

afi

Comparison Operators e

When you compare things you get either True or False (Boolean Type).

Basic comparators:

> greater than

< less than

== is equal to

<= less than or equal to

>= greater than or equal to

!= is not equal to

>>> 10 < 100 
True

>>> 3 != 3
False

>>> 55 != 44 >>> 
True

some_list = [1,2,3,4]
sum(some_list) == 10
True 

>>> person = "Alfredo"

>>> person.startswith('a')

False

>>> person.startswith('A')

True

* Not exactly a comparator but the same idea

STEVENS INSTITUTE of TECHNOLOGY | 7

Sequential Steps Y


When a program Is running, It flows from one step to the next. We as
programmers set up “paths” for the program to follow

STEVENS INSTITUTE of TECHNOLOGY | 9

 

Conditionals lw


Certain parts of your code can be made to depend on certain
conditions.

The indented part of the code is executed only if the condition
is True.

if today == ‘October 30’:
print (‘Happy Birthday, John’)
elif today == ‘June 21’:
print (‘Happy Birthday, Lauren’ )
else:
print (‘Good morning?’ )

The ‘else’ provides a default response if none of the other
conditions match

STEVENS INSTITUTE of TECHNOLOGY | 9

Conditionals

>>> truck_weight = 10000 #1bs

>>> bridge capacity = 5000 #lbs

>>> if truck_weight > bridge_capacity:
print "DO NOT CROSS! !"

DO NOT CROSS!!

 

>>> day_of_week = 4
>>> if day_of_week == 1:
print "it is Monday"
elif day_of_week == 2:
print "it is Tuesday"
else:
print “it is a different day"

 

it is a different day

STEVENS INSTITUTE of TECHNOLOGY | 10

 



Loops 

Sometimes we want to repeat parts of the code multiple times. If the
number of times is pre-determined we'll use a “for” loop (which Is ‘definite’);
if we don’t know how many times we'll use a “while” loop (‘indefinite’)*.



 
* We still might know how many times this will repeat, see next example

_
STEVENS INSTITUTE of TECHNOLOGY | 11

Loops

While loops require a conditional

When the conditional is False the loop stops

countdown = 25
while countdown < 100:

print countdown
countdown = countdown —- 1

Be careful not fo make
Infinite loops!

 



In [1]: countdown = 12

In [2]: while countdown != 0:
          print countdown
          countdown = countdown —- 1



STEVENS INSTITUTE of TECHNOLOGY | 12

 

Control Statements in Loops &

Sometimes you want your loop to end early or skid back to the top. You can
use break and continue.


numbers = range(10) 

for i in numbers: 
print i 
if i= 5
break



_
STEVENS INSTITUTE of TECHNOLOGY | 13

 

Put it all together 

What this output does?

people = ["Mark", "Alex", "Laura", "Amy"]

for person in people:
if person.startswith("A"):
print person.upper()

How about this?

age = 20
while age < 66:
age = age + 1

print age
print "Retirement?"

_
STEVENS INSTITUTE of TECHNOLOGY | 14

 

Python Scripts (Programs) 

¢ Interactive Python is good for experiments and
programs of 3-4 lines long

¢ But most programs are much longer so we type
them into a file and tell python to run the
commands in the file

¢ As convention, we add “.py” as the suffix on the
end of these Tiles to indicate they contain Python

STEVENS INSTITUTE of TECHNOLOGY | 45

What's wrong with these?

weight = 100.0
if weight < 200:
print "You weigh less than 200lbs"

scores = [22, 55, 70]

if sum(scores) = 100:
print "The sum of the scores is 100!"

STEVENS INSTITUTE of TECHNOLOGY |

 

cfs
Common mistakes lw

When dividing, if you divide by an integer, you get an integer! Divide by a
float to get a float

In [7]: x = 3 
In [8]: y = 10
In [9]: print y / x
3

In [10]: a = 3.0

 In [11]: b = 10
 In [12]: print b/a
3 3033353555333

Some programs indent with tabs, others with 4 soaces. These don’t mix well
together!

Don't confuse = and ==

More on floats next time!

_
STEVENS INSTITUTE of TECHNOLOGY | 17

 

Useful Links lw

http://learnoythonthehardway.org/book/ex33.html
htto://www.learnoython.org/en/Loops

hitos://www.codecademy.com/courses thon-beginner-en-

CxMGTt/0/1 ¢curriculum_id=4f89dab3d788890003000096

https://en.wikibooks.org/wiki/Python_Programming/Conditional Statements

htto://www.oeachpit.com/articles/article.asox?0=1312792&seqNum=3

 

STEVENS INSTITUTE of TECHNOLOGY | 18

 

ft STEVENS

lw INSTITUTE of TECHNOLOGY
iy

Developing Software

Deemeee UII

bess lh 6k hd WED BAL ib (a



clipizzi@stevens.edu

SSE

 

 

Beginning of software S

¢ Software separated trom the hardware in 1950's
— emerged as a distinct technology
— became independent product

¢ Original programmers recruited from the ranks of
hardware engineers and mathematicians
— used ad-hoc techniques from their former fields

STEVENS INSTITUTE of TECHNOLOGY | 2

What are the costs of SW development? &

¢ Roughly 60% are develooment costs, 40% are testing costs. For
custom software, evolution costs offen exceed development
costs

¢ Costs vary depending on the type of system being developed
and the requirements of system attributes such as performance
and system reliability

¢ Distribution of costs depends on the develooment model used

STEVENS INSTITUTE of TECHNOLOGY | 3

 

 

Design Larger/Complex Programs jw

¢ Building something larger requires good software

engineering

— Top-down: Start from requirements, then identify the
pieces to write, then write the pieces

— Bottom-up: Start building pieces you know, test them,
combine them, and keep going until you have your
program

— Debugging: Programming is the art of debugging a
blank sheet of paper

— Testing: Because nothing complicated and man-
made is flawless

— Maintenance: By far, the most expensive part of any
program

STEVENS INSTITUTE of TECHNOLOGY | 4

Waterfall metaphor ¢

¢ Used in construction and manufacturing
— collect the requirements
— create a design
— follow the design during the entire construction
— transfer finished product to the user
— solve residual problems through maintenance

¢ Intuitively appealing metaphor

— good design avoids the expensive late rework
— waterfall became the dominant paradigm

STEVENS INSTITUTE of TECHNOLOGY | 5

 

 

 

Waterfall model ie

  
   

¢ Elaborate up-front activities

¢ Most of the “legacy” systems are
still largely based on the
waterfall


STEVENS INSTITUTE of TECHNOLOGY | 4

Advantages & Disadvantages S

¢ Thorough requirements
definition

¢ Design proven

¢ Documentation emphasized

¢ Planning details

¢ Known quantities

Lack of flexibility for change
Less opportunity for
innovation

Test compressed

Customer only sees result at
end

Developer works from static
specification, not with
customer

Time lag between design and
results

STEVENS INSTITUTE of TECHNOLOGY | 7

 

 

Fact Checks we

¢ In 1995
— 31% of all software projects were cancelled

— 53% were “challenged” (completed only with
great difficulty, with large cost or time overruns, or
substantially reduced functionality)

— only 16% could be called successful

¢ Obviously, the waterfall metaphor did not
solve the problems of software develooment

STEVENS INSTITUTE of TECHNOLOGY | 8

as

What is SW Engineering? iw

¢ SW engineering Is an engineering discipline
— concerned with all aspects of SW production starting trom
the early stages of system specification through to the
maintenance of the system after it has started to be used
¢ all aspects of SW production:
— not only the technical processes
— but also deals with project management, development of
tools, methods and theories to support SW production

STEVENS INSTITUTE of TECHNOLOGY | 9

 

 

Basic Practical Principles iw

1. Use Open Source Software (as much as possible}
— Open and cost effective
— Media production

2. Use Industry Standards
— Again, open & cost effective

3. Make GUIl's as separate components
— Easy to simulate, allows automated testing

STEVENS INSTITUTE of TECHNOLOGY | 10

: Hi, Professor. 1:52 : Hello, she you! How are you? 1:54 : Thank you. 1:58 : I am on campus. 1:59 : Oh. yeah, in your office. 2:01 : Yeah. Yeah. Yeah, yeah. I mean I wanted to do the class from home, but then I had medium things to do, and I couldn't do it. 2:05 : Hello, Christina! 2:22 : So it's 6, 29. We will wait another 2:31 : couple of minutes. 2:39 : No problem. 2:40 : 6, 29. Let's wait for 6, 30 to officially start in the class. 3:00 : Okay, so now is a 6, 30, and we can officially start. So it's a April the eleventh. 3:09 : and it's a 6, 30. 3:19 : This is going to be the last class for this courts, when I will present something next class will be you presenting your finals. 3:21 : and and that will be the end of the course. So I really hope that it was interesting and useful for you. 3:41 : But we will talk next week about that. So for up this week we will change things a little bit compared to what was originally scheduled, and in particular we will have a 3:49 : couple of things. One will be she you presenting a a portion of the research she's doing with me for our Phd program. 4:08 : Then I will. A couple of 4:25 : research is applications that I developed. 4:33 : Then I would talk briefly about a couple of scripts that they want to show to you to see what you could do with the 4:37 : I mean that in practical uses all the writing scripts. Then we will talk about the final. I will present again the template for developing it. I will go through some examples of the final. 4:49 : There are a couple in particular that I just want to show you. I will not share with you the actual presentation, because is one of the topics that you could eventually a peak for your final. 5:09 : and then I will give you 1015 min to work on an in class assignment that could be a little bit different from the usual. So I will ask you to use a chat gpt to do some tasks. 5:25 : so it will be fun. 5:45 Okay. So let's start with the she you just because I don't want to keep her waiting for a portion for the entire class. So once 5:48 : she will finish, she can go, or she can say, obviously, but she doesn't need to say. See you on the floor if you are. 6:00 : Hi, Professor, thank you, and 6:10 : I. I like your course very insightful. I I took 6, 24, and 6, 26 Highly recommend for you guys for this for yeah. 6:13 : and the 6:23 : I I share my screen. Sure. Sure. 6:25 Okay. 6:28 : Can you guys see my screen? 6:33 Mina Shafik: Yup: okay. Good. Thank you. 6:38 : Okay. 6:47 : First of all, let me introduce myself. I'm. Sure, you pretty probably you guys have already familiar with me. My email address. Actually. 6:49 : I'm. Currently a Phd students working with Dr. Carlo and also the Ta. For this session. Today I will talk about one of the research we are doing now. It's about a sequence labeling task. 6:58 : Probably you guys are not familiar with what is sequence levy and task, but I believe after this presentation you can link this task to your normal life 7:14 : in case you are using Chij. Bt. Now, because this task is related to Ted Bt: the Chatbot. Okay. So I will present to this topic in the following. 7:25 : all the first I will explain what his sequence labeling, and what have the applications of second. Slowly, then, I will divide the sequence, label into 3 sub tasks and introduce how to do the what the method to do. Sequence labeling task. 7:38 : So first let's look at what is exactly sequence labeling 7:57 : according to a definition sequence, labeling task attempts to assign a categorical labor 8:02 : label in each member in the sequence. Actually, this is a very general definition for sequence labeling because we are doing the natural language processing research. So in natural language processing. The sequence is the sentence one 8:08 : and the member in the sequence. The tokens are the words: Probably you guys are not familiar with token that's totally fine. 8:23 : you can trade to the member, or as worse, the worst in the sentence and the sequence labeling task in an Rp. Is to assign each word a token in the sentence 8:31 : to a category based on their meaning or function in the sentence. 8:44 : The goal of sequence lovely is to understand the structure and meaning of the input sequence, which is the sentence by classifying the word and all the token according to a predefined set of categories. 8:49 the predefined the set of category. I will. I will explain this part in the following slides, probably in the third section, the sequence lovely sub tasks. 9:04 : So let's look at the application of sequence level. And this is why we are doing this: An Rp. Research. 9:15 : The most common application of sequence labeling are the information extraction and the checkbox like a. G. 9:24 : So in information extraction we first need to know what is information extraction. It's a subfield of an Lp. Research that focuses on identifying and extracting structure the information from unstructured text that data 9:33 : as well know the machine or the computer cannot read the character of the words exactly. It can only read worse. It can only read numeric data. So 9:48 : we need to transform the raw text, the data. the row row text that data is the the character of the world you are seeing. Now : we need to transfer the real text that data into a more structured or machine read by format and through this machine with a format to data. We extract to the head information from the real text that data : so through sequence labeling tasks, we can do the name, the entity, recognition and the semantic role, liberty. Actually, these 2 tasks are this: 2 of the 3 sub tasks belongs to Sequence laby. You can treat the named, the entity and the semantic roles as a nun : in the sentence all they : significant information in the sentence : sequence, labeling can provide information, extraction, tasks, the name, the entity and the semantic rows and : event, extraction, knowledge based construction, and the P. On a 3 subtest of information, extraction, for example, through name to entity, we can. We can extract the named entity from the raw text. : and we can do the event tracking through the extracted entities. : Also we can use the extracted name to entity from the raw text data and based on their relationship to construct. : to construct the knowledge base. : And through this knowledge base we can do semantic search. I'll come all construct. I I partly to graph to do the recommendation: yeah, build a recommendation system. : And for this third, the subtask of information, extraction, the question Answer. Actually, this is related to the next application of sequence. Lively. I will combine these 2 together. : which is the chat box. The definition of Tbots is a software that designed to simulate a conversation with him. He my research, if you did not use the custom at the : commercial check out to like that inserted in the website to deal with some of the some of your purchase, or some of the question you concern. If you did not use that : kind of chat about you. My heart of Ch. I bt, and you may use Chi Gp to do some information retrieval out to just the entertainment. : So this is the use usage of about to do information, retrieval, virtual assistant entertainment. : Yeah, that's for the application for the usage of chat about. So how how sequence labeling tasks have with about : also : sequence labeling tasks that can provide the name to entity. And as part of speech to the Chatbot, for example, the talk about through the named entity extracted from your text. : The tap out can detect or law or locate the entities, such as people, organization, location dates, or product : from the raw text. and once the chat about : detect the name, the entities in the real text, the row text that can be the question you asked : in the : the tabout can provide accurate and relevant information and provide you a customized response to your theory. : Probably if you use travel before you may find that sometimes about can only provide us very channel, but and : useless information : through accurately locate the name. The entities in the question in your question or to in the customer's question, the checkout can locate. : They : salient information like these entities in this question, and to provide a more customized response. Also, part of speech can help the tap out to understand the grammar took place. Structure. : and can help that about to generate more coherent and contextually accurate response. So this is what sequence labeling I have with Chatbot. : So next I will introduce 3 : month subtas of sequence living. : I've already mentioned them, or in the previous slides. They are the part of speech, hacking them to entity, recognition, and semantic, probably probably, for now you still feel a little bit confusion. So let's go step by step. : Part of speech. Tagging is a a part of speech is a category to which a word is assigned to in accordance with it's syntactic fe a function. This definition is from the Oxford language : in English. The main part of speech are now pronoun adjective, determined verb, etc., and these capitalize the character at the representation of part of speech in our research in I actually this representation are widely used in the research field. : and let me, with a concrete example to illustrate what is part of speech. for example, in this sentence. : But yeah, so, se shell on the shop. Our task is to assign each of the elements, which is the word in the sentence to its corresponding part of speech. : So both of you announced so the verb se shell. Another now : is interject interaction that is determined : sure is another. Now. : if you are, if you want to use data driven method, we want to train a model to learn the relationship between the part of speech and the world in our sequence. : So this is part of speech tagging : next the name to entity. Recognition : is to identify and to characterize the named entities mentioned in the text and extract the entity span with levels. : You may wonder why entity span, because sometimes the named entity is not a single word. It's a word freeze, such as Steve Jobs Steve is an is a verb is an entity belongs to person Entity Class : jobs is another person is another entity that belongs to an person entity class. So Steve Jobs is actually the entity span : the level format of named the entity are these: for actually different to data set has different to pre-defined the Ca category. : Set. I just risk an example to illustrate the label format in in some larger : named entity training data set. They I remember the biggest one has 37 named entities. : So here is a just a example. It depends on what training data set you choose. : So then. me illustrated this concept of through the same sentence : in name to entity regulation. Our task ago has changed. We want to assign each of the element. Still each of the elements in the sentence, but 2 different function. : The public yeah belongs to person : like I said. I like, I explained, this is actually this to our entity. Span mobile. You is the beginner of the person. Entity UN is the inside of person entity : another part, another entity is. : it's plans to miss us, and sure. : which belongs to the location. : So the : and now entities. So we tacked as now. : So we want to build our we want to build a model to learn this relationship : between the anti-tax entity type, all the not, and today, with their corresponding token in the sentence. So the last task is semantic, row labeling. : This task is to like. : and then labels towards our freezes in a sentence indicating this meant to grow. The semantic rules focus on who did what we use, subject, predicate, and object to represent, hold it? What? And we use a a capitalized as you be : to represent a subject we capitalized, we to represent, to practice, predicate and capitalize the obj to represent the object. : Actually, semantic role is related to the part of speech I just mentioned. Part of speech provided the statement of the syntax representation of each token, and a semantic role can indicate the relationship between the part of speeches of each world. : So let me still use the same sentence to illustrate this concept. : The for here we concern who did what? Who is for you : which they are the subject Date? What about you? He sold his sell. So what se shell? : So this is the semantic rose we care about in this sentence. : mobile, and so shell. But you may wonder how about on the show, because we do not care who did what, to whom we'd only care about who did what? So on the show. Now, semantic crops we ignore that, but we still need to tech them as now. : These 3 are more 3 sub tasks of semantic roles of sequence labeling tasks. And next, I will introduce the method to do this tasks : we have classic sequence, label and the modern sequence label you can understand as a traditional sequence, labeling way and the modern a current, a more current sequence, leveling way : for traditional sequence leveling, we have to. There are 2 branches, the header mark of model and conditional random field hidden. Markov model is : the head of Markov is one. : Oh. : once it to the parents of one situation only depend on its immediate previous token : a situation. So : I will explain how he, the Markov model, use the impact of speech. Let's review what is part of speech a little bit. It's taking a sequence of words. Assign each word, a part of speech like not a verb. : And in this section we need a 2 probability matrix. The first one is called the transition probability. It doesn't matter. We do not care what is called. We care what is calculated. : We. We want to the proper the conditional probability of we be based on the occurrence of Md. And we calculate to this probability through the through counting the currents of Md. And they be divided by the current of Md. : Actually this, for this calculation is from human engineer, and this whole match may measure the head, and Markov model is a feature engineering. It's belong to the traditional : aspect. We actually we did not use this method anymore. : Okay, so from the next, that is emission probability. : Actually, this is a likelihood we want to pretty. We want to the likelihood, the conditional probability of the talking well based on it's : based on it's part of speech. It's a Md. : They for we formula we formulated this probability by counting the co-currence of Md. And will, by dividing by the current of Md. Actually this probability also from human engineered. : Our goal is to get the probability of the it. We want to get the probability of each part of speech for this token, and we want to get the : high is the probability. : So this is. This is the example of the probability matrix. : Like I said, these numbers, these probabilities are from human engineered, so in the traditional way. They use a lot of human labor to calculate the statistic and build the statistic model. But now we are not doing things in this way : we're doing the : your data driven method. : This method has a to branch : arm based model, and the transformer based model are is designed to sequence analysis, but : it has some limitations. So the men's room is to use transformer based model. : and I will introduce this very classic Fontaine approach. So first we divide the sentence into each tokens you can treat the token as word. So this sentence has an : numbers of words, and we vectorize this because the machine cannot to read word or tokens. After vector after vectorization we put the token vector into the model, and we fix the previous several layers to maintain the : parameter, and which one, the last several layers in the pretend model and the build a dense layer, because we are because we are doing the multi class classification. So we use soft Max. : So actually, this is the end to end. Approach. We assign each token to one entity. If the second token is not a entity, we assign it as a : 0. So this is the : transformer based all the data driven method to do, and you are one of the sequence labeling tasks. : So this is this: what I want to share with you guys, I don't know if I explain clearly. If you guys have any questions, please let me know. : Thank you : all right. Thanks a lot to you. : True, Professor. : Should I stop? Share all I I don't know if you can stop sharing unless we have some questions on on one particular slide, and I mentioned you. You can just retrieve it. : Okay. So one of the goals, all the the presentation that she you gave to us I mean, the most obvious is to give you an example of of the research that we are doing, but the second is to frame the research in a broader scope. : So the sequence labeling seems to be a very specific, deep niche research topic. : But when you do complex things as more details. We'd really make the difference in the old research. I think. Last class. I mentioned the fact that : that one of the things that that we do in a natural language processing basic things is a a programming. So you want to put together words with the same semantic meaning system, engineering, project management school of business, a Webinar mass destruction. All of those : are the same semantic concept spanning over multiple worlds. : So we we can do it the the way we do. We did the in the past, using a sort of brute force. You take the the words so one next to the other, and then you take only the most common, and those are the end grams. : That's a possibility. But if you want to do things the right way, then you may want to work on a the so forth. You want to remove the stopports, but if you remove the stop for so before doing the programming school of business, we never exist. : So you want to remove the so forth. So. But after you do the programming, and only if the stop for the is either at the beginning or at the end of the end. So that's an example. We wrote a paper on that : I I wrote a script that there is probably 200 lines just for that, just for a and gramming. : and seems to be a very niche problem. : But if you don't solve that, then Theola interpretation is not going to be right, because you will never know what is going on about the school of business or about the systems engineering. So you want to have that 250. : Another example is done. You You want to get some insights on on a a document. : but documents can be big. So you have a document or several 100 page, and the same concept can be scattered in different places in the document. : How do you do that. So the medal that I created was basically : deconstruct the document and reconstructed the in a visual paragraphs that are semantically coherent. So what I did was basically victorize the document 250, : and then create clusters based on the vectors. So at that point, and then going back to the actual world. So I recreated the the document 2 : with a completely different structure. : Again. It : it's a very niche problem, teeny tiny, but without that you don't get results. : So again, we just wanted to give you an example of a thinking big, so a chatboard, but then a digging deep : going into one detail, and : so with. : So if you don't do the right labeling, you will never get the information you want, that you will never understand. If you are talking about a person or you. You are talking about the CD. : So that's the name, the entity, re recognition, and then the part of speech, the part of speech. It's particularly tree, because the role that the words are playing : is changing in time. If you consider Google Uber, so they are nouns. But we use them as verbs : meaning over the years language changed. So if you use a : methods that are static, so if you use that, we use the natural language, toolkit, natural language toolkit as a module, doing it the part of speech tagging. But it's based on a classification of the names and the other parts that is 20 years old. : and Uber may not be there, meaning, if you use it, Weber as a verb, we not get it. : and language is changing constantly. And then there are a jargons. So in everyday life we are using words with. : That is a not. The fish are one. But if you want to understand what people is saying, you need to recreate that so. And : the way you extract those it's really gridy gala. So : erez agmoni, we are exploring, creating a network and then applying network metrics to recreate these causality in the sequences of awards meaning in phrases 142. : If you don't, do that. : you Don't understand the text. So for human beings, so life is easy because you live in the present time. You use the words that people around you are using. : You are using them. It's probably in the same way, and you understand each other. : But machines do not have this common sense, and that's why we need to dig deep and create those algorithms, those methods that can really help us fulfill the the the task that that we have. : So again. : sometimes you can go high level. But sometimes you need to go deep. So probably is not even when not is definitely not the deepest : point that you can reach in a natural language processing. There are many other things that that are more I mean smaller targets, but pretty complex. But that's an example. : See you? What what do you think of this type on an analysis? : Yes, Professor, actually, we think we have another kind of sequence labeling task in the recent days that is prompt to learning. : That is also we design different part of a sequence. And this another story. Yes, i'll go with you, Professor, and I don't think our research is trivial. It's build a. So it's a foundation to the top tasks. : Yep. : Okay. Great thanks again to you. : Okay. : All right. Okay. So let's move on with the the other topics we have, so : I want to spend as much time as possible on the final and then sometime on some other example. So let's let's start with the easy portion of examples, all the applications. : So I want to share with you : couple of researches that we did in recent times. : So let me share the screen and let me go here. : So this project that we call the Greek connect is a project that we started. : and yet enough to go pretty much and let me go into a presentation model. : It's it's it's it's, it's, it's, it's it's it's it's it's a : yeah : on 7 different tasks with the 1,400, the people participating all over the world. One of the 7 tasks was a financing, a sustainable future. : So we : created the a team. So the the point of contact, Martin Powell was a former a student. All the George this, who was was the Provost Stevens 2 problems. Ago : E. And Martin encouraged Georgia to participate. I I work with Georgia on a a couple of Dod projects, 250, : and he knew that they had some background in in natural language processing. Can we put together a the thing that I going to show to you? : So the : problem that we want to. That we want to to address. So was a : how to find projects that can be funded by Siemens financial services. : projects that are in the sustainability area. So that was the target that that that we had. So again we won a the hackathon : and let me go. Here we use the the room theory that I presented the last time. The room theory is basically a a knowledge driven approach : that is based on analyzing documents using a computational representation of the knowledge of the specific domain. : The architecture is basically you create the the corpus collecting documents in this case, but related to what Zoom's financial services was doing : so. All the documentation on all of past projects, the way you're doing things, what is the background of the people doing the same job? : And then we transform the this : ere : that we're computational representation of the of the domain. : then that with the same knowledge you can do multiple things. So we define the specific tasks on with the keywords and related weights : mit ctl. And so keywords are at basic keywords. And then all the the synonyms, all the misspelling. And again, weights, because not all the keywords are created equal. 101. : We don't do an exact match. But we basically measure the proximity of those keywords with the documents, so that we are analyzing. : So then, we built a a webcroller going into the web and getting those projects to be analyzed. : Then, using this room theory, we had the projects that are potentially of interest for Siemens financial services to be funded. : So then, you have those elements, and you do some visualization to present the the results. : So we developed a a basic user interface where you have a you can select the target industry. : You have the different documents with a caller code for the potential interest they may have, and a distribution : in terms of what are the keywords, I mean based on the keywords that you provide in the in the benchmark. : The proof of concept was a a simpler than that. So W. Was basically a room. We had a relatively small room, initially, I mean for the hackad on we had the 450 documents. : The benchmark was 300. The words of races, I mean, I mean, what's can be engrams again. : because : we wanted to provide that semantic concept, not just the work. : And a : for the hackad on. We didn't have the wet roller ready, and we collected manually the documents. Now, in the current version we do have a crawler : and the algorithm for matching. We are the same of the room theory, and we use the some sort of basic graphic visualizations. : So that's basically, what you have. You have all the different benchmarks or keywords, and you have the different projects, and you have a this sort of a correlation magic. So that is telling you what are the projects that are potentially more interesting. : We added that this : project, 0. There is a sort of a a control project that was not related with the topic to see if the system was doing his job. But not. : And then you couldn't have a a little bit more. That will be on a a view for : the different I mean the intersection between documents and and benchmark, and you can visually see what are the projects that are matching. : or better, what are the keywords that are matched? The the most by the different benchmarks. : So a. And that's basically it. We are after the hackaton. Siemens gave us a another. Here of a funding. : We are expanding the the system. The system again has a crawler. The roller is working fine. We expanded the the benchmark, the corpus. : the crawler, so far, is a dumb crawler. So it's just collecting a elements based on keywords that we provide. And then the system is actually doing the job : We are thinking about adding a layer all the you know common sense, because one of the things that seems to be easy, but it's it's kind of complicated. You have a document that is a white paper, so it's a commercial pro document. : It is not a request for a a proposal or an idea of a project to be founded for a human that would be easy to say. It's a commercial promotion is now the Vla request for funds : for a machine it is. It's more complicated. : Machines do not have the common sense, so we will use something like a chat gpt equivalent to to filter out those projects that are only : not projects, but they are just commercial products to commercial documents. : Same thing for the location. So sometimes you have a document. : and you have Cds. You have vitro it. But actually, the the project is going to happen in Nairobi. So because Siemens financial services is is for use in this case on North America, Nairobi would be or no interest for them. : So for a human it would be easy just to go through everything and say, okay, this is not of interest to us. Again. We will use a a a lunch language model to do this software pre screening. : That was the I mean the the 3 of us, the the founding members are still there. : We are thinking about the next stage, meaning the : Erez Agmoni. The system is now working, I mean. Obviously, there is always room for improvement, but it's delivering results. 150 : Siemens wants to continue working with us, want to use the system. : So we are thinking about creating a company that could be with Stevens inside. : So I had conversations with the Provost, with the senior Vice provost for research and anthropologists with our general counselor, and we are going in the directions : when there is a sort of the organization in our point of contact move to a another company. : So right now the idea of the company is kind of on old. But before our point of contact left. : I mean. He involved the the Cfo, the the CEO, or Siemens financial services, North America. That is, 20,000 people. Then he seems to be on board. But : again. The direction is to spin off this activity, creating a company, and then we'd see what is going to happen. So that's an example. It's not end to end. But I mean it is a pretty, let's say mature example. and then : of application of something that we discussed. : something completely different. Let me share this research that we did about probably 2 years ago. and was a a join effort with unicef : during the pandemic A. There there was a a a a spike in domestic violence, and children abuse. So we we wanted to track that. We wanted to create some awareness on on the problem. So : we created the a. : A joint team with some people from Stevens, some people from Unicef. and that's the Unicef team. : That's the Stevens's team. So myself. : The 2 of us are 2 of the main point of contacts when is about analytics in a, in a broad sense, is more focus on on a resilience and more on data, analytics and natural language processing it's. : We are a 2 of our Phd students, both employed. Now. : Korea is in Bangladesh as a senior data scientist, and Fernando same thing in a data consulting company. : So : there is no need to to give you the background on the team. But I just want to go into the project. : So we we wanted to again create the some sort of a a awareness we for use the on a different countries for sure, on the Us. But we wanted to expand a a little bit. : and that we use the social media to get what people was : thinking was doing and trying to map somehow the facts with a story Again, it's all about the storytelling : 1 million issues. Some : data may be misleading. We are talking about a topic that is : very sensible and sensitive. : and some people it's not comfortable discussing those experiences with me. : I mean on public records. And then there is the issue, or a multi-languages. : So we collected the tweets from a 15 different countries. In those data frames we collected the data from red. It : tweets red. It's a they are very different. So tweets are are less for use, the shorter red. It's a more for use longer. : We'd read it. : You have communities with tweets, not so much so. We use the : modeling to represent all of that. So the first case study is a using tweed. : So again, there are some advantages. It's a easy to access was so easy to access before you know. Mask. It's widely used, based on text. : So I I mean Instagram. Will it be great, but it's way much more difficult to process images than text. : and because you have a the possibility to comment, I mean to to side. There is a sort of conversation that can and gone. : Okay. We collected the the data, we : and identify the conversation so that we are more related to the topic of our research. We mapped the results by geography, all 3 : location, and we analyze the results. So : those are our hate speeches in different States before and after. : So : before Covid and after Covid. So that was the increase for this Rito Columbia, for example. So this is hate speech : around the world so abusive, non abusive. : There. : I mean that again, as usual. : Swedish are always the best in the world. But unfortunately, what this big and the number of people in Sweden, unfortunately, is not as much as the people in Indonesia or in Brazil, meaning : they're having so much of abuses, but compared to non abusive. : it's really warring up there. : So then : we created a sort of a an index to measure it. and that this is the distribution. So you have an average, and and countries that are above, over below that : similar thing, using a red. It : so again, red. It is very different from to either. It's a social media, but it's a different animal : you have Subreddits subreddits are very focus on a given topic. Mit, ctl and so we focus on on abuse-related, subreddits, abuse survivors, abuse survivors of abuse and a massive violence. 150. : We determine a control group just to have a a a way to create a context for the results that that we had. : And again, one of the limitations in this case is that the credit is used primarily by the Us. Meaning you, Don't have a : the same amount of data from other countries that you have with with them. : That's an example of a one on the post : collected data, analyzing in time expecting topics. That's what we did. : Number of newly active users on a abuse Re related the Reddit. So there is definitely a a a spike, even just considering the number : of users, and not the content or what they said. Then, using nda, we had that I mean. We : kind of manipulated a little bit, the nda adding labels to the different topics. : So that's basically where those were more : related. : So abuse related subred. It's a are among the topics with the highest growth during the lockdown. So : that was a something scaring somehow. : So let me skip that. : So those are the : topics. : So again we detected the 5 topics. : and we label them, and we extracted the keywords that are meaning the the different topics. : and that that's basically for each one of the topics, the distribution in time : that is giving you an idea of what was going on. So if you look at the first one, that's : really growing a lot a lot I mean it's it's it's carries out so 33 more exposure to abusive languages, language, and cyber bowling on twitter : 37 us states so a large and increase in tweets containing abusive language. 94 more child abuse on ready during the lockdown 88 more intimate partner abuse re but already during the lockdown. : So : those are the facts. So the paper that we published the following this presentation was a a highly sided : There are 2 : more out to to the story. So the the first one, when you write something, stay on facts that can really drive attention. : because at that point you will get more exposure. : The sag on the most important. : What we do can really be useful to expose Some situations, create or increase awareness. : Some of those researches are not so easy meaning you can. You may not find them on the New York Times, not because they they are not good, but but because I mean it. It took us : 3 months to develop it. : Jordan is may not have the same, the the the actually I'll spend so much time. But again you can do a lot of good or a lot of working with data and leveraging on them and presenting them in in the proper way. : So that's something that they wanted to share with you. Another thing that they want to share with you is a few scripts. : So in particular. : 2 of them : let me share this screen again. : Let me go here. So 1: : analyzing text is a pretty general need. 1: : So she you was mentioning 1: : retrieval in in text. There is a genetic term. It really depends what you want to do with the the text. What are the information that you want to extract? Some of the tasks kind of a repetitive. So we 1: : spend some time, for for example, not analyzing text and extracting the most frequent words, the 1: : diagrams. 1: : By the way, let me stop here for a second and let me go into diagrams. 1: : Yesterday I was teaching a a class on a a visualization course, of course 6, 22, 1: : with Professor Ramirez 1: : pretty much each semester. We do a sort of an exchange of of classes at a certain point, and at each one of these classes, using my my 1: : experience in a natural language processing and applying that to visualizations and or to network analysis, or a combination of the 2, 1: : one of the students, and asked me Why, she's also a student in my other 6 24 class. She asked me, why you are asking us to do diagrams. So what's the use of diagrams. 1: : Well, diagrams are an essential component of the or engram, so are an essential component of of the conversation. I mentioned that half an hour ago. 1: : so school of business project management information technology. So all of those are are in Grams diagrams, 3 grams or more, with the same semantic meaning. 1: : So you don't want to leave the words as individual words, but you want to aggregate them, creating some sort or representing 1: : one single semantic concept. 1: : Once you have that that could be diagram. It could be engram again, a web on a mass destruction. There are quite some words, but we just one single semantic element. 1: You want to use them 1: : erez agmoni, in pretty much all the representations you have. You want to use them for the word cloud. The most frequent words, even if They are not really words, but they are those chunks, those semantic elements, 150 1: : you want to use. When you do some topic analysis. 1: : you do, Lda, as an example of topic for a presentation, and you want to have a 1: : business, a single word, or a business management as a diagram. 1: : So that's the reason why you want to have a diagrams, because you want to extract from the text not only the words, but you want to extract the the 1: : concepts, the the semantic elements in those who can span across multiple words. So that's something that I mean that you may have the same question that 1: : this you have 1: : all right. So 1: : this script is kind of a a genetic script. It's relatively long, 240, and change lines of code. 1: : and he is a doing a 1: : kind of a repetitive tasks in N IP: so is a text cleaning 1: : Most common words generated in the Engrams doing the core. And so let me stop here for a moment. Co-corance 1: : A co-currence means we mentioned briefly last week means 1: : appearing together. So you have a a large text, and you have a 1: : some words, so that they are appearing together. 1: : So the assumption is that if they are repeating together, that means they are related. So this being related as a value, because when you have more words that are related, then you have a 1: : a set of related topic. What that will create a topic. 1: : So, using this call quorance awards across the document is a way to analyze the document. 1: : Eventually you can create a network out of it with what's the that are related because they are appealing one next to the other? 1: : Then what is next to the other is to be defined, meaning, if they are one next to the other, I mean 1: : one following the other, then he is a diagram. But if they are in works of distance, so degree of separation. 1: : Then you can define the degrees of separation between the words. That will be that that threshold, Obviously the larger is the document, and the the smaller is going to be the number of degrees of separation, because otherwise you, you would have a a, a medium words. 1: : and then you may want to count 1: : the number of times the words are appearing, because if they appear together only once, they may not be relevant. So 1: : with those parameters you can create a network based on this call quorance. So, and that's why in my script i'm calculating the coordinates. 1: : Then what cloud some topic, modeling and some statistics. 1: : So basically I imported the All the different libraries. 1: : Some are for visualization, some are for calculation. 1: This is a a 1: : and 1: very basic cleaning function. 1: : This is to 1: : calculate the the most common elements 1: : generating the engram, sir. generating this matrix of cocaine. 1: : then generating the work cloud 1: : generating the the visualization for the nda model for representing topics. 1: : And then you define whatever is the file you read the software file you, I mean, load the the file files into. 1: : At least you said that the minimal length Sorry about that. 1: : You set the window separation, and that I was mentioning to you 1: the number of engrams that you want. 1: : How many of 1: : I mean, how big and as should be in Engrams, how many of them you want. and then that some naming a number of topics 1: : here i'm generating from the Madrid. So the Co. Currents madrics what is in the network analysis? It's called the adjacency madrics. 1: and 1: : from that I'm. Generating the the graph 1: : and then saving it for a future analysis 1: : Then. 1: : and 1: : from the cleaning I I get the vocabulary that is the least of unique words 1: : and 1: creating the work cloud 1: : topic, modeling topic, modeling a more that I think needs the vocabulary. That's why I generated it. 1: : And then I basically generating some statistics. The number of words. the number of unique words. So the entropy meaning how diversified is the user awards. So a text with the higher entropy than another 1: : means that there are a wider variety awards. 1: : So when you run it, let me run it. And 1: : so you are going to have a 1: : the what cloud! 1: : And that's Why not? You have a 1: : the engrams. the topics 1: : That's the this that I was mentioning. and there are some graphs. So this, for example. 1: : is the graph 1: i'm opening it 1: : with the her FDA that is being generated. So when you go into the different, so we define a for topics. So when you go on each one on the right side, that you have the wars or and Graham. 1: So you have a natural language processing senior business consultant. 1: : So those are engrams. 1: and 1: : this is actually from something different. But the the the same concept. 1: : You can eventually change the relevance. Madrid. 1: : So this was for the resume. Okay, let me go to what has been generated in this case. 1: : It is right here. This is what has been generated in this case. Sorry about that 1: : was 5, not for 1: topics. And again 1: : you have the different topics. The relevance is all of them. 1: : You have a 1: another interesting representation that is a 1: this one up. 1: : So I was mentioning that that you can generate a network out of the awards, using the call quor, and so and that's basically what it's been generated. 1: : So you have the different elements and how they are connected. 1: So, going into it. 1: : you can get a sense eventually from that you can do a clustering, using a a, an algorithm and it's called the lobbying community detection. And you can get more insights. 1: : You can change what is called the gravity, meaning how much the notes are attracted, either one to the other or to the center. You can change the what it's called the solver, that is the visualization. 1: : and then you have the work cloud. So you have the work cloud. You have the Mba. You have the network, and you have some statistics. I use that the when we were hiding 1: new people at Ssc. So I wanted to compare those results 1: : so, and it's pretty much the same as the previous one. I'm not 1: : keeping in mind that when you have a a resume, most likely the resume is in a Pdf. So I had this. 1: : There's a script that was a reading a Pdf. And transforming the the Pdf. Into a text is a relatively straightforward 1: : script, using a library. There are several of them 1: : is the one that I use in this case. So, using this one, it's basically where I got 1: : this one. So 1: : so 1: : this Lda is related to my resume. 1: : So you have a business intelligence. You have a natural language processing a step, and Cecil of technology. 1: : So the things that you can expect. So I I did the something very straightforward. I went to Linkedin. I downloaded the I transformed the profile into a Pdf. And then I use the Pdf 1: again. 1: : It's just an example. you can see, actually, the user. All the 1: the engrams. 1: : So Stevens is the technology, business intelligence, natural language processing lab. In this case, senior business consultant, principal investigator 1: : a developing solution. So you want to have a machine learning. So you want to have those and grabs 1: : all right. So, very briefly, I want to show you 1: couple of finals. 1: : So in particular, I want to show you this one that is, from last year. 1: : and my opinion is one of the 1: best 1: : in 1: : recent semesters, and was for sure, the best 1: : in that particular semester. So let me share the screen. 1: Let me go here. 1: : It's on one of the tasks of the problems that you have as options for this year. So global migration dynamics. 1: : Joshua Johnson 1: : a good, the table of content. I mean that that was a kind of massive 30 page, and you would see the quality of the visualizations and the analysis and the conclusions 1: : in particular. So you have a introduction, research questions, data, description, preparation, representation, conclusions, references, appendixes, list of figures. 1: : list of Tables abstract. 1: : So why we are doing what we we are doing why this problem is relevant. Why, we are talking about the migration. So that's the description. I'm not going to spend much time, but I just want to give you a sense of the the way it was done. 1: : Some terminology that is always good, because we assume that we are using sometimes. But everybody knows, but not necessarily. Everybody does 1: : so. Factors push and pool. 1: : so push job opportunities, hyp population. 1: : cool availability, or regular work. 1: : higher wages, education, opportunities, and so on. Results 1: : questions. 1: : So you want to have questions. What are the major global migration dynamics and so on. 1: : Data, description. 1: : exploratory data analysis. 1: : preparation. 1: : and then some visualization. So 1: : with this one, you really see who is going where. 1: : So you'll see. I don't know people from Europe, North America going 1: to North Africa eventually, and so on. 1: : So source, destination numbers 1: : the range that some from the different regions 1: : global migration in terms of immigration, immigration, self, immigration. meaning from 1: I don't know Latin America to Latin America, different 1: : countries eventually. 1: : and so on. That's another interesting one. 1: : So you have going Where? From? Where? 1: : So this is a sun key diagram created for analyzing flows, and this is a flow of people. I mean it's not fluids, but it's the same concept. 1: : same thing. So I none 1: : want to go into each one 1: : conclusions. So that's 1: : I mean, you want to have something at that, because that's your take. 1: : So 1: : migration is ongoing and happens from all over to all over. So 1: : they are working side by side. Somehow. 1: : Obviously. 1: : people in less developed the lower income region are more likely to migrate to more developed, and that's something 1: : that it is not a surprise. A little bit more surprising is people in more developed region, so do not always migrate to more developed region, but may actually migrate back to less developed regions. 1: : I think about the the countries that are growing I think about. I mean, the typical example is China. 1: : I mean. 30 years ago China was a an underdeveloped country. Now it's definitely a a developed country people who migrated from China to us 150, 1: : 30 years ago, or 4 years ago. Now they may want to go back. So it it. It's a an inverse migration that is happening 1: : anyway, so that and then references and some appendix is so that's a way to use the the concept of appendix is 1: : you don't need to have them another big fun of having a lot of tables, but I mean that it's an appendix. You can use it or you can out. 1: : Okay, so I really wanted to share with you those very briefly. Another, I think that they want to share with you is again at something that they already presented. But I want to be sure 1: : that you have it, and you consider it in the proper way is a a data exploration template. So be sure that you have a a structure that is somehow similar to this one. So you want to have a the goals you want to cover those faces that are 1: : business understanding, date, understanding, data, preparation and data representation. So you want to cover all those phases those can be like in 1: : in the document that they represented before as a table of content. You can call them in different ways. So you don't want to call like in this case a data understanding data description that's absolutely fine. 1: but you need to have those steps 1: : so, and that for each one you will add the your content. So you have this. Use it not necessarily to create a presentation that the but as a table of content for your a final document. 1: : One more thing that I want to share with you, and then we will spend a few minutes playing with the the in class assignment. 1: : I probably already 1: : share with you a little bit. 1: : Let me share the screen now. 1: : So we are now talking about the chat, Gpt. 1: : The way you present you prompt you 1: : type. The query 1: : will effect heavily the way they bought the 1: will provide the answer. 1: : So this case I created the several chats from for 1: : different topics. So this is mixed, then I have a future AI technologies. So 1: : when I place a a query in this chat, I will have an answer that is different from the answer that I could get from another chat. 1: : So 1: the more 1: : context you provide for your query, the more accurate is going to be your answer. 1: : So use Chat Gpt. Use it wisely. So keep in mind that that it is likely that those bots will take out the 1: : some of the jobs. 1: : but most likely the jobs that will be lost will be the bottom 10%. 1: : So mit ctl, and you don't want to be in the bottom 10%. So you need to use the boats. But now it be replaced by the bolts 150. 1: So the idea of the next assignment 1: : in class you will have just 10 min, so no more than that I just want you to think in those terms 1: : is about. 1: : Let me share again the screen 1: : and let me go here. 1: So 1: : this in classics that sizes, exercises on a chat gpt that I, calling the the B in the rest of the description, you want to perform. One of the following tasks, using the Board 1: : called the documentation 1: : input your code to the both and ask for a documentation 1: creating a narrative, input your results and get a narrative from the boat 1: : code, writing input, the the requirements. So from one of the first assignment and get the Python code. 1: So 1: : i'm going to share a 1: : this in class assignment. You have it. Okay. So let's have a just 10 min, and then we will recombine that. So 1: : I will create breakout rooms. But they are not really necessary, but I didn't do that. 1: : So i'm creating a 3 breakout rooms with 3 participants each. 1: : They are open to you in a less than 10 min. 1: : Okay. So 1: : so we have another 15 s. 1: : 10 s. 1: : All right. So 1: : I want your input so what is your experience? What do you think? What did you do? What did you get 1: : anyone? 1: : So we had noticed like it needed a login. So we kind of just my group and I were just. I was kind of telling a narrative about how my team at work actually use Chat Gpt because we were just doing release planning. 1: : and our chief architect actually wondered if it could write a description and a acceptance criteria for a story we wanted done for a feature in our Jira, and 1: : it was shockingly long, and like, quite accurate to what our goal was for writing the acceptance criteria. 1: : and then, like talking, as in the English, was 1: : quite correct as well. So we actually ended up. 1: : you know, taking inspiration from it. 1: : And I thought, you know, in a professional setting. I thought that was pretty interesting. 1: : but you use it as these at the very end, or you did some modifications. 1: : I think our chief architect was like for my own 1: : pride. I'm going to change like, I'm going to add my own tidbits like. For the most part he did keep it 1: : pretty much the same. 1: : Wow: okay, that's great. Okay, Thank you. 1: : Anyone else. 1: : Yeah. So, Professor, this is Kevin at the At our organization level and in in Lockheed we we do block the use of it. 1: : But there, there. Isn't really a policy of of using on assigned as a supplement to like. 1: : you know, for for some some, you know, textual or verbiage, you know, assistance. 1: : but I found that 1: : it. It is really helpful if you provide the proper questions, and sometimes you just have to reformat the question. It is nice that it has a context of your previous submissions. 1: : So that's really cool. I found that really interesting and powerful. 1: : Yeah, I mean, at the very end. It's a sort of next level of a so changing them. So what the search engine is doing? It has a crawler going into the websites and tang. 1: : and then you have a sort of database with the page and tags some. Then you have your query, and is is matching the tags in your query with the tags in the database and giving you the URL. So that's 1: : a very straightforward the approach with the those bots. Instead of having a a tag, you have a pattern or a pattern so, and it is taking the patterns in your query and matching them with patterns 1: : in the database it is, and then is adding a layer of a conversation. 1: : The more patents you provide the in your query, the more matches will be in this sort of pattern, matching that that that you have, and that's why it's so important to to formulate the questions. So in the proper way. 1: : It is becoming a a job, but it is called the the Prompt Engineering. We mentioned that probably last time up. But the way you formulate the query it's really important for the the quality of the answer. 1: : So out of curiosity, what they did the just before the class was a to 1: : fast. So 1: : a script, this one, so saying, write a documentation for the following code. 1: : And it was just a code. 1: : And that's basically what I got. 1: I mean, that 1: : is not something that I can use directly. 1: : but consider that this daily support is not a specific chat. If I 1: : would 1: : create something more on the documentation, I would probably get 1: something better. But it's still a giving something. 1: : I mean that writing documentation is a pain in the neck, and most of the people write the code, the I mean in theory. You should write code, the and documentation at the same time. The majority of developers don't do that. 1: : So having something that will help you can be helpful. 1: Anyway. 1: : It's 806. Does the end of the class? Let me know. Let us know if you have any question on the final. 1: : We we are definitely here to help. If you want to change at the last minute your project and you have a something that you really want to do it. 1: : That's not a problem. But you need to send me not just the overall genetic idea, but more detail the the description all the project you want to do, and the data set you are going to use. Not 1: : 2 page 2 paragraphs is absolutely fine, but they really need to understand what you have in mind. So that's 1: : basically it. And again feel free to work in teams. So that's absolutely fine. Be sure that you are all working on the same project. I mean. 1: : The main reason why I have no problem with this class is because you are professionals. So yes, you are here to get a a good grade. But you are here to learn now. 1: : So 1: : while in my other 6, 24 are things that different, the students may have different goals, they may be less mature. In this class I have no problem. I i'm pretty sure that you know how to do things in a broad way. 1: : So again 1: : thank you for being with me till 8 0 7, and again, if you have questions, so send me and see you an email, and we will get back to you. By the way just out of curiosity. We are 1: : working on creating a sort of Ssc. Gpt. And I will use the transcripts of my classes as a sorts of data that that i'm going to use for training the bought 1: : with those larger language models. You also have the possibility to do what is called fine-tuning. 1: : providing a couple question answers. So those would be conceded before using the 1: : matching of patterns. 1: : So we are collecting the email from the past few semesters and creating a. A. Q. A. That we will feed the to the system to do that 1: : for extracting a. Q. A. From the email. We are using a combination of our chat, Gpt. And New months. 1: : so the initial Ssc. Gpt will be on 6, 24, and will be a sort of a tube or a for 6, 24, so just to let you know. 1: : anyway. So thank you again. 809. Have a good night, and I see you next week again. If you have any question, let me know. 1: : Next week you will present, so we will exchange her emails. In the meantime you will present your final projects, and we will start a sort of a a discussion on that.
  
: So it's 6, 32. 0:00   : So I changed the the plan because some of you were not sure if if they could 0:04   making time. 0:13   the the courts were scheduled to end the this week. 0:16   : but the very end at at Stevens, the final p of the we last until the beginning of may. So I thought that. Why don't we use the extra time to do things 0:22   the best we can, so and that's why i'm giving you an additional week. 0:39   : So I hope that you like the idea. But again, if for any reason, you want to close it to today, that's absolutely fine, I mean that that if you have your presentation, feel free to present it, that. 0:45   : and otherwise we'll be next week. 1:01   : So what do you think 1:05   : any one of you time? What do you think? 1:14   Kyle Jonas: Yeah. So I mean, I thought I just assumed the presentation was next week, because that's when the last module is. 1:20   : So I was not ready for this week. So yeah, I was happy to see your your message. Okay, Good. Good, good, good 1:26   : Christina. You okay? 1:33   : Okay, Sorry. I'm. Actually, between 2 computers the computer I normally use. 1:40   : We started on me and I jumped on my back up. 1:44   : Okay. 1:48   : I actually was under the impression that the presentation was done today. It was due today, so I was 1:52   : kind of put in pedal the metal to get everything ready with my group for today. 1:58   : Okay, we we can go today if you don't. If you don't mind, we're we're ready. No, no, no, no, no, I mean a. As in my email, if you prefer to do it today. Just make sure that you submitted it. So we don't have a 2:02   : spending issue, and and I just want to be fully transparent with you a professor and i'm not sure it. It doesn't matter who who's who's a fall? It is, or is it? It is a fall? But on Sunday night. I think I saw some instructions posted on Module 13 for how to do the analysis. 2:19   : but regardless, I think I think we're fine. 2:40   : I just wanted to plan out there that we didn't realize that there was a you know, additional instructions for module and module 13. But we we we did started early. 2:43   : I just want to to set that straight. Okay, then that's why we're ready. Okay, okay, go ahead. Alright. So I i'll. I'll share my screen. Thank you 2:54   : sure? 3:03   : All right, let's see. 3:04   : All right. 3:09   : We have a live audience today. You see my screen. 3:11   : Yep. Okay. So 3:15   : what what we did Our analysis on was on aviation. Specifically accidents that occur in the aviation space. I found that really, really, you know, our group found it really interesting. 3:18   : The specifically it was a data set where we were able to apply filters to in a specific website, which i'll get into details later. 3:30   : But we were able to partition. You know the the data set into 3 3 separate decades, and then we performed our analysis a month amongst those 3 decades. Right? So our our team is is Elise, Christina, and and myself, Kevin. 3:40   : So this is a summary of what we did right. So the purpose is. Again, we were interested in the aviation accents over time to see how the accent that changed with a with a heavy focus on on injuries. 3:54   : We decide it's best to analyze the data in in the 3 separate decades, right? Because it's a large data set right. And I I think it's started out in like 1960, S. And I. It Eventually the data for some reason stopped recording that around 2,015. But 4:05   : our our our analysis was focused between like 19 eighties and 2,011. 4:23   : So what we did first is a data preparation the aviation data contain. They have full inconsistencies. We we clean the data up before performing the analysis. 4:29   : Obviously for the columns of dial that we were interested in. If it had missing data, we would just remove the entire role If there were columns that we're not print, you know, that did not apply to our analysis. We just left it alone, because it provide no value in just removing that. 4:39   : So what we did. In Our strategy is to partition it, as you know, discussed into 3 separate decades. Right? So we have a one spanning 1979 to 1989, 4:57   : 1990 to 2,000, 2,000and one to 2,011. So the the reason why we partition this data is so that we can perform our analysis and and see the trends right. The accent trends. My team will discuss that later on on on their approach to do that. 5:08   : What we did last was we generate that right? We we pl at the data and comparisons between the 3 separate decades that we analyze. 5:26   : and some methods we use identify columns that should be grouped together and identified, which comes that we can compare against. 5:36   : So the that we we obtain data from Ntsb aviation query 5:44   : we we left. We intentionally left the query right country, anywhere, state anywhere, month, all, and including all accidents, right, and the injury, severity would be fatal 5:50   : that details. So in this data set we we have a a a good amount of columns, right? But we we focused on ones that made more sense for analysis. Specifically, events, data, engine type. 6:03   : a far description which is the Federal aviation regulation description if it is present that means that that particular aviation rule is being scrutinized by this. That organization. Right? Let's say you had an accent, and it was related to maybe maintenance or something. Then there will be a a a. You know, a respective description to to indicate that 6:16   : total fatal injuries, serious injuries, minor injuries, the phase of flight and the weather conditions which is pretty interesting. We decide to ignore certain ones because they're very unique per accident, like event. Id investigation, type, accent, number. 6:39   : location country. Most of them fall under, like the United States. So it didn't really make sense to analyze the others, because there's only a few that that from from other countries, for for whatever reason, I guess it's more flexible out of the United States 6:57   : that are preparation. So this is just like a brief summary of our strategy, right? We we did the filtering, and this our strategy for getting our 3 partitions 7:10   : all right. We we had like a a data frame for a start, date and an end date right between those 3 3 years, right? Those 3 years start and start and and start end. And this is how we stored and returned our new data frame 7:21   : for those those timeframes. Right? Of course, we ran to some issues where the data, the the date time fields were either empty or had a tab space. So we we had a strategy. We had a function to to filter that out. 7:37   : That's it for me. I'm gonna pass this to 7:53   : Christina. She'll talk about the the scope of our analysis now 7:59   all right. So the 8:04   : broad trends that we looked into were the types of injuries over the decades. the phase of flight versus the number of accidents. 8:07   : the weather conditions, and how they affected the accidents, the purpose of the flight. There's a variety of different reasons why planes are flying, so what are they doing, and 8:13   : how that affected the trends for accidents? The engine type for the airplane itself. 8:24   : And then just generic accident trends over the years. Kevin, you can go the next slide. 8:29   : Okay. So this is just the data that we looked at. This is just comparing fatal, serious and minor injuries over the decades. 8:38   : The overall trend that we noticed at the highest number tended to be in the 1990 to 2,000 category, a decade that held true for total overall injuries, minor injuries, and serious injuries. 8:47   : The only one that was a little different was fatal. Injuries actually had a higher 9:00   : number in the 1,979 to 1,989 decade 9:05   : for all categories. The lowest decade was 2,001 to 2,011, 9:08   : and across each of the 3 decades as well. The highest injury type was fatal. 9:14   : You know the next I have. 9:23   : Alright, so this is just the correlation analysis that we ran on the data for each of the decades. So it's actually comparing 9:26   : the types of injuries to each other, and seeing the relationship between one type of injury and another in both 1979 to 1989, and 1990 to 2,000. There was all positive correlations between the types of injuries. 9:33   : So, as one injury type increase, you'd expect to see an increase in the other injury types as well. 9:49   : The strongest correlation in both sets of both decades was between serious injuries and minor injuries. 2,001 to 2,011 was a little different. There was still a very small positive correlation between serious injuries and minor injuries. 9:55   : but the other combinations actually saw negative correlations between them, so, as one increase, the other would be expected to decrease, which was   : vastly different than the previous 2 decades.   : But the overall turn was actually a weakening of the correlation over time between the injury types.   : All right, Kevin, go on the next one.   : So in this chart we're just comparing the number of accidents that occurred at each phase of flight.   : For each of the 3 decades   : they look kind of similar across the years, the top phases of flight for injury.   : for accidents, i'm sorry, would be cruise, maneuvering and take off.   : and the lowest we're seeing the lowest number of accidents are seen echo around, landing, standing and taxing across all 3 decades.   : All right, Kev.   : Thank you. Thank you. This is I got one more at least Takes over an 11 last one. So for weather conditions we also compared them to face the flight.   : So across all 3 decades we saw a trend where the highest number of accidents actually occurred in good weather, conditions visible.   : visible, meteorological. That's a tough word conditions. So when the pilot's actually able to rely on what he can see as opposed to having to rely on the instruments.   : and at first we kind of thought that was a bit alarming. Is it something up with the pilots? But it actually may be a factor of the fact that statistically, most flights actually occur in good weather a lot of times. If there's   : poor weather or visibility flight, so we cancel delay. They're not going to take off. So it may just be a factor of the number of flights that actually occur, some statistically be more likely to have an accident there   : that that was actually something. We said that with another data set might be an interesting comparison to look into.   : But across all the decades as well, we noticed that in maneuvering was the highest number of accidents that occurred even during good weather.   : All right. i'm going to pass it over to at least. Now   : Yeah, go for at least   : Alright, perfect thanks, guys. So now we're going into specifically over each decade, the purpose of flight.   : So we see here there is a trending very large percentage   : which is represented by the purpose of personal flying. So from 1979 through 2,011 this is represented by approximately 50 to greater percentage from that 1,979   : to 2011 window. Now.   : being a factor of you know, thinking what is the most common purpose of flight   : about the time of the sixties is when commercial flights started to become popular, and so gradually air travel was less exclusive. By about the eighties via a popularized, You know, commercial flying experience. And so into the 2 thousands more people had access   : to plane travel, had the money to also had via the Internet. You know more of a desire to see places that were farther away.   : So that is   : a reasoning for that. 50% approximate growing to so much larger to about 70% over the years. And then the subsequent larger percentages here are represented by in order, business, unknown purpose, and instructional flights.   : and so obviously business can be justified by saying that this is a necessary means of flight in terms of obviously those who are employing that need to travel for their.   : for their work   : would quantify a mass amount of people. Then we have an unknown reasoning which could be for any very smaller and significant purpose of flying, and then instructional flights. For obviously there are   : pilots that are in school that would be flying planes where   : any number of situations could possibly go awry.   : So the next slide, please, Kevin. So now we move into the engine type of what the plane would have. So here   : all decades, analyze the engine type with the most injuries would be the reciprocating engine. Aka, a piston engine which uses one or more pistons, you know, convert pressure into a rotational motion and a rotational source of power.   : So   : over the years we see that this actually grows the use of this engine, and it is   : that the piston engine has become the most popular type of engine to date. So that is a very literal reasoning for   : this percentage going up. And so   : also the reason for this going up is that, you know, advancements in technology have happened over the years which supports us, the use of this type of engine   : more so over any other type.   : so you can go to the next slide. So now we move into.   : They look in the Federal aviation regulations for the far rules. So these are sets of rules set by the Federal administration to   : kind of look at this data to see how every regulation was documented over each decade, and which rule was under the most scrutiny in terms of looking at accidents that were reported.   : So we did notice a majority of accidents that were looked at. We're missing a data value, regardless of what year we were looking at.   : and also the rule that was under the most scrutiny over all decades by the Federal Aviation   : Association was general aviation. Aka. These personal flying   : instances   : Some next slide.   : Oh, you're done. At least we're going to do a quick conclusion just to wrap up. So, for over all 3 decades the greatest number of accidents that we saw occurring was during that Vmc. Period, or that visual meter meteor lodge. Oh, my goodness, Christine, that you're absolutely correct   : conditions. So, just to reiterate. That was when the pilot was able to use good visual cues to be able to navigate the aircraft.   : and   : therefore would have you know, a better sense of surroundings when in flight. Another trend that happened to occur for all 3 decades was that the most occurring injury type was that of fatal.   : And for the if they are data values, President   : present the general aviation category which could mostly categorize that personal and business purposes of flying was scrutinized the most, and was therefore put under the most investigation.   : And now we'll open anything up to more questions from everyone.   : Okay, great. Thank you.   : So I have a few questions. So one is more a general question. Why, you didn't consider the making model.   : Oh, yeah, so I we we explain. In the report.   : There were a lot of nick and models to the point where it wasn't. It was very difficult to to consider. I mean, I guess it can, but it would take a lot more effort, not sure what value would bring.   : but from from what we we try doing. They They end up having a lot of   : variations making models in the data set.   : And I guess if you were going to move into the type of plane. A piece of that aspect that we took was being the engine there like being that there were a much more succinct data set to look through, and you were able to get numbers that put more value toward the analysis.   : Yeah, it's kind of interesting because i'm a former student of mine did the in the 800 project on a a a similar problem. She   : he's working, I think, for lock it.   : and and then she did the the on the same topic. So i'm not saying that they became an expert. But I a little bit familiar with the   : the the topic on your project. The second question is why you think there are so many less fatal accidents in the last decade   : I can go first. If   : yeah, I can follow up I   : the investments in technology, right? Maybe the robustness and the design of like the landing gear. For example, I I I've seen a lot of documentaries   : because I just like it on documentary nerd, but they they just made everything better. There's more strict regulations, more may, maybe more maintenance, right the panel. There's I have a cousin that works in the maintenance side of of aircraft, and I think the requirements are more stricter, right? Just everything overall.   : It's just a better over the years.   : Yeah. I mean that it it's a quite a relevant point, because when you do with an analysis like what you did, the the the main question is why you are doing it. Who can benefit from it?   : So when you see those differences in breeding down on why there are a such a difference. So it could be an indication, I mean, if it is a a matter of a a different regulatory system, what are the regulations that that impacted the the most.   : and that the second answer, I think kind of a following the first one. How can I reduce even in farther? So did did they reach a a sort of plateau for those changes.   : So I mean, i'm not at all like criticizing the very good work that that that you did but keep in mind that that you can get some actionable. So somehow, from a research like like this one.   : and the actionable in this case would be okay. I see a reduction. Let let me better understand why   : a a and a. Is there any margin for improvement? So those things could be the next step for the analysis that that that you did not not saying that that is not great, but it it's really good.   : The other question I have a, and that's unfortunately something that is a common in mostly analysis, that that I see the lack of a normalization by the size of the population that you are comparing.   : So if you don't normalize for a a number of flights, so   : then the comparison may not be that much relevant. So either you go in in terms of percentage, or you go with the normalization with with the that there is the sort of a person that January.   : So what do you? Because at the certain point you mentioned also the the fact that the numbers can be different, because the number of flights in the different periods could be different. So how did you consider that   : I'll be honest, Professor? We do not consider that, but that's a very good point. I I I think, what you're saying, it makes absolute sense. We should probably   : do, maybe like an approach where we do 1,000 from this year, 1,000 for this decade, another 1,000 from the second decade, another 1,000 from the third decade for some type of of a way to make it equal comparison.   : Yeah. I mean that in any form of a a normalization would would be fine. Because I I mean, if you had the percentage that the percentage, in a sense, is the normalization, because you have a the the the the type of   : A. As a percentage of the total. What they are, they total is.   : and then you can compare them   : order. You can normalize using a a any form of a normalization, the meeting, Max, or whatever is the most, the the   : the normalization that that that you pick. But I mean, like, keep in mind that that that's something that you may want to have a a for future analysis, because otherwise, if they feel to to compare.   : Thank you. That's very good feedback. I appreciate that.   : To go back to your second question also about you know why you might have the injuries   : and accidents that, like went down over the years, I would say to there's definitely, besides the less human factors of new technologies. There is also probably better training and risk mitigation factors with   : instructing new pilots and teaching like new pilots, how to flying. Also that I imagine   : there is a more of a demand for pilots, and so, therefore over greater number of them there is a a safer flight.   : Yeah, in the work we did with the with the other student. We know this the same fact that that accident happened more in clear of sky.   : and it it was a kind of surprising, so   : she didn't do much of the normalization that would help, but because I mean, if you have all the data that are in clear of sky and few that are with cloud this guy, then obviously you will have more on the clear sky cases.   : But another consideration that we did was. and when you go in clear sky condition. You rely on the equipment.   : and the the pilot may pay less attention because there is nothing to be worried about.   : and that could be a another factor. Yeah, I I went to note that that's exactly what I me and my team were discussing specifically. Christina, I was saying. We can add that to the conclusion how they could be less reliant on technology. But maybe they should use a combination of the technology and the clear skies to make a judgment.   : So that's another good point.   : Yep. Yep. Okay, sounds great. You did good. You'd be very good.   : How big was your code, then? Do you have the code? The By the way, yeah, yeah, we have the code. I I believe, minus the the comments. The code should be around 350 lines, but not   : Okay? So as a final conclusion, was this courts useful for you?   : Yeah, absolutely. I think it. It forced me to learn python, but certainly it. It's a new. It's a new language for me. And do we learning the different? You know, various ways to process data and the what? What was certainly interesting is the the analysis that we had to do.   : I I never had to do analysis in in this kind of way before, so it definitely was a a new, I opening experience for me, and and of course allowing us to choose a topic of our choice, for the final was very enjoyable experience.   : That's for me   : good Christina Elise.   : you like I before. They only used Python a   : a handful of times for very specific, like AI purposes.   : but because they were so specific. I I kind of lost touch with it quite quickly after ending the assignment. So   : I certainly feel like I actually put, you know, methods to memory, which is for me pretty impressive. I only usually code with like Bash scripting at work. So   : I think this, you know everything culminating together in this project   : was very useful and made sense to You know how we moved through everything.   : Good. Good.   : Yeah. I don't use. I probably haven't looked at coding since my underground number of years ago. So it's definitely   : different than my norm. I was   : interesting to see different ways of analyzing data. I'm generally in excel person. So it was interesting to see I could find the information in excel, but I could also do it another way for other purposes.   Okay.   : okay. So I I mean I I was not fishing for compliments. It was just getting your input because I mean this: this courts is running, since quite a while   : I   : change a little bit each semester and getting input. It's really important to me. So is there any part that you think I could could have been different or didn't work Well, as you expected, the   : not   really   : not necessarily   : my head right now. Not necessarily.   : Yeah, not not for me. But I I was actually curious. Oh, Professor, is there any class that incorporates   : students to contribute to like a code repository like a collective, you know way of developing or working on something or an assignment.   : not really. We have more research projects right now. As an example, I'm. Working with some students on a project to create   in a Ssc. Chat box based on on a larger language model that could be sort of a a Tudor for a students.   : and we are going to start with the with this course with the 6, 24, and and then we use the from the classes as data along with the the the teaching material so the slides, the readings and all the rest.   : All of that. We'll generate code, and they code will be available to students as well. We don't have a real repository.   : I mean that   : repository for the the the   : wouldn't   make not much sense. I share a   : some parts of code during the the like. I don't know what cloud or cleaning data, so that those are either scripts or as nippets. It's something that they share.   : Probably it would be good that to create a sort of small a gee tab with the all those pieces. We, our students can just go there and get it   : for the larger scripts or problems. I mean.   : they go into research projects, and for those we do have a repository. But then the the research projects sometimes have some some restrictions.   : We do research properties for the Dod, and a while at the results are the public, the the code, the maybe now after. So that's   : my experience. So far.   : all right. So it's really interesting that you mentioned the the chat boss because i'm working on a a software as a service platform. And we're also trying to incorporate. You know a AI into into that as well. More and more of so to   : facilitate the whole, it help desk that we're shooting, you know, basically automating the   : the the way we we handle requests some request to be automated to a certain degree.   : Yeah. I mean that this paradigm of the larger language models it. It's very interesting because   : you can have an interaction with the data that is more conversational, that is, lowering the threshold for people to use it.   : We are a considering. I mean   : the model that they developed on on a specific   : knowledge base that is a a specific one domain as the limitation of not having the the common sense. So I think so. That seems to be obvious, may not be noticed by the system, and that it's why we want to incorporate the the existing   : large language models either open AI like or a those from a hugging face, and they have a some models that are available   : for the common sense meaning. Before going into the analysis that is more a domain specific, we go to the common sense to eliminate the obvious.   : It would be great to have a resources enough to have something that is a domain specific and big enough like bloom. But the so Bloomberg had the the   actually all the I don't know how many years, all financial   : data and publications, and they use that to create the the entire model.   : We don't have that. So even with the all the transcripts and all the the material. It would be a tiny, tiny fraction compared to what could give the the system a a real, enjoyable interaction.   : Yeah, it's as good as the that you have right.   : It is a separate project is to use. I mean, the large language models have a component that is generating a language.   : The idea is, to use the language, generation, capability of a large language model for presenting the results of a different model.   : So I developed my mode a lot with the let's say, a combination, let's say, generating the data like those that would go in a dashboard.   : So I have quite a lot of data, and I could create a a a dashboard instead of creating the dashboard. I feed the a language model, and the model will present the results in plain English.   : and that could be useful in a when you are in critical conditions. So you are on the bottom field. You are in a intensive care unit.   : You are a in a nuclear plant with an emergency. So in those cases you you don't really rely much on what you see you are doing something, and you want your explanation in English.   : So when you have those models so generating the text, then the text to speech is a no-brainer. There are a a medium module for that.   : So that's another thing that that we are exploring there so language as a an equivalent to the visualization of the next a blow, something like that.   : Yeah, Professor, I have 2 questions. Thanks for asking   : some some of my work pose a question about language models. Specifically, I think they're concerned about how it would drive. You know it. And and is that something we're we're we're you know, concerned about, or adapting right where, where your thoughts on on that is, that is that the future is that going to define?   : Well, I I mean, we. We need to set the expectations in the proper way.   : We tend to have a way too much expectation from technology. So when the first computers we're used there, we are people saying the machine side that and has to be through.   : But the reality, the algorithm that the machine was using and the data that the machine was using. You know we are not so   : well done, or a strong or tested or larger to, I mean, justify the fate.   : We are pretty much in the same situation here. So those models   they call them   : generative models, but they're not generating anything they are generating. Answers to that are the equivalent of what Google is giving us   : with the layer of conversation.   : So this is to be very intelligent, but in reality they're not   : erez agmoni, creating something like an intelligent mind that could do. They are as teaching together pieces, and presented it 150 in a sort of a a probabilistic way.   : If we continue increasing the size of all the models we will be. I mean, we we will have a both, so that are more accurate in analyzing the date than presenting the data.   : But there is no generation of a new concept, new ideas.   and there is no domain.   : So we didn't solve the the the problem. All the the specific knowledge that you can have in that one specific domain.   : The   : yeah, If you have it, it should be relying on the the the basis of   : of what it it it knows right to to generate the answers. Yeah, I mean, if you add the all your knowledge in a   : the   : aircraft industry, and you dump into an open AI.   : It will be a drop in the ocean, meaning the part that it will discover will be sort of a deluded in a homeopathic way in a way that there will be no trace of the specific knowledge.   : So what we need to do is to come out with a better   : or more representation of the knowledge that can value the different components, so the generic and the specific in the proper way.   : And then I mean that we will be able to reach the the point that we will have those types of representation working in a in a a proper way, most likely. Yes, probably not in the next couple of years, but in in the next 10 years, probably.   : or even 5 years. At that point it would be interesting, because I mean.   : I don't know if I already mentioned that i'm writing a book on a societal implication of a. I am machine learning.   : and I am analyzing those issues.   : jobs will be destroyed, but jobs will be created. So if you look back   : to to the past revolutions driven by technology.   : the revolution, so they did destroy the the entire categories of of jobs.   : If you think the the music industry who is buying buying it. Yes, there is a little bit, but it the marginal business. So who is buying a cassettes? Who is buying a analogue? Music in general.   : blockbuster renting Dvds or Cassettes that they are gone or developing teachers printing pictures. So all of those were big businesses, but they are gone   : even now with the streaming. I mean videos move these.   : They they they! They they are gone, but new industries will come. So we really need to be very flexible and very open   : and fast reacting, because things will change. Things are changing very fast. The major issue that they really see is more   : on the concentration of   that that we see more and more.   : If we stay with this paradigm like open AI. There will be 4 or 5 companies able to create those giant models.   : and we will need to use them. Yeah.   and that's an oligopoly.   : but it would be a noticeably of the knowledge, because we will all use the know that that is collected by them.   : So with the chat gpt, the vast majority of the data is in English. That that means that we have a a, a, a large section of the population in the world that is under represented.   : So we don't want that   : there are many changes that we need to do. The current proof of concept that we are experiencing with with with the open AI, and similar   to to make it work.   We need to be aware of what what is wrong, and try to do our best to work better.   : So distributing intelligence is something that is coming up   the   I mean.   : In fact, we already have it with the I mean most of our phones. They have a a dedicated components for our machine learning meaning, and there is some computing 150   : on the machine learning side that is happening in our phone, and probably on our smart watches, 250. But we need more than that. So we need more of a distributed computing, more of a distributed intelligence.   : So anyway, that's going to be an interesting period, though. Yeah, we're living an interesting times.   : Yup, yup, Yup, yeah. I'm always interested on how how the you know. Does the the data preparation that cleaning for for their learning models? Very, very interesting stuff. My, My, my last question is so nice exciting is I? I message you and the Ta. About submitting the file project. We just need one person to submit. Everything. Is that correct?   : Yes, yes, Yes, yes, so I mean that   : just just to be sure that they everything will be done in the proper way.   Either one of you would submit, or all of you would submit the same thing.   : Okay, the last. The last time I checked the submission button was not available. But you know it could be available. Now. Yeah, okay, All right, Thank you.   : Sure. I mean the the reason why I would be more   : on all of you with submit is because if at the certain point there will be an audit. I don't want someone that checking and a normally. Why, those students didn't submit.   : so I mean we know it, but it could look   : better if you all will submit.   : Okay.   : And it would make a much professor that you that that you who will will will get it.   : Thank you, Professor, for sharing all your insights   : absolutely, absolutely. So again. Next week there will be other presentations, so feel free to join my opinion. It's a good learning moment, because you would see what I did the on probably   different areas, different topics.   : So I   : encourage you to join us next week.   : Okay, Thank you for us, sir.   : Okay, thank you all.   : If you don't have a question. So that's the end of this sort of a class. This is a very short one.   : and I appreciated your presentation. You did a a great job.   
 

fis STEVENS

lw INSTITUTE of TECHNOLOGY
is
i
i

Variable Types and
Conversions

Repel



clipizzi@stevens.edu

SSE

 

~~ )h—$|slUui

Variable Types &

 

In the previous class we said that variables
have names, types, and values

There are many types of variables but a few
that you will come across most offen

STEVENS INSTITUTE of TECHNOLOGY | 2

Variable Types &

You will use strings, numbers, and lists most to start with.
Strings are strings of characters
“hello”
“goodbye”
“100”
«Numbers can be integers (ints) or floating point numbers
7 (int)
10.0 (float)
5.5 (float)

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 3

 

 

cfs
Variable Types and Operators &

Depending on the type of the variable, the
same operators do different things

>>> first = John 

>>> last = Smith
>>> print (first + last)
JohnSmith

>>> dollars = 1.0
 >>> cents = 0.75
 >>> print (dollars + cents)
1.75

>>> word = ‘Hi’ 
>>> print (word * 5)
HiHiHiHiHi

>>> hourly_wage = 14.50
 >>> print (hourly_wage * 8)
 116.0

>>> print (‘Hi + 5)
Traceback (most recent call last):
Type Error: can only concatenate str (not “int') to str

 

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 4

 

Which type is a variable? &

If you need to know or fest the type of a variable, you can use Python's
type() function:

>>> name = "Julia" 
>>> print (type(name)) 
<type ‘str'> 

>>> height = 178 #cm
>>> print (type(height))
<type ‘int'>


>>> scores = [99, 80, 78] 
>>> print (type(scores)) 
<type 'list'>

>>> version_no = 2.7
>>> print (type(version_no))
 <type 'float'>


height = "seven feet"

if type(height) == str:

print("I am " + height + " tall")
elif type(height) == int: ?
print("My height is:", height )



else:
print("There is no information")

I am seven feet tall

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 5

several Types of Numbers je

Numbers have two main types
— Integers are whole numbers: -14, -2, 0,

1, 100, 401233

— Floating Point Numbers have decimal

parts: -2.5 , 0.0, 98.6, 14.0

There are other number types - they are
variations on float and integer

>>> xx = |

>>> type (xx)
<type ‘int'>

>>> Temp = 98.6
>>> type(temp)
<type ‘float'>
>>> type(1)
<type ‘int'>

>>> type(1.0)
<type ‘float'>
>>>

STEVENS INSTITUTE of TECHNOLOGY |

 

Type Conversions

When you put an integer and
floating point in an expression
the integer Is implicitly
converted to a float

You can control this with the
built in functions int() and
float()

>>> print (float(99) / 100)
0.99

>>>i = 42

>>> type(i)

<type ‘int'>
>>> f = float(i)

>>> print (f)

42.0

>>> type(f)

<type ‘float'>

>>> print (1 + 2 * float(3) / 4-5)
-2.5

>>>

STEVENS INSTITUTE of TECHNOLOGY | 7

 

Type Conversion lw

You can turn numbers into strings and (some) strings into numbers using the
conversion functions int(), float(), and str()

Id 2]: a = "5"

In [3]: print a*5 
55555

In [7]: x = 576.3

In [8]: y = str(x)
In [9]: print "Number: " + y*2

 Number: 576.3576.3

In [4]: b = int(a)


In [5]: print bxb
25




In [10]: age = "fifteen"

In [11]: new_age = int(age)
Traceback (most recent call last):

ValueError: invalid literal for int() with base 10: 'fifteen'

 

STEVENS INSTITUTE of TECHNOLOGY |

 

String Conversions lw

You can also use int()
and float() to convert
between strings and
integers

You will get an error if the
string does not contain
numeric characters

aft

>>> sval = '123'

>>> type(sval)

<type ‘str'>

>>> print (sval + 1)

Traceback (most recent call last):
File "<stdin>", line 1, in <module>

TypeError: cannot concatenate ‘str and

‘int

>>> ival = int(svall)

>>> type(ival)

<type ‘int'>

>>> print (ival + 1)

124

>>> nsv = ‘hello bob’

>>> niv = int(nsv)

Traceback (most recent call last):
File "<stdin>", line 1, in <module>

ValueError: invalid literal for int()

STEVENS INSTITUTE of TECHNOLOGY | g

 

 

~~ )h—$|slUui

a won

Preventing/Managing Error Type &
Conversion

When you try to convert info numeric a string without ag number inside, you
will get an error

In [10]: age = "fifteen"

In [11]: new_age = int(age)

Traceback (most recent call last):

 

ValueError: invalid literal for int() with base 10: 'fifteen'


STEVENS INSTITUTE of TECHNOLOGY | 10

 

User Input lw
¢ Wecan instruct name = input(‘Who are you?’)
Python to pause print (‘Welcome’, name)
and read data
from the user using Who are you? Carlo

the inout function Welcome Carlo

¢ The input function
returns a string

STEVENS INSTITUTE of TECHNOLOGY | 9

Converting User Input

 

lif we want to read ino = input(‘Europe floor?’)
a number from the usf = int(inp) + 1
print (“US floor’, usf)
User, we must
convert It from a Europe flooré 0
string to a number US floor |
UsINg a Type

conversion function

Later we will deal
with bad input
data

STEVENS INSTITUTE of TECHNOLOGY | 10

String Operations

¢ Some operators apply to
strings
— +implies “concatenation”
— *implies “multiple

concatenation”

¢ Python knows when It Is
dealing with a string or a
number and behaves
appropriately

ots
we

>>> print (‘abc' + '123’)
abc123

>>> print (‘Hi * 5)
HiHIHIHiHI

>>>

STEVENS INSTITUTE of TECHNOLOGY | 14

 

 

Lists lw

Lists are containers for other types. For example, you can have a list of
strings, a list of integers, a list of floating point numbers, and also lists of mixed

Types.

The square brackets, [ and ], are used to define lists

Items inside lists (‘elements’) are separated by commas

example_list = [123, "abc", 2.0, "Fish"]

STEVENS INSTITUTE of TECHNOLOGY | 14

 

ER )h—lsl«Uri

Lists lw

You can access elements in a list by their position (‘index’). Indices in Python
Start at O:

In [1]: example_list = [123, “abc", 2.0, "Fish"]

In [2]: print example_list[@]
123

In [3]: print example_list [3]
Fish

You can change the elements of a list too:

In [23]: example_list[3] = "Tilapia"

In [24]: print example_list
[123, 'abc', 2.0, 'Tilapia']

STEVENS INSTITUTE of TECHNOLOGY | 15

List Indexes and Slicing ye

In addition to selecting a particular element you can select a ‘slice’ of a list:

In [14]: a_thru_h = ['‘a','b','c','d','e','f','g',‘h']
In [15]: print a_thru_h[3]

d

In [16]: print a_thru_h[3:]

['d', ‘e', “Tw 2s ‘hh

In [17]: print a_thru_h[:3]
['a', hi", 'c']

In [18]: print a_thru_h[3:5]
['d', 'e']

You can even count backwards from the end:

In [19]: print a_thru_h[-1]
h

In [20]: print a_thru_h[-2:]
"ae", 'h']

STEVENS INSTITUTE of TECHNOLOGY | 16

 

 


Using Lists 

In [1]: example_list = [123, “abc", 2.0, "Fish"] Y

You can use the index method of a list to find the index of a particular value:

In [5]: print example_list. index("abc")

In [7]: print example_list. index(123)
Q

Add something to the end of a list with append:
In [25]: example_list. append (42)

In [26]: print example_list

{[123, ‘abc', 2.0, 'Tilapia', 42]
Delete something form a list with remove:

In [27]: example_list. remove('abc')

In [28]: print example_list
[123, 2.0, 'Tilapia', 42]
SS rrr—“i—‘—SS
STEVENS INSTITUTE of TECHNOLOGY | 17

Concatenation vs. append method 

>>> a = [1,2,3]

>>> b = [4,5,6]

>>> c = a + b # concatenation
>>> print (c)

[1,2,3,4,5,6]


>>> a = [1,2,3]

>>> b = [4,5,6]

>>> a.append(b) # append method
>>> print (a)

[1,2,3,[4,5,6]]

STEVENS INSTITUTE of TECHNOLOGY | 18

 

Ils an element in a list?

>>> a = [234, 133, 42]
>>> 42 ina

True

>>> 55 ina

False

>>> 55 not ina

True

STEVENS INSTITUTE of TECHNOLOGY | 19

 

 

Lists are Ordered 

The order of elements in a list does not change (unless you change it) so
they are useful for keeping track of sequential events. You can also loop
through them in order easily.

In [29]: schedule = ["Math", "Gym", "French", "History"]
In [30]: periods - ["P1", "p2" "p3" apa)

In [31]: for i in [0,1,2,3]:
print periods [il, schedule [i]

P1 Math
P2 Gym
P3 French
P4 History

STEVENS INSTITUTE of TECHNOLOGY | 20

Lists can be sorted

>>> a = [457, 246, 110, 111]
>>> a.sort()
>>> print (a)

[110, 111, 246, 457]

>>> b _— [“sopam”, “eggs”, “bacon” |
>>> b.sort()
>>> print (b)

[“bacon”, “eggs”, “soam’

 

STEVENS INSTITUTE of TECHNOLOGY | 21

Strings

We already know that strings [of characters] are the data type for words

and letters

You can loop through (‘iterate’), slice, and select by index the same way

you can with lists



STEVENS INSTITUTE of TECHNOLOGY | 22

 

String Methods

There are several useful methods that strings have

upper(), lower(), capitalize(), and title() change the capitalization of the string

a_string = "HeLlO. hOw ArE yOu?"


print a_string. lower() 
print a_string.upper() 
print a_string.title() 
print a_string.capitalize()

hello. how are you?
HELLO. HOW ARE YOU?
Hello. How Are You?
Hello. how are you?

startswith(}) is useful too

a_string = "“HeLlO. hOw ArE yOu?"


print a_string.startswith("He") 
print a_string.startswith("he")

True
False

There are ~30 string methods!
https://docs.python.org/2/library/stdtypes.html#string-methods


STEVENS INSTITUTE of TECHNOLOGY | 23

 

String Methods &

lower() (or Upper) is useful for comparing words

Say we want to check If a word Is in a list
original = ['fISH', ‘cow', 'DOG', ‘cAt']
word = ‘Dog' __
print word in original

lowercase = [x.lower() for x in original] 

print word. lower() in lowercase


STEVENS INSTITUTE of TECHNOLOGY | 24

 

 

replace() Method &

‘The replace() method lets you replace part of a string with
something else, which is offen useful.

Notice that this doesn’t change the original string:

name = "Jonathan"
print name.replace("a","?") 

print name
 Jon?th?n
 Jonathan


STEVENS INSTITUTE of TECHNOLOGY | 25

a won

Type Conversion - 2 Y

Type conversion can be useful when geiting input from users. The
raw_input() function gets input from a user as a string.

In [13]: age = raw_input("Please enter your age:")
Please enter your age:27

In [14]: print "In 5 years you will be", int(age) + 5
In 5 years you will be 32

The list() function will turn a string into a list too:

In [36]: print list("Stevens")
Ss", "E" ‘e', “a ‘e', "is is

STEVENS INSTITUTE of TECHNOLOGY | 26

 

 

Ly A
i

a won

Some Tips &

You can use the range(n) function to quickly oe Ct ar 3 a beagel ee

print i
create d list of numbers O — (n-1) to loop through: ;
1
2
3
Lists can even have other lists as elements
my_list = [1, "two", [3,"three",3.0], 4.0]


print my_list[2] [1]
> three

The index method will only return the position of
the first match!
In [32]: letters = ['H','e','l','l','o']

In [33]: print letters. index('1l')
2

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 27

 

~~. )3=h(lUui

a won

Conditional Steps &

sometimes we want to repeat parts of the code multiple times. If the
number of times is pre-determined we'll use a “for” loop (which is ‘definite’);
if we don't know how many times we'll use a “while” loop (‘indefinite’)*.



* We still might Know how many times this will repeat, see next example

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 28

Comparison Operators

Boolean expressions ask a
question and produce a
Yes or No resulf which we
use to control program flow

Boolean expressions using
comparison operators
evaluate to - True / False -
Yes / No

Comparison operators look
at variables but do not
change the variables

Python
<

<=

 

Y

Meaning
Less than
Less than or Equa
Equal to
Greater than or Equal
Greater than

Not equal

STEVENS INSTITUTE of TECHNOLOGY | 29

 

Breaking Out of a Loop lu

¢ The break statement ends the current loop and
jumps To the statement immediately following the
loop

¢ It is like a loop test that can happen anywhere in
the body of the loop

while True:
line = input('> ')
if line == 'done' :
break
print (line)
print ('Done!’)

Ø hello there
Ø hello there
Ø > finished
Ø finished
Ø > done
Ø Done!

STEVENS INSTITUTE of TECHNOLOGY | 30

Finishing an Iteration with continue S

¢ The continue statement ends the current iteration
and jumps fo the top of the loop and starts the

next iteration

while True:
line = input('> ')
if line[O] == '#':
continue
If line == 'done':
break
print (line)
print (‘Donel’)

> hello there
hello there

> # don't print this
> print this!

print this!

> done

Done!

STEVENS INSTITUTE of TECHNOLOGY | 34

 

 

° ° . fs
Finding the Average in a Loop we

count = 0 
sum = 0 B
print ('Before', count, sum) 
for value in [9, 41, 12, 3, 74, 15]

count = count + 1

sum = sum + value

print (count, sum, value)

print (‘After’, count, sum, sum / count) 


$ python averageloop.py
Before 0 0
1 9 9
2 50 41
3 62 12
4 65 3
5 139 74
6 154 15
After 6 154 25


An average just combines the counting and sum
patterns and divides when the loop is done

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 32

 

Useful Links we

htto://www.tutorialspoint.com/python/python_ variable _types.htm
http://www. informit.com/articles/article.aspx? p=459269 &SeqNUM=7
https://docs.python.org/2.7/tutorial/datastructures.html
http://learnpythonthehardway.org/book/ex32.html

STEVENS INSTITUTE of TECHNOLOGY | 33

  
And probably you can reduce the code using functions doing the same thing. 1:   I mean some of the plots are the same plot with different data. 1:   Right. 1:   So I mean nothing too bad. But keep in mind that when you have a code that is so long, if there is something wrong in one of the logical blocks, then you need to change each one of them, and then things can be different, you do. 1:   Think in one, but not in the other, and then it will become problematic. 1:   So when you have a sort of a common denominator, you change in one place, and we'll take effect to all the times that you will call it. 1:   Great that makes sense. I need to practice a little more, since we're starting off right. 2:   So I'll try and improve on this, you know. Just. 2:   Thank you. 2:   Absolutely. I mean, you did a good job. Did you enjoy analyzing the data overall? 2:   I really did it. This is the data set that made most sense to me, and I like it. 2:   So it was really. Oh, I learned a lot by doing this. 2:   So so yes, so thank you. 2:   Good, good, good. Thank you. Alright. Okay. This is basically concluding our class, including our courts. 2:   Shortly we will post the final grades. If you have issues, questions, send us an email as soon as possible. 2:   It was a good semester as soon as possible. It was a good semester. I really thank you all. 2:   I really hope that you learned something that was a useful, or will be useful for the rest of your professional career. 2:   Studies, whatever it's going to use that you will have a for what you learn. 2:   So again, that's been a pleasure working with you, and if you have any question on the course or something that is related, or on the program of your studies, or in your professional activities, if there is something that you can benefit from on the data side or AI machine learning Nlp feel free 2:   to contact me, I will be super happy to help you. So we are really here, not for the money, because Academy is not exactly one of the most highest paid job. 2:   So, but because we want to help you. So we want to be able as much as possible to touch lives, to help you, to succeed and to I mean get the most you can get from.
  
Okay. So again, we started recording it's February the fourth and is 631. 0:01   This is M 624 w us. 0:11   And we have a relatively full agenda for the day. 0:15   So let me go here for a second. So we are right here February 4th. 0:23   We will talk about variables, control. 0:33   We will talk about testing. And we will start I mean, I will introduce exercise number two. 0:38   Before doing that, let me go here. 0:51   And it is new to your cell phone. 0:55   You will be a. Okay. 1:06   So can you mute yourself this? All right. 1:14   Okay. So. The assignment that we had asked for. 1:18   But this week was a writer program named Convert, the API that will convert the temperature in Celsius into a temperature in Fahrenheit. 1:31   So that was the specs and the specifications for the assignment. 1:43   And that's basically one of the possible solutions. 1:52   So again, there are a million other ways for doing it. 1:57   So in this case and using is digital, we all know that this is not the best way to do it, but it's the easiest way. 2:01   So inputting the temperature in Celsius, uh, then if the user will type down, it will go here to print. 2:12   Thanks for using this tool break meaning is leaving the loop and is leaving the program. 2:24   If is not done then I'm checking if the input is numeric. 2:30   If it's not, I will print the message and go back. 2:39   If yes, I do the calculation on the temperature and I will print the and that's the end of it. 2:44   So I run it. So if the temperature is 12. 2:55   The equivalent is 53.6. And then. 3:04   Is that? I mean, obviously, that was not the only way of doing it. 3:09   There is also the possibility to use a try. 3:21   Except that is always recommended. I just wanted to use a metaphor. 3:25   We can seriously. Oh, thank you for letting me know. 3:31   Sorry about that. All right, so let me start all over. 3:35   My apologies. Okay. 3:43   So the program. So again, my apologies. 3:46   So it's basically the same wide through Looper that we did the million other times. 3:53   So the looper is on what is the temperature insult is asking. 4:02   I mean, the input is what is the temperature in Celsius then when finished. 4:06   So if the input is done, then it will print a message and it will break. 4:14   We'd go out of the loop out of the program. 4:21   If it's not done using is digit again is not the best way to do it is the more straightforward but it's not the most recommended. 4:23   But I mean try except the would work better. 4:34   So if it falls meaning is now the digital will print the you need to input the number on the letter. 4:40   Continue going back and asking again if will pass the test, then I will do the calculation and I will print the results again. 4:47   Right again. So if the number is 33. 5:03   The temperature, the 91.4 in Fahrenheit. 5:12   And if I type, then we'll exit. 5:15   So is that straight forward? 5:21   So let me stop sharing for a second the questions. 5:24   All right. So if. 5:35   No question, sir. Again, you can use the right accepter or you can use the is digital. 5:38   Keeping in mind that if you use is digit and you import a name, a number that is a protein pointer, it has the decimal, 5:50   but then you will not pass the test and you will get the message input the number on the letter and that will state over you. 6:03   All right. So just a quick question. 6:16   Are you all okay with? With Python, with the environment, with Pi Sharma. 6:23   It's all okay. Is there any problem? It is working fine for you. 6:36   Are you still debating if you want to use a notebook or something else? 6:42   So. What do you think? 6:50   Well, good. So far, so good. 6:55   Okay. That's what I want to know. All right. 6:58   Okay. Okay. 7:02   So thank you for the feedback. 7:07   Let me share the screen again and let me go now and the PowerPoint. 7:13   So I want to spend just a few slides on testing. 7:22   The slides are more sort of a placeholder than something to really describe a method or something like that. 7:30   We mentioned in the first class that when you develop software. 7:44   You have. Apart from the maintenance. So the two main components are the development and the testing. 7:51   Testing code is really essential. 7:58   So it's close to impossible to write a code with certain level of complexity with zero error or the first round. 8:03   So when you start writing something with a certain level of complexity, meaning not just three lines, 8:16   but something like 300 lines or 100 lines, whatever it is, it is very likely that there is a bug. 8:23   So you want to be sure that when you develop your software. 8:33   You tested that in the proper way. 8:40   As we said that most of the time you have different people developing and testing. 8:44   When you develop, your goal is to make sure that the program is running and is generating something that seems to be in line with the specs. 8:50   When you are testing, you actually are looking for points of failure. 9:02   So you want the software, the program to fail to break because at that point you did your job and you found something, 9:07   then obviously it could happen that there is no way or most likely this is not going to happen at the first round, 9:18   but it could happen after an aberration. 9:25   So testing most of the time, it's really back and forth. 9:31   So you do the first round you tested, you fix something, and then once you fix that, you realize that there is something else. 9:36   So you need to make sure that when you are developing the software, when you are testing the software, you test all the possible options. 9:45   So even before you write the code, you need to have a mental view of what the options can be for your program, 9:57   meaning all the different grounds, the different branches in you are a logical tree that is your program.   The user can go and you want to be sure that you will test each one of them.   So it's like having a flow chart and testing each branch of the flow chart.   So again, it's really essential to do a proper testing.   There are different schools of testing.   So in this case is a bay.   You develop a code based on testing.   So you do force a flowchart.   You determine all the alternatives that are in your program and you define the data that would drive the program into those branches.   And when you write the code, you tested that using those data.   So either you do a development that is based on tests or you do something that is more retrofitting the test,   the software you have, but meaning you wrote it and then you tested and you test every single aspect.   But testing, it's really essential for the development.   Um. It sometimes is not easy to find airdrops.   Sometimes the airdrops can be logical, sometimes can be of a different nature.   So let me go for a second. To think it's.   Yeah. It's probably. Yeah. Yeah. So let me go for a second to the different types of areas that you can get.   So one error is a syntax error. So for example, you had no call on after an E for else statement.   So that's a syntax said or meaning is an error on the syntax or the language.   So it's a formal, let's say, error, rather.   In other cases, it could be an execution error.   So the syntax is correct. But you are I don't know.   You are adding a string with a number and you will get an error.   So. The interpreter will tell you the type of error.   So that's an execution error, meaning syntactically it's nothing wrong, but when applied on will execute the code will generate the error.   Other. Option is the design error.   That's the trickiest type of arrow because there is no syntactic error.   There is no execution. Arora. But the program is not doing what it's supposed to do.   So at that point, you really need to go into the code and I mean, be like the Python interpreter and do each step.   Top from top to bottom, left to right, all the alternatives and see what is wrong.   What is generally very useful is to insert some temporary print statements for   the variables to make sure that the output at that point is like a checkpoint.   At that point of the code is exactly what you expect.   You can write the printer and then you can use a you can comment the printer when you do not need to again.   And eventually you can remove the numbered sign and have it again.   You bring the variable, but the value of the variable that you can print the type of variable.   So sometimes you got an error because instead of having a number, you have a string.   So at that point you may want to type to print the type of variable instead of the content itself.   So generally speaking, those check points that you will remove when the code will work fine.   But those check points are what we use the most when we do the debugging.   So you run the program, you see that it's not generating any error or meaning.   There is no syntax error or no execution. Arora.   But the results are not what you expected.   So at that point, what you do is to place those check points and see what's going on.   So is this variable at this point having the value that I expected?   If not, I go back and I check the way that value has been calculated and then you go back.   So again, inserting those check points is really essential when you do the debugging of your code.   So now let's go back to Python and let's talk about the other features that we will use down the road and from next assignment on.   So variable types. So we so the strings we so the numbers that are that are variable types in Python.   Strings numbers. Again, strings are normally in a single or double quotation.   For Biden, it's pretty much the same numbers can be.   Integers can be floating. You have operators working with both of their mustering and numbers.   So those are some of the examples. You can do some of the same operations.   So you can add the strings. You can divide strings, obviously.   But you can add the strings. When you add the strings, you attach one to the other.   There is no space because you are physically taking one string and the other and collapse that is.   But you can do multiplication. That is like adding in that time when an is the number that multiplier.   You cannot and you can do the same thing with numbers with the result that you expect.   So addition multiplication. It's obviously not my way.   You cannot mix and match meaning that would be an execution error.   If you add a string to a number, you will get an error.   So I mean, the interpreter will tell you the type of error that you are getting.   As I was mentioning before, you can ask a python.   What is the type of variable that you are working with?   So if you have a variable name, the name and you are.   Hope. And you assign a value to that variable.   If you do print that type the name of the variable, you will get that.   What is the type of the variable? Meaning this case is a string.   In this case, it is a list. In this case is an integer and so on.   So those can be useful because sometimes a.   You have one, you expect one type, but you did something and the result is not the type that you expected.   So at that point, you probably need to go back and do something in your logic and change it.   So keep that in mind. This is something that you may want to consider.   Um. We mention that there are two main types of numbers integers floating point.   So there are also array. We will talk about that later on.   Arrays, her vectors. We are the elements of numbers.   But this is something that we will do later on.   For the time being, integers and floating points are the two types of numbers that we are going to see them.   As we know, because we did two assignments on that, you can convert the string into integer and vice versa.   So obviously you need to be sure if you are converting a string into integer or numbers, either floating or integer.   If you are converting as we know, if you are converting a string into a number and the content of the string is not numeric, you will get an error.   So again. You can I mean, convert one into another.   You can convert an integer into floating the string into integer or floating thinks that that.   So that's an example. So you have a stringer with the number inside.   This is sort of pseudocode in Python three point something which should have the parentheses.   This is not there. So in this case you will get no error.   This case you will get no error as well in this case because the content of the string is a non numerical.   If you try to do the conversion of the string into number, you will get the net.   And we already knew that. So again, be aware of what could be inside.   Again, that's another case where you may want to add that eventually in a print statement before doing the operation.   If you see that there is something wrong and then you will remove it.   So it is just a temporary check point. So.   Again, we know that there are statements like is the jitter that can prevent the error.   We also know that we can use the trai statement that will intercept the error.   And that. So we talked about the user input.   We use that. So in Python three point something is just input in two point something was a row input.   But what you get is a string as. Meaning that eventually you need to transform the input into a number if you want to do operations with that.   Again strings. You can add the string.   You can multiply string. Lisa is another data variable type that is very useful.   We will use it quite a lot. It is probably one is not the most common.   Variable type that we use this like the number and the name is a sequence of elements within square brackets.   So the elements are separated by a comma.   You can have one or you cannot have a space before or after.   The comma is not required.   I generally use it for better readability, but you do not have to.   So the elements inside the the list can be numbers, can be string or can be lists.   So you can have a nested the list,   meaning get a list of lists and then you need to address those in the proper way   since you'll be a little bit convoluted but is something that is commonly used.   We would do an exercise on that. Those are examples.   So you have this list with four elements.   So a number, a string, an integer, a string, and a floating point number and then other string.   So python up for the list for the strings.   Pretty much whatever you think is starting counting from zero.   So the first element of a string is the element zero.   So if you want to print the example list, the first element that will be zero and you will get one, two, three, three is the fourth element.   And you will get a fish. So lists are mutable, meaning you can change the content of the entire list of elements of the list.   In this case, I replace the the last element from fish to tilapia.   And then when I printed, I will get the instead of the fish that was initially I have tilapia or whatever.   So. If you do print typer for this list, you will get this.   So again, sometimes when you do, you are coding.   You expect a list, but instead of a list you got a string.   So meaning that either you do the conversion or there is something wrong in the logic of your program.   And that's where the print type of the variable can help.   Again, it's a sort of a temporary checkpoint.   Both lists and strings can be as allies.   So if you consider or if you consider what I am.   If you consider this. Mr.   If you consider this list. So the fourth element would be D if you do.   Three. Fall. That means that you start from the fourth element and you go all the way to the end.   So you are at least in the list, taking from one certain point that you specify up to the end,   or you can do vice versa from the beginning to that point, or you can just slice from a certain point to a certain point.   So in this case, you are taking from this string that from the fourth element, the up to the fifth element, meaning you have the fourth and the fifth.   So. You can do also with the minus sign, you can get the last element.   Minus one. Minus two would be G.   I in the last two G and a. And that means you can really do a lot of slicing.   Um. Listen, as we said before.   Can be addressed by that number.   And it is a two ways method to access the content.   Well, let me go back to this one. Uh.   With up to three, you have the and then five means you stop before the five.   So that means is DNA. So it's up to that.   So you can do. List the name of the list, the square brackets one, and you will get to ABC, but you can do vice versa.   So you can get the index from the value.   In a given position.   So in this case, that is the reverse of getting the content based on the position so you can get the position based on the content.   So in this case, you are doing you are getting one.   And it is the second position calling the content ABC.   With, I don't know, four. I mean, you can do the first.   You can do. Uh, that's kind of tricky.   You can add the elements, or you can join a list.   So with a pen, there you are adding an element to an existing list.   So you have that this list. You add the 4 to 2.   You are extending the list with the new value.   That's particularly useful when you do a loop and in each loop you add one element to your list.   So you use and append within the loop, adding at each adoration.   One value you can do removal meaning same thing again.   Lists are mutable. You can change them the way you want.   So you can remove elements. So you can remove the element calling the value.   Or you can also remove it, calling the position.   So what is a pre he was mentioning before is that.   Difference between concatenation and appending.   So concatenation, you have this list one, two, three, these other lists, four, five, six, and you are adding one to the other.   But when you do the concatenation, you are concatenating the two lists and the result will be a list with all the elements of the fourth.   And all the elements of this is one single list, plain, simple, one place to this concatenation append as you.   So over here is a pending an element at the end as last element of the of an existing list.   So if you have the same A and B lists, you are pending B to A at this point,   you are appending new element the B meaning the entire list as a fourth element, as an additional element.   So you really need to address the drawing of lists in the proper way.   Concatenation is creating a list that is a sort of summation of the two lists.   Append is just a pending the second list as an element of the existing list.   So in this case, you have five elements or six elements with the sixth that is addressed by the number five.   In this case, you have four elements where the first three elements are simple numbers, and the fourth element is a list.   So if you want to address that, let's say this value five, it will be a.   Three because it is the fourth element to any square bracket that you will.   Right one. So there are two indexes at that point.   We will go back to that. You can use that in for a list.   So you can I mean, that could be in a brief statement, but just to explain how it works,   you have a value and you are asking the interpreter if this value is in the list.   So you just do value, let's say 42 in a this list here and you will get through 55 is not in a list that you will get at folds.   You can do either in or not in. In this case, if you do not in with 55, you will get through.   Obviously, again, those could be part of an effort statement and you can do different things based on either a true or false value.   Police are ordered the scenes to be obvious, but not all the variables in Python are ordered.   So when you create the list, that order will say, so is the order of your creation or the list.   So then you can change it.   Meaning you can add the elements, you can remove elements, you can rewrite the entire content.   But if you don't do anything called those, the order will say the same again.   Seems to be obvious, but not all the variables implied on have the same behavior.   And we will go there shortly. So again, if this is the order when you printed that will say exactly the way it is.   So in this case, this loop, what is doing?   You have a scheduler impedance for I in this list of value.   So the first iteration I would be zero and will print the periods zero schedule zero.   Meaning you have a one map.   The other iteration of the second element you will have P to Jim and so on till the end of the list till the number three.   That is the fourth element. Um.   I mentioned before, the order will stay the same unless you change it, because again, these are immutable.   So you can sort of ascending or descending to a list, if is numerical.   Is either a numerical descending on ascending if is a alphabetical strings, the sort will be by alphabetical order.   The. The strings. We know strings, so we use strings a few times.   So in a sense they have a similar behavior as this.   So if you have this string here.   You can slice it. Meaning if you print from a.   Five on meaning ops. You are printing from a deep space on and you'll have a Morning America.   If you ask for the fourth element that would be this the here.   Uh, if you do a loop. Um.   When you loop into a string, you are looping into each letter.   So when you print, you will get something like that.   So G and so on. I think certainly we use that with is digital.   They have a method to get we.   Additional I mean, to transform somehow the or to test what is in the string.   So if you have a string like this one and you can lower the values in the stringer getting this one,   you can have all capital with upper you have tidal meaning the capital will be after each space.   You can capitalize meaning the copy.   The letter will be only on the fourth letter of the string you can use.   Start with the meaning you are testing.   You are getting true or false based on if the condition is true.   I mean,   if in this case you have a low and a start with you are asking basically if the string it is starting with the capital h e and you will get through.   Biden is sensitive of the capital, a lot of small letters and that meaning if you ask her a small letter, h e, you will get false.   There are about 30 medals and you can get them on Python.   That's because. So in this case, you have a word.   That is a stronger dog.   You have a list and you are asking if this word is in the list and you get a false because a dog in the string in the list is all capital.   And in the string it's only the first letter in cabinet.   If you do a loop. So that's a compact way for doing loops.   So in this case, the way you read this loop is you have four peaks in the origin of the list that we had.   Meaning is looping in each one of the values in the list.   So x the first round will be fish cycle, round would be cow and so on and is lowering it and is creating a list,   new list with all the values in that lowercase.   And then when you printed that, when you ask if it's lowercase, you would get true.   You can do replace pretty much like in this, you can replace a value with another value.   So you have name or replace the original value, the new value.   You can replace chunks as well.   You can do conversions when you convert a string into a list.   You will get a list of the characters, the single letters if letters or characters in general that are in the string.   I'm saying characters because it could be a punctuation sign that it could be a space, could be a special character or whatever it was.   So each one will be one element of the list.   We pretty much covered this just one moment driving your attention on this one.   We already mentioned something similar. So this list is a list of lists.   So you have the first element that is a number. The second element is a string.   The third element that is a list. And the fourth element that is a number.   So when you do, when you bring the two, meaning the third element that without this one, you would get the entire list in the third position.   When you say one, that means that on that element you want to have the second element and you will get three.   So that's the way to get the an element that is inside the list.   That is inside the list. They can be nested as much as you want, meaning you can have eventually, if instead of three, there was another place.   Then you need to have another number in square brackets to address.   Eventually. What is inside? We know that you can do conditional steps.   So conditional steps in a list,   meaning you can have a list and then you can loop into the list and getting each one of the values that are in the list.   Again, you can do all the comparison that you normally do.   We mentioned breaking out of loops we mention continue.   You can do averages, and that's pretty much it.   Okay. So we finished the, uh, the slides, uh, quite early.   So I'm stopping sharing for a moment.   Just to check if you have any question.   If no question, I will introduce the in-class assignment.   So let me publish first the content.   So I'm publishing. The script for the previous exercise and.   And publishing. The slides and the in-class exercise.   All right. So let me share again the screen.   And let me go here.   So the in-class exercise is actually two exercises.   So one up is ask the user for a number depending on whether the number is even or the printout an appropriate message to the user.   Meaning, if I'd say is to, then the message would be the number that you input.   It was an event and so on, and you want to print a different message if the input is a multiple of four.   So if it's a multiple of four instead of printing.   It's an event that you will print that the number is a multiple of four instead of to determine if he is even or all the you would use.   You will divide the number by two and if the reminder is zero, that means it is an even number.   But the second would be create a program that asks the user to enter the name and age and will print out a method that will tell the year   when the user will take the hundred and the number of days from today till the first day of their hundredth year or not for the year.   So we don't know the birthday, but we are just based on yet.   Okay. So you have that in your canvas.   So now I will create a.   Some break rooms. So you will.   There would be 11 breakout rooms, three, four participants per room.   And just to be sure, is clear, for the first assignments, there will be no grading in any way for these in-class exercises.   They are only to be sure that you are practicing languages.   The best way to learn a language. I said that medium time.   So sorry for repeating it, but it's true. Learning languages are the real key for learning languages.   Either computer languages or spoken languages is to practice.   So you really need to practice a lot to be sure that you will be proficient in using it.   So at this point, I will pull.   All right, so I'm recording again.   There is no need to submit anything.   As I was mentioning before, for the fourth class and we are still in the first classes.   So there will be no submission, no grading for the class exercises they've been done,   only to give you the opportunity to practice a little bit more in a sort of protect the no judgment,   no grading environment, working with your colleagues.   And that's it based on how the class will go.   I can give students the opportunity to get more points and then the in-class exercises.   We get some grading. It will be, I mean, not much on the quality,   but more on the fact that you did something or you did not much or anything or you didn't attend to a class.   So the real scope is to be sure that you are using the time of the lectures for what they are meaning then in the lectures,   doing the in-class exercises for practicing.   So I mean, you are paying quite some money and you want to use all the resources that we are giving to you for the money you are paying.   So anyway. Let's start with your versions.   Anyone want to present what you did?   Again, there is no judgment. There is no mapping of.   Yep. Quite. I could present.   Please somebody else. Okay. Cool. Yep. Okay.   You want to comment a little bit? Yeah. So I have defined function call as is number which tests whether it is all number order or not voice.   I have tried to plot so that although one point number ago are taken and   secondly for negative and positive also it's considering your by expanding it,   whether it's complex or not. And if it's not true, it will break out of the loop and the invalid statement will be shown.   So secondly, I have then taken the input and after that, if Loop is running poised, I have checked whether it is all multiple of four or not.   And as it does, my role of audit will also be or even number.   And secondly, if it's not a multiple of four, but it's still a even number, then additive is shown.   And after that, if it neither of them, then it will show it's an odd number.   So. I'll just run a glass.   So this isn't. This is our number. Mm hmm.   The. It is an even number as well.   A little more colorful. Mm hmm. And what about if it's not a number and if it's not a number?   For example, if I would look at a cutoff, then it would show I'm getting an ad off.   I just. Yeah, you are getting an error because you defined the function, but you didn't use it.   So in line 12, you do or if is number well 930.   Yeah. But I mean if is not the number you will get an error in line 12.   That is what you have in the message.   Mm hmm. So what will how how will it solves?   Well, I mean, you need to test before doing the integer.   So you need to. I mean, about the normal, I guess.   Still, yeah. You need to remove from line 12 the ante like this.   Well, you don't have the number yet, so you cannot test the number.   So, so remove the one that you just added.   So this one will go then in line 12, you have a space that you want to remove this base before n because at this point,   okay, then you want to remove the anti.   Yeah. And the parentheses at the end.   And at that point, it should work.   I just testo character. Yeah, yeah, yeah.   Now it's looking okay. Thank you so sure that gets awfully not dull competing for a startup one input into your name.   And secondly, it was the age of the person.   Then to show the person's Adrian, when the person returned 100, that will be displayed.   So I just run. But it's. Or whatever you.   So this is the year Paulson moved on.   Mm hmm. All right. Okay. So in both cases, you didn't do the loop for, uh, keep using the tool till the user will type done.   But. But that's okay. Right.   I mean, in the previous exercises, we used the the while loop just to be sure that the user can stay in the loop till will type done.   But I mean what was not required. Yeah. And I mean, you, you the function.   We didn't introduce the function for the rest of the of the students in the class.   Don't worry about the function. You do not need to use functions yet.   You will use it in two or three classes.   All right. Okay. Thanks a lot. Thank you, sir.   Mm hmm. Anyone else? You can stop sharing.   Whatever. Anyone else want to present what to do?   So I tried the first one. Go ahead. Yeah.   Murder. I haven't heard of that thing with Lego.   If there is input of a like. If there is a string as input.   I have got that. Okay. Go ahead. You.   It is the program that I tried. So I have input now as if that's done or done.   And yeah, from Yaron, if an integer is less than zero, then only natural numbers are allowed.   And please try again. Message will pop up and continue.   If a number is equal to 000 that I would not even then continue.   If it's less, it's divided by four and the reminder is zero, then the number is multiple of four and hence it is an even number.   Continue again. Then if the number is divided by two and the remainder is zero, then the given number is even others.   It's an hour and we just need a you.   Yeah. So here it is. Like, please insert a number if it's in negative two or something.   It will only natural numbers that are alert. And please try again. If the number is zero, then it will show zero is neither or not an even number.   If it's a multiple of four, then it will show this number is multiple foreign hands.   It's a even number. If it's an odd number, maybe like the spin rate, which is an even number.   And if it's an R. And if everything has been.   Okay. Sounds good. Yeah. You didn't check for the nomadic, but that's fine.   Yeah. All right. Great. Thanks a lot. Anyone else?   Professor Paul, the second question, what I want to say to my screen.   Please. Yeah.   So. This is worth something.   Okay. So, Anderson. Actually, I was trying to locate what I would do, but still without looking, it is working fine.   Just. I'm in the presentation that you are doing.   It's really useful because it's giving the other students a way of doing things that 1:   they may not think that was possible or was useful or you made better or had that. 1:   They also did. It's a good learning way of doing things. 1:   Thank you. Thank you, Professor. So, Professor, this is my program. 1:   So on the first to last day, was it about his name? 1:   It is. And. And then, like here and giving my name. 1:   Mm hmm. The rest is printed. 1:   So then it will ask me, what is the current year 2023 and then the calculation books. 1:   So the 100 is will be under minus is plus the current year. 1:   And here I am also saying the remaining days for 400 year and also the remaining year for the hundred. 1:   That's a. That's is a simple problem. 1:   Okay. Sounds good. So you did the printing in a different way. 1:   But that's absolutely fine. Yeah. Okay. 1:   So here I have used this extreme to call this a defined name, year 100 as to call this thing. 1:   I have used extreme. Mm hmm. Mm hmm. Well, if I. 1:   If I remove this upstream and run this program, then. 1:   Lagos State considered to be the largest. 1:   It will not recognize the name. So that is a given this upstream. 1:   That's good. That's very good. Okay. 1:   So thanks a lot. And you want to thank you. Thank you. 1:   Anyone else. Well, mine is relatively simpler, but I concentrated. 1:   Please. So this was the first one, what I just said while true, and if it is a desert, 1:   then it will check the multiples or else it'll say please and draw a numerical value. 1:   Mm hmm. So you'll give it a number to say for it says it's an even number and a multiple of 47. 1:   It says it's an odd number. And if it's oh, if it's a letter, it'll say please and add a numerical value. 1:   And it'll just look if you enter another. So it'll asking for a number. 1:   So that was plus one. That's good. I mean, you may want to add an exit condition in the Y loop. 1:   The usual done when you want to exit. 1:   But that's fine. Okay. 1:   And then for the next one, this one was also pretty simple, but name id int enter your ID and then I took it directly as two doesn't do anything. 1:   So you print, you will be hundred in this year and then the number of days you have. 1:   So to. Yeah. Yeah. You need to stop the other one so it'll go on the bottom left. 1:   Okay, that's fine. Yeah. No, unfortunately. 1:   Yeah, yeah, yeah. Yeah. Okay. Yeah. 1:   Okay. Once again, I think I can just around here. Yep. So you and your name. 1:   And then you and do your age. So it tells you you will do 100 in the year 20, 101, and then you have this many days before your hundred year. 1:   Sounds great. Yeah. I mean, the only thing you may want to check if is nomadic or not, 1:   because if the user will type something that is not nomadic, then you will get an article. 1:   Okay. Thank you. Yep. Sure. All right. 1:   Winona. What did you do now that you need to present? 1:   But just want your comments. Sure. 1:   I mean, I can present both. Yeah, sure. 1:   One second. Okay. I guess I'll start with the first one. 1:   So I mean, one of my group members I've already presented, but I'll show what I did in case I did it a little differently. 1:   So this is the first assignment. 1:   First question. So I did a print enter a number equals integer input, and then the conditions were if a module I learned the module from forgot. 1:   So thank you for that. If the module two double equals zero print numbers even. 1:   Same thing for for. If a module for w equals zero print number is odd. 1:   So if I run it. I say by a numbers even number is odd. 1:   I don't know if I did that one right, but that's what I get. 1:   All right? Yeah. I mean, there is no. 1:   I mean, there is a connection. Well, I mean, the condition is intrinsically there because there is no loop. 1:   Okay. But what is missing is a. 1:   I mean that if you look at the code in this case if is a multiple it's even is even. 1:   But then on line six, it shouldn't be. 1:   It's odd, but it should be is a multiple of four. 1:   Right. Right. And then there should be an Elsa, that is. 1:   Print. I mean that column. 1:   Before. Yeah. You want to delete this piece? 1:   Hold on a sec. So you held a call on that. 1:   And then new line. After Elsa. 1:   Okay, great. And then that print number is on. 1:   Run it. All right. 1:   All right. Okay. And if you try. And an odd number. 1:   All right. You got it. Thanks. 1:   So there are a couple of things. One, you may want to check if the input is really numeric or not. 1:   Mm hmm. And the second is, you may want to check. 1:   You may want to have a look just to give the user the possibility to keep using the tool till he or she would say, that's enough. 1:   But that is not was not in the requirements. 1:   Yeah. Did you want me to share the second question. Confident in this one. 1:   So for the second. For a second one, I did. 1:   Name equals input. Hi. What is your name? Age equals integer parentheses input. 1:   What is your age? I use current year equals integer input. 1:   What's the current year? And then year underscore 100 equals 100 minus age plus current year days. 1:   So 100 equals 365 times 100 minus current year. 1:   And then print f name. You will turn 100 years old and you're 100% F. 1:   That's days to 100 days. Now, similar to Debbie Rogers, you helped me with this one. 1:   I think mine's a little bit different. Maybe shorter, so. 1:   But I think. If I run it so. 1:   I think it does work. Yep. Yeah. 1:   Okay. Sounds great. I mean, the same thing. 1:   It check on being numeric for the input would be appropriate and then the loop eventually to give the user the possibility to keep using the tool. 1:   But I mean, it's minor. All right. Thanks a lot. 1:   Okay. So. Let me share the screen and let me show you. 1:   So this one was the program for checking if the number is on the ribbon and multiple of four. 1:   So again, generally speaking, you may want to have some comments, obviously not in half an hour time as you had. 1:   But when you do, when you have more time, it's always good to. 1:   Add comments because it will increase the readability. 1:   So Lupo on on the requests. 1:   Just to be sure that the user can keep using the tool, then getting and testing if it's done, if done will break after message. 1:   Then I have to try. Except on. 1:   On the first input. So I try getting the integer out of the value that is been imported. 1:   If I get an error, I will print an error message and will go back. 1:   Then I am checking for if is a multiple of four. 1:   If yes, multiple of four. If not, I will go here and check if it is a multiple to a message. 1:   Otherwise, it's odd. And when I run into. 1:   So if I have a body for. 1:   Table for 22. It's even not 33. 1:   It's the. Uh, that I can do. 1:   W. W. That's America. 1:   Dun dun. The other one. 1:   Um. Same thing. Some comments, same looper asking. 1:   Name. If done, break holds. 1:   Asking The Age. Transforming an integer if no error. 1:   If there is an error, I go back, I read in here those values that I'm in that is not great to write values into code, but that's just an example. 1:   So you have what is the year today, what is the month, the day, and what is the number of years? 1:   200 and then that calculating the centennial birth here printing. 1:   The days same think. I'm printing. 1:   So it's a little bit more complex because I'm also taking into consideration the day and the month, 1:   but it's pretty much the same that some of you already did. 1:   Name whatever it is. Age or whatever it is. 1:   And then you have all the rest. 1:   And then when you finish ups where I was, we would look. 1:   All right. So. That's basically it. 1:   I will. Both those two on campus. 1:   Let me go now to the assignment. 1:   So for next week we have an assignment with the two parts. 1:   So the part one, it's riding. 1:   I mean, they are both related to the same program. 1:   So the testing and the program are related to the same matter. 1:   That is what the program is going to do. But you want to do to design the testing. 1:   So designing testing is basically using an approach like this one. 1:   So you will use this doc phyla as a sort of template. 1:   So you want to have the goal of the program. 1:   And the goal of the program will be. From here, write a program that has a loop and so on. 1:   So that's the goal of the program. What is the testing strategy? 1:   So the testing strategy is a checking each one of the option. 1:   For example, using Don to exit the program, printing a proper statement when the user enter done checking that the password enter is acceptable. 1:   Printing a proper statement that when the user enters a non acceptable password. 1:   So those are examples of the alternatives that there will be in your program and then you want to have data to go into each one of those elements. 1:   Those are alternatives that you'll have. So if the input is ABC, then the output will be the password is too short. 1:   If the input is done, the output will be good by things like that. 1:   And you want to be sure that you are checking all the bases where the bases are. 1:   Each one of the options so that your program can go in running. 1:   So the programmer is a writer programmer as a looper to ground the user for input, 1:   checking it and printing the proper message until the user it will print done. 1:   That is very generic. So in this case. 1:   The idea is to enter a password and check if the password is a meeting the requirements so the user will input 1:   a password or done when finished and the output will be either password accepted or not accepted or good by. 1:   So the password needs to have a minimum length five character, maximum length 12 character, 1:   and then any combination of numerical and alphabetical character, either as more or capital letter. 1:   So you will you will prompt so you will to the rules of the game that are daughter. 1:   Please type a password with the following characteristics. 1:   Then the user will input the password. 1:   If the user will type done, you will print goodbye. 1:   And so then you will check the password. 1:   Meaning you will take the input that you had from number two and you will check if the requirements are meaning meaning a minimum length, 1:   maximum length and combination, or maybe going out for alphabetical characters if. 1:   The requirements are okay are met. 1:   Then password accepted. If not, you will print the proper message so they can be. 1:   The password is too short. The password is too long. 1:   Or that there is. There are not the numerical values or things that they. 1:   So each one will have a separate message if multiple errors, of course occur. 1:   Then you can use any of them. If is accepted, then you will print password accepted and that's basically it. 1:   So you need to submit your document, 1:   your PDF or doc document with the testing and that you will submit and you will submit your program in the pie format. 1:   So that's basically it that it's a little bit more complex. 1:   Again, each assignment will be a little bit more complex than the previous one. 1:   That's the rules of the game, and that's the reason why you do not want to fall behind. 1:   Because catching up could be complicated. 1:   So be sure that you are current, that you are okay if there is something that is not going right. 1:   Check with me. Check with the DEA, with you. 1:   And we would be happy to help you. Questions. 1:   As someone questioned the test driven, the testing driven part, are you essentially explaining what you will be writing in the program or. 1:   I didn't I didn't understand. What exactly are we supposed to describe? You will describe what will be the. 1:   I mean, in a sense, you are describing in plain English what you are doing. 1:   But let me go back to sharing the screen and let me go to the document again. 1:   So you are writing a what are every single alternative that you are going to have in your program? 1:   So basically one alternative is the minimum length. 1:   The requirement for minimum length A is not met, meaning I don't know the password that is for correctness. 1:   And in this case you will have a uh so that logic would be the first one would be the testing for the length of the password. 1:   The second that will be testing for the presence of a numerical character. 1:   The third, that will be testing for the presence of alphabetical character. 1:   And each one will be elements that you will need to test. 1:   You want to be sure that the output would be what you expect. 1:   Meaning if the password is too long and then you need to screen the message, the password is too long or is too short. 1:   The password is too short. And then when you do this sample, you need to provide the sample of the password. 1:   So if the input is ABC because the requirement is a minimum of five, 1:   this part is a is three characters and the output that will be the password is too short. 1:   ABC is just an example. It's any of a character. 1:   It's a combination of characters or numbers that are not up to five. 1:   So. It's really describing the logic of your program and when and how you will do the testing. 1:
: So. Hello, everybody is Wednesday, April 20, sixth, and it's 6, 32 right now in this class is not a a Br. Class. It's kind of a office time. 0:01 : So the only thing that I want to show you is basically how to proceed. The with the the last assignment to that size 9, whatever it was, and then I would be here to 0:18 : get your questions, if any, if there will be no question that will be the end of the class. 0:36 : So again, if you have any question on the final or other things, so that are related to the courts, so feel free to ask me now, or whenever you want. But now would be a a good moment for that. 0:46 : So 1:02 : let me start sharing the screen and going to 1:04 : this to the 1:11 : proposed solution. So the idea was to work on this file. That is a a a subset of what we normally get from 1:15 : mit ctl, and what day for each semester. We have something like what you have on your screen with the information 150 1:30 : on the individual courses. So each court, so is one line, and so I kind of sanitize the some of the content distractors. You will not see the names. You would see numbers 1:40 : the to 2:00 : corporate courses, and there is a a a name of the sponsor. I eliminated that. But apart from that, it, it's, it's 100%. What we get from a work day 2:01 : you you were required to do some analysis. So some of the analysis where in the requirements, some other, maybe something that you want to do on top of what W. Was asked by the requirements. 2:18 : So let me go into the code, and 2:35 : so 2:41 : it's obviously heavily relying on a on a pandas. So without Pandas, so it would take way much more than the current 89 lines of code and I mean considering that there are comments that are blank lines. The code is called the the the pure 2:43 : lines of code, that 3:08 the number could be probably 60. 3:11 : So it is a a short script, doing a quite a lot of things. 3:14 : So I imported the batch of libraries, including a library for the palate. So you do not need to do that, but it is nice to have call off. So 3:21 : that's the given palette. You can in pick another pilot, if you like. 3:37 : So because we have 2 chats that are pie shots, so that I mean that the same, apart from the data, that and title that that we passed to 3:42 : the the chart, I created the a function called the Create Pie Shop. 3:58 : Not much imagination. And I pass the data structure, the the call on that they want to analyze. So 4:06 : all the shots are based on the enrollment, count, but it's a role, my account as a instructor. So as a a program. So that's 4:18 the I mean the the the name, and then passing, and then the title that we change. 4:30 : So I did my group buy in the in this function, so i'm grouping by 4:38 : whatever is the name of the call on that could be, we will not see over the year. so that will be a program or a delivery mode. 4:49 : So let's say it's programmer. I want to do a a chat program, a roll my account. So i'm group buying a program, selecting the the column enrollment count. 5:01 : and then doing the the the summation on the value. So that's something this nomadic, only true, that was introduced in the latest version of all the Pandas 5:16 : in the past, the nomadic only through was the default. Now you need to specify. If you don't do that, you will get a a warning. 5:29 : Nothing that will crash your code. But if you can avoid it, that would be better 5:39 : calculating the percentage, because I mean with the bouquet you you're gonna just pass the number so. 5:47 : and the the the the library. We do the calculation with the percentage. You need to do it kind of manually so, and then 5:58 : converting the series into a Pandas so data frame, because this is required as an input for the pie chart, i'm passing the angle so that I calculated using the a percentage. 6:10 : and 6:29 : I'm using a pi, 3.14, 6:31 : and and another number. So 6:39 : to calculate the the angles. 6:43 : So, using the library. 6:47 : we adding colors and then creating the source data for the visualization 6:51 : setting the parameters. I'm. Having the 7:01 : over on the tool tip by group. 7:04 : you will see the results 7:12 : creating the the actual pie shaft. saving it, showing it. 7:14 : And that's what the function is doing. 7:21 : The main program reading the Csv file. 7:24 : You don't need to to do what is in the room? 59 7:28 : replacing the not available with 0. That is no, no, no, not available. But I mean that you never know. That's why I added it. If there is no not not available, then 7:35 it would not replace anything. 7:49 : so 7:51 they do nothing. But if you don't have it, and you have some none available. Then you may have a problem 7:53 : calculating and printing the different metrics. 8:02 : So this is a the largest for enrollment 8:07 : Again, some of you. One of you asked me. 8:12 : You have a some courses that are on a multiple sections like am 6, 24, or that is a and I don't know if this semester. 8:16 : But let's let's say 6, 12, so there are 7 out 6, 12, so 8:33 : you could have the combination of those giving them more indication of the entire courts, and not just the section. But this was not a. Was a W. What was a, asked the by the requirements. 8:41 : so 8:56 : eventually is an additional level of analysis that you can definitely do so we can. When we talk about 8:57 : the the largest enrollment, count per instructor, we are counting the the individual sections for the different courses, no matter if it is the same course in multiple sections or not. 9:07 : students in graduate courses. So i'm reducing and selecting only the part of the data set we are Level is equal. G 9:25 : for graduate, Same thing for undergraduate corporate. 9:36 : then, and for each one. I'm. Creating a additional data structure. 9:42 : then open courses, close cis, and then 9:48 : i'm printing, then creating the pie shards. 9:55 : So the first one i'm passing a program at the column, and the title is Distribution of enrollment by program, and the second delivery mode, the distribution of all meant by delivery, mode, and and no processing. : So if I run it : so, those are the shots. So this is the distribution of enrollment by the delivery mode that you had the over on with the the values. : So, and this is a distribution of enrollment by program and same thing with the legend that is telling me what the different sections : on : in terms of a printing. So I have the largest enrollment, count, but court. So I mean, it is not the nicest, definitely way of presenting the data, but is what you need. : So those E. Ng. R. Are cross school courses. They are undergraduate. They are some of the first courses that under undergraduate are taking. : They are not only school of systems and enterprises, but cross the : entire Institute, and that's why you have those monuments in terms of numbers. : Ss. W. 5 55, but is a very popular court. So and he's on that. : and he is a graduate undergraduate : when you have a the numbering of the course, so there is nothing with 5. That means that that can be both rather than an undergrad right : largest enrollment, count but structure. So you have the Id of the instructor. There is no name. I remove the name and the counter. : It's quite likely that those 2, I mean. Obviously those are the instructors for those courses. : The total number of students in graduate courts is undergraduate courses, corporate courses. : But we have quite a lot of : corporate students, so it's almost half for the ground, as they are all graduate. : How for the grad weight : ere. : and a little bit more than 900 undergraduates. Traditionally, this school of systems and enterprises has a a more, a graduate than undergraduate. Things are. : I know, in changing a bit, so we are having more and more undergraduate. But Still, that's a distribution. The open courses meaning that courts is that they are running and not at capacity. And those are the courses that running at. : So that's basically it for the and that's pretty much all I have to say on my side. : So i'm not sure that all of you already submitted it. We are not going to : to be too strict in the submission time, so we we Obviously, if you would submit a in a week you will get 0. But but : erez agmoni, if you submit a little bit late, that's not a a major problem, and that's the reason why i'm not publishing the solution yet 101. : So questions Are you working on the final : mit ctl? And again, just as the recap final are due in a few days 250 : next week. That will be the presentation of the finals, meaning some of you. 5, 6 of you will be asked to present : day jobs. You will be notified the the day before. We don't have a a lot of time, so it it's kind of a trade off if I give you less days : to complete the final. Then I I could give you more days in advance to know who is going to present. But the very end, I thought, that is better to give you as many days extra as possible to complete the final instead of giving us that. So : when you will develop the document. Keep in mind that you may be asked the to present it. You don't need to have a a powerpoint, the regular : 5, 6, 10, page 15 page, whatever it's going to be for, for for the fine of 5 would be too short at once. Let's say saying that plus page for the document that will be enough for the presentation. So again, you don't need to have the Powerpoint. : If you feel more comfortable developing a a a Powerpoint on top of your Pdf. Or word document. That's absolutely fine. : What else? During the presentation you will have a 10 : you can do 12, or you can do 13. You cannot do 20 min for the presentation. : You cannot do 3 min, so be sure that you I mean I get the into the mindset of doing a presentation all around 10 min : after the final up after the presentation of the final. So I will have a 72 h to submit the your final grade. : meaning, there will be not much time for any complain that you may have. : So go back to to the grades that you have. You basically have everything apart from the except size Number 9, the one that I presented a few minutes ago, and the final. : So : you and you had the formula to calculate the final grade. So there shouldn't be that much in terms of surprise for most of you meaning, if you have any issue any question, ask me now : because I I can do something now, if it appropriate, it would be way much more difficult if you ask me. Once I posted the final grade. : I mean that I cannot think about anything else in terms of instruction. Be sure that you will submit the : your document, whatever format is the format that you are using, and the script we need to run the script. So we need to make sure that the script is working. When you will do the presentation you will spend few minutes explaining : what you did in terms of writing the script. : So be sure that you are prepared for that. : That's Basically, it questions Ava: Hi, Professor! So as I was working through the final, I had like 2 questions. One's like more of a general one, and then the second is more specific to the problem that I chose. So it's the more general one. First, as i'm like running this script, i'm fine that's taking a long time to run. So I was wondering if you had any comments on that like, is it okay? If the runtime is really long, or is there any like workarounds for that. : Well, I mean there is no generalized the answer for that. When I run my model victorizing tests : it's run for a couple of days. : But I mean, in that case is a lot of calculation. It's a deep learning that are a lot. It's a : I mean a TV show, a neural network with a lot of a he delay it, and it takes long when I use another metal, the using what is called the shallow neural network, meaning with just one hidden layer. It takes few hours, but still is few hours. : It it really depends. Keep in mind that that sometimes, when you don't, have a very complex calculations, so i'll go it that you are implementing in your tool. You may do something to optimize it. : An example. Now, if you do loops into loops so nested loops, generally seeking python is not very efficient on that. : If you can transform a debt into operations between data structures like what I presented for the solution of the the current assignment that that would help a lot. : E, c. So with C. I think that are a faster because it's compiled. So : revise a little bit the code, the try to reduce the the loops as much as possible. Try to use a pandas and a data structure in general a as much as possible. That's the most generalize the answer that that I can give you. : Obviously, each code that is different. I I don't know exactly why you are called the it's not as fast as you want, and the but, generally speaking up, that those could be some some of the issues. Obviously, another issue that is quite obvious if you do quite a lot of of a of writing files. So we defiance that is lowing down. : so : do all the readings, and then I keep the the the data in memory, and then write whatever you need to write at the end, and don't do Megan for that, because this this is low in down. That's probably it. It's not the case in your case. Ava: Hmm. Okay, that makes sense. Yeah, I mean it's not hours. It's more like a couple of minutes, so I guess it's not something too much to worry about, but i'll take a look at the loops and try to limit those as much as possible. Ava: Second question. Ava: And yeah, so the second question. So i'm working on the people migration data, and I want a quick clarification. On the fourth question asks, what are the dynamics by income and geographic region? Ava: So I just wanted to clarify like, Are you looking for like one Ava: visual that has both income and geographic reach and data on it, or like, are you looking for 2 separate visuals, one about income and one about : that's up to you. : It really depends on the type of visualization that that that you are using. If you're using the the San que diagram, and in a one of the an example, so that I presented the the the Sanky diagram we'd allow you to give both the ways. : But it may be a little bit complex, so probably it would be easier to do 2 graphs. But again it's totally up to you. Ava: Okay, yeah, it was a little bit tricky, because we only have, like the income level and regions for, like the destinations of the origin. So I was trying. Ava: But yeah, if it's okay, just to do it. As to that's great. Okay. And then like for each question like, I mean, of course, if the first one is just like an overall. : But so are you looking for about like one to 2 visuals per question, or how like? What do you expect? : Go beyond the the questions you have. So the questions are are kind of samples. So you you want to understand what's going on. I will give you an example. So : yesterday it was the presentation day for the other 6, 24 that that i'm teaching a student what on : the mass shooting and she was analyzing both the text and numbers. So : and what she did that was was basically to see what is the distribution in time? What what are the States where there is more : mass shooting, and then on the tech side, she collected data from that some news outlets, so that they are more on the Conservative side and the news out outlets, so that that more on the limit outside. : and then doing some natural language processing, she extracted the the diagram. So the visuals so visuals. In that case we are. What Cloud : and what she got was a something that she didn't really expect, that that I mean, if you think for a second it's reasonable, but may not be the first thing that could come. : E. C. : The the psychological conditions of the shoe that's not all the other problems. So that's an example. I mean. : I I did then ask in this particular case what was the most stress, the aspect, the of the conversation. : But when you have a problem. You want to drill down and see what are the reasons? What is the problem? : So in those cases you may even use a external sources like in our case. She downloaded the the text from the different news outlets. So again, obviously. : eventually, there is no limit. You can spend a months doing. The analysis in particular. M migration is a complicated problem. But : just to be sure that you are not limited to the 4 questions you have. So the questions are a example of questions. So if you address those 4, that's fine. : If you think that there are other elements that would be more relevant to better understand the problem. That's absolutely fine. You will explain it. You would say, I think that this point is more relevant. : and then you you will analyze it. Ava: Okay, yeah, that definitely makes sense, You know. If something, you know sticks out definitely, you know. Try to find some more info on it to go. : I mean that don't see this course, this assignment as a a traditional one. When you have questions you need to provide answers. You need to use the tools. So to understand what's going on. : and then your vision is different from mine is different from anyone else. : So use your vision, your interests, your experience to address the I mean to understand the the problem. Ava: Right? Okay? I think that answers my questions. Thank you so much. : Other questions. : I've got a so I think go ahead. So : yeah, I have a I have a couple of questions on the project, but before that I will. I wanted to talk about the exercise 9 that you just showed. : So you seem to use bouquet plots, I I mean. I try to do it that way, but I've found it a bit challenging, so I dropped it halfway down and used blockly. I don't know if that is, that's absolutely fine. : It's fine. Okay. : And I can relate to that. : Yeah, it was. It was pretty challenging, and there was no time. But yeah, okay, that's fine. So : yeah, coming to the project, I picked the COVID-19 as my project. So I was. : I was analyzing the counties data, and for that flip codes were already given, so it was pretty easy to find the shape, file and merge it with that and create a to that. That was : easy. But when I was trying to merge the data sets of the Gdp and the counties and the housing units. It was a it was a bit challenging to : actually create a new column and put those flip codes on the columns for Gdp and the housing unit. So I I did did it manually, so I created a new column, and : I actually went back to. I downloaded the code file and actually : copied it manually. So I I don't know if that is, that is yeah, I mean, sometimes doing it with the school the way much more complicated than doing it. : The reason was both the excel files format was a bit, I mean they they didn't match quite a bit, so I I : I had to do it manually. So just to get the High Gdp low Gdp Counties and the corresponding deaths and the cases and : yeah, such. And you can explain that in your report to saying I did the the with a little bit of a : human contribution, not 100% automatic. And that's absolutely fine. : Okay, that that's the only area that I just manually inputed stuff. Actually, there was one more. So I wanted to. : So the Gdp was only up till 2,018. If i'm not mistaken. Yeah, it was only up till 2,018, so I wanted to see how the : like. I wanted to build a time series graph, so to check how the Gdp was on the 2,019, 2,020, and did it go up? Did it go down? So : at that point I wanted to merge the new Gdp file that I found, and with the with the one that you gave that that was : also a bit challenging. So I just merge just the United States, the top the top level one, just to get the general consensus, like the General Gdp of the the whole country as a whole, and for the all 3 years. So that was also a bit of manual. : My. : just to create the one graph just to create the time series. But that is : okay, 2, right? : Yeah. I mean, it would be good to do the analysis at the county level, because you would see how the Gdp at the county level may have an impact on : things related to the : Yeah. So yeah, you have a I don't know less resources, meaning less intensive care units, and people will die more so, just as an example. But at the very end. You need to deal with the data you have. : That is true. So I I I still want to build those graphs, but i'm already up to, I think, 13 or something, so I I : I don't know if I can build a report for all the that that makes sense explain the limitations. So the issues that that you had : consider it a a as a sort of the the same criteria that we use when we do the academic publications : when you publish : your paper is reviewed by Ps. And they review it a. And they do it sort of critique all what what is inside. : So when I see something like in this case, why the analysis is not done at the county level, it would make more sense. So you don't have the time you don't have the the data set. But you need to explain that. : So you need to say, okay, it would be good. I I know that it would be good to go at that level. But I didn't have the data set that can assist me, and I didn't have the time and to find another one doing it. : and that's fine. So you are explaining the the limitations of your : I will do that. So I didn't do it for the 2,019, 2,020, because I just found the data set like 2 days ago. But for the 2,018 the one which you gave. I went down deep down to the country level, and from the High Gdp Countries, Low Gdp Countries, and how they dealt with this in that. And same with the : high housing units, countries, and low housing units counties, and how the cases progressed over there. So : I I just didn't go above 2018, 2019. So yeah, another way to go around I mean at the county level it is. It's unlikely that the Gdp will go to : from being very low to being very high. : so you can create categories : meaning you. You can have a that will not give you a a time series for the Gdp. But it will give you a data point in the in the analysis. : So you you can class. The account is by 3 4 5 categories so medium and high. So : and it's likely that they will stay like that quarter the remaining a year. So that you don't have data. : Okay, that that's fine. Okay, I just did it for the low and high. But yeah, I could ben it for the medium level. And yeah, yeah. : A better story. : Yeah, the last question. So there was another file professor, which was, I think, the : the total census data of : the entire country based on the whole 10 years. : so I : you don't need to use it. You don't need to use it. You can use it. : This data Set the if you think that can be read around. : Oh, yeah, I've found one use where I just looked at the age group. I mean the total populations of these counties which the the debts were higher, and I looked at the population, and then I looked down and looked at the age group of each counties where there were more debts, and whether all the people who were : likely affected by that. So that's all that I could do with that data. But if I could merge merger with another one, it would have been better. But I I I found it really challenging to. : I mean the age is one of those things that we know that is highly correlated with the that's from Covid. : Yeah, the : Yeah, I I yeah it. It. It is pretty much common sense that : the older the people are more likely. They're you know, to be effective. But so I wanted to do a regression analysis. But I couldn't : get it. Get the 2 data sets to merge. So I that's that's where i'm at right now. But : yeah, I think that clears my doubts that that that's all I have. : Yup alright, great. : all right. Other questions. Sure. : Question. : Okay. So that was short. And again, if you have questions from now till the deadline feel free to send me and she, you be sure to include she, you : just to increase the the chances that you will get an answer right away. : and we will be happy to address the issues you may have, or the questions you might have. : All right. So enjoy the rest of the evening, and we talk in a week, if not before, so I would be on campus tomorrow if you need me most of the time. But I would be available anyway via email.
 

fis STEVENS

lw INSTITUTE of TECHNOLOGY
Is
Fi
it

Input & Outputs,
Dictionaries and Tuples



clipizzi@stevens.edu

SSE

 

at
Files we

 

¢ Offen when you are writing code you will want To
access data that is stored in a file. For example:
—Text file (.1xt)
—Comma Separated Values file (.csv) (really just a
text Tile Too but the extension lets you know the
format of the data inside]

¢ You can use the open() function in Python to open
a file for reading or writing data

STEVENS INSTITUTE of TECHNOLOGY | 2

 

File Processing lw

¢ A text file can be thought of as a sequence of
lines

From steohen.marquard@uct.ac.za Sat Jan 5 :16 2008
Return-Path: <postmaster@collab.sakaiproject.org>

Date: Sat, 5 Jan 2008 :18 -O500To:
source@collab.sakaiproject.orgFrom:
steohen.marquard@uct.ac.zaSubject: [sakai] svn commit: r39772 -

content/branches/Detalls:
htto://source.sakaiproject.org/viewsvn/?Vview=rev &feV=39772

http://www. py4inf.com/code/mbox-short.txt

STEVENS INSTITUTE of TECHNOLOGY | 3

Opening a File lw

¢ Before we can read the contents of the file we
must tell Python which file we are going to work
with and what we will be doing with the file

¢ This is done with the open() function

¢ open() returns a “Tile handle” - a variable used
to perform operations on the Tile

¢ Kind of like “File -> Open” in a Word Processor

STEVENS INSTITUTE of TECHNOLOGY | 4

 

Using open() lw

¢ handle = open(tilename, mode}
— returns a handle use to manipulate the file
— filename is a string file = open(‘mbox.txt’, 'r)
— mode Is optional and should be 'r' if we are

planning reading the file and 'w' if we are going to
write To the file.

http://docs.python.org/lib/built-in-funcs.hAtml|

STEVENS INSTITUTE of TECHNOLOGY | 5

 

What is a Handle? ie

>>> file = open('mbox.txt’)
>>> print (file)
<open file 'mbox.txt', mode 'r' at Ox1005088b0>


Note: Since file is not a 
reserved word, it can be
used as the handle. 
 

STEVENS INSTITUTE of TECHNOLOGY | ¢

 

 

a won

Simple Example 

Say we have a fext file like this, the simple code below gives the output to
the right.

test.txt
The quick brown fox jumped over the lazy dog.
Goodbye!


file_handle = open('test.txt', 'r') 
Linecount = @ 
for text_line in file_handle: 
Linecount = linecount + 1
‘
print linecount, text_line 


1 The quick brown fox jumped over the lazy dog.
2
 3 Goodbye!
STEVENS INSTITUTE of TECHNOLOGY | 7

 

‘ won

Simple Example

Say we have a fext file like this, the simple code below gives the output to
the right.

eee test.txt

The quick brown fox jumped over the lazy dog.
Goodbye!

file_handle = open('test.txt', 'r')
Llinecount = 0
for text_line in file_handle: 
Linecount = Linecount + 1 
print linecount, text_line

1 The quick brown fox jumped over the lazy dog.
2
3 Goodbye!


STEVENS INSTITUTE of TECHNOLOGY |

File Handle as a Sequence

A file handle open for read can
be treated as a sequence of
strings where each line in the

file is a string in the sequence 
We can use the for statement 
to iterate through a sequence

Remember - a sequence Is an
ordered set

STEVENS INSTITUTE of TECHNOLOGY | 9

 

Counting Lines in a File iw

° Open a file read-only
¢ Use a for loop to read 
1 |
each line 
¢ Count the lines and 
print out the number of ‘ine Geunh 1aaosds
lines

 file = open('mbox.txt’, ‘r’)
count = 0
for line in file:
count = count +  1
print (‘Line Count:', count)

Output:
Line Count: 132045

STEVENS INSTITUTE of TECHNOLOGY | 10

 

Reading the *Whole* File lw

¢ We can read the
whole file (newlines
and all) into a single
string.

>>> file= open('mbox-short.txt)
>>> inp = file.read()

>>> print len(inp)

94626

>>> print (ino[:20])

From steohen.marquar

STEVENS INSTITUTE of TECHNOLOGY | 11

 

Practical Example

Say we have a fext file with a Tweet on each line and we want to extract all
the hashtags...

Our file looks like this:

eee tweets.txt

jaFlunkie we have @etdragonpunch . even more dank than trump

Rand Paul Supposes He Might Support Trump As Nominee After All {rssmicro} https://t.co/ohPsEsO9Rf

@slushstuff Things are looking pretty bleak for him. Would you vote at all if based Trump-sama won the primary?

No doubt about it Cruz is a good man.Trump or Cruz. https://t.co/iDH9X1ANDs

Trump voters too dangerous for kids on Election Day? {rssmicro} https://t.co/ohPsEs@9Rf

This is what happens when you attack Trump! Each candidate falls big-time! Beware Cruz!!!https://t.co/lBZBHIv01I4

I hate Donald Trump, he is just racist

Donald Trump Says His Money Drew Hillary Clinton to His Wedding - ABC News —- https://t.co/kj@xJu@V7r via @ABC

»@slone Trump being rude to disabled, ties with mob, bribing politicians S&amp; more. Sounds like NY to me. @AndrewArlink @Schu64 @lesko_mike
Excellent piece in @guardian by #KaddieAbdul about humanizing the #other, even if it's #Trump supporters: https://t.co/Onb4Pho2Gy

Obama&U2s Failings Are the Reasons for Trump&U#s Rise by Victor Davis Hanson https://t.co/smr4e0yy16 via @NRO

#Politics Politics analyst Mike Tyson predicts Trump vs. Hillary for 2016 &UO USA TODAY https://t.co/DxsaMItZju

Obama's Failings Among The Reasons For Trump's Rise {rssmicro} https://t.co/ohPsEs@9Rf

Behind Cruz's attack on Trump's 'New York values' https://t.co/uoZyUQPINL via @DCExaminer

@CNN @jaketapper @realDonaldTrump I hate Iran as much as anyone, but it's about time Trump started to tell the truth !

The end of Ted&U#s excellent adventure? How a New York Times bombshell and Donald Trump could finish Cruz {rssmicro} https://t.co/ohPsEsQ9Rf
This 'Freedom Kids' Trump VIDEO is making the internet GO NUTS https://t.co/wq0eQWUb87

Is it just me or is that video of the three little girls from the Trump rally vibe with North Korea in style (not to mention substance)?
U.S. Chamber chief calls Trump "politically stupid" https://t.co/PxTGGhBz0k

Clinton's Real Bimbo Eruptions vs. Trump's Alleged 'Sexism' https://t.co/vDXx8GSOsr - American Thinker - https://t.co/pCKoA0g8xi 123
@AnnCoulter Trump is right take it to the courts let it be decided,you have to ask if he believes there isn't a problem ???

I take this as evidence that GOP billionaires don't really care if Trump wins the nomination https://t.co/a0@LgkoB3u

Donald Trump Campaign Now Clearly Endangering the Well-Being of Children https://t.co/eXlcqcVzsD

@ss31704_s @cat_1012000 @LiberatedCit @jensan1332 @TheLastRefuge2 Funny! Trump started with $1M load THEN got &£20@M from DADDY when he died
I'm going to respectfully ask trump supporters if they are troubled by white supremacists openly supporting trump and he won't denounce.
@Nation_altile @TylerDaDragon people are hoping #3 is Donald trump.

Each line is a Tweet (election debate discussions)

STEVENS INSTITUTE of TECHNOLOGY | 12

 

 

Practical Example

This is Our code:

 

 

 

 

8 tweetfile = open("tweets.txt","r")

9
10 # An empty list that we will save our hashtags to when we find them
11 hashtags = [] 
12
13 # Loop through each tweet (line of file) 
14 for tweet in tweetfile:
15 +
16 # The split() method of a string splits it into a list
a7 # It splits at spaces by default 
18 words_list = tweet.split() 
19
20 # Loop through the words of the tweet 
21 for word in words_list: 
22 if word.startswith("#"):
23 hashtags. append (word)
24
25 # Print first 10 hashtags
26 for hashtag in hashtags[:10]:

| 27 print hashtag

 

 

 

 

EE
STEVENS INSTITUTE of TECHNOLOGY | 13

 

Skipping with continue lw

¢ We can conveniently
Skip a line by using
the continue
statement

fhand = open('mbox-short.txt’}
for line in fhand:
line = line. rstrip ()
# Skip ‘uninteresting lines’
if not line.startswith(‘From:’) :
continue
# Process our ‘interesting’ line
print (line)

STEVENS INSTITUTE of TECHNOLOGY | 14

 

Using in to select lines lu

We can look for a
string anywhere in
a line as our
selection criteria

fhand = open('mbox-short.txt')
for line in fhand:
line = line.rstrip()
if not '@uct.ac.za' in line :
continue
print (line)


From steohen.marquard@uct.ac.za Sat Jan 5 :16 2008
X-Authentication-Warning: set sender to steohen.marquard@uct.ac.za using —f

From: steohen.marquard@uct.dc.za
Author: steophen.marquard@uct.dac.za

STEVENS INSTITUTE of TECHNOLOGY | 15

 

 

split() Y

The split() method of a string can be very useful when reading files.
It will split a string into a list.
It will solit at every instance of the separator you fell it to. E.g.:

In [2]: print "Hello! My name is Frosty. I'm Cold! Who are you?".split("!")
['Hello', " My name is Frosty. I'm Cold", ' Who are you?']

The default separator is whitespace:

In [3]: print "Hello! My name is Frosty. I'm Cold! Who are you?".split()
['Hello!', 'My', 'name', ‘is', 'Frosty.', "I'm", 'Cold!', 'Who', ‘are', ‘you?']

STEVENS INSTITUTE of TECHNOLOGY | 16

split() and CSV Files Y

We know CSV files have data separated by commas:
names.csv
John, American
Juan,Mexican
Ioannis,Greek

Jean, French

We can split at the commas to get the information we want:


people_file = open("names.csv","r")

print "Nationalities:" >
for row in people_file: 
print row.split(",") [1]



Nationalities:
American
Mexican
Greek
French

But why the extra soaces between lines?

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 17

 

 

Newlines lw

¢ Each line in the .CSV file (in all text files) has an invisible newline character
at the end. In Python it’s written as “\n”.

¢ The strip() method of a string removes all whitespace (including newlines)
from each end of the string.

¢ If we add it to our code:

people_file = open("names.csv","r") 
print “Nationalities: . 
for row in people_life:
print row. strip().split(", ") [1] 

Nationalities:
American
Mexican
Greek
French

¢ The print statement starts a newline anyway so each output is on its own
line

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 18

 

Write to a File we

We can also use the open() function with the “w” option for writing:

our_file = open("newfile.txt","w") ec50e adios
our_file.write("The first line\n") The first line
our_file.write("\n") > .

our_file.write("The third line") Lil a la

our_file.close()

STEVENS INSTITUTE of TECHNOLOGY | 19

Some Tips &

¢ It's good practice to close the file once you're done using It
people_file = open("names.csv","r")
print "Nationalities:"

for line in people_file:
print line.strip().split(",") [1]

people_file.close()

¢ If you’re using a Mac and are opening CSV files from PCs, use “rU” rather
than just “r’ when opening (for Universal Newline). (It doesn’t hurt to do it
always)

people_file = open("“names.csv"|,"rU")|

print "Nationalities:"
for line in people_file:
print line.strip().split(",") [1]

people_file.close()

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 20

 

 

Useful Links le

hitp://www.pythontorbeginners.com/Tiles/reading-and-writing-files-in-python
hitps://docs.python.org/2.7/tutorial/inputoutput.Atml

STEVENS INSTITUTE of TECHNOLOGY | 21

 

Dictionaries and Tuples &

We have already seen and used lists

Remember that they are ordered collections and you can access elements
using their index
In [1]: alphabet = ['a','b','c','d']

In [2]: print alphabet [2]
Cc

Tuples and dictionaries are other collections that operate in different ways.

STEVENS INSTITUTE of TECHNOLOGY | 22

e @ 46 e@ Vb) oft
Lists are like “collection we

¢ A collection allows us to put many values in a single
“variable”

¢ In acollection we can carry all many values around
IN one convenient package

friends = [ ‘Joseph’, ‘Glenn’, ‘Sally’ ]

carryon = [ ‘socks’, ‘shirt’, ‘oerfume’ |

STEVENS INSTITUTE of TECHNOLOGY | 23

 

 

Looking Inside Lists ie

¢ Just like strings, we can get at any single element in
a list Using an Index specified in square brackets



STEVENS INSTITUTE of TECHNOLOGY | 24

Lists are Mutable

¢ Strings are “immutable” -

we cannot change the
contents of a string - we
must make ad new string to
make any change

Lists are "mutable" - we
can change an element
of a list using the index
operator

>>> fruit = ‘Banana’

>>> fruit[O] = 'b’
Traceback...TypeeError...
>>> x = fruit.lower()

>>> print (x)

banana

>>> print (fruit)

Banana

>>> lotto = [2, 14, 26, 41, 63]
>>> print (lotto)

[2, 14, 26, 41, 63]

>>> lotto[2] = 28

>>> print (lotto)

[2, 14, 28, 41, 63]

STEVENS INSTITUTE of TECHNOLOGY | 25

 

Getting the length of a List 

¢ The len() function takes a
list GS A paramefter and
returns the number of
elements in the list

¢ Actually len() tells us the
number of elements of any
set or sequence (i.e. such
as a string...)

>>> greet = ‘Hello Bob’
>>> print (len(greet))

9

>>>x=[1, 2, joe’, 99]

>>> print (len(x))
4

>>>

STEVENS INSTITUTE of TECHNOLOGY | 26

 

Using the range function ie

¢ The range function
returns a list of
numbers that range
from zero To one less
than the parameter

¢ Wecan construct an
index loop using for
and an integer
iterator

>>> print (range (4))

[O, 1, 2, 3]

>>> friends = ['Joseph’, 'Glenn’, ‘Sally’]
>>> print (len(friends))

>>> print (range (len(friends)))

[O, 1, 2]
>>>

STEVENS INSTITUTE of TECHNOLOGY | 27

 

Built in Functions and Lists lw

There are a number of
functions built into
Python that take lists as
parameters

Remember the loops we
built¢ These are much
simpler

http://docs.python.org/lib/built-in-funcs.hAtml|

af

>>> nums = [3, 41, 12, 9, 74, 15]
>>> print (len(nums))

>>> print (max(nums))
74
>>> print (min(nums))

>>> print (sum(nums)})

154

>>> print (sum(nums)/len(nums))
25

STEVENS INSTITUTE of TECHNOLOGY | 28

 

Multiple splitting S

¢ Sometimes we split a line one way and then grab

one of the pieces of the line and split that piece
again

From steohen.marquard@uct.ac.za Sat Jan 5 :16 2008

words = ine.split() 
email = words[1]

pieces = email.split('@') a']
print (pieces[1])



STEVENS INSTITUTE of TECHNOLOGY | 29

 

 

Tuples &

Tuples act very similarly to lists. The two main differences are:
They are defined with parentheses, ( and }), not square brackets.

They are immutable. This means you can't change the elements once you've
created the tuple.

my_tuple = (‘a’, ‘b’, ‘c’)

print (my_tuple[1])

b

my_tuple[1] = ‘x’

Traceback (most recent call last);
TypeError: ‘tuple’ object does not support item assignment

 

STEVENS INSTITUTE of TECHNOLOGY | 30

Tuple assignment le

Tuples allow to assign many variables at the same Time.

(a, b) = (27, 31)

Tuple assignment allows to swap values in variables without the use of a
temporary variable using:

(a, b) = (b, a)

instead of:
temp =a
a=b)b

b = temp

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 31

 

Dictionaries

Dictionaries are very different
They are unordered
They are made from key, value pairs

Each value in the dictionary has a key associated with it that can be used to look
up the value

Curly braces, { and }, are used and the key is followed by a colon and then the
value

populations = {“USA":318, “ltaly”":59, “Japan”:127}

STEVENS INSTITUTE of TECHNOLOGY | 32

 

 

Dictionaries 
You can add new key, value pairs simply by assigning a value to a new key
populations = {“USA”:318, “Italy”:59, “Japan”:127}
print (populations)
{“Japan”:127, “Italy”:59, “USA”:318}
Populations[“UK”] = 64
print (populations)
{“Japan”:127, “Italy”:59, “UK”:64, “USA”:318}
As with lists, the dictionaries values can be any type, even other dictionaries
or lists
The keys have to be immutable types (e.g., strings, ints, tuples – not lists)

STEVENS INSTITUTE of TECHNOLOGY | 33

 

ae at
Dictionaries we

You can add new key, value pairs simply by assigning a value to ad new key

populations = {“USA":318, “ltaly”:59, “Japan”:127}
print (populations)

{“Japan”:127, “Italy”:59, “USA”:318}
Populations[“UK”] = 64

print (populations)

{“Japan”:127, “ltaly’:59, “UK":64, “USA”:318}

As with lists, the dictionaries values can be any type, even other dictionaries
or lists

The keys have to be immutable types (e.g., strings, ints, tuples — not lists)

STEVENS INSTITUTE of TECHNOLOGY | 34

 

Definite Loops and Dictionaries S

¢« Even though dictionaries are not stored in order, we can write
a for loop that goes through all the entries in a dictionary -
actually if goes through all of the keys in the dictionary and
looks Up the values

>>> counts = { charlie’: 1 , ‘fred’ : 42, ‘jan’: 100}
>>> for key in counts:
print (key, counts[key])

jan 100
charlie 1

fred 42
>>>

STEVENS INSTITUTE of TECHNOLOGY | 35

Retrieving lists of Keys and Values S

¢ You can get d list of
keys, values or
items (both) from a
dictionary

>>> my_dict= {charlie ': 1, ‘fred’ : 42, ‘jan’: 100}
>>> print (list(my_dict))

[jan’, charlie ’, ‘fred’]

>>> print (my_dict.keys()}

[jan’, charlie ’, ‘fred’]

>>> print (my_dict.values())

[100, 1, 42]

>>> print (my_dict.items())

[(‘jan', 100), (charlie ', 1), (‘fred', 42)]

>>>

STEVENS INSTITUTE of TECHNOLOGY | 36

 

_ oft
A Dictionary Example &

We will make use of the "try... except” construction
This will try some code, and if an error occurs if will run the code listed under
except
y= 10

try:
print x > 

except:
print "There is no variable x"

There is no variable x

The code tries to print x but if doesn’t exist so it throws an error so it prints the
other statement

STEVENS INSTITUTE of TECHNOLOGY | 37

 

 

. ge cis
A Dictionary Example &

Say we have a text file and we want to count how many words there are

eee news_story.txt

In the minutes before an EgyptAir flight from Paris to Cairo crashed into the
Mediterranean Sea, killing all 66 people on board, there were rapid and escalating
failures in the plane’s flight control system, according to sensor data transmitted
by the aircraft to operators on the ground that was published Friday by a respected
aviation journal.

Ac awiatinon officiale cortead throinh the data which wac noacted nnline hv The

We will:
Read one line at a time
Split each line into words
Try to add | to the value of that word’s dictionary entry

If that fails, create a dictionary entry with value 1

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 38

a won

A Dictionary Example 

The code:

newsfile = open("news_story.txt","r")

word_counter = {}


for line in newsfile: 
words = line.strip().split() 
for word in words: 
try: 
word_counter[word] = word_counter[word] + 1 1
except: 
word_counter[word] = 1 


first_10 = word_counter.keys() [:10]

for key in first_10:
print key, word_counter [key]

STEVENS INSTITUTE of TECHNOLOGY | 39

 

Counter object 

The same code from the previous example can be rewritten using Counter(),
which is a particular type of dictionary used for counting

>>> # Tally occurrences of words ina list

>>> cnt = Counter()

>>> for word in ['red', ‘blue', ‘red', ‘green', ‘blue', ‘blue']:
cnt[word] += 1

>>> cnt

Counter({'blue': 3, ‘red': 2, ‘green': 1})

STEVENS INSTITUTE of TECHNOLOGY | 40

 

Counter object

newsfile = open("news_story.txt","r")
word_counter = {}

for line in newsfile:
words = line.strip().split()
for word in words:
try:
word_counter[word] = word_counter[word] + 1
except:
word_counter [word]



first_10 = word_counter.keys() [:10]

for key in first_10:
print key, word_counter [key]

newsfile = open(“news_story.txt”,”r”)
word_counter = Counter()
for line in newsfile:

words = line.strip().split()

for word in words:
word_counter[word] += 1

first_10 = word_counter.keys()[:10]

for key in first_10:
print (key, word_counter[key])

STEVENS INSTITUTE of TECHNOLOGY | 41

 

 

Useful Links we

hitp://learnpythonthehardway.org/book/ex39.html
hitp://www.tutorialsooint.com/python/python_dictionary.htm
hitps://en.wikibooks.org/wiki/Python_Programming/Tuples

hitp://stackoverflow.com/questions/348907 1 /in-python-when-to-use-a-
dictionary-list-or-set

STEVENS INSTITUTE of TECHNOLOGY | 42

Learn Python The Hard Way
Release 2.0

Zed A. Shaw

June 24, 2011


The Hard Way Is Easier

Reading and Writing ..................
Attentionto Detail ................0..
Spotting Differences ..............20.0.
Do Not Copy-Paste .................-.4.
A Note On Practice And Persistence ..........
LICENSE same se ew Hee EE EMME

Exercise 0: The Setup

Mae'OSX «se eeit it eee eet ct trameues
Windows .. 1... . ee ee
Linux 2... ee

Exercise 1: A Good First Program

What You ShouldSee..........0........
ExtraCredit .. 2... ee ee

Exercise 2: Comments And Pound Characters

What You ShouldSee..................
ExtraCredit .. 2... ee ee ee

Exercise 3: Numbers And Math

What You ShouldSee..................
Extra Credit... 2... 2. eee

Exercise 4: Variables And Names

What. You Should See: « - «se eestititaweues
ExtraCredit ... 2 ee ee

CONTENTS

aAnb BH WW Ww

17

Le ee 17
Le ee 17

19

Se fee Ghee ss ogame es S 20
Se 20

 

Exercise 5: More Variables And Printing

What You Should See... 2... 2... ee ee
Extra Credit 2. 2... ee

Exercise 6: Strings And Text

What You Should See... 2... ee
Extra Credit 2. 2. ee

Exercise 7: More Printing

What You Should See... 2... 2 ee
Extra Credit... ee

Exercise 8: Printing, Printing

What You.Should.See:. ..265 . 6 ga ow meee tc EM mH ee EE
Extra Credit... ee

Exercise 9: Printing, Printing, Printing

What You,Should.See «2 as::¢i@eUeH ete caw B RHE EES
Extra Credit 2... 0. 2 ee

Exercise 10: What Was That?

What You,SHGuIG.SGE::. os sis tte eee wee ERM BBR ee
BetreCGredt . 5. iw sane eS ts eh HEE MAH EE

Exercise 11: Asking Questions

What You Should See... 2... ee
Extra Credit : sc ae me eens tb ta ERM BE BHEE EEG

Exercise 12: Prompting People

What You Should See... 2... ee
Extfa Grédif. . cca mew mit tt REHM MMB wee ee

Exercise 13: Parameters, Unpacking, Variables

Hold Up! Features Have Another Name .................
What You Should See = «26: .: cee eee eet eee ew HE Eee
Extra Credit... see eae st ee eB HHEET EER BH HEE EES

Exercise 14: Prompting And Passing

What You Should See... 2... ee
Extfa Grédif. . cca mew mit tt REHM MMB wee ee

Exercise 15: Reading Files

What You Should See... 2... ee
Extra Credit... 0. eee

Exercise 16: Reading And Writing Files

What You Should See... 2... ee

23
24
24

25
26
26

27
27
28

29
29
29

31
31
32

33
34
34

35
36
36

37
34
38

 

Extra Credit... ee

Exercise 17: More Files

What’ You Should See: ; -«ese@8:itteeeeeenceeraewaee
PRAVCTEO se as km he EE EA lm ae OE wl me

Exercise 18: Names, Variables, Code, Functions

What: You Should Sé@. -. cee e uit tc RM EB HEE ERM B Be
Extra Credit «a ei3 st dM RBH EE MARES ws

Exercise 19: Functions And Variables

What You Should See... 2... ee
Extra@CHediE cw uees it kM MMH RRO BEEBE

Exercise 20: Functions And Files

What You Should See... 2... ee ee
Extra Credit... ee

Exercise 21: Functions Can Return Something

What You Should See... 2... ee et
Extra Credit... ee

Exercise 22: What Do You Know So Far?

What Youare Learning .............2. 002-002 eee eee

Exercise 23: Read Some Code

Exercise 24: More Practice

What You Should See... 2... 2.0... 2 ee ee ee et
Extra Credit... ee

Exercise 25: Even More Practice

What You Should See... 2... 2... ee ee
Extra Credit... ee

Exercise 26: Congratulations, Take A Test!

Exercise 27: Memorizing Logic

The Truth Terms... 2... 2 2, ee
The Truth Tables .. 2... 2... ee ee

Exercise 28: Boolean Practice

What You Should See... . 2. 2 2 ee ee
Extra Credit... ee

Exercise 29: What If

What You Should See... 2... 2.2... ee et

53
54
54

55
56

59
60
60

61
62
62

63
64
64

67
67

69

71
72
72

73
74
75

77

79
80
80

83
85
85

87
88

 

Extra Credit 2. 2 0 ee 88

Exercise 30: Else And If 89
What You,Should:See «sass: stage @MHA SE ERM R Hae ECR MH Bae Te REM 90
Extrared : 5s cms ee EAE EO mG Ewa mE Gk Ew wl 90

Exercise 31: Making Decisions 91
What You.SHould.Séé «eens st ctw ee HEE ERB BDH EER BBE HEE Ew 92
Extra Credit: cs cee eat CTA RBH EAR HAGE ERR BR Ae TC RRM 93

Exercise 32: Loops And Lists 95
What You Should See... 2... ee 96
Extfa Credit. . sce eee HE EE EME ERR MEH EEE EH HHS HEE EE aE 97

Exercise 33: While Loops 99
What You Should See... 2... ee 100
BXifa Cred. , sc cee mewte E MRAP Ee Re H  em ee ee 100

Exercise 34: Accessing Elements Of Lists 103
Extra Credit... 2. ee eee 104

Exercise 35: Branches and Functions 105
What You. Should.See...65:: tases kw s EA ee sd Ee ae 107
Extra Credit... 2. ee 107

Exercise 36: Designing and Debugging 109
Rules:For li-Statements 2 asi: sia e HHS TRAM HHT ERR BR Ae eC RRM 109
Rules For Loops... 2... 2. ee 110
Tips For Debugging ... 2... 2... eee 110
Homework... 2.2... ee 110

Exercise 37: Symbol Review 111
Keywords .. 2... ee 111
Data Types 2... eee 112
Stting Escapes Sequences... . 2s cee eee ett eee we ee ewe ee ew eS 113
String Formats; « see aguas te Ce RM HES TERR HEE ERE MRE RR 113
Operators 2.2... ee 114

Exercise 38: Reading Code 117
Extfa Credit. . sce eee HE EE EME ERR MEH EEE EH HHS HEE EE aE 117

Exercise 39: Doing Things To Lists 119
What You Should See... 2... ee 121
EXifa Gredi® . , cs cee meet EMRE APE eRe em ee we 121

Exercise 40: Dictionaries, Oh Lovely Dictionaries 123
What You Should See... 2... ee 125

 

ExtraCredit . 2... ee 125

Exercise 41: Gothons From Planet Percal #25 127
What You Should See: - sss e823: eee ¥HH EEE ERT HERE EE RHR Ee ae 132
Extpas@remit, 3 eae cc em EEE EMM EEG EA eG Em we GG 8 134

Exercise 42: Gothons Are Getting Classy 135
What You Shoild See... -esee8uiitt eee MERE RR RR HB ee 140
Extra Credit «22.22: se 888Gb EERE EHH REE RB RR EG ae 140

Exercise 43: You Make A Game 143

Exercise 44: Evaluating Your Game 145
POMEHOMSOVIE., cc 2 5 oi me SEEM EEO ae EE ee eS 145
Class Style. 2... ee 146
Code Style... 2. ee 146
Good Comments ... 2... . ce ee 147
Evaluate YourGame .: .e2 888322 t ade OH CHEM HEE ERM RMBES TS 147

Exercise 45: Is-A, Has-A, Objects, and Classes 149
How This Looks InCode .. 2... 0. 2. ee 150

Exercise 46: A Project Skeleton 153
Skeleton Contents: Linux/OSX .. 0... 0. ee 153
Testing YourSetup .... 2... 2.2... 2.2.2 2 ee ee 155
Usme The Skel6toh . .- cum eee ce eww Ee eR we ee we ee 155
Required QUIZ): e222 2 se eR BHE SEER BMH EET EMH HEE EMM BAe se 156

Exercise 47: Automated Testing 157
Writing A Test Case. 2... eee 157
Testine GuidGlNES .. stew ewe HH EEE ESHER B EHH EES 159
What You Should See). . cc ma ee tm Ee EEA md Em ee 159
Extra Credit . 2... ee 159

Exercise 48: Advanced User Input 161
OurGame Lexicon : : - see e8e3 ib eee RHEE ERT HH REE RH BREE ae 161
What You Should Test 2.2... . ee 163
Design Hints 2... 2... ee 165
Extra Credit 2... eee 165

Exercise 49: Making Sentences 167
Match And Peek... 2... ee 167
The Sentence Grammar... 1. 1 ee 168
A: Word'On EXceptionS « . see eet ce ieee ee ER ww eee mw ee 170
What.You Should Test . 2.225883 22 ta@@@OHs bce H HHH Ee ERM RMBES TS 170
Extra Credit . 2... ee 171

 

Exercise 50: Your First Website

Fixing Errors... 2 ee ee
Create Basic Templates... 2... ee
Extra Credit... 2. ee

Exercise 51: Getting Input From A Browser
How The Web Works .. 2... 2... 2 ee
How Forms Work .. 2... 2 ee
Creatine HTML, Fotms «222. seem eee ee we ee wee ew
Creating A Layout Template 2.2 cae eee ee ewe HEE HH BREE eee HS
Writing Automated Tests For Forms .............2. 0.000222 eee ee eee
ExtraCredit . 2...

Exercise 52: The Start Of Your Web Game
Refactoring The Exercise 42 Game... 0. 6 be me ee
Sessions And Tracking Users... 2... ee ee
Creating AnEngine.... 2... ee ee
Your FinalLbxam «2c eee ct eRe we ee wee ewe

Next Steps

Advice From An Old Programmer

173
173
174
175
175
176
178

179
179
181
182
184
185
188

189
189
194
195
198

199

201

 

vi

Learn Python The Hard Way, Release 2.0

 

Welcome to the 2nd Edition of Learn Python the hard way. You can visit the companion site to the book
at http://learnpythonthehardway.org/ where you can purchase digital downloads and paper versions of the
book. The free HTML version of the book is available at http://learnpythonthehardway.org/book/.

 

CONTENTS 1

Learn Python The Hard Way, Release 2.0

 

 

2 CONTENTS

The Hard Way Is Easier

This simple book is meant to get you started in programming. The title says it’s the hard way to learn to
write code; but it’s actually not. It’s only the “hard” way because it’s the way people used to teach things.
With the help of this book, you will do the incredibly simple things that all programmers need to do to
learn a language:

1. Go through each exercise.
2. Type in each sample exactly.
3. Make it run.

That’s it. This will be very difficult at first, but stick with it. If you go through this book, and do each
exercise for one or two hours a night, you will have a good foundation for moving onto another book.
You might not really learn “programming” from this book, but you will learn the foundation skills you
need to start learning the language.

This book’s job is to teach you the three most essential skills that a beginning programmer needs to know:
Reading and Writing, Attention to Detail, Spotting Differences.

Reading and Writing

It seems stupidly obvious, but, if you have a problem typing, you will have a problem learning to code.
Especially if you have a problem typing the fairly odd characters in source code. Without this simple
skill you will be unable to learn even the most basic things about how software works.

Typing the code samples and getting them to run will help you learn the names of the symbols, get
familiar with typing them, and get you reading the language.

Attention to Detail

The one skill that separates bad programmers from good programmers is attention to detail. In fact, it’s
what separates the good from the bad in any profession. Without paying attention to the tiniest details of

 

Learn Python The Hard Way, Release 2.0

 

your work, you will miss key elements of what you create. In programming, this is how you end up with
bugs and difficult-to-use systems.

By going through this book, and copying each example exactly, you will be training your brain to focus
on the details of what you are doing, as you are doing it.

Spotting Differences

A very important skill — that most programmers develop over time — is the ability to visually notice
differences between things. An experienced programmer can take two pieces of code that are slightly
different and immediately start pointing out the differences. Programmers have invented tools to make
this even easier, but we won’t be using any of these. You first have to train your brain the hard way, then
you can use the tools.

While you do these exercises, typing each one in, you will be making mistakes. It’s inevitable; even
seasoned programmers would make a few. Your job is to compare what you have written to what’s
required, and fix all the differences. By doing so, you will train yourself to notice mistakes, bugs, and
other problems.

Do Not Copy-Paste

You must type each of these exercises in, manually. If you copy and paste, you might as well just not even
do them. The point of these exercises is to train your hands, your brain, and your mind in how to read,
write, and see code. If you copy-paste, you are cheating yourself out of the effectiveness of the lessons.

A Note On Practice And Persistence

While you are studying programming, I’m studying how to play guitar. I practice it every day for at least
2 hours a day. I play scales, chords, and arpeggios for an hour at least and then learn music theory, ear
training, songs and anything else I can. Some days I study guitar and music for 8 hours because I feel
like it and it’s fun. To me repetitive practice is natural and just how to learn something. I know that to get
good at anything you have to practice every day, even if I suck that day (which is often) or it’s difficult.
Keep trying and eventually itll be easier and fun.

As you study this book, and continue with programming, remember that anything worth doing is difficult
at first. Maybe you are the kind of person who is afraid of failure so you give up at the first sign of
difficulty. Maybe you never learned self-discipline so you can’t do anything that’s “boring”. Maybe you
were told that you are “gifted” so you never attempt anything that might make you seem stupid or not
a prodigy. Maybe you are competitive and unfairly compare yourself to someone like me who’s been
programming for 20+ years.

 

4 The Hard Way Is Easier

Learn Python The Hard Way, Release 2.0

 

Whatever your reason for wanting to quit, keep at it. Force yourself. If you run into an Extra Credit you
can’t do, or a lesson you just do not understand, then skip it and come back to it later. Just keep going
because with programming there’s this very odd thing that happens.

At first, you will not understand anything. It’ll be weird, just like with learning any human language.
You will struggle with words, and not know what symbols are what, and it’ll all be very confusing. Then
one day BANG your brain will snap and you will suddenly “get it”. If you keep doing the exercises and
keep trying to understand them, you will get it. You might not be a master coder, but you will at least
understand how programming works.

If you give up, you won’t ever reach this point. You will hit the first confusing thing (which is everything
at first) and then stop. If you keep trying, keep typing it in, trying to understand it and reading about it,
you will eventually get it.

But, if you go through this whole book, and you still do not understand how to code, at least you gave it
a shot. You can say you tried your best and a little more and it didn’t work out, but at least you tried. You
can be proud of that.

License

This book is Copyright (C) 2010 by Zed A. Shaw. You are free to distribute this book to anyone you
want, so long as you do not charge anything for it, and it is not altered. You must give away the book in
its entirety, or not at all. This means it’s alright for you to teach a class using the book, so long as you
aren’t charging students for the book and you give them the whole book unmodified.

Special Thanks

Id like to thank a few people who helped with this edition of the book. First is my editor at Pretty Girl
Editing Services who help me edit the book and is just lovely all by herself. Then there’s Greg Newman,
who did the cover jacket and artwork, plus reviewed copies of the book. His artwork made the book look
like a real book, and didn’t mind that I totally forgot to give him credit in the first edition. I’d also like to
thank Brian Shumate for doing the website landing page and other site design help, which I need a lot of
help on.

Finally, I'd like to thank the hundreds of thousands of people who read the first edition and especially the
ones who submitted bug reports and comments to improve the book. It really made this edition solid and
I couldn’t have done it without all of you. Thank you.

 

License 5

Learn Python The Hard Way, Release 2.0

 

 

6 The Hard Way Is Easier

Exercise 0: The Setup

This exercise has no code. It is simply the exercise you complete to get your computer setup to run
Python. You should follow these instructions as exactly as possible. For example, Mac OSX computers
already have Python 2, so do not install Python 3 (or any Python).

Mac OSX

To complete this exercise, complete the following tasks:

1.

2.

NH nn ff W

~

Go to http://learnpythonthehardway.org/exerciseO.html with your browser, get the gedit text ed-
itor, and install it.

Put gedit (your editor) in your Dock so you can reach it easily.

(a) Run gedit so we can fix some stupid defaults it has.

 

(b) Open Preferences from the gedit menu and select the Editor tab.

(c) Change Tab width: to4.

(d) Select (make sure a check mark isin) Insert spaces instead of tabs.
(e) Turn on “Automatic indentation” as well.

(f) Open the View tab and turn on “Display line numbers”.

. Find your “Terminal” program. Search for it. You will find it.
. Put your Terminal in your Dock as well.
. Run your Terminal program. It won’t look like much.

. In your Terminal program, run python. You run things in Terminal by just typing their name and

hitting RETURN.

. Hit CTRL-D (4D) and get out of python.

. You should be back at a prompt similar to what you had before you typed python. If not find out

why.

 

Learn Python The Hard Way, Release 2.0

 

9. Learn how to make a directory in the Terminal. Search online for help.
10. Learn how to change into a directory in the Terminal. Again search online.

11. Use your editor to create a file in this directory. You will make the file, “Save” or “Save As...”, and
pick this directory.

12. Go back to Terminal using just the keyboard to switch windows. Look it up if you can’t figure it
out.

13. Back in Terminal, see if you can list the directory to see your newly created file. Search online for
how to list a directory.

 

Note: If you have problems with gedit, which is possible with non-English keyboard layouts, then I
suggest you try Textwrangler found at http://www.barebones.com/products/textwrangler/ instead.

OSX: What You Should See

Here’s me doing the above on my computer in Terminal. Your computer would be different, so see if you
can figure out all the differences between what I did and what you should do.

Last login: Sat Apr 24 :54 on ttys001

~ S$ python

Python 2.5.1 (r2863, Feb 6 2009, :12)
[GCC 4.0.1 (Apple Inc. build 5465)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> *D

~ § mkdir mystuff

~ S$ cd mystuff

mystuff $ ls

# ... Use Gedit here to edit test.txt....
mystuff $ ls

bests Cxt

mystuff $

 

Windows

Note: Contributed by zhmark.

 

1. Go to http://learnpythonthehardway.org/wiki/ExerciseZero with your browser, get the gedit text
editor, and install it. You do not need to be administrator to do this.

2. Make sure you can get to gedit easily by putting it on your desktop and/or in Quick Launch.
Both options are available during setup.

 

8 Exercise 0: The Setup

Learn Python The Hard Way, Release 2.0

 

NH nn fS W

12.

13.

(a) Run gedit so we can fix some stupid defaults it has.

 

 

(b) Open Edit-—>Preferences select the Editor tab.

(c) Change Tab width: to4.

(d) Select (make sure a check mark isin) Insert spaces instead of tabs.
(e) Turn on “Automatic indentation” as well.

(f) Open the View tab turn on “Display line numbers”.

. Find your “Terminal” program. It’s called Command Prompt. Alternatively just run cmd.
. Make a shortcut to it on your desktop and/or Quick Launch for your convenience.
. Run your Terminal program. It won’t look like much.

. In your Terminal program, run python. You run things in Terminal by just typing their name and

hitting RETURN.

(a) If yourun python andit’s not there(python is not recognized. .). Install it from
http://python.org/download

(b) Make sure you install Python 2 not Python 3.

(c) You may be better off with ActiveState Python especially when you miss Administrative
rights

 

. Hit CTRL-Z (4Z), Enter and get out of python.

. You should be back at a prompt similar to what you had before you typed python. If not find out

why.

. Learn how to make a directory in the Terminal. Search online for help.
10.
11.

Learn how to change into a directory in the Terminal. Again search online.

Use your editor to create a file in this directory. Make the file, “Save” or “Save As...”, and pick this
directory.

Go back to Terminal using just the keyboard to switch windows. Look it up if you can’t figure it
out.

Back in Terminal, see if you can list the directory to see your newly created file. Search online for
how to list a directory.

Warning: Windows is a big problem for Python. Sometimes you install Python and one com-

puter will have no problems, and another computer will be missing important features. If you have
problems, please visit: http://docs.python.org/faq/windows.html

 

 

Windows 9

Learn Python The Hard Way, Release 2.0

 

Windows: What You Should See

C:\Documents and Settings\you>python

ActivePython 2.6.5.12 (ActiveState Software Inc.) based on

Python 2.6.5 (r2063, Mar 20 2010, :52) [MSC v.1500 32 bit (Intel)] on win32
Type "help", "copyright", "credits" or "license" for more information.

Soe “2

C:\Documents and Settings\you>mkdir mystuff
C:\Documents and Settings\you>cd mystuff
Here you would use gedit to make test.txt in mystuff
C:\Documents and Settings\you\mystuff>
<bunch of unimportant errors if you istalled it as non-admin - ignore them - hit Enter>
C:\Documents and Settings\you\mystuff>dir

Volume in drive C is
Volume Serial Number is 085C-7E02

 

 

Directory of C:\Documents and Settings\you\mystuff

04.05.2010  <DIR>
04.05.2010  <DIR> a
04.05.2010  6 test.txt
1 File(s) 6 bytes
2 Dir(s) 14 804 623 360 bytes free

C:\Documents and Settings\you\mystuff>

You will probably see a very different prompt, Python information, and other stuff but this is the general
idea. If your system is different let us know at http://learnpythonthehardway.org and we’ Il fix it.

Linux

Linux is a varied operating system with a bunch of different ways to install software. I’m assuming if
you are running Linux then you know how to install packages so here are your instructions:

1. Go to http://learnpythonthehardway.org/wiki/ExerciseZero with your browser, get the gedit text
editor, and install it.

2. Make sure you can get to gedit easily by putting it in your window manager’s menu.
(a) Run gedit so we can fix some stupid defaults it has.

(b) Open Preferences select the Editor tab.

 

10 Exercise 0: The Setup

Learn Python The Hard Way, Release 2.0

 

(c) Change Tab width: to4.
(d) Select (make sure a check mark isin) Insert spaces instead of tabs.
(e) Turn on “Automatic indentation” as well.

(f) Open the View tab turn on “Display line numbers”.

 

. Find your “Terminal” program. It could be called GNOME Terminal, Konsole, or xterm.
. Put your Terminal in your Dock as well.

. Run your Terminal program. It won’t look like much.

nH On HR W

. In your Terminal program, run python. You run things in Terminal by just typing their name and
hitting RETURN.

(a) If you run python and it’s not there, install it. Make sure you install Python 2 not Python 3.
7. Hit CTRL-D (‘D) and get out of python.

8. You should be back at a prompt similar to what you had before you typed python. If not find out
why.

9. Learn how to make a directory in the Terminal. Search online for help.
10. Learn how to change into a directory in the Terminal. Again search online.

11. Use your editor to create a file in this directory. Typically you will make the file, “Save” or “Save
As..”, and pick this directory.

12. Go back to Terminal using just the keyboard to switch windows. Look it up if you can’t figure it
out.

13. Back in Terminal see if you can list the directory to see your newly created file. Search online for
how to list a directory.

Linux: What You Should See

S python

Python 2.6.5 (r2063, Apr 1 2010, :39)

[GCC 4.4.3 20100316 (prerelease) ]J on linux2

Type "help", "copyright", "credits" or "license" for more information.
>>>

S$ mkdir mystuff

S cd mystuff

 

# ... Use gedit here to edit test.txt ...
S is

test .0xC

$

You will probably see a very different prompt, Python information, and other stuff but this is the general
idea.

 

Linux 11

Learn Python The Hard Way, Release 2.0

 

Warnings For Beginners

You are done with this exercise. This exercise might be hard for you depending on your familiarity with
your computer. If it is difficult, take the time to read and study and get through it, because until you can
do these very basic things you will find it difficult to get much programming done.

If a programmer tells you to use vim or emacs, tell them, “No.” These editors are for when you are
a better programmer. All you need right now is an editor that lets you put text into a file. We will use
gedit because it is simple and the same on all computers. Professional programmers use gedit so it’s
good enough for you starting out.

A programmer may try to get you to install Python 3 and learn that. You should tell them, “When all of
the python code on your computer is Python 3, then I'll try to learn it.’ That should keep them busy for
about 10 years.

A programmer will eventually tell you to use Mac OSX or Linux. If the programmer likes fonts and
typography, they’ll tell you to get a Mac OSX computer. If they like control and have a huge beard,
they’ll tell you to install Linux. Again, use whatever computer you have right now that works. All you
need is gedit, a Terminal, and python.

Finally the purpose of this setup is so you can do three things very reliably while you work on the
exercises:

1. Write exercises using gedit.
2. Run the exercises you wrote.

3. Fix them when they are broken.
4. Repeat.

Anything else will only confuse you, so stick to the plan.

 

12 Exercise 0: The Setup

YA QA WwW Fw HY

Exercise 1: A Good First Program

Remember, you should have spent a good amount of time in Exercise 0 learning how to install a text
editor, run the text editor, run the Terminal, and work with both of them. If you haven’t done that then do
not go on. You will not have a good time. This is the only time I’ll start an exercise with a warning that
you should not skip or get ahead of yourself.

print "Hello World!"

print "Hello Again"

print "I Jake typing this,"

Print "This is fun."

print “Sey! Printing:

print "I'd much rather you 'not'."
print 'I "said" do not touch this.!

Type the above into a single file named ex1. py. This is important as python works best with files ending
in .py.

Warning: Do not type the numbers on the far left of these lines. Those are called “line numbers”

and they are used by programmers to talk about what part of a program is wrong. Python will tell you
errors related to these line numbers, but you do not type them in.

 

Then in Terminal run the file by typing:

python exl.py

If you did it right then you should see the same output I have below. If not, you have done something
wrong. No, the computer is not wrong.

What You Should See

S python exl.py
Hello World!

Hello Again

I like typing this.

 

13

Ww Fk Bw NY

Learn Python The Hard Way, Release 2.0

 

This is fun.

Yay! Printing.

I'd much rather you ‘not’.
I "said" do not touch this.

$
You may see the name of your directory before the $ which is fine, but if your output is not exactly the
same, find out why and fix it.

If you have an error it will look like this:

S python ex/exl.py
File "ex/exl.py", line 3
print "I like typing this.

A

SyntaxError: EOL while scanning string literal
It’s important that you can read these since you will be making many of these mistakes. Even I make
many of these mistakes. Let’s look at this line-by-line.

1. Here we ran our command in the terminal to run the ex1. py script.

2. Python then tells us that the file ex1.py has an error on line 3.
3. It then prints this line for us.
4

. Then it puts a * (caret) character to point at where the problem is. Notice the missing " (double-
quote) character?

5. Finally, it prints out a “SyntaxError” and tells us something about what might be the error. Usually
these are very cryptic, but if you copy that text into a search engine, you will find someone else
who’s had that error and you can probably figure out how to fix it.

Extra Credit

You will also have Extra Credit. The Extra Credit contains things you should try to do. If you can’t,
skip it and come back later.

 

For this exercise, try these things:
1. Make your script print another line.
2. Make your script print only one of the lines.

3. Put a ‘#’ (octothorpe) character at the beginning of a line. What did it do? Try to find out what this
character does.

From now on, I won’t explain how each exercise works unless an exercise is different.

 

 

14 Exercise 1: A Good First Program

Learn Python The Hard Way, Release 2.0

 

Note: An ‘octothorpe’ is also called a ‘pound’, ‘hash’, ‘mesh’, or any number of names. Pick the one
that makes you chill out.

 

 

Extra Credit 15

Learn Python The Hard Way, Release 2.0

 

 

16 Exercise 1: A Good First Program

Exercise 2: Comments And Pound
Characters

Comments are very important in your programs. They are used to tell you what something does in
English, and they also are used to disable parts of your program if you need to remove them temporarily.
Here’s how you use comments in Python:

# A comment, this is so you can read your program later.
# Anything after the # is ignored by python.

print "I could have code like this." # and the comment after is ignored

# You can also use a comment to "disable" or comment out a piece of code:
# print "This won't run."

print "This Wii 2iti«"

What You Should See

S python ex2.py
I could have code like this.
This will run.

$

Extra Credit

1. Find out if you were right about what the # character does and make sure you know what it’s called
(octothorpe or pound character).

2. Take your ex2 . py file and review each line going backwards. Start at the last line, and check each
word in reverse against what you should have typed.

 

17

Learn Python The Hard Way, Release 2.0

 

3. Did you find more mistakes? Fix them.

4. Read what you typed above out loud, including saying each character by its name. Did you find
more mistakes? Fix them.

 

18 Exercise 2: Comments And Pound Characters

Exercise 3: Numbers And Math

Every programming language has some kind of way of doing numbers and math. Do not worry, program-
mers lie frequently about being math geniuses when they really aren’t. If they were math geniuses, they
would be doing math, not writing ads and social network games to steal people’s money.

This exercise has lots of math symbols. Let’s name them right away so you know what they are called.
As you type this one in, say the names. When saying them feels boring you can stop saying them. Here
are the names:

¢ + plus

* — minus

e / slash

¢ « asterisk

° % percent

e < less-than

* > greater-than

¢ <= less-than-equal

* >= greater-than-equal

Notice how the operations are missing? After you type in the code for this exercise, go back and figure
out what each of these does and complete the table. For example, + does addition.

print "I will now count my chickens:"

print "Hens", 25 + 30 / 6
print "Roosters", 100 -25*3%4

print "Now I will count the eggs:"
print 342¢1-5+4%2-1/4+6

print "Is it true that 3 + 2< 5 = 72"

 

19

20

21

22

23

Learn Python The Hard Way, Release 2.0

 

print 3+4+2< 52 7

prant "What ts 34+ 27", 3 4 2
print "Wheat is 5- 72", 5=7

print "Oh, that"s why it's False."
print "How about some more."

print "Is it greater?", 5 > -2
print "Is it greater or equal?", 5 >= -2
print "Is it less or equal?", 5 <= -2

What You Should See

S python ex3.py

I will now count my chickens:
Hens 30

Roosters 97

Now I will count the eggs:

7

Is it true that 3 +2 <5 - 7?
False

What is 3 + 2? 5

What is 5 — 7? -2

Oh, that's why it's False.
How about some more.

Is it greater? True

Is it greater or equal? True
Is it less or equal? False

$

Extra Credit

1. Above each line, use the # to write a comment to yourself explaining what the line does.

2. Remember in Exercise 0 when you started python? Start python this way again and using the above
characters and what you know, use python as a calculator.

3. Find something you need to calculate and write a new . py file that does it.

4. Notice the math seems “wrong”? There are no fractions, only whole numbers. Find out why by
researching what a “floating point” number is.

5. Rewrite ex3.py to use floating point numbers so it’s more accurate (hint: 20.0 is floating point).

 

20 Exercise 3: Numbers And Math

Exercise 4: Variables And Names

Now you can print things with print and you can do math. The next step is to learn about variables.
In programming a variable is nothing more than a name for something so you can use the name rather
than the something as you code. Programmers use these variable names to make their code read more
like English, and because they have lousy memories. If they didn’t use good names for things in their
software, they’d get lost when they tried to read their code again.

If you get stuck with this exercise, remember the tricks you have been taught so far of finding differences
and focusing on details:

1. Write a comment above each line explaining to yourself what it does in English.
2. Read your . py file backwards.

3. Read your . py file out loud saying even the characters.

cars = 100

space_in_a_car = 4.0

drivers = 30

passengers = 90

cars_not_driven = cars - drivers
cars_driven = drivers

carpool_capacity = cars_driven * space_in_a_car
average_passengers_per_car = passengers / cars_driven

print "There are", cars, "cars available."

print "There are only", drivers, “drivers available."

print "There will be", cars_not_driven, "empty cars today."

print "We can transport", carpool_capacity, "people today."

print "We have", passengers, "to carpool today."

print "We need to put about", average_passengers_per_car, "in each car."

 

 

Note: The _ in space_in_a_car is called an underscore character. Find out how to type
it if you do not already know. We use this character a lot to put an imaginary space between words in
variable names.

 

 

21

Learn Python The Hard Way, Release 2.0

 

What You Should See

S python ex4.py
There are 100 cars available.

There are only 30 drivers available.

There will be 70 empty cars today.

We can transport 120.0 people today.

We have 90 to carpool today.
We need to put about 3 in each car.

$

Extra Credit

When I wrote this program the first time I had a mistake, and python told me about it like this:

Traceback (most recent call last):

File "ex4.py", line 8, in <module>

 

average_passengers_per_car = car_pool_capacity / passenger

 

NameError: name 'car_pool_capacity'

is not defined

Explain this error in your own words. Make sure you use line numbers and explain why.

Here’s more extra credit:

1. Explain why the 4.0 is used instead of just 4.

. Remember that 4.0 is a “floating point” number. Find out what that means.

. Write comments above each of the variable assignments.

. Remember _ is an underscore character.

2
3
4. Make sure you know what = is called (equals) and that it’s making names for things.
5
6

. Try running python as a calculator like you did before and use variable names to do your calcu-
lations. Popular variable names are also i, x, and 3.

 

22

Exercise 4: Variables And Names

Exercise 5: More Variables And
Printing

Now we’ll do even more typing of variables and printing them out. This time we’ll use something called
a “format string”. Every time you put " (double-quotes) around a piece of text you have been making a
string. A string is how you make something that your program might give to a human. You print them,
save them to files, send them to web servers, all sorts of things.

Strings are really handy, so in this exercise you will learn how to make strings that have variables embed-
ded in them. You embed variables inside a string by using specialized format sequences and then putting
the variables at the end with a special syntax that tells Python, “Hey, this is a format string, put these
variables in there.”

As usual, just type this in even if you do not understand it and make it exactly the same.

my_name = 'Zed A. Shaw'
my_age = 35 # not a lie
my_height = 74 # inches
my_weight = 180 # lbs

my_eyes = 'Blue'

my_teeth = 'White'

my_hair = 'Brown'

print "Let's talk about %s." % my_name

print "He's %@d inches tall." % my_height

print "He's %d pounds heavy." % my_weight

print "Actually that's not too heavy."

print "He's got %s eyes and %s hair." % (my_eyes, my_hair)

print "His teeth are usually %*s depending on the coffee." % my_teeth

# this line is tricky, try to get it exactly right
print "If I add %d, %d, and %d I get %d." & (
my_age, my_height, my_weight, my_age + my_height + my_weight)

 

23

Learn Python The Hard Way, Release 2.0

 

What You Should See

S python ex5.py

Let's talk about Zed A. Shaw.

He's 74 inches tall.

He's 180 pounds heavy.

Actually that's not too heavy.

He's got Blue eyes and Brown hair.

His teeth are usually White depending on the coffee.
If I add 35, 74, and 180 I get 289.

$

Extra Credit

1. Change all the variables so there isn’t the my_ in front. Make sure you change the name every-
where, not just where you used = to set them.

2. Try more format characters. Sr is a very useful one. It’s like saying “print this no matter what”.
3. Search online for all of the Python format characters.

4. Try to write some variables that convert the inches and pounds to centimeters and kilos. Do not
just type in the measurements. Work out the math in Python.

 

24 Exercise 5: More Variables And Printing

Exercise 6: Strings And Text

While you have already been writing strings, you still do not know what they do. In this exercise we
create a bunch of variables with complex strings so you can see what they are for. First an explanation of
strings.

A string is usually a bit of text you want to display to someone, or “export” out of the program you are
writing. Python knows you want something to be a string when you put either " (double-quotes) or ’
(single-quotes) around the text. You saw this many times with your use of print when you put the text
you want to go to the string inside " or ’ after the print. Then Python prints it.

Strings may contain the format characters you have discovered so far. You simply put the formatted
variables in the string, and then a % (percent) character, followed by the variable. The only catch is that
if you want multiple formats in your string to print multiple variables, you need to put them inside (_ )
(parenthesis) separated by , (commas). It’s as if you were telling me to buy you a list of items from the
store and you said, “I want milk, eggs, bread, and soup.” Only as a programmer we say, “(milk, eggs,
bread, soup)”.

We will now type in a whole bunch of strings, variables, formats, and print them. You will also practice
using short abbreviated variable names. Programmers love saving themselves time at your expense by
using annoying cryptic variable names, so let’s get you started being able to read and write them early
on.

x = "There are %d types of people." % 10

binary = "binary"

do_not = "don't"

y = "Those who know %s and those who %s." % (binary, do_not)
print x

print y

print "I said? 4r." % xX

print "I also said: ‘"ss"." & y
hilarious = False
joke_evaluation = "Isn't that joke so funny?! Sr"

print joke_evaluation % hilarious

 

25

Learn Python The Hard Way, Release 2.0

 

"THiS 18 the Jefe Side of..."
e = "a string with a right side."

=
u

print wte

What You Should See

S python ex6.py
There are 10 types of people.

Those who know binary and those who don't.
I said: 'There are 10 types of people.'.

I also said: 'Those who know binary and those who don't.'.

Isn't that joke so funny?! False

This is the left side of...a string with a right side.

$

Extra Credit

1. Go through this program and write a comment above each line explaining it.

2. Find all the places where a string is put inside a string. There are four places.

3. Are you sure there’s only four places? How do you know? Maybe I like lying.

4. Explain why adding the two string w and e with + makes a longer string.

 

26

Exercise 6: Strings And Text

Exercise 7: More Printing

Now we are going to do a bunch of exercises where you just type code in and make it run. I won’t be
explaining much since it is just more of the same. The purpose is to build up your chops. See you in a
few exercises, and do not skip! Do not paste!

print "Mary had a little lamb."
print "Its fleece was white as %s."

print "And everywhere that Mary went."

print “.

endl = "
end2 = "
end3 =
end4 =
end5 = "
end6 =
end7 = "
end8g =
endg9 = "

# watch that comma at the end.
nd2 + end3 + end4 +

c"
h"

Won
Won

s"

Won

BU"

wy"

rr

x 10

mg"
Won
wr

# what'd that do?

%

nd5

"snow'

try removing it to see what happens
+ end6,

 

print endl +

print end7 + end8 + end9 + end10 + endll + endl2

What You Should See

S python

Mary had a little lamb.
Its fleece was white as snow.
And everywhere that Mary went.

 

27

Learn Python The Hard Way, Release 2.0

 

Cheese Burger

$

Extra Credit

For these next few exercises, you will have the exact same extra credit.
1. Go back through and write a comment on what each line does.
2. Read each one backwards or out loud to find your errors.

3. From now on, when you make mistakes write down on a piece of paper what kind of mistake you
made.

4. When you go to the next exercise, look at the last mistakes you made and try not to make them in
this new one.

5. Remember that everyone makes mistakes. Programmers are like magicians who like everyone to
think they are perfect and never wrong, but it’s all an act. They make mistakes all the time.

 

28 Exercise 7: More Printing

Exercise 8: Printing, Printing

formatter = "Sr Sr Sr Sr"

print formatter % (1, 2, 3, 4)
print formatter % ("one", "two", "three", "four")
print formatter % (True, False, False, True)
print formatter % (formatter, formatter, formatter, formatter)
print formatter % (
WE Hed, Lats tiie .",
"That you could type up right.",
"But dt didn't sing.",
"So I said goodnight."

What You Should See

S python ex8.py

1234

'one' 'two' 'three' 'four'

True False False True

"Sr Sr Sr br' 'Sr Sr br @r' 'Sr Sr br Sr' 'Sr Br Br Sr'

'I had this thing.' 'That you could type up right.' "But it didn't sing."
$

Extra Credit

'So I said goodnight.'

1. Do your checks of your work, write down your mistakes, try not to make them on the next exercise.

2. Notice that the last line of output uses both single and double quotes for individual pieces. Why do

you think that is?

 

29

Learn Python The Hard Way, Release 2.0

 

 

30 Exercise 8: Printing, Printing

Exercise 9: Printing, Printing, Printing

# Here's some new strange stuff, remember type it exactly.

days = "Mon Tue Wed Thu Fri Sat Sun"

months = "Jan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug"
print "Here are the days: ", days

print "Here are the months: ", months

print wow

There's something going on here.
With the three double-quotes.
We'll be able to type as much as we like.

Even 4 lines if we want, or 5, or 6.
wee

What You Should See

S python ex9.py

Here are the days: Mon Tue Wed Thu Fri Sat Sun
Here are the months: Jan

Feb

Mar

Apr

May

Jun

Jul

Aug

There's something going on here.

With the three double-quotes.

We'll be able to type as much as we like.
Even 4 lines if we want, or 5, or 6.

 

31

Learn Python The Hard Way, Release 2.0

 

Extra Credit

1. Do your checks of your work, write down your mistakes, try not to make them on the next exercise.

 

32 Exercise 9: Printing, Printing, Printing

Exercise 10: What Was That?

In Exercise 9 I threw you some new stuff, just to keep you on your toes. I showed you two ways to make
a string that goes across multiple lines. In the first way, I put the characters \n (back-slash n) between
the names of the months. What these two characters do is putanew line character into the string
at that point.

This use of the \ (back-slash) character is a way we can put difficult-to-type characters into a string.
There are plenty of these “escape sequences” available for different characters you might want to put
in, but there’s a special one, the double back-slash which is just two of them \\. These two
characters will print just one back-slash. We’ll try a few of these sequences so you can see what I mean.

Another important escape sequence is to escape a single-quote ’ or double-quote ". Imagine you
have a string that uses double-quotes and you want to put a double-quote in for the output. If you
do this "I "understand" joe." then Python will get confused since it will think the " around
"understand" actually ends the string. You need a way to tell Python that the " inside the string isn’t
a real double-quote.

To solve this problem you escape double-quotes and single-quotes so Python knows to include in the
string. Here’s an example:

"T am 6'2\" tall." # escape double-quote inside string

'T am 6\'2" tall.' # escape single-quote inside string

The second way is by using triple-quotes, which is just """ and works like a string, but you also can put
as many lines of text you as want until you type """ again. We’ll also play with these.

tabby_cat = "\tI'm tabbed in."

persian_cat = "I'm split\non a line."

backslash_cat = "I'm \\ a \\ cat."

fat_cat = ww

i) dea disty

\t* Cat food

\t* Fishies

\t* Catnip\n\t* Grass

mew

 

33

Learn Python The Hard Way, Release 2.0

 

print tabby_cat
print persian_cat
print backslash_cat
print fat_cat

What You Should See

Look for the tab characters that you made. In this exercise the spacing is important to get right.

S python exl10.py
I'm tabbed in.
I'm split
on a line.
I'm \ a \ cat.

i'ii do a lists
Cat food
Fishies
Catnip
Grass

+ + %

Extra Credit

1. Search online to see what other escape sequences are available.
. Use ”’ (triple-single-quote) instead. Can you see why you might use that instead of """?

. Combine escape sequences and format strings to create a more complex format.

-F wYW NY

Remember the %r format? Combine %r with double-quote and single-quote escapes and print
them out. Compare %r with %s. Notice how %r prints it the way you’d write it in your file, but %s
prints it the way you'd like to see it?

 

34 Exercise 10: What Was That?

Exercise 11: Asking Questions

Now it is time to pick up the pace. I have got you doing a lot of printing so that you get used to typing
simple things, but those simple things are fairly boring. What we want to do now is get data into your
programs. This is a little tricky because you have learn to do two things that may not make sense right
away, but trust me and do it anyway. It will make sense in a few exercises.

Most of what software does is the following:
1. Take some kind of input from a person.
2. Change it.
3. Print out something to show how it changed.

So far you have only been printing, but you haven’t been able to get any input from a person, or change
it. You may not even know what “input” means, so rather than talk about it, let’s have you do some and
see if you get it. Next exercise we’ll do more to explain it.

print "How old are you?",

age = raw_input ()

print "How tall are you?",
height = raw_input ()

print "How much do you weigh?",
weight = raw_input ()

print "So, you're %r old, %r tall and %r heavy." & (
age, height, weight)

 

Note: Notice that we put a , (comma) at the end of each print line. This is so that print doesn’t
end the line with a newline and go to the next line.

 

35

Learn Python The Hard Way, Release 2.0

 

What You Should See

S python exll.py

How old are you? 35

How tall are you? 6'2"

How much do you weigh? 1801bs

So, you're '35' old, '6\'2"' tall and '180lbs' heavy.
$

Extra Credit

1. Go online and find out what Python’s raw_input does.
. Can you find other ways to use it? Try some of the samples you find.

. Write another “form” like this to ask some other questions.

KR WwW WN

. Related to escape sequences, try to find out why the last line has ’ 6\’ 2"’ with that \’ sequence.
See how the single-quote needs to be escaped because otherwise it would end the string?

 

36 Exercise 11: Asking Questions

Exercise 12: Prompting People

When you typed raw_input() you were typing the ( and ) characters which are parenthesis.
This is similar to when you used them to do a format with extra variables, asin"Ss %Ss" % (x, y).
For raw_input you can also put in a prompt to show to a person so they know what to type. Put a
string that you want for the prompt inside the () so that it looks like this:

y = raw_input ("Name? ")
This prompts the user with “Name?” and puts the result into the variable y. This is how you ask someone
a question and get their answer.

This means we can completely rewrite our previous exercise using just raw_input to do all the prompt-
ing.

age = raw_input ("How old are you? ")
height = raw_input ("How tall are you? ")
weight = raw_input ("How much do you weigh? ")

print "So, you're %r old, %r tall and %r heavy." & (
age, height, weight)

What You Should See

S python exl2.py

How old are you? 35

How many tall are you? 6'2"

How much do you weight? 180l1bs

So, you're '35' old, '6\'2"' tall and '180lbs' heavy.
$

 

37

Learn Python The Hard Way, Release 2.0

 

Extra Credit

1. In Terminal where you normally run python to run your scripts, type: pydoc raw_input.
Read what it says.

2. Get out of pydoc by typing q to quit.
3. Go look online for what the pydoc command does.

4. Use pydoc to also read about open, file, os, and sys. It’s alright if you do not understand
those, just read through and take notes about interesting things.

 

38 Exercise 12: Prompting People

a oe a

Exercise 13: Parameters, Unpacking,
Variables

In this exercise we will cover one more input method you can use to pass variables to a script (script being
another name for your .py files). You know how you type python ex13.py to run the ex13.py
file? Well the ex13.py part of the command is called an “argument”. What we’ll do now is write a
script that also accepts arguments.

Type this program and I'll explain it in detail:

from sys import argv
script, first, second, third = argv

print "The seript is calleds", script
print "Your first variable is:", Eirst
print "Your second variable is:", second
print "Your Third variable 18°"; third

On line 1 we have what’s called an “import”. This is how you add features to your script from the Python
feature set. Rather than give you all the features at once, Python asks you to say what you plan to use.
This keeps your programs small, but it also acts as documentation for other programmers who read your
code later.

The argv is the “argument variable”, a very standard name in programming. that you will find used in
many other languages. This variable holds the arguments you pass to your Python script when you run
it. In the exercises you will get to play with this more and see what happens.

Line 3 “unpacks” argv so that, rather than holding all the arguments, it gets assigned to four variables
you can work with: script, first, second, and third. This may look strange, but “unpack” is
probably the best word to describe what it does. It just says, “Take whatever is in argv, unpack it, and
assign it to all of these variables on the left in order.”

After that we just print them out like normal.

 

39

Learn Python The Hard Way, Release 2.0

 

Hold Up! Features Have Another Name

I call them “features” here (these little things you import to make your Python program do more) but
nobody else calls them features. I just used that name because I needed to trick you into learning what
they are without jargon. Before you can continue, you need to learn their real name: modules.

From now on we will be calling these “features” that we import modules. I'll say things like, “You
want to import the sys module.” They are also called “libraries” by other programmers, but let’s just
stick with modules.

What You Should See

Run the program like this (and you must pass three command line arguments):

python ex13.py first 2nd 3rd

This is what you should see when you do a few different runs with different arguments:

S python exl3.py first 2nd 3rd
The script is called: ex/exl13.py
Your first variable is: first
Your second variable is: 2nd
Your third variable is: 3rd

S python exl3.py cheese apples bread
The script is called: ex/exl13.py
Your first variable is: cheese

Your second variable is: apples

Your third variable is: bread

S python exl3.py Zed A. Shaw

The script is called: ex/exl13.py
Your first variable is: Zed

Your second variable is: A.

Your third variable is: Shaw

You can actually replace “first”, “2nd”, and “3rd” with any three things. You do not have to give these
parameters either, you can give any 3 strings you want:

python ex13.py stuff I like
python exl3.py anything 6 7

If you do not run it correctly, then you will get an error like this:

python exl3.py first 2nd
Traceback (most recent call last):
File "ex/exl3.py", line 3, in <module>

 

40 Exercise 13: Parameters, Unpacking, Variables

Learn Python The Hard Way, Release 2.0

 

script, first, second, third = argv
ValueError: need more than 3 values to unpack

This happens when you do not put enough arguments on the command when you run it (in this case just
first 2nd). Notice when Irun itI give it first 2nd 3rd, which caused it to give an error about
“need more than 3 values to unpack” telling you that you didn’t give it enough parameters.

Extra Credit

1. Try giving fewer than three arguments to your script. See that error you get? See if you can explain
it.

2. Write a script that has fewer arguments and one that has more. Make sure you give the unpacked
variables good names.

3. Combine raw_input with argv to make a script that gets more input from a user.

4. Remember that modules give you features. Modules. Modules. Remember this because we’ ll need
it later.

 

Extra Credit 41

Learn Python The Hard Way, Release 2.0

 

 

42

Exercise 13: Parameters, Unpacking, Variables

Exercise 14: Prompting And Passing

Let’s do one exercise that uses argv and raw_input together to ask the user something specific. You
will need this for the next exercise where we learn to read and write files. In this exercise we’ll use
raw_input slightly differently by having it just print a simple > prompt. This is similar to a game like

Zork or Adventure.

from sys import argv

script, user_name = argv
prompt = '> '!
print "Hi %s, I'm the %s script." % (user_name,

print "I'd like to ask you a few questions."
print "Do you like me %s?" % user_name
likes = raw_input (prompt)

print "Where do you live %s?" % user_name
lives = raw_input (prompt)

print "What kind of computer do you have?"
computer = raw_input (prompt)

print wee

Alright, so you said %r about liking me.
You live in tr. Not sure where that is.
And you have a %r computer. Nice.

wun og (likes, lives, computer)

script)

Notice though that we make a variable prompt that is set to the prompt we want, and we give that to
raw_input instead of typing it over and over. Now if we want to make the prompt something else, we

just change it in this one spot and rerun the script.

Very handy.

 

43

Learn Python The Hard Way, Release 2.0

 

What You Should See

When you run this, remember that you have to give the script your name for the argv arguments.

S python exl4.py Zed

Hi Zed, I'm the exl4.py script.

I'd like to ask you a few questions.
Do you like me Zed?

> yes

Where do you live Zed?

> America

What kind of computer do you have?
> Tandy

Alright, so you said 'yes' about liking me.
You live in 'America'. Not sure where that is.
And you have a 'Tandy' computer. Nice.

Extra Credit

1. Find out what Zork and Adventure were. Try to find a copy and play it.
. Change the prompt variable to something else entirely.

Add another argument and use it in your script.

A YN

Make sure you understand how I combined a """ style multi-line string with the % format activator
as the last print.

 

44 Exercise 14: Prompting And Passing

Exercise 15: Reading Files

Everything you’ve learned about raw_input and argv is so you can start reading files. You may have
to play with this exercise the most to understand what’s going on, so do it carefully and remember your
checks. Working with files is an easy way to erase your work if you are not careful.

This exercise involves writing two files. One is your usual ex15. py file that you will run, but the other
is named ex15_sample.txt. This second file isn’t a script but a plain text file we’ll be reading in our
script. Here are the contents of that file:

This is stuff I typed into a file.
It is really cool stuff.
Lots and lots of fun to have in here.

 

What we want to do is “open” that file in our script and print it out. However, we do not want to just
“hard code” the name ex15_sample.txt into our script. “Hard coding” means putting some bit of
information that should come from the user as a string right in our program. That’s bad because we want
it to load other files later. The solution is to use argv and raw_input to ask the user what file they
want instead of “hard coding” the file’s name.

from sys import argv
script, filename = argv
txt = open(filename)

print "Here's your file tr:" % filename
print txt.read()

print "I'll also ask you to type it again:"
file_again = raw_input("> ")

txt_again = open(file_again)
print txt_again.read()

A few fancy things are going on in this file, so let’s break it down real quick:

Line 1-3 should be a familiar use of argv to get a filename. Next we have line 5 where we use a new

 

45

Learn Python The Hard Way, Release 2.0

 

command open. Right now, run pydoc open and read the instructions. Notice how like your own
scripts and raw_input, it takes a parameter and returns a value you can set to your own variable. You
just opened a file.

Line 7 we print a little line, but on line 8 we have something very new and exciting. We call a function
on txt. What you got back from open is a file, and it’s also got commands you can give it. You give a
file a command by using the . (dot or period), the name of the command, and parameters. Just like with
open and raw_input. The difference is that when you say txt .read() you are saying, “Hey txt!
Do your read command with no parameters!”

The remainder of the file is more of the same, but we’ll leave the analysis to you in the extra credit.

What You Should See

I made a file called “ex15_sample.txt” and ran my script.

S python exl5.py exl15_sample.txt
Here's your file 'exl5_sample.txt':
This is stuff I typed into a file.

It is really cool stuff.

Lots and lots of fun to have in here.

I'll also ask you to type it again:

> ex1l5_sample.txt

This is stuff I typed into a file.

It is really cool stuff.

Lots and lots of fun to have in here.

Extra Credit

This is a big jump so be sure you do this extra credit as best you can before moving on.
1. Above each line write out in English what that line does.

2. If you are not sure ask someone for help or search online. Many times searching for “python
THING?” will find answers for what that THING does in Python. Try searching for “python open”.

3. I used the name “commands” here, but they are also called “functions” and “methods”. Search
around online to see what other people do to define these. Do not worry if they confuse you. It’s
normal for a programmer to confuse you with their vast extensive knowledge.

4. Get rid of the part from line 10-15 where you use raw_input and try the script then.

 

46 Exercise 15: Reading Files

Learn Python The Hard Way, Release 2.0

 

5. Use only raw_input and try the script that way. Think of why one way of getting the filename
would be better than another.

6. Run pydoc file and scroll down until you see the read () command (method/function). See
all the other ones you can use? Skip the ones that have ___ (two underscores) in front because those
are junk. Try some of the other commands.

7. Startup python again and use open from the prompt. Notice how you can open files and run
read on them right there?

8. Have your script also do a close() on the txt and txt_again variables. It’s important to
close files when you are done with them.

 

Extra Credit 47

Learn Python The Hard Way, Release 2.0

 

 

48

Exercise 15: Reading Files

Exercise 16: Reading And Writing

Files

If you did the extra credit from the last exercise you should have seen all sorts of commands (meth-
ods/functions) you can give to files. Here’s the list of commands I want you to remember:

¢ close — Closes the file. Like File->Save. . in your editor.

e read — Reads the contents of the file, you can assign the result to a variable.

¢ readline — Reads just one line of a text file.

* truncate — Empties the file, watch out if you care about the file.

* write(stuff) — Writes stuff to the file.

For now these are the important commands you need to know. Some of them take parameters, but we do
not really care about that. You only need to remember that write takes a parameter of a string you want

to write to the file.
Let’s use some of this to make a simple little text editor:

from sys import argv
script, filename = argv

print "We're going to erase @r." % filename

Print "IT You don"L Want that, Ait CIRE-C (*€)i:

print "If you do want that, hit RETURN."
raw_input ("?")

print "Opening the file...
target = open(filename, 'w')

print "Truncating the file. Goodbye!"
target .truncate()

print "Now I'm going to ask you for three lines."

 

49

20

21

22

23

24

25

26

27

28

29

30

31

32.

33

Learn Python The Hard Way, Release 2.0

 

linel = raw_input ("line 1: ")
line2 = raw_input ("line 2: ")
line3 = raw_input ("line 3: ")

print "I'm going to write these to the file."

target .write(linel)
target .write("\n")
target .write(line2)
target .write("\n")
target .write(line3)
target .write("\n")

print "And finally, we close it."
target .close()

That’s a large file, probably the largest you have typed in. So go slow, do your checks, and make it run.
One trick is to get bits of it running at a time. Get lines 1-8 running, then 5 more, then a few more, etc.,

until it’s all done and running.

What You Should See

There are actually two things you will see, first the output of your new script:

S python exl6.py test.txt

We're going to erase 'test.txt'.

If you don't want that, hit CTRL-C (*C).
If you do want that, hit RETURN.

2

Opening the file...

Truncating the file. Goodbye!

Now I'm going to ask you for three lines.
line 1: To all the people out there.
line 2: I say I don't like my hair.

line 3: I need to shave it off.

I'm going to write these to the file.
And finally, we close it.

$

Now, open up the file you made (in my case test .t xt) in your editor and check it out. Neat right?

 

50 Exercise 16: Reading And Writing Files

Learn Python The Hard Way, Release 2.0

 

Extra Credit

1. If you feel you do not understand this, go back through and use the comment trick to get it squared
away in your mind. One simple English comment above each line will help you understand, or at
least let you know what you need to research more.

2. Write a script similar to the last exercise that uses read and argv to read the file you just created.

3. There’s too much repetition in this file. Use strings, formats, and escapes to print out linel,
line2, and line3 with just one target .write() command instead of 6.

4. Find out why we had to pass a ‘ w’ as an extra parameter to open. Hint: open tries to be safe by
making you explicitly say you want to write a file.

5. If you open the file with ’w’ mode, then do you really need the target .truncate()? Go
read the docs for Python’s open function and see if that’s true.

 

Extra Credit 51

Learn Python The Hard Way, Release 2.0

 

 

52

Exercise 16: Reading And Writing Files

20

21

22

23

24

Exercise 17: More Files

Now let’s do a few more things with files. We’re going to actually write a Python script to copy one file
to another. It’ll be very short but will give you some ideas about other things you can do with files.

from sys import argv
from os.path import exists

script, from_file, to_file = argv

print "Copying from %s to %s" % (from_file, to_file)

# we could do these two on one line too, how?

input = open(from_file)

indata = input.read()

print "The input file is %d bytes long" % len(indata)
print "Does the output file exist? Sr" % exists(to_file)

print "Ready; hit RETURN to continue, CTRL-C to abort."
raw_input ()

 

output = open(to_file, 'w')
output .write (indata)

print "Alright, all done."

output.close()
input.close()

You should immediately notice that we import another handy command named exists. This returns
True if a file exists, based on its name in a string as an argument. It returns False if not. We’ll be
using this function in the second half of this book to do lots of things, but right now you should see how
you can import it.

Using import is a way to get tons of free code other better (well, usually) programmers have written so
you do not have to write it.

 

53

Learn Python The Hard Way, Release 2.0

 

What You Should See

Just like your other scripts, run this one with two arguments, the file to copy from and the file to copy it
to. If we use your test .t xt file from before we get this:

S$ python exl7.py test.txt copied.txt

Copying from test.txt to copied.txt

The input file is 81 bytes long

Does the output file exist? False

Ready, hit RETURN to continue, CTRL-C to abort.

Alright, all done.

S cat copied.txt

To all the people out there.
I say I don't like my hair.
I need to shave it off.

$

It should work with any file. Try a bunch more and see what happens. Just be careful you do not blast an
important file.

Warning: Did you see that trick I did with cat? It only works on Linux or OSX, on Windows use

type to do the same thing.

 

Extra Credit

1. Go read up on Python’s import statement, and start python to try it out. Try importing some
things and see if you can get it right. It’s alright if you do not.

2. This script is really annoying. There’s no need to ask you before doing the copy, and it prints too
much out to the screen. Try to make it more friendly to use by removing features.

3. See how short you can make the script. I could make this | line long.

4. Notice at the end of the WYSS I used something called cat? It’s an old command that
“con*cat*enates” files together, but mostly it’s just an easy way to print a file to the screen. Type
man cat to read about it.

5. Windows people, find the alternative to cat that Linux/OSX people have. Do not worry about
man since there is nothing like that.

6. Find out why you had to do output .close() in the code.

 

54 Exercise 17: More Files

Exercise 18: Names, Variables, Code,
Functions

Big title right? I am about to introduce you to the function! Dum dum dah! Every programmer will go
on and on about functions and all the different ideas about how they work and what they do, but I will
give you the simplest explanation you can use right now.

Functions do three things:
1. They name pieces of code the way variables name strings and numbers.
2. They take arguments the way your scripts take argv.
3. Using #1 and #2 they let you make your own “mini scripts” or “tiny commands”.

You can create a function by using the word def in Python. I’m going to have you make four different
functions that work like your scripts, and then show you how each one is related.

# this one is like your scripts with argv
def print_two(*args):

argl, arg2 = args

print “argl: %r, arg2: %r" %& (argl, arg2)

# ok, that *args is actually pointless, we can just do this
def print_two_again(argl, arg2):
print "argl: Sr, arg2: %r" % (argl, arg2)

# this just takes one argument
def print_one(argl):
print "argl: @r" % argl

# this one takes no arguments
def print_none():
print “Ll get nothin".

print_two("Zed", "Shaw")
print_two_again("Zed","Shaw")

 

55

21

Learn Python The Hard Way, Release 2.0

 

print_one("First!")
print_none()

Let’s break down the first function, print_two which is the most similar to what you already know
from making scripts:
1. First we tell Python we want to make a function using def for “define”.

2. On the same line as def we then give the function a name, in this case we just called it “print_two”
but it could be “peanuts” too. It doesn’t matter, except that your function should have a short name
that says what it does.

3. Then we tell it we want *args (asterisk args) which is a lot like your argv parameter but for
functions. This has to go inside () parenthesis to work.

4. Then we end this line with a : colon, and start indenting.

5. After the colon all the lines that are indented 4 spaces will become attached to this name,
print_two. Our first indented line is one that unpacks the arguments the same as with your
scripts.

6. To demonstrate how it works we print these arguments out, just like we would in a script.

Now, the problem with print_two is that it’s not the easiest way to make a function. In Python
we can skip the whole unpacking args and just use the names we want right inside (). That’s what
print_two_again does.

After that you have an example of how you make a function that takes one argument in print_one.

Finally you have a function that has no arguments in print_none.

Warning: This is very important. Do not get discouraged right now if this doesn’t quite make sense.

We’re going to do a few exercises linking functions to your scripts and show you how to make more.
For now just keep thinking “mini script” when I say “function” and keep playing with them.

 

What You Should See

If you run the above script you should see:

S$ python exl8.py

argl: 'Zed', arg2: 'Shaw'
argl: 'Zed', arg2: 'Shaw'
argi: 'First!'

I got nothin'.

$

Right away you can see how a function works. Notice that you used your functions the way you use
things like exists, open, and other “commands”. In fact, I’ve been tricking you because in Python

 

56 Exercise 18: Names, Variables, Code, Functions

Learn Python The Hard Way, Release 2.0

 

those “commands” are just functions. This means you can make your own commands and use them in
your scripts too.

Extra Credit

Write out a function checklist for later exercises. Write these on an index card and keep it by
you while you complete the rest of these exercises or until you feel you do not need it:

1. Did you start your function definition with def?

Does your function name have only characters and _ (underscore) characters?
Did you put an open parenthesis ( right after the function name?

Did you put your arguments after the parenthesis ( separated by commas?
Did you make each argument unique (meaning no duplicated names).

Did you put a close parenthesis and a colon ) : after the arguments?

SN wv EP Yb

Did you indent all lines of code you want in the function 4 spaces? No more, no less.
8. Did you “end” your function by going back to writing with no indent (dedent ing we call it)?
And when you run (aka “use” or “call’”’) a function, check these things:
1. Did you call/use/run this function by typing its name?
2. Did you put ( character after the name to run it?
3. Did you put the values you want into the parenthesis separated by commas?
4. Did you end the function call with a ) character.
Use these two checklists on the remaining lessons until you do not need them anymore.
Finally, repeat this a few times:

“To ‘run’, ‘call’, or ‘use’ a function all mean the same thing.”

 

Extra Credit 57

Learn Python The Hard Way, Release 2.0

 

 

58

Exercise 18: Names, Variables, Code, Functions

20

21

22

23

24

Exercise 19: Functions And Variables

Functions may have been a mind-blowing amount of information, but do not worry. Just keep doing these
exercises and going through your checklist from the last exercise and you will eventually get it.

There is one tiny point though that you might not have realized which we’ll reinforce right now: The
variables in your function are not connected to the variables in your script. Here’s an exercise to get you
thinking about this:

def cheese_and_crackers(cheese_count, boxes_of_crackers):
print "You have *d cheeses!" % cheese_count
print "You have %d boxes of crackers!" % boxes_of_crackers
print "Man that's enough for a party!"
print "Get a blanket.\n"

print "We can just give the function numbers directly:"
cheese_and_crackers(20, 30)

print "OR, we can use variables from our script:"
amount_of_cheese = 10
amount_of_crackers = 50

cheese_and_crackers (amount_of_cheese, amount_of_crackers)

print "We can even do math inside too:"
cheese_and_crackers(10 + 20, 5 + 6)

print "And we can combine the two, variables and math:"
cheese_and_crackers (amount_of_cheese + 100, amount_of_crackers + 1000)

This shows all different ways we’re able to give our function cheese_and_crackers the values it
needs to print them. We can give it straight numbers. We can give it variables. We can give it math. We
can even combine math and variables.

In a way, the arguments to a function are kind of like our = character when we make a variable. In fact,

 

59

Learn Python The Hard Way, Release 2.0

 

if you can use = to name something, you can usually pass it to a function as an argument.

What You Should See

You should study the output of this script and compare it with what you think you should get for each of
the examples in the script.

S python exl19.py

We can just give the function numbers directly:
You have 20 cheeses!

You have 30 boxes of crackers!

Man that's enough for a party!

Get a blanket.

OR, we can use variables from our script:
You have 10 cheeses!

You have 50 boxes of crackers!

Man that's enough for a party!

Get a blanket.

We can even do math inside too:
You have 30 cheeses!

You have 11 boxes of crackers!

Man that's enough for a party!

Get a blanket.

And we can combine the two, variables and math:
You have 110 cheeses!

You have 1050 boxes of crackers!

Man that's enough for a party!

Get a blanket.

$

Extra Credit

1. Go back through the script and type a comment above each line explaining in English what it does.
2. Start at the bottom and read each line backwards, saying all the important characters.

3. Write at least one more function of your own design, and run it 10 different ways.

 

60 Exercise 19: Functions And Variables

20

21

22

23

24

25

26

27

28

29

30

31

32

33

Exercise 20: Functions And Files

Remember your checklist for functions, then do this exercise paying close attention to how functions and
files can work together to make useful stuff.

from sys import argv
script, input_file = argv

def print_all(f):
print f.read()

def rewind(f):
£.seek (0)

def print_a_line(line_count, f):
print line_count, f.readline()

current_file = open(input_file)

print "First let's print the whole file:\n"

print_all(current_file)

print "Now let's rewind, kind of like 4 tape.”

rewind(current_file)

print "Let's print thtes Jines:"

current_line = 1

current_file)

 

print_a_line(current_line,

current_line = current_line + 1

current_file)

 

print_a_line(current_line,

current_line = current_line + 1

 

print_a_line(current_line,

current_file)

 

61

Learn Python The Hard Way, Release 2.0

 

Pay close attention to how we pass in the current line number each time we run print_a_line.

What You Should See

S python ex20.py test.txt
First let's print the whole file:

To all the people out there.
I say I don't like my hair.
I need to shave it off.

Now let's rewind, kind of like a tape.
Let's print three lines:
1 To all the people out there.

2 I say I don't like my hair.

3 I need to shave it off.

Extra Credit

. Go through and write English comments for each line to understand what’s going on.

. Each time print_a_line is run you are passing in a variable current_line. Write out what

current_line is equal to on each function call, and trace how it becomes line_count in
print_a_line.

 

. Find each place a function is used, and go check its def to make sure that you are giving it the

right arguments.

. Research online what the seek function for file does. Try pydoc file and see if you can

figure it out from there.

. Research the shorthand notation += and rewrite the script to use that.

 

62

Exercise 20: Functions And Files

20

21

22

23

24

25

26

27

Exercise 21: Functions Can Return
Something

You have been using the = character to name variables and set them to numbers or strings. We’re now
going to blow your mind again by showing you how to use = and a new Python word return to set
variables to be a value from a function. There will be one thing to pay close attention to, but first type

this in:
def add(a, b):

print "ADDING %d + %d" % (a, b)
return a +b

def subtract(a, b):
print "SUBTRACTING %d - %d" % (a, b)
return a - b

def multiply(a, b):
print "MULTIPLYING %d * %d" % (a, b)
return a * b

def divide(a, b):

print "DIVIDING %d / $d" % (a, b)
return a / b

print "Let's do some math with just functions!"
age = add(30, 5)

height = subtract (78, 4)

weight = multiply(90, 2)

iq = divide(100, 2)

print "Age: %d, Height: %d, Weight: %d, IQ: %d" % (age, height, weight,

# A puzzle for the extra credit, type it in anyway.

iq)

 

63

29

30

31

32

33

Learn Python The Hard Way, Release 2.0

 

print “Here 15 4 piazle."
what = add(age, subtract (height, multiply(weight, divide(ig, 2))))
print "That becomes: ", what, "Can you do it by hand?"

We are now doing our own math functions for add, subtract, multiply, and divide. The im-
portant thing to notice is the last line where we say return a + b (in add). What this does is the
following:

1. Our function is called with two arguments: a and b.
2. We print out what our function is doing, in this case “ADDING”.

3. Then we tell Python to do something kind of backward: we return the addition of a + b. You
might say this as, “I add a and b then return them.”

4. Python adds the two numbers. Then when the function ends any line that runs it will be able to
assign this a + b result to a variable.

As with many other things in this book, you should take this real slow, break it down and try to trace
what’s going on. To help there’s extra credit to get you to solve a puzzle and learn something cool.

What You Should See

S python ex21.py

Let's do some math with just functions!
ADDING 30 + 5

SUBTRACTING 78 - 4

MULTIPLYING 90 * 2

DIVIDING 100 / 2

Age: 35, Height: 74, Weight: 180, IQ: 50
Here is a puzzle.

DIVIDING 50 / 2

MULTIPLYING 180 * 25

SUBTRACTING 74 - 4500

ADDING 35 + —-4426

That becomes: -4391 Can you do it by hand?
$

Extra Credit

1. If you aren’t really sure what return does, try writing a few of your own functions and have them
return some values. You can return anything that you can put to the right of an =.

 

64 Exercise 21: Functions Can Return Something

Learn Python The Hard Way, Release 2.0

 

2. At the end of the script is a puzzle. I’m taking the return value of one function, and using it as the
argument of another function. I’m doing this in a chain so that I’m kind of creating a formula using
the functions. It looks really weird, but if you run the script you can see the results. What you
should do is try to figure out the normal formula that would recreate this same set of operations.

3. Once you have the formula worked out for the puzzle, get in there and see what happens when you
modify the parts of the functions. Try to change it on purpose to make another value.

4. Finally, do the inverse. Write out a simple formula and use the functions in the same way to
calculate it.

This exercise might really whack your brain out, but take it slow and easy and treat it like a little game.
Figuring out puzzles like this is what makes programming fun, so I'll be giving you more little problems
like this as we go.

 

Extra Credit 65

Learn Python The Hard Way, Release 2.0

 

 

66 Exercise 21: Functions Can Return Something

Exercise 22: What Do You Know So
Far?

There won’t be any code in this exercise or the next one, so there’s no WYSS or Extra Credit either. In
fact, this exercise is like one giant Extra Credit. I’m going to have you do a form of review what you have
learned so far.

First, go back through every exercise you have done so far and write down every word and symbol
(another name for “character’) that you have used. Make sure your list of symbols is complete.

Next to each word or symbol, write its name and what it does. If you can’t find a name for a symbol in
this book, then look for it online. If you do not know what a word or symbol does, then go read about it
again and try using it in some code.

You may run into a few things you just can’t find out or know, so just keep those on the list and be ready
to look them up when you find them.

Once you have your list, spend a few days rewriting the list and double checking that it’s correct. This
may get boring but push through and really nail it down.

Once you have memorized the list and what they do, then you should step it up by writing out tables of
symbols, their names, and what they do from memory. When you hit some you can’t recall from memory,
go back and memorize them again.

 

What You are Learning

It’s important when you are doing a boring mindless memorization exercise like this to know why. It
helps you focus on a goal and know the purpose of all your efforts.

In this exercise you are learning the names of symbols so that you can read source code more easily.
It’s similar to learning the alphabet and basic words of English, except this Python alphabet has extra
symbols you might not know.

 

67

Learn Python The Hard Way, Release 2.0

Just take it slow and do not hurt your brain. Hopefully by now these symbols are natural for you so this
isn’t a big effort. It’s best to take 15 minutes at a time with your list and then take a break. Giving your
brain a rest will help you learn faster with less frustration.

 

68 Exercise 22: What Do You Know So Far?

Exercise 23: Read Some Code

You should have spent last week getting your list of symbols straight and locked in your mind. Now you
get to apply this to another week reading code on the internet. This exercise will be daunting at first. ’m
going to throw you in the deep end for a few days and have you just try your best to read and understand
some source code from real projects. The goal isn’t to get you to understand code, but to teach you the
following three skills:

1. Finding Python source code for things you need.
2. Reading through the code and looking for files.
3. Trying to understand code you find.

At your level you really do not have the skills to evaluate the things you find, but you can benefit from
getting exposure and seeing how things look.

When you do this exercise, think of yourself as an anthropologist, trucking through a new land with just
barely enough of the local language to get around and survive. Except, of course, that you will actually
get out alive because the internet isn’t a jungle. Anyway.

Here’s what you do:
1. Go to bitbucket.org with your favorite web browser and search for “python”.
2. Avoid any project with “Python 3” mentioned. That’ll only confuse you.
3. Pick a random project and click on it.

4. Click on the Source tab and browse through the list of files and directories until you find a .py
file (but not setup.py, that’s useless).

5. Start at the top and read through it, taking notes on what you think it does.
6. If any symbols or strange words seem to interest you, write them down to research later.

That’s it. Your job is to use what you know so far and see if you can read the code and get a grasp of what
it does. Try skimming the code first, and then read it in detail. Maybe also try taking very difficult parts
and reading each symbol you know outloud.

Now try several three other sites:

 

69

Learn Python The Hard Way, Release 2.0

 

¢ github.com
¢ launchpad.net
¢ koders.com

On each of these sites you may find weird files ending in .c so stick to . py files like the ones you have
written in this book.

A final fun thing to do is use the above four sources of Python code and type in topics you are interested
in instead of “python”. Search for “journalism”, “cooking”, “physics”, or anything you are curious about.
Chances are there’s some code out there you could use right away.

 

70 Exercise 23: Read Some Code

20

21

22

23

24

25

26

27

28

29

30

31

Exercise 24: More Practice

You are getting to the end of this section. You should have enough Python “under your fingers” to move
onto learning about how programming really works, but you should do some more practice. This exercise
is longer and all about building up stamina. The next exercise will be similar. Do them, get them exactly
right, and do your checks.

print "Let's practice everything."
print 'You\'d need to know \'bout escapes with \\ that do \n newlines and \t tabs.'

poem = """

\tThe lovely world

with logic so firmly planted

cannot discern \n the needs of love
nor comprehend passion from intuition
and requires an explanation

\n\t\twhere there is none.
ww

five = 10 -2+3- 6
print "This should be five: %s" % five

def secret_formula(started):
jelly_beans = started * 500
jars = jelly_beans / 1000
crates = jars / 100
return jelly_beans, jars, crates

start_point = 10000
beans, jars, crates = secret_formula(start_point)

print "With a starting point of: td" % start_point

 

71

32

33

34

35:

36

37

Learn Python The Hard Way, Release 2.0

 

print "We'd have %d beans, %d jars, and %d crates." % (beans, jars, crates)
start_point = start_point / 10

print "We can also do that this way:"
print "We'd have %d beans, %d jars, and %d crates." % secret_formula(start_point)

What You Should See

S python ex24.py

Let's practice everything.

You'd need to know 'bout escapes with \ that do
newlines and tabs.

The lovely world
with logic so firmly planted
cannot discern
the needs of love
nor comprehend passion from intuition
and requires an explanation

where there is none.

This should be five: 5

With a starting point of: 10000

We'd have 5000000 beans, 5000 jars, and 50 crates.
We can also do that this way:

We'd have 500000 beans, 500 jars, and 5 crates.

$

Extra Credit

1. Make sure to do your checks: read it backwards, read it out loud, put comments above confusing
parts.

2. Break the file on purpose, then run it to see what kinds of errors you get. Make sure you can fix it.

 

72 Exercise 24: More Practice

20

21

22

23

24

25

26

27

28

29

30

Exercise 25: Even More Practice

We’re going to do some more practice involving functions and variables to make sure you know them

well.

This exercise should be straight forward for you to type in, break down, and understand.

However, this exercise is a little different. You won’t be running it. Instead you will import it into your
python and run the functions yourself.

def

def

def

def

def

def

break_words (stuff):

"""This function will break up words for us."""
words = stuff.split(' ')

return words

sort_words (words):
muNserts the words. """
return sorted(words)

print_first_word(words):

"""Prints the first word after popping it off.@""
word = words.pop (0)

print word

print_last_word(words):

"""Prints the last word after popping it off."""
word = words.pop(-1)

print word

sort_sentence (sentence):

"""Takes in a full sentence and returns the sorted words."""
words = break_words (sentence)

return sort_words (words)

print_first_and_last (sentence):

"""Drints the first and last words of the sentence."""
words = break_words (sentence)

print_first_word (words)

print_last_word (words)

 

73

31

32.

33

34

35

20

21

22

23

24

25

26

27

28

29

30

Learn Python The Hard Way, Release 2.0

 

def print_first_and_last_sorted(sentence):
"""Sorts the words then prints the first and last one."""
words = sort_sentence (sentence)
print_first_word (words)
print_last_word (words)

First, run this like normal with python ex25.>py to find any errors you have made. Once you have
found all of the errors you can and fixed them, you will then want to follow the WYSS section to complete
the exercise.

What You Should See

In this exercise we’re going to interact with your . py file inside the python interpreter you used peri-
odically to do calculations.

Here’s what it looks like when I do it:

S python
Python 2.5.1 (r2863, Feb 6 2009, :12)
[GCC 4.0.1 (Apple Inc. build 5465)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import ex25
>>> sentence = "All good things come to those who wait."
>>> words = ex25.break_words (sentence)
>>> words
['All', 'good', 'things', 'come', 'to', 'those', 'who', 'wait.']
>>> sorted_words = ex25.sort_words (words)
>>> sorted_words
['All', 'come', 'good', 'things', 'those', 'to', 'wait.', 'who']
>>> ex25.print_first_word(words)
All
>>> ex25.print_last_word (words)
wait.
>>> wrods
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
NameError: name 'wrods' is not defined
>>> words
['good', 'things', 'come', 'to', 'those', 'who']
>>> ex25.print_first_word(sorted_words)
All
>>> ex25.print_last_word(sorted_words)
who
>>> sorted_words
['come', 'good', 'things', 'those', 'to', 'wait.']
>>> sorted_words = ex25.sort_sentence (sentence)
>>> sorted_words

 

 

74 Exercise 25: Even More Practice

31

32

33

34

35

36

37

38

39

Learn Python The Hard Way, Release 2.0

 

['All', 'come', 'good', 'things', 'those', 'to', 'wait.', 'who']
>>> ex25.print_first_and_last (sentence)

All

wait.

S>>
All
who
Sis

$s

ex25.print_first_and_last_sorted (sentence)

“D

Let’s break this down line by line to make sure you know what’s going on:

Line 5 you import your ex25.py python file, just like other imports you have done. Notice you
do not need to put the . py at the end to import it. When you do this you make a module that has
all your functions in it to use.

Line 6 you made a sentence to work with.

Line 7 you use the ex25 module and call your first function ex25.break_words. The . (dot,
period) symbol is how you tell python, “Hey, inside ex25 there’s a function called break_words
and I want to run it.”

Line 8 we just type words, and python will print out what’s in that variable (line 9). It looks weird
but this is a List which you will learn about later.

Lines 10-11 we do the same thing with ex25.sort_words to get a sorted sentence.

Lines 13-16 we use ex25.print_first_word and ex25.print_last_word to get the
first and last word printed out.

Line 17 is interesting. I made a mistake and typed the words variable as wrods so python gave
me an error on Lines 18-20.

Line 21-22 is where we print the modified words list. Notice that since we printed the first and last
one, those words are now missing.

The remaining lines are for you to figure out and analyze in the extra credit.

Extra Credit

1. Take the remaining lines of the WYSS output and figure out what they are doing. Make sure you

understand how you are running your functions in the ex25 module.

Try doing this: help (ex25) and also help (ex25.break_words). Notice how you get
help for your module, and how the help is those odd """ strings you put after each function in
ex25? Those special strings are called documentation comments and we’ll be seeing more
of them.

 

Extra Credit 75

Learn Python The Hard Way, Release 2.0

3. Typing ex25. is annoying. A shortcut is to do your import like this: from ex25 import *
which is like saying, “Import everything from ex25.” Programmers like saying things backwards.
Start a new session and see how all your functions are right there.

4. Try breaking your file and see what it looks like in python when you use it. You will have to quit
python with CTRL-D (CTRL-Z on windows) to be able to reload it.

 

76 Exercise 25: Even More Practice

Exercise 26: Congratulations, Take A
Test!

You are almost done with the first half of the book. The second half is where things get interesting. You
will learn logic and be able to do useful things like make decisions.

Before you continue, I have a quiz for you. This quiz will be very hard because it requires you to fix
someone else’s code. When you are a programmer you often have to deal with another programmer’s
code, and also with their arrogance. They will very frequently claim that their code is perfect.

These programmers are stupid people who care little for others. A good programmer assumes, like a
good scientist, that there’s always some probability their code is wrong. Good programmers start from
the premise that their software is broken and then work to rule out all possible ways it could be wrong
before finally admitting that maybe it really is the other guy’s code.

In this exercise, you will practice dealing with a bad programmer by fixing a bad programmer’s code. I
have poorly copied exercises 24 and 25 into a file and removed random characters and added flaws. Most
of the errors are things Python will tell you, while some of them are math errors you should find. Others
are formatting errors or spelling mistakes in the strings.

All of these errors are very common mistakes all programmers make. Even experienced ones.

Your job in this exercise is to correct this file. Use all of your skills to make this file better. Analyze it
first, maybe printing it out to edit it like you would a school term paper. Fix each flaw and keep running
it and fixing it until the script runs perfectly. Try not to get help, and instead if you get stuck take a break
and come back to it later.

Even if this takes days to do, bust through it and make it right.
Finally, the point of this exercise isn’t to type it in, but to fix an existing file. To do that, you must go to:
¢ http://learnpythonthehardway.com/exercise26.txt

Copy-paste the code into a file named ex26.py. This is the only time you are allowed to copy-paste.

 

77

Learn Python The Hard Way, Release 2.0

 

 

78

Exercise 26: Congratulations, Take A Test!

Exercise 27: Memorizing Logic

Today is the day you start learning about logic. Up to this point you have done everything you possibly
can reading and writing files, to the terminal, and have learned quite a lot of the math capabilities of
Python.

From now on, you will be learning /ogic. You won’t learn complex theories that academics love to study,
but just the simple basic logic that makes real programs work and that real programmers need every day.

Learning logic has to come after you do some memorization. I want you to do this exercise for an entire
week. Do not falter. Even if you are bored out of your mind, keep doing it. This exercise has a set of
logic tables you must memorize to make it easier for you to do the later exercises.

I’m warning you this won’t be fun at first. It will be downright boring and tedious but this is to teach you
a very important skill you will need as a programmer. You will need to be able to memorize important
concepts as you go in your life. Most of these concepts will be exciting once you get them. You will
struggle with them, like wrestling a squid, then one day snap you will understand it. All that work
memorizing the basics pays off big later.

Here’s a tip on how to memorize something without going insane: Do a tiny bit at a time throughout the
day and mark down what you need to work on most. Do not try to sit down for two hours straight and
memorize these tables. This won’t work. Your brain will really only retain whatever you studied in the
first 15 or 30 minutes anyway.

Instead, what you should do is create a bunch of index cards with each column on the left on one side
(True or False) and the column on the right on the back. You should then pull them out, see the “True or
False” and be able to immediately say “True!” Keep practicing until you can do this.

Once you can do that, start writing out your own truth tables each night into a notebook. Do not just copy
them. Try to do them from memory, and when you get stuck glance quickly at the ones I have here to
refresh your memory. Doing this will train your brain to remember the whole table.

Do not spend more than one week on this, because you will be applying it as you go.

 

79

Learn Python The Hard Way, Release 2.0

The Truth Terms

In python we have the following terms (characters and phrases) for determining if something is “True”
or “False”. Logic on a computer is all about seeing if some combination of these characters and some
variables is True at that point in the program.

° and
* or

* not

!= (not equal)

== (equal)

>= (greater-than-equal)

<= (less-than-equal)
¢ True
¢ False

You actually have run into these characters before, but maybe not the phrases. The phrases (and, or, not)
actually work the way you expect them to, just like in English.

The Truth Tables

We will now use these characters to make the truth tables you need to memorize.

NOT True?
not False | True
not True | False

 

 

 

 

 

 

OR True?
True or False | True
True or True True
False or True | True
False or False | False

 

 

 

 

 

AND True?
True and False | False
True and True True
False and True | False
False and False | False

 

 

 

 

 

 

80 Exercise 27: Memorizing Logic

Learn Python The Hard Way, Release 2.0

 

 

 

 

 

 

 

 

NOT OR True?
not (True or False) | False
not (True or True) False
not (False or True) | False
not (False or False) | True
NOT AND True?
not (True and False) | True
not (True and True) False
not (False and True) | True
not (False and False) | True

 

 

 

 

 

 

 

 

 

 

f= True?
1!=0 | True
1!=1 | False
0!=1 | True
0!=0 | False
== True?
1==0 | False
l= True
QO==1 | False
0 == True

 

 

 

 

Now use these tables to write up your own cards and spend the week memorizing them. Remember
though, there is no failing in this book, just trying as hard as you can each day, and then a Jittle bit more.

 

The Truth Tables

81

Learn Python The Hard Way, Release 2.0

 

 

82

Exercise 27: Memorizing Logic

Exercise 28: Boolean Practice

The logic combinations you learned from the last exercise are called “boolean” logic expressions.
Boolean logic is used everywhere in programming. They are essential fundamental parts of computa-
tion and knowing them very well is akin to knowing your scales in music.

In this exercise you will be taking the logic exercises you memorized and start trying them out in
python. Take each of these logic problems, and write out what you think the answer will be. In each
case it will be either True or False. Once you have the answers written down, you will start python in
your terminal and type them in to confirm your answers.

1. True and True
False and True
1 == 1 and 2 ==
"test" == "test"
or 2 !=1
True and 1 ==

False and 0 != 0

True or 1 == 1

Caos a YM FS YS PL
ll
ll
=

"test" == "testing"

 

S

'= 0 and 2 ==

—
—

. "test" != "testing"

—
N

. "test" == 1

—
Ww

. not (True and False)

_
A

. not (1 == 1 and 0 != 1)

—
Nn

. not (10 == 1 or 1000 == 1000)

—
Oo

. not (1 != 10 or 3 == 4)

—
~

.- not ("testing" == "testing" and "Zed" == "Cool Guy")

 

83

Learn Python The Hard Way, Release 2.0

 

18. 1 == 1 and not ("testing" == 1 or 1 == 0)
19. "Chunky" == "bacon" and not (3 == 4 or 3 == 3)
20. 3 == 3 and not ("testing" == "testing" or "Python" == "Fun")

I will also give you a trick to help you figure out the more complicated ones toward the end.
Whenever you see these boolean logic statements, you can solve them easily by this simple process:
1. Find equality test (== or !=) and replace it with its truth.
2. Find each and/or inside a parenthesis and solve those first.
3. Find each not and invert it.
4. Find any remaining and/or and solve it.
5. When you are done you should have True or False.
I will demonstrate with a variation on #20:
3 != 4 and not ("testing" != "test" or "Python" == "Python")
Here’s me going through each of the steps and showing you the translation until I’ve boiled it down to a
single result:

1. Solve each equality test:

(a) 3 != 4is True: True and not ("testing" != "test" or "Python"
== "Python")

(b) "testing" != "test" is True: True and not (True or "Python"
== "Python")

(c) "Python" == "Python": True and not (True or True)

2. Find each and/or in parenthesis ():

(a) (True or True) is True: True and not (True)
3. Find each not and invert it:

(a) not (True) is False: True and False
4. Find any remaining and/or and solve them:

(a) True and False is False

With that we’re done and know the result is False.

Warning: The more complicated ones may seem very hard at first. You should be able to give a
good first stab at solving them, but do not get discouraged. I’m just getting you primed for more of

these “logic gymnastics” so that later cool stuff is much easier. Just stick with it, and keep track of
what you get wrong, but do not worry that it’s not getting in your head quite yet. It’1l come.

 

 

84 Exercise 28: Boolean Practice

Learn Python The Hard Way, Release 2.0

What You Should See

After you have tried to guess at these, this is what your session with python might look like:

S python

Python 2.5.1 (r2863, Feb 6 2009, :12)

[GCC 4.0.1 (Apple Inc. build 5465)] on darwin

Type "help", "copyright", "credits" or "license" for more information.
>>> True and True

rrue

>>> 1 == 1 and 2 ==

True

Extra Credit

1. There are a lot of operators in Python similar to != and ==. Try to find out as many “equality
operators” as you can. They should be like: < or <=.

2. Write out the names of each of these equality operators. For example, I call ! = “not equal”.

3. Play with the python by typing out new boolean operators, and before you hit enter try to shout
out what it is. Do not think about it, just the first thing that comes to mind. Write it down then hit
enter, and keep track of how many you get right and wrong.

4. Throw away that piece of paper from #3 away so you do not accidentally try to use it later.

 

What You Should See 85

Learn Python The Hard Way, Release 2.0

 

 

86

Exercise 28: Boolean Practice

20

21

22

23

24

25

26

27

28

Exercise 29: What If

Here is the next script of Python you will enter, which introduces you to the if-statement. Type this
in, make it run exactly right, and then we’ll try see if your practice has paid off.

people = 20

cats = 30
dogs = 15

if people
print

if people
print

if people
print

if people
print
dogs += 5

if people
print

if people

print

if people
print

< cats:
"Too many cats!

> Cats:

 

[The world is doomed!"

"Not many cats! The world is saved!"

< dogs:
"The world is drooled on!"

> dogs:
"The world is dry!"

>= dogs:
"People are greater than or equal

<= dogs:

"People are less than or equal to

== dogs:
"People are dogs."

to dogs.

dogs."

 

87

Learn Python The Hard Way, Release 2.0

 

What You Should See

S python ex29.py

Too many cats! The world is doomed!
The world is dry!

People are greater than equal to dogs.
People are less than equal to dogs.
People are dogs.

$

Extra Credit

In this extra credit, try to guess what you think the if-statement is and what it does. Try to answer
these questions in your own words before moving onto the next exercise:

1.

What do you think the if does to the code under it?

2. Why does the code under the if need to be indented 4 spaces?
3.
4
5

What happens if it isn’t indented?

. Can you put other boolean expressions from Ex. 27 in the if-statement? Try it.

. What happens if you change the initial variables for people, cats, and dogs?

 

88

Exercise 29: What If

A Fw YN

Exercise 30: Else And If

In the last exercise you worked out some if—statements, and then tried to guess what they are and
how they work. Before you learn more [ll explain what everything is by answering the questions you
had from extra credit. You did the extra credit right?

1.

What do you think the if does to the code under it? An if statement creates what is called a
“branch” in the code. It’s kind of like those choose your own adventure books where you are
asked to turn to one page if you make one choice, and another if you go a different direction. The
if-—statement tells your script, “If this boolean expression is True, then run the code under it,
otherwise skip it.”

. Why does the code under the if need to be indented 4 spaces? A colon at the end of a line is how

you tell Python you are going to create a new “block” of code, and then indenting 4 spaces tells
Python what lines of code are in that block. This is exactly the same thing you did when you made
functions in the first half of the book.

. What happens if it isn’t indented? If it isn’t indented, you will most likely create a Python error.

Python expects you to indent something after you end a line with a : (colon).

. Can you put other boolean expressions from Ex. 27 in the if statement? Try it. Yes you can, and

they can be as complex as you like, although really complex things generally are bad style.

. What happens if you change the initial variables for people, cats, and dogs? Because you

are comparing numbers, if you change the numbers, different if-statements will evaluate to
True and the blocks of code under them will run. Go back and put different numbers in and see if
you can figure out in your head what blocks of code will run.

Compare my answers to your answers, and make sure you really understand the concept of a “block”
of code. This is important for when you do the next exercise where you write all the parts of
if—statements that you can use.

Type this one in and make it work too.

people = 30

cars

= 40

buses = 15

 

89

20

21

22

23

Learn Python The Hard Way, Release 2.0

if cars > people:

print "We should take the cars."
elif cars < people:

print "We should not take the cars."
else:

print "We can't decide."

if buses > cars:

print "That's too many buses."
elif buses < cars:

print "Maybe we could take the buses."
else:

print "We still can't decide."

if people > buses:

print "Alright, let's just take the buses,"
else:

print "Fine, let's stay home then."

What You Should See

S python ex.py

We should take the cars.

Maybe we could take the buses.
Alright, let's just take the buses.

$

Extra Credit

1. Try to guess what elif and else are doing.

2. Change the numbers of cars, people, and buses and then trace through each

if—statement to see what will be printed.

3. Try some more complex boolean expressions like cars > people and buses < cars.

4. Above each line write an English description of what the line does.

 

90

Exercise 30: Else And If

19

20

21

22

23

24

25

26

Exercise 31: Making Decisions

In the first half of this book you mostly just printed out things and called functions, but everything was
basically in a straight line. Your scripts ran starting at the top, and went to the bottom where they ended.
If you made a function you could run that function later, but it still didn’t have the kind of branching you
need to really make decisions. Now that you have if, else, and elif you can start to make scripts
that decide things.

In the last script you wrote out a simple set of tests asking some questions. In this script you will ask the
user questions and make decisions based on their answers. Write this script, and then play with it quite a
lot to figure it out.

print "You enter a dark room with two doors. Do you go through door #1 or door #2?"
door = raw_input("> ")
if door == "1":

print "There's a giant bear here eating a cheese cake. What do you do?"
print "1. Take the cake."
print "2... Screai. at. the bear.”

bear = raw_input("> ")
if bear == "1":

print "The bear eats your face off. Good job!"
elif bear == "2":

print "The bear eats your legs off. Good job!"
else:

print "Well, doing %s is probably better. Bear runs away." % bear

elif door == "2";
print "You stare into the endless abyss at Cthuhlu's retina."
print “1... Blueberries."
print "2. Yellow jacket clothespins."
print "3. Understanding revolvers yelling melodies."

insanity = raw_input("> ")

 

91

27

28

29

30

31

32.

33

Learn Python The Hard Way, Release 2.0

 

if insanity == "1" or insanity == "2":
print "Your body survives powered by a mind of jello. Good job!"
else:
print "The insanity rots your eyes into a pool of muck. Good job!"
else:

print "You stumble around and fall on a knife and die. Good job!"

A key point here is that you are now putting the if-statements inside if-statements as code
that can run. This is very powerful and can be used to create “nested” decisions, where one branch leads
to another and another.

Make sure you understand this concept of if-statements inside if-statements. In fact, do the extra credit
to really nail it.

What You Should See

Here is me playing this little adventure game. I do not do so well.

S$ python ex31.py

You enter a dark room with two doors. Do you go through door #1 or door #2?
> 1

There's a giant bear here eating a cheese cake. What do you do?

1. Take the cake.

2. Scream at the bear.

= 2

The bear eats your legs off. Good job!

S$ python ex31l.py

You enter a dark room with two doors. Do you go through door #1 or door #2?
> 1

There's a giant bear here eating a cheese cake. What do you do?

1. Take the cake.

2. Scream at the bear.

& a.

The bear eats your face off. Good job!

S python ex31.py

You enter a dark room with two doors. Do you go through door #1 or door #2?
= 2

You stare into the endless abyss at Cthuhlu's retina.

1. Blueberries.

2. Yellow jacket clothespins.

3. Understanding revolvers yelling melodies.

> 1

Your body survives powered by a mind of jello. Good job!

 

92 Exercise 31: Making Decisions

Learn Python The Hard Way, Release 2.0

 

S python ex31l.py

You enter a dark room with two doors. Do you go through door #1 or door #2?
> 2

You stare into the endless abyss at Cthuhlu's retina.

1. Blueberries.

2. Yellow jacket clothespins.

3. Understanding revolvers yelling melodies.

> 3

The insanity rots your eyes into a pool of muck. Good job!

S python ex31.py

You enter a dark room with two doors. Do you go through door #1 or door #2?
> stuff

You stumble around and fall on a knife and die. Good job!

S python ex31l.py

You enter a dark room with two doors. Do you go through door #1 or door #2?
= 1

There's a giant bear here eating a cheese cake. What do you do?

1. Take the cake.

2. Scream at the bear.

> apples

Well, doing apples is probably better. Bear runs away.

Extra Credit

Make new parts of the game and change what decisions people can make. Expand the game out as much
as you can before it gets ridiculous.

 

Extra Credit 93

Learn Python The Hard Way, Release 2.0

 

 

94

Exercise 31: Making Decisions

Exercise 32: Loops And Lists

You should now be able to do some programs that are much more interesting. If you have been
keeping up, you should realize that now you can combine all the other things you have learned with
if—statements and boolean expressions to make your programs do smart things.

However, programs also need to do repetitive things very quickly. We are going to use a for—loop in
this exercise to build and print various lists. When you do the exercise, you will start to figure out what
they are. I won’t tell you right now. You have to figure it out.

Before you can use a for-loop, you need a way to store the results of loops somewhere. The best way to
do this is with a list. A list is exactly what its name says, a container of things that are organized in
order. It’s not complicated; you just have to learn a new syntax. First, there’s how you make a list:

hairs = ['brown', 'blond', 'red']
eyes = ['brown', 'blue', 'green']
weights = [1, 2, 3, 4]

What you do is start the list with the [ (left-bracket) which “opens” the list. Then you put each item
you want in the list separated by commas, just like when you did function arguments. Lastly you end the
list with a ] (right-bracket) to indicate that it’s over. Python then takes this list and all its contents, and
assigns them to the variable.

Warning: This is where things get tricky for people who can’t program. Your brain has been
taught that the world is flat. Remember in the last exercise where you put if-statements inside
if—statements? That probably made your brain hurt because most people do not ponder how to

“nest” things inside things. In programming this is all over the place. You will find functions that
call other functions that have if-statements that have lists with lists inside lists. If you see a
structure like this that you can’t figure out, take out pencil and paper and break it down manually bit
by bit until you understand it.

 

We now will build some lists using some loops and print them out:

the_count = [1, 2, 3, 4, 5]
fruits = ['apples', 'oranges', 'pears', 'apricots']
change = [1, 'pennies', 2, 'dimes', 3, 'quarters']

 

95

20

21

22:

23

24

25

26

27

28

29

Learn Python The Hard Way, Release 2.0

 

# this first kind of for-loop goes through a list
for number in the_count:
print "This is count %d" % number

# same as above
for fruit in fruits:
print "A fruit of type: %s" % fruit

# also we can go through mixed lists too
# notice we have to use @r since we don't know what's in it
for i in change:

print "I got tr" % i.

# we can also build lists, first start with an empty one
elements = []

# then use the range function to do 0 to 5 counts
for i in range(0, 6):
print "Adding %d to the list." $i
# append is a function that lists understand
elements.append (i)

# now we can print them out too
for i in elements:
print "Element was: %d" % i

 

What You Should See

S python ex32.
This is count
This is count
This is count
This is count
This is count
fruit of type: apples
fruit of type: oranges
fruit of type: pears
fruit of type: apricots
got 1

got 'pennies'

got 2

got 'dimes'

got 3

got 'quarters'

Adding 0 to the list.
Adding 1 to the list.

GQ ws WN HE'D

HN NON NON OD DS OD

 

96 Exercise 32: Loops And Lists

Learn Python The Hard Way, Release 2.0

 

Adding 2
Adding 3
Adding 4
Adding 5
Element
Element
Element
Element
Element
Element

$

to the
to the
to the
to the
was: O
was: 1
was: 2
was: 3
was: 4
was: 5

Extra Credit

list.
LAST 5
dist.
dist.

1. Take a look at how you used range. Look up the range function to understand it.

2. Could you have avoided that for—loop entirely on line 22 and just assigned range (0, 6)

directly to elements?

3. Find the Python documentation on lists and read about them. What other operations can you do to

lists besides append?

 

Extra Credit

97

Learn Python The Hard Way, Release 2.0

 

 

98

Exercise 32: Loops And Lists

DA we Bw KR

Exercise 33: While Loops

Now to totally blow your mind with a new loop, the while-loop. A while-loop will keep executing
the code block under it as long as a boolean expression is True.

Wait, you have been keeping up with the terminology right? That if we write a line and end it witha :
(colon) then that tells Python to start a new block of code? Then we indent and that’s the new code. This
is all about structuring your programs so that Python knows what you mean. If you do not get that idea
then go back and do some more work with if—statements, functions, and the for—loop until you
get it.

Later on we’ll have some exercises that will train your brain to read these structures, similar to how we
burned boolean expressions into your brain.

Back to while-loops. What they do is simply do a test like an if-statement, but instead of
running the code block once, they jump back to the “top” where the while is, and repeat. It keeps doing
this until the expression is False.

Here’s the problem with while-—loops: Sometimes they do not stop. This is great if your intention is
to just keep looping until the end of the universe. Otherwise you almost always want your loops to end
eventually.

To avoid these problems, there’s some rules to follow:
1. Make sure that you use while—loops sparingly. Usually a for—loop is better.

2. Review your while statements and make sure that the thing you are testing will become False at
some point.

3. When in doubt, print out your test variable at the top and bottom of the while-loop to see what
it’s doing.
In this exercise, you will learn the while—loop by doing the above three things:
i=0

numbers = []

while i < 6:
print “AC the top 1.18 da” % 1
numbers.append (i)

 

99

Learn Python The Hard Way, Release 2.0

 

i=id¢il

print "Numbers now:

print "The numbers: "

for num in numbers:
print num

"
,

What You Should See

S python ex.py

At the top iis 0
Numbers now: [0]

At the bottom iis 1
At the top iis 1
Numbers now: [0y, LJ
At the bottom iis 2
At the top iis 2
Numbers now: [On Le
At the bottom i is 3
At the top iis 3
Numbers now: [Gz Le
At the bottom iis 4
At the top iis 4
Numbers now: [0, 1,
At the bottom iis 5
At the top iis 5
Numbers now: [0, 1,
At the bottom iis 6
The numbers:

GW wNHE OS

Extra Credit

1. Convert this while loop to a function that you can call, and replace 6 in the test (i < 6) witha

variable.

2]

2,

2,

2,

numbers
print "At the bottom iis %d" % i

 

100

Exercise 33: While Loops

Learn Python The Hard Way, Release 2.0

 

2. Now use this function to rewrite the script to try different numbers.

3. Add another variable to the function arguments that you can pass in that lets you change the + 1
on line 8 so you can change how much it increments by.

4. Rewrite the script again to use this function to see what effect that has.

5. Now, write it to use for—loops and range instead. Do you need the incrementor in the middle
anymore? What happens if you do not get rid of it?

If at any time that you are doing this it goes crazy (it probably will), just hold down CTRL and hit c
(CTRL-c) and the program will abort.

 

Extra Credit 101

Learn Python The Hard Way, Release 2.0

 

 

102 Exercise 33: While Loops

Exercise 34: Accessing Elements Of
Lists

Lists are pretty useful, but unless you can get at the things in them they aren’t all that good. You can
already go through the elements of a list in order, but what if you want say, the 5th element? You need to
know how to access the elements of a list. Here’s how you would access the first element of a list:

animals = ['bear', 'tiger', 'penguin', 'zebra']
bear = animals[0]

You take a list of animals, and then you get the first one using 0?! How does that work? Because of the
way math works, Python start its lists at 0 rather than 1. It seems weird, but there’s many advantages to
this, even though it is mostly arbitrary.

The best way to explain why is by showing you the difference between how you use numbers and how
programmers use numbers.

Imagine you are watching the four animals in our list above ([’ bear’, ‘tiger’, ‘penguin’,
‘zebra’ ])runinarace. They win in the order we have them in this list. The race was really exciting
because, the animals didn’t eat each other and somehow managed to run a race. Your friend however
shows up late and wants to know who won. Does your friend say, “Hey, who came in zeroth?” No, he
says, “Hey, who came in first?”

This is because the order of the animals is important. You can’t have the second animal without the first
animal, and can’t have the third without the second. It’s also impossible to have a “zeroth” animal since
zero means nothing. How can you have a nothing win a race? It just doesn’t make sense. We call these
kinds of numbers “ordinal” numbers, because they indicate an ordering of things.

Programmers, however, can’t think this way because they can pick any element out of a list at any point.
To a programmer, the above list is more like a deck of cards. If they want the tiger, they grab it. If they
want the zebra, they can take it too. This need to pull elements out of lists at random means that they
need a way to indicate elements consistently by an address, or an “index”, and the best way to do that is
to start the indices at 0. Trust me on this, the math is way easier for these kinds of accesses. This kind of
number is a “cardinal” number and means you can pick at random, so there needs to be a 0 element.

So, how does this help you work with lists? Simple, every time you say to yourself, “I want the 3rd
animal,” you translate this “ordinal” number to a “cardinal” number by subtracting 1. The “3rd” animal

 

103

Learn Python The Hard Way, Release 2.0

 

is at index 2 and is the penguin. You have to do this because you have spent your whole life using ordinal
numbers, and now you have to think in cardinal. Just subtract 1 and you will be good.

Remember: ordinal == ordered, 1st; cardinal == cards at random, 0.

Let’s practice this. Take this list of animals, and follow the exercises where I tell you to write down what
animal you get for that ordinal or cardinal number. Remember if I say “first”, “second”, etc. then ’m
using ordinal, so subtract 1. If I give you cardinal (0, 1, 2) then use it directly.

animals = ['bear', 'python', 'peacock', 'kangaroo', 'whale', 'platypus']

1. The animal at 1.
2. The 3rd animal.
3. The Ist animal.
4. The animal at 3.
5. The 5th animal.
6. The animal at 2.
7. The 6th animal.
8. The animal at 4.

For each of these, write out a full sentence of the form: “The Ist animal is at 0 and is a bear.” Then say it
backwards, “The animal at 0 is the Ist animal and is a bear.”

Use your python to check your answers.

Extra Credit

1. Read about ordinal and cardinal numbers online.

2. With what you know of the difference between these types of numbers, can you explain why this
really is 2010? (Hint, you can’t pick years at random.)

3. Write some more lists and work out similar indexes until you can translate them.

4. Use Python to check your answers to this as well.

Warning: Programmers will tell you to read this guy named “Dijkstra” on this subject. I recommend

you avoid his writings on this unless you enjoy being yelled at by someone who stopped programming
at the same time programming started.

 

 

104 Exercise 34: Accessing Elements Of Lists

20

21

22

23

24

25

26

27

28

29

30

31

32

33

Exercise 35: Branches and Functions

You have learned to do if-statements, functions, and arrays. Now it’s time to bend your mind. Type
this in, and see if you can figure out what it’s doing.

from sys import exit

def

def

gold_room():

print "This room is full of gold. How much do you take?"

next = raw_input("> ")
af "O"™ tn nese or “1™ in mexe:
how_much = int (next)
else:
dead("Man, learn to type a number.")

if how_much < 50:
print "Nice, you're not greedy, you win!"
exit (0)

else:
dead("You greedy bastard!")

bear_room():

prant “There 15 a bear here,”

print "The bear has a bunch of honey."

print "The fat bear is in front of another door."
print "How are you going to move the bear?"

 

bear_moved = False

while True:
next = raw_input("> ")

if next == "take honey":

dead("The bear looks at you then slaps your face off.")

elif next == "taunt bear" and not bear_moved:
print "The bear has moved from the door.
bear_moved = True

You can go through it now."

 

105

34

35

36

37

38

39

41

42

43

45

47

48

49

50

51

52

53

54

55

56

57

58

59

61
62

63

65

66

67

68

69

70

71

72

73

74

vi)

76

Learn Python The Hard Way, Release 2.0

 

elif next == "taunt bear" and bear_moved:
dead("The bear gets pissed off and chews your leg off.")
elif next == "open door" and bear_moved:

gold_room()
else:

print "I got no idea what that means."

def cthulu_room():

print "Here you see the gréat evil Cthulu."
print "He, it, whatever stares at you and you go insane."
print "Do you flee for your life or eat your head?"

next = raw_input("> ")

if "flee" in next:
start ()
elif "head" in next:

dead("Well that was tasty!")

else:
cthulu_room()

def dead(why):
print why, "Good job!"
exit (0)

def start():
print "You are in a dark room."

print "There is a door to your right and left."

print "Which one do you take?"
next = raw_input("> ")

if next == "left":
bear_room()

elif next == "right":
cthulu_room()

else:

dead("You stumble around the room until you starve.")

start ()

 

106

Exercise 35: Branches and Functions

Learn Python The Hard Way, Release 2.0

 

What You Should See

Here’s me playing the game:

S python ex35.py

You are in a dark room.

There is a door to your right and left.

Which one do you take?

> left

There is a bear here.

The bear has a bunch of honey.

The fat bear is in front of another door.

How are you going to move the bear?

> taunt bear

The bear has moved from the door. You can go through it now.
> open door

This room is full of gold. How much do you take?
> asf

Man, learn to type a number. Good job!

$

Extra Credit

1. Draw a map of the game and how you flow through it.
Fix all of your mistakes, including spelling mistakes.
Write comments for the functions you do not understand. Remember doc comments?

Add more to the game. What can you do to both simplify and expand it.

ae Bb

The gold_room has a weird way of getting you to type a number. What are all the bugs in this
way of doing it? Can you make it better than just checking if “1” or “0” are in the number? Look
at how int () works for clues.

 

What You Should See 107

Learn Python The Hard Way, Release 2.0

 

 

108 Exercise 35: Branches and Functions

Exercise 36: Designing and Debugging

Now that you know if-statements, I’m going to give you some rules for for-—loops and
while-loops that will keep you out of trouble. [’m also going to give you some tips on debug-
ging so that you can figure out problems with your program. Finally, you are going to design a similar
little game as in the last exercise but with a slight twist.

Rules For If-Statements

1. Every if-statement must have an else.

2. If this else should never be run because it doesn’t make sense, then you must use a die function
in the else that prints out an error message and dies, just like we did in the last exercise. This will
find many errors.

3. Never nest if-statements more than 2 deep and always try to do them | deep. This means if
you put an if in an if then you should be looking to move that second if into another function.

4. Treat if-statements like paragraphs, where each if, elif, else grouping is like a set of
sentences. Put blank lines before and after.

5. Your boolean tests should be simple. If they are complex, move their calculations to variables
earlier in your function and use a good name for the variable.

If you follow these simple rules, you will start writing better code than most programmers. Go back to
the last exercise and see if I followed all of these rules. If not, fix it.

Warning: Never be a slave to the rules in real life. For training purposes you need to follow these
rules to make your mind strong, but in real life sometimes these rules are just stupid. If you think a

tule is stupid, try not using it.

 

 

109

Learn Python The Hard Way, Release 2.0

 

Rules For Loops

1. Use a while-loop only to loop forever, and that means probably never. This only applies to
Python, other languages are different.

2. Use a for—loop for all other kinds of looping, especially if there is a fixed or limited number of
things to loop over.

Tips For Debugging

1. Do not use a “debugger”. A debugger is like doing a full-body scan on a sick person. You do not
get any specific useful information, and you find a whole lot of information that doesn’t help and
is just confusing.

2. The best way to debug a program is to use print to print out the values of variables at points in
the program to see where they go wrong.

3. Make sure parts of your programs work as you work on them. Do not write massive files of code
before you try to run them. Code a little, run a little, fix a little.

Homework

Now write a similar game to the one that I created in the last exercise. It can be any kind of game you
want in the same flavor. Spend a week on it making it as interesting as possible. For extra credit, use
lists, functions, and modules (remember those from Ex. 13?) as much as possible, and find as many new
pieces of Python as you can to make the game work.

There is one catch though, write up your idea for the game first. Before you start coding you must write
up a map for your game. Create the rooms, monsters, and traps that the player must go through on paper
before you code.

Once you have your map, try to code it up. If you find problems with the map then adjust it and make the
code match.

One final word of advice: Every programmer becomes paralyzed by irrational fear starting a new large
project. They then use procrastination to avoid confronting this fear and end up not getting their program
working or even started. I do this. Everyone does this. The best way to avoid this is to make a list of
things you should do, and then do them one at a time.

Just start doing it, do a small version, make it bigger, keep a list of things to do, and do them.

 

110 Exercise 36: Designing and Debugging

Exercise 37: Symbol Review

It’s time to review the symbols and Python words you know, and to try to pick up a few more for the next
few lessons. What I’ve done here is written out all the Python symbols and keywords that are important
to know.

In this lesson take each keyword, and first try to write out what it does from memory. Next, search online
for it and see what it really does. It may be hard because some of these are going to be impossible to
search for, but keep trying.

If you get one of these wrong from memory, write up an index card with the correct definition and try to
“correct” your memory. If you just didn’t know about it, write it down, and save it for later.

Finally, use each of these in a small Python program, or as many as you can get done. The key here is to
find out what the symbol does, make sure you got it right, correct it if you do not, then use it to lock it in.

Keywords

* and

* del

¢ from
* not

e while
* as

e elif

 

* global
* or

e with

assert

 

111

Learn Python The Hard Way, Release 2.0

 

* else

e if

* pass

° yield

° break

* except
¢ import
* print

* class

* exec
ein

* raise

* continue
* finally
°is

* return
* def

° for

¢ lambda

© Gry

Data Types

For data types, write out what makes up each one. For example, with strings write out how you create a
string. For numbers write out a few numbers.

¢ True

° False

¢ None

* strings
* numbers

e floats

 

112 Exercise 37: Symbol Review

Learn Python The Hard Way, Release 2.0

 

e lists

String Escapes Sequences

For string escape sequences, use them in strings to make sure they do what you think they do.
o VA
e\l
\\"
°\a
° \b
° \f
e\n
e\r
°\t
e \v

String Formats

Same thing for string formats: use them in some strings to know what they do.

ole
oO.

ole
Kh

e e e
ole \ ole \
x x G Oo

e
ole
)

e
ole
OI

 

e e e e
ole rs ole ole
Q Q yooh

 

String Escapes Sequences 113

Learn Python The Hard Way, Release 2.0

 

Operators

oe ole ole
n 6 Q

ol?
ol?

Some of these may be unfamiliar to you, but look them up anyway. Find out what they do, and if you still

can’t figure it out, save it for later.

+

*

 

114

Exercise 37: Symbol Review

Learn Python The Hard Way, Release 2.0

 

Spend about a week on this, but if you finish faster that’s great. The point is to try to get coverage on all
these symbols and make sure they are locked in your head. What’s also important is to find out what you
do not know so you can fix it later.

 

Operators 115

Learn Python The Hard Way, Release 2.0

 

 

116 Exercise 37: Symbol Review

Exercise 38: Reading Code

Now go find some Python code to read. You should be reading any Python code you can and trying to
steal ideas that you find. You actually should have enough knowledge to be able to read, but maybe not
understand what the code does. What I’m going to teach you in this lesson is how to apply things you
have learned to understand other people’s code.

First, print out the code you want to understand. Yes, print it out, because your eyes and brain are more
used to reading paper than computer screens. Make sure you only print a few pages at a time.

Second, go through your printout and take notes of the following:
1. Functions and what they do.
2. Where each variable is first given a value.
3. Any variables with the same names in different parts of the program. These may be trouble later.
4. Any if-statements without else clauses. Are they right?
5. Any while-loops that might not end.
6. Finally, any parts of code that you can’t understand for whatever reason.

Third, once you have all of this marked up, try to explain it to yourself by writing comments as you go.
Explain the functions, how they are used, what variables are involved, anything you can to figure this
code out.

Lastly, on all of the difficult parts, trace the values of each variable line by line, function by function. In
fact, do another printout and write in the margin the value of each variable that you need to “trace”.

Once you have a good idea of what the code does, go back to the computer and read it again to see if you
find new things. Keep finding more code and doing this until you do not need the printouts anymore.

Extra Credit

1. Find out what a “flow chart” is and write a few.

2. If you find errors in code you are reading, try to fix them and send the author your changes.

 

117

Learn Python The Hard Way, Release 2.0

 

3. Another technique for when you are not using paper is to put # comments with your notes in the
code. Sometimes, these could become the actual comments to help the next person.

 

118 Exercise 38: Reading Code

Exercise 39: Doing Things To Lists

You have learned about lists. When you learned about while-lLoops you “appended” numbers to the
end of a list and printed them out. There was also extra credit where you were supposed to find all the
other things you can do to lists in the Python documentation. That was a while back, so go find in the
book where you did that and review if you do not know what I’m talking about.

Found it? Remember it? Good. When you did this you had a list, and you “called” the function append
on it. However, you may not really understand what’s going on so let’s see what we can do to lists, and
how doing things with “on” them works.

When you type Python code that reads mystuff.append(’ hello’) you are actually setting off a
chain of events inside Python to cause something to happen to the myst uf f list. Here’s how it works:

1. Python sees you mentioned mystuff and looks up that variable. It might have to look backwards
to see if you created with =, look and see if it is a function argument, or maybe it’s a global variable.
Either way it has to find the mystuff first.

2. Once it finds mystuff it then hits the . (period) operator and starts to look at variables that are
a part of mystuff. Since mystuf f is a list, it knows that mystuff has a bunch of functions.

3. It then hits append and compares the name “append” to all the ones that mystuff says it owns.
If append is in there (it is) then it grabs that to use.

4. Next Python sees the ( (parenthesis) and realizes, “Oh hey, this should be a function.” At this point
it calls (aka runs, executes) the function just like normally, but instead it calls the function with an
extra argument.

5. That extra argument is ... mystuff! I know, weird right? But that’s how Python works so it’s
best to just remember it and assume that’s alright. What happens then, at the end of all this is a
function call that looks like: append (mystuff, ‘hello’ ) instead of what you read which
ismystuff.append(’hello’).

For the most part you do not have to know that this is going on, but it helps when you get error messages
from python like this:

S python
Python 2.6:5 (£2063, Apr 16 2010, :41)
[GCC 4.4.3] on linux2

 

119

Learn Python The Hard Way, Release 2.0

 

Type "help", "copyright", "credits" or "license" for more information.
>>> class Thing(object):
def test(hi):
print "hi"

>>> a = Thing()
>>> a.test ("hello")
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
TypeError: test() takes exactly 1 argument (2 given)
Sa>

 

What was all that? Well, this is me typing into the Python shell and showing you some magic. You
haven’t seen class yet but we’ll get into those later. For now you see how Python said test ()
takes exactly 1 argument (2 given). If you see this it means that python changed
a.test ("hello") totest (a, "hello") and that somewhere someone messed up and didn’t
add the argument for a.

 

That might be a lot to take in, but we’re going to spend a few exercises getting this concept firm in your
brain. To kick things off, here’s an exercise that mixes strings and lists for all kinds of fun.

ten_things = "Apples Oranges Crows Telephone Light Sugar"
print "Wait there's net 10 things in that. 11st, let"s fix that."

stuff = ten_things.split(' ')
more_stuff = ["Day", "Night", "Song", “Frisbee”, "Cern", "Banana", "Girl", "Boy"™]

while len(stuff) != 10:
next_one = more_stuff.pop()
print "Adding: ", next_one

stuff.append(next_one)
print "There's %d items now." % len(stuff)

print "There we go: ", stuff
print "Let's do some things with stuff."

print stuff[1]

print stuff[-1] # whoa! fancy

print stuff.pop()

print ' '.join(stuff) # what? cool!

print '#'.join(stuff[3:5]) # super stellar!

 

120 Exercise 39: Doing Things To Lists

Learn Python The Hard Way, Release 2.0

 

What You Should See

S python ex39.py

Wait there's not 10 things in that list, let's fix that.

Adding: Boy

There's 7 items now.

Adding: Girl

There's 8 items now.

Adding: Banana

There's 9 items now.

Adding: Corn

There's 10 items now.

There we go: ['Apples', 'Oranges', 'Crows', 'Telephone', 'Light',
"Boy', 'Girl', 'Banana', 'Corn']

Let's do some things with stuff.

Oranges

Corn

Corn

Apples Oranges Crows Telephone Light Sugar Boy Girl Banana

Telephone#Light

Extra Credit

"Sugar',

1. Take each function that is called, and go through the steps outlined above to translate them to what

Python does. For example,’ ’.join(things) isjoin(’ ’, things).

2. Translate these two ways to view the function calls in English. For example, ’
’ ,join(things) reads as, “Join things with ‘ ‘ between them.’ Meanwhile, join(’ ',

things) means, “Call join with ‘ ‘ and things.” Understand how they are really the same thing.

. Go read about “Object Oriented Programming” online. Confused? Yeah I was too. Do not worry.
You will learn enough to be dangerous, and you can slowly learn more later.

. Read up on what a “class” is in Python. Do not read about how other languages use the word
“class”. That will only mess you up.

. What’s the relationship between dir (something) and the “class” of something?

. If you do not have any idea what I’m talking about do not worry. Programmers like to feel smart
so they invented Object Oriented Programming, named it OOP, and then used it way too much. If
you think that’s hard, you should try to use “functional programming”.

 

What You Should See 121

Learn Python The Hard Way, Release 2.0

 

 

122 Exercise 39: Doing Things To Lists

Exercise 40: Dictionaries, Oh Lovely
Dictionaries

Now I have to hurt you with another container you can use, because once you learn this container a
massive world of ultra-cool will be yours. It is the most useful container ever: the dictionary.

Python calls them “dicts”, other languages call them, “hashes”. I tend to use both names, but it doesn’t
matter. What does matter is what they do when compared to lists. You see, a list lets you do this:

>>> things = ["a'; “bt; “e'; “d']
>>> print things[1]

b

>>> things[1] = 'z'

>>> print things[1]

Z

>>> print things

['a', "Zi", "e', 'd'J

>>>

You can use numbers to “index” into a list, meaning you can use numbers to find out what’s in lists. You
should know this by now, but what a dict does is let you use anything, not just numbers. Yes, a dict
associates one thing to another, no matter what it is. Take a look:

>>> stuff = {'name': 'Zed', 'age': 36, 'height': 6*12+2}
>>> print stuff['name']

Zed

>>> print stuff['age']

36

>>> print stuff['height']

74

>>> stuff[eity!] = “San. Francisco"

>>> print stuff['city']
San Francisco
>>>

You will see that instead of just numbers we’re using strings to say what we want from the stuff
dictionary. We can also put new things into the dictionary with strings. It doesn’t have to be strings

 

123

Learn Python The Hard Way, Release 2.0

 

though, we can also do this:

>>> stuff[1] = "Wow"

>>> stuff[2] = "Neato"

>>> print stuff[1]

Wow

>>> print stuff[2]

Neato

>>> print stuff

{'city': 'San Francisco', 2: 'Neato',
'name': 'Zed', 1: 'Wow', 'age': 36,
"height': 74}

>>>

In this one I just used numbers. I could use anything. Well almost but just pretend you can use anything
for now.

Of course, a dictionary that you can only put things in is pretty stupid, so here’s how you delete things,
with the del keyword:

>>> del stuff['city']

>>> del stuff[1]

>>> del stuff[2]

>>> stuff

{'name': 'Zed', 'age': 36, 'height': 74}
Sos

We’ ll now do an exercise that you must study very carefully. I want you to type this exercise in and try to
understand what’s going on. It is a very interesting exercise that will hopefully make a big light turn on
in your head very soon.

eltzles = {"CA's "San Francisco", "MI": "Detxvoeit",
'FL': 'Jacksonville'}

cities['NY'] = 'New York'

cities['OR'] = 'Portland'

def find_city(themap, state):
if state in themap:
return themap[state]
else:
return "Not found."

# ok pay attention!
cities['_find'] = find_city

while True:
print "State? (ENTER to quit)",
state = raw_input("> ")

 

 

 

124 Exercise 40: Dictionaries, Oh Lovely Dictionaries

20

21

22

23

24

Learn Python The Hard Way, Release 2.0

 

if not state: break

# this line is the most important ever! study!
city_found = cities['_find'] (cities, state)

print city_found

Warning: Notice how I use themap instead of map? That’s because Python has a function called

map, so if you try to use that you can have problems later.

 

What You Should See

S python ex40.py

State? (ENTER to quit) > CA
San Francisco

State? (ENTER to quit) > FL
Jacksonville

State? (ENTER to quit) > O

Not found.

State? (ENTER to quit) > OR
Portland

State? (ENTER to quit) > VT
Not found.

State? (ENTER to quit) >

Extra Credit

1. Go find the Python documentation for dictionaries (a.k.a. dicts, dict) and try to do even more things
to them.

2. Find out what you can’t do with dictionaries. A big one is that they do not have order, so try
playing with that.

3. Try doing a for—loop over them, and then try the items () function ina for—loop.

 

What You Should See 125

Learn Python The Hard Way, Release 2.0

 

 

126 Exercise 40: Dictionaries, Oh Lovely Dictionaries

Exercise 41: Gothons From Planet
Percal #25

Did you figure out the secret of the function in the dict from the last exercise? Can you explain it to
yourself? [ll explain it and you can compare your explanation with mine. Here are the lines of code we
are talking about:

cities['_find'] = find_city

city_found = cities['_find'] (cities, state)

Remember that functions can be variables too. The def find_city just makes another variable
name in your current module that you can use anywhere. In this code first we are putting the function
find_city into the dict cities as ’_find’. This is the same as all the others where we set states
to some cities, but in this case it’s actually the function.

Alright, so once we know that find_city is in the dict at __f ind, that means we can do work with it.
The 2nd line of code (used later in the previous exercise) can be broken down like this:

1. Python sees cit y_found = and knows we want to make a new variable.
2. It then reads cities and finds that variable, it’s a dict.

3. Then there’s [’_find’] which will index into the cities dict and pull out whatever is at
_ find.

4. What is at [’_find’ ] is our function find_city so Python then knows it’s got a function, and
when it hits ( it does the function call.

5. The parameters cities, state are passed to this function find_city, and it runs because
it’s called.

6. find_city then tries to look up states inside cities, and returns what it finds or a message
saying it didn’t find anything.

7. Python takes what find_city returned, and finally that is what is assigned to cit y_found all
the way at the beginning.

Here’s a trick. Sometimes these things read better in English if you read the code backwards. This is how
I would do it for that same line (remember backwards):

 

127

Learn Python The Hard Way, Release 2.0

1. state and city are...
2. passed as parameters to...
3. a function at...
4. '_ find’ inside...
5. the dict cities...
6. and finally assigned to cit y_found.
Here’s another way to read it, this time “inside-out”.
1. Find the center item of the expression, in this case [’_find’].
2. Go counter-clock-wise and you have a dict cities, so this finds the element _ find in cities.
3. That gives us a function. Keep going counter-clock-wise and you get to the parameters.
4. The parameters are passed to the function, and that returns a result. Go counter-clock-wise again.
5. Finally, we are at the city_found = assignment, and we have our end result.

After decades of programming I don’t even think about these three ways to read code. I just glance at it
and know what it means. I can even glance at a whole screen of code, and all the bugs and errors jump
out at me. That took an incredibly long time and quite a bit more study than is sane. To get that way, I
learned these three ways of reading most any programming language:

1. Front to back.
2. Back to front.
3. Counter-clock-wise.
Try them out when you have a difficult statement to figure out.
Now type in your next exercise, then go over it. This one is gonna be fun.

1 £rom sys import exit
2 from random import randint

4 def death():

5 quips = ["You died. You kinda suck at this.",

6 "Nice job, you died ...jackass.",

7 "Such a luser..",

8 "TI have a small puppy that's better at this."]
9

10 print quips[randint(0, len(quips)-1)]

ul exit (1)

4 def central_corridor():
15 print "The Gothons of Planet Percal #25 have invaded your ship and destroyed"
16 print "your entire crew. You are the last surviving member and your last"

 

128 Exercise 41: Gothons From Planet Percal #25

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

45

47

48

49

50

51

52

53

54

55

56

57

58

59

60

61

62

63

Learn Python The Hard Way, Release 2.0

 

def

costume"

print "mission is to get the neutron destruct bomb from the Weapons Armory,"
print "put it in the bridge, and blow the ship up after getting into an "
print "escape pod."
print "\n"
print "You're running down the central corridor to the Weapons Armory when"
print "a Gothon jumps out, red scaly skin, dark grimy teeth, and evil clown
print "flowing around his hate filled body. He's blocking the door to the"
print "Armory and about to pull a weapon to blast you."
action = raw_input("> ")
if action == "shoot!":

print "Quick on the draw you yank out your blaster and fire it at the Gothon."

print "His clown costume is flowing and moving around his body, which throws"
print "off your aim. Your laser hits his costume but misses him entirely. This"
print "completely ruins his brand new costume his mother bought him, which"

print

print "you are dead. Then he eats you."
return 'death'

elif action == "dodge!":
print "Like a world class boxer you dodge, weave, slip and slide right"
print "as the Gothon's blaster cranks a laser past your head."
print "In the middle of your artful dodge your foot slips and you"
print "bang your head on the metal wall and pass out."
print "You wake up shortly after only to die as the Gothon stomps on"
print "your head and eats you."
return 'death'

elif action == "tell a joke":
print "Lucky for you they made you learn Gothon insults in the academy."
print "You tell the one Gothon joke you know:"

print "Lbhe zbgure vf fb sng, jura fur fvgf nebhaq gur ubhfr, fur fvgf nebhaq gur ubhfr."

print "The Gothon stops, tries not to laugh, then busts out laughing and can't move."

print "While he's laughing you run up and shoot him square in the head"
print "putting him down, then jump through the Weapon Armory door."
return 'laser_weapon_armory'

else:

print "DOES NOT COMPUTE!"
return 'central_corridor'

laser_weapon_armory():

print
print
print
print
print

"You do a dive roll into the Weapon Armory, crouch and scan the room"
"for more Gothons that might be hiding. It's dead quiet, too quiet."
"You stand up and run to the far side of the room and find the"
"neutron bomb in its container. There's a keypad lock on the box"
"and you need the code to get the bomb out. If you get the code"

 

129

"makes him fly into an insane rage and blast you repeatedly in the face until"

65

67
68
69
70
71
72
73
74
vi)
76
71
78
79
80
81
82
83
84
85
86
87
88
89
90
OL
92
93
94
95
96
97
98

Learn Python The Hard Way, Release 2.0

 

def

print "wrong 10 times then the lock closes forever and you can't"

print "get the bomb. The code is 3 digits."

code = "Sd%d%d" % (randint(1,9), randint(1,9), randint(1,9))

guess = raw_input ("[keypad]> ")

guesses = 0

while guess != code and guesses < 10:
print "BZZZZEDDD!"
guesses += 1
guess = raw_input("[keypad]> ")

if guess == code:
print "The container clicks open and the seal breaks, letting gas out."
print "You grab the neutron bomb and run as fast as you can to the"
print "bridge where you must place it in the right spot."
return 'the_bridge'

else:
print "The lock buzzes one last time and then you hear a sickening"
print "melting sound as the mechanism is fused together."
print "You decide to sit there, and finally the Gothons blow up the"
print "ship from their ship and you die."
return 'death'

the_bridge():

print "You
print "und
print "tak
print "clo
print "wea
print "arm

action =r

if action
print
print
print
print
print
print
return

elif actio
print
print
print
print

burst onto the Bridge with the netron destruct bomb"
er your arm and surprise 5 Gothons who are trying to"
e control of the ship. Each of them has an even uglier"
wn costume than the last. They haven't pulled their"
pons out yet, as they see the active bomb under your"
and don't want to set it off."

aw_input("> ")

== "throw the bomb":
"In a panic you throw the bomb at the group of Gothons"
"and make a leap for the door. Right as you drop it a"
"Gothon shoots you right in the back killing you."
"As you die you see another Gothon frantically try to disarm"
"the bomb. You die knowing they will probably blow up when"
"it goes off."

‘death'

n == "slowly place the bomb":

"You point your blaster at the bomb under your arm"

"and the Gothons put their hands up and start to sweat."
"You inch backward to the door, open it, and then carefully"
"place the bomb on the floor, pointing your blaster at it."

 

130

Exercise 41: Gothons From Planet Percal #25

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

145

146

147

148

149

150

151

153

154

155

156

157

158

159

160

Learn Python The Hard Way, Release 2.0

 

def

print "You then jump back through the door, punch the close button"
print "and blast the lock so the Gothons can't get out."
print "Now that the bomb is placed you run to the escape pod to"
print. "get. off this tin can.”
return 'escape_pod'
else:
print "DOES NOT COMPUTE!"
return "the_bridge"

escape_pod():
print "You rush through the ship desperately trying to make it to"
print "the escape pod before the whole ship explodes. It seems like"

print "hardly any Gothons are on the ship, so your run is clear of"
print "interference. You get to the chamber with the escape pods, and"
print "now need to pick one to take. Some of them could be damaged"
print "but you don't have time to look. There's 5 pods, which one"
print "do you take?"

good_pod = randint (1,5)
guess = raw_input("[pod #]> ")

if int(guess) != good_pod:
print "You jump into pod %s and hit the eject button." % guess
print "The pod escapes out into the void of space, then"
print "implodes as the hull ruptures, crushing your body"
print "into jam jelly."
return 'death'

else:
print "You jump into pod %s and hit the eject button." % guess
print "The pod easily slides out into space heading to"
print "the planet below. As it flies to the planet, you look"
print "back and see your ship implode then explode like a"
print "bright star, taking out the Gothon ship at the same"
print "time. You won!"

exit (0)
ROOMS = {
‘death': death,
"central_corridor': central_corridor,
'laser_weapon_armory': laser_weapon_armory,

def

'the_bridge': the_bridge,
"escape_pod': escape_pod

runner(map, start):

 

131

161

162

163

165

166

167

168

Learn Python The Hard Way, Release 2.0

 

next = start

while True:
room = map[next]

print "\a-————— "
next = room()
runner (ROOMS, 'central_corridor')

It’s a lot of code, but go through it, make sure it works, play it.

What You Should See

Here’s me playing the game.

S python ex/ex4l.py

The Gothons of Planet Percal #25 have invaded your ship and destroyed
your entire crew. You are the last surviving member and your last
mission is to get the neutron destruct bomb from the Weapons Armory,
put it in the bridge, and blow the ship up after getting into an
escape pod.

You're running down the central corridor to the Weapons Armory when
a Gothon jumps out, red scaly skin, dark grimy teeth, and evil clown costume
flowing around his hate filled body. He's blocking the door to the
Armory and about to pull a weapon to blast you.

> dodge!

Like a world class boxer you dodge, weave, slip and slide right

as the Gothon's blaster cranks a laser past your head.

In the middle of your artful dodge your foot slips and you

bang your head on the metal wall and pass out.

You wake up shortly after only to die as the Gothon stomps on

your head and eats you.

Such a luser.
learnpythehardway $ python ex/ex4l.py

The Gothons of Planet Percal #25 have invaded your ship and destroyed
your entire crew. You are the last surviving member and your last
mission is to get the neutron destruct bomb from the Weapons Armory,
put it in the bridge, and blow the ship up after getting into an
escape pod.

 

132 Exercise 41: Gothons From Planet Percal #25

Learn Python The Hard Way, Release 2.0

 

You're running down the central corridor to the Weapons Armory when

a Gothon jumps out, red scaly skin, dark grimy teeth, and evil clown costume
flowing around his hate filled body. He's blocking the door to the

Armory and about to pull a weapon to blast you.

> tell a joke

Lucky for you they made you learn Gothon insults in the academy.

You tell the one Gothon joke you know:

Lbhe zbgure vf fb sng, jura fur fvgf nebhaq gur ubhfr, fur fvgf nebhag gur ubhfr.
The Gothon stops, tries not to laugh, then busts out laughing and can't move.
While he's laughing you run up and shoot him square in the head

putting him down, then jump through the Weapon Armory door.

You do a dive roll into the Weapon Armory, crouch and scan the room
for more Gothons that might be hiding. It's dead quiet, too quiet.
You stand up and run to the far side of the room and find the
neutron bomb in its container. There's a keypad lock on the box
and you need the code to get the bomb out. If you get the code
wrong 10 times then the lock closes forever and you can't

get the bomb. The code is 3 digits.

[keypad]> 123

BZZZZEDDD!

[keypad]> 234

BZZZZEDDD!

[keypad]> 345

BZZZZEDDD!

[keypad]> 456

BZZZZEDDD!

[keypad]> 567

BZZZZEDDD!

[keypad]> 678

BZZZZEDDD!

[keypad]> 789

BZZZZEDDD!

[keypad]> 384

BZZZZEDDD!

[keypad]> 764

BZZZZEDDD!

[keypad]> 354

BZZZZEDDD!

[keypad]> 263

The lock buzzes one last time and then you hear a sickening
melting sound as the mechanism is fused together.

You decide to sit there, and finally the Gothons blow up the
ship from their ship and you die.

 

 

What You Should See 133

Learn Python The Hard Way, Release 2.0

 

You died. You kinda suck at this.

Extra Credit

. Explain how returning the next room works.
. Add cheat codes to the game so you can get past the more difficult rooms.

. Instead of having each function print itself, learn about “doc string” style comments. Write the

room description as doc comments, and change the runner to print them.

. Once you have doc comments as the room description, do you need to have the function prompt

even? Have the runner prompt the user, and pass that in to each function. Your functions should
just be if-statements printing the result and returning the next room.

. This is actually a small version of something called a “finite state machine”. Read about them.

They might not make sense but try anyway.

 

134

Exercise 41: Gothons From Planet Percal #25

Exercise 42: Gothons Are Getting
Classy

While it’s fun to put functions inside of dictionaries, you’d think there’d be something in Python that does
this for you. There is: the class keyword. Using class is how you create an even more awesome “dict
with functions” than the one you made in the last exercise. Classes have all sorts of powerful features and
uses that I could never go into in this book. Instead, you’ll just use them like they’re fancy dictionaries
with functions.

A programming language that uses classes is called “Object Oriented Programming”. This is an old style
of programming where you make “things” and you “tell” those things to do work. You’ve been doing a
lot of this. A whole lot. You just didn’t know it. Remember when you were doing this:

stuff = ['Test', 'This', 'Out']

print ' '.join(stuff)

You were actually using classes. The variable stuff is actually a list class. The ’
’.join(stuff) is calling the join function of the string’ ’ (just an empty space) is also a class, a
string class. It’s all classes!

Well, and objects, but let’s just skip that word for now. You’ll learn what those are after you make some
classes. How do you make classes? Very similar to how you made the ROOMS dict, but easier:

class TheThing(object):

def init__(self):
self.number = 0

def some_function(self):
print "I got called.”

def add_me_up(self, more):
self.number += more
return self.number

# two different things
a = TheThing()

 

135

Learn Python The Hard Way, Release 2.0

 

b = TheThing()

a.some_function()
b.some_function ()

print a.add_me_up (20)
print a.add_me_up (20)
print b.add_me_up (30)
print b.add_me_up (30)

print a.number
print b.number

Warning: Alright, this is where you start learning about “warts”. Python is an old language with
lots of really ugly obnoxious pieces that were bad decisions. To cover up these bad decisions they
make new bad decisions and then yell at people to adopt the new bad decisions. The phrase class

TheThing (object) is an example of a bad decision. I won’t get into it right here, but don’t worry
about why your class has to have (object) after its name. Just always type it this way or other
Python programmers will yell at you. We’ll get into why later.

 

You see that self in the parameters? You know what that is? That’s right, it’s the “extra” parameter
that Python creates so you can type a.some_function() and then it will translate that to really
be some_function(a). Why use self? Your function has no idea what you are calling any one
“instance” of TheThing or another, you just use a generic name self. That way you can write your
function and it will always work.

You could actually use another name rather than se1f but then every Python programmer on the planet
would hate you, so don’t. Only jerks change things like that and I taught you better. Be nice to people
who have to read what you write because ten years later all code is horrible.

Next, see the __init__ function? That is how you set up a Python class with internal variables. You
can set them on self with the . (period) just like I'll show you here. See also how we then use this in
add_me_up () later which lets you add to the sel f.number you created. Later you can see how we
use this to add to our number and print it.

Classes are very powerful, so you should read everything you can about them and play with them. You
actually know how to use them, you just have to try it. In fact, I want to play some guitar right now so
I’m not going to give you an exercise to type. You’re going to write an exercise using classes.

Here’s how we’d do exercise 41 using classes instead of the thing we created:

from sys import exit
from random import randint

class Game(object):

def __ init__(self, start):
self.quips = [

 

136 Exercise 42: Gothons Are Getting Classy

20

21

22

23

24

22

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

45

46

47

48

49

50

51

52

53

54

55

Learn Python The Hard Way, Release 2.0

 

def

def

def

"You died. You kinda suck at this.",
"Your mom would be proud. If she were smarter.",
"Such. a luiser.”,
"T have a small puppy that's better at this."
]

self.start = start

play(self):
next = self.start

while True:

prank Sg o

room = getattr(self, next)

next = room()
death(self):
print self.quips[randint(0, len(self.quips)-1)]
exit (1)

central_corridor(self):

print "The Gothons of Planet Percal #25 have invaded your ship and destroyed"
print "your entire crew. You are the last surviving member and your last"
print "mission is to get the neutron destruct bomb from the Weapons Armory,"
print "put it in the bridge, and blow the ship up after getting into an "
print "escape pod."

print: "\n"

print "You're running down the central corridor to the Weapons Armory when"

print "a Gothon jumps out, red scaly skin, dark grimy teeth, and evil clown costume"

print "flowing around his hate filled body. He's blocking the door to the"
print "Armory and about to pull a weapon to blast you."

action = raw_input("> ")

if action == "sShoot!":

print "Quick on the draw you yank out your blaster and fire it at the Gothon."
print "His clown costume is flowing and moving around his body, which throws"

print "off your aim. Your laser hits his costume but misses him entirely.
print "completely ruins his brand new costume his mother bought him, which"

print "makes him fly into an insane rage and blast you repeatedly in the face until"

print "you are dead. Then he eats you."
return 'death'

elif action == "dodge!":
print "Like a world class boxer you dodge, weave, slip and slide right"
print "as the Gothon's blaster cranks a laser past your head."
print "In the middle of your artful dodge your foot slips and you"
print "bang your head on the metal wall and pass out."

 

137

56
57
58

59

61

62

63

65

67

68

69

70

7

2:

2B

74

75

76

71

78

79

80

81

82

83

84

85

86

87

88

89

OL

92

93

94

95

96

97

98

103

Learn Python The Hard Way, Release 2.0

 

 

 

print "You wake up shortly after only to die as the Gothon stomps on"
print "your head and eats you."
return 'death'
elif action == "tell a joke":
print "Lucky for you they made you learn Gothon insults in the academy."
print "You tell the one Gothon joke you know:"
print "Lbhe zbgure vf fb sng, jura fur fvgf nebhaq gur ubhfr, fur fvgf nebhaq <
print "The Gothon stops, tries not to laugh, then busts out laughing and can't
print "While he's laughing you run up and shoot him square in the head"
print "putting him down, then jump through the Weapon Armory door."
return 'laser_weapon_armory'
else:
print "DOES NOT COMPUTE!"
return 'central_corridor'

def laser_weapon_armory (self):

print "You
print "for
print "You

do a dive roll into the Weapon Armory, crouch and scan the room"
more Gothons that might be hiding. It's dead quiet, too quiet."
stand up and run to the far side of the room and find the"

print "neutron bomb in its container. There's a keypad lock on the box"

print "and

you need the code to get the bomb out. If you get the code"

print "wrong 10 times then the lock closes forever and you can't"

print "get the bomb. The code is 3 digits."

code = "Sd%d%sd" % (randint (1,9), randint(1,9), randint (1,9))

guess = raw_input("[keypad]> ")

guesses = 0

while guess != code and guesses < 10:
print "BZZZZEDDD!"
guesses += 1
guess = raw_input ("[keypad]> ")

if guess == code:
print "The container clicks open and the seal breaks, letting gas out."
print "You grab the neutron bomb and run as fast as you can to the"
print "bridge where you must place it in the right spot."
return 'the_bridge'

else:
print "The lock buzzes one last time and then you hear a sickening"
print "melting sound as the mechanism is fused together."
print "You decide to sit there, and finally the Gothons blow up the"
print "Ship from their ship and you die."
return 'death'

def the_bridge(self):

 

138

Exercise 42: Gothons Are Getting Classy

110

lll

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

129

130

131

132

133

134

135

136

137

138

139

140

141

142

143

145

146

147

148

149

150

151

Learn Python The Hard Way, Release 2.0

 

def

print "You burst onto the Bridge with the netron destruct bomb"
print "under your arm and surprise 5 Gothons who are trying to"
print "take control of the ship. Each of them has an even uglier"
print "clown costume than the last. They haven't pulled their"
print "weapons out yet, as they see the active bomb under your"
print "arm and don't want to set it off."

action = raw_input("> ")

if action == "throw the bomb":
print "In a panic you throw the bomb at the group of Gothons"
print "and make a leap for the door. Right as you drop it a"
print "Gothon shoots you right in the back killing you."
print "As you die you see another Gothon frantically try to disarm"
print "the bomb. You die knowing they will probably blow up when"
print "it goes off."
return 'death'

elif action == "slowly place the bomb":
print "You point your blaster at the bomb under your arm"
print "and the Gothons put their hands up and start to sweat."
print "You inch backward to the door, open it, and then carefully"
print "place the bomb on the floor, pointing your blaster at it."
print "You then jump back through the door, punch the close button"
print "and blast the lock so the Sothons can'’c get otc."
print "Now that the bomb is placed you run to the escape pod to"
print "get off this tin ean.”
return 'escape_pod'

else:
print. "DOES NOT COMPUTE!"
return "the_bridge"

escape_pod(self):
print "You rush through the ship desperately trying to make it to"
print "the escape pod before the whole ship explodes. It seems like"

print "hardly any Gothons are on the ship, so your run is clear of"
print "interference. You get to the chamber with the escape pods, and"
print "now need to pick one to take. Some of them could be damaged"
print "but you don't have time to look. There's 5 pods, which one"
print "do you take?"

good_pod = randint (1,5)
guess = raw_input("[pod #]> ")

if int(guess) != good_pod:
print "You jump into pod %s and hit the eject button." % guess
print "The pod escapes out into the void of space, then"

 

139

152

153

154

155

156

157

158

159

161

162

163

165

166

Learn Python The Hard Way, Release 2.0

 

print "implodes as the hull ruptures, crushing your body"
print "into jam jelly."
return 'death'

else:
print "You jump into pod %s and hit the eject button." % guess
print "The pod easily slides out into space heading to"
print "the planet below. As it flies to the planet, you look"
print "back and see your ship implode then explode like a"
print "bright star, taking out the Gothon ship at the same"
print "time. You wen!"
exit (0)

a_game = Game ("central_corridor")
a_game.play()

What You Should See

The output from this version of the game should be exactly the same as the previous version. In fact
you'll notice that some of the code is nearly the same. Compare this new version of the game with the
last one so you understand the changes that were made. Key things to really get are:

1.
2.

ww

= ok Oe

How you madeaclass Game (object) and put functions inside it.
How __init__ isa special intialization method that sets up important variables.

How you added functions to the class by indenting them so they were deeper under the class
keyword. This is important so study carefully how indentation creates the class structure.

How you indented again to put the contents of the functions under their names.
How colons are being used.

The concept of self and how it’s used in__init__, play, and death.

 

Go find out what getattr does inside play so that you understand what’s going on with the
operation of play. In fact, try doing this by hand inside Python to really get it.

How a Game was created at the end and then told to play () and how that got everything started.

Extra Credit

1.
2.

Find out what the __dict___ is and figure out how to get at it.

Add some rooms to make sure you know how to work with a class.

 

140

Exercise 42: Gothons Are Getting Classy

Learn Python The Hard Way, Release 2.0

 

 

3. Create a two-class version of this, where one is the Map and the other is the Engine. Hint: play
goes in the Engine.

 

 

Extra Credit 141

Learn Python The Hard Way, Release 2.0

 

 

142 Exercise 42: Gothons Are Getting Classy

Exercise 43: You Make A Game

You need to start learning to feed yourself. Hopefully as you have worked through this book, you have
learned that all the information you need is on the internet, you just have to go search for it. The only
thing you have been missing are the right words and what to look for when you search. Now you should
have a sense of it, so it’s about time you struggled through a big project and tried to get it working.

Here are your requirements:
1. Make a different game from the one I made.
2. Use more than one file, and use import to use them. Make sure you know what that is.

3. Use one class per room and give the classes names that fit their purpose. Like GoldRoom,
KoiPondRoom.

4. Your runner will need to know about these rooms, so make a class that runs them and knows about
them. There’s plenty of ways to do this, but consider having each room return what room is next
or setting a variable of what room is next.

Other than that I leave it to you. Spend a whole week on this and make it the best game you can. Use
classes, functions, dicts, lists anything you can to make it nice. The purpose of this lesson is to teach you
how to structure classes that need other classes inside other files.

Remember, I’m not telling you exactly how to do this because you have to do this yourself. Go figure
it out. Programming is problem solving, and that means trying things, experimenting, failing, scrapping
your work, and trying again. When you get stuck, ask for help and show people your code. If they are
mean to you, ignore them, focus on the people who are not mean and offer to help. Keep working it and
cleaning it until it’s good, then show it some more.

Good luck, and see you in a week with your game.

 

143

Learn Python The Hard Way, Release 2.0

 

 

144 Exercise 43: You Make A Game

Exercise 44: Evaluating Your Game

In this exercise you will evaluate the game you just made. Maybe you got part-way through it and you
got stuck. Maybe you got it working but just barely. Either way, we’re going to go through a bunch of
things you should know now and make sure you covered them in your game. We’re going to study how
to properly format a class, common conventions in using classes, and a lot of “textbook” knowledge.

Why would I have you try to do it yourself and then show you how to do it right? From now on in the
book I’m going to try to make you self-sufficient. ’'ve been holding your hand mostly this whole time,
and I can’t do that for much longer. I’m now instead going to give you things to do, have you do them on
your own, and then give you ways to improve what you did.

You will struggle at first and probably be very frustrated but stick with it and eventually you will build
a mind for solving problems. You will start to find creative solutions to problems rather than just copy
solutions out of textbooks.

Function Style

All the other rules ve taught you about how to make a function nice apply here, but add these things:

¢ For various reasons, programmers call functions that are part of classes methods. It’s mostly
marketing but just be warned that every time you say “function” they’ll annoyingly correct you and
say “method”. If they get too annoying, just ask them to demonstrate the mathematical basis that
determines how a “method” is different from a “function” and they’ ll shut up.

When you work with classes much of your time is spent talking about making the class “do things”.
Instead of naming your functions after what the function does, instead name it as if it’s a com-
mand you are giving to the class. Same as pop is saying “Hey list, pop this off.” It isn’t called
remove_from_end_of_list because even though that’s what it does, that’s not a command
to a list.

Keep your functions small and simple. For some reason when people start learning about classes
they forget this.

 

145

Learn Python The Hard Way, Release 2.0

 

Class Style

Your class should use “camel case” like SuperGoldFactory rather than
super_gold_factory.

Try not to do too much in your__init___ functions. It makes them harder to use.

Your other functions should use “underscore format” so write my_awesome_hair and not
myawesomehair or MyAwesomeHair.

Be consistent in how you organize your function arguments. If your class has to deal with users,
dogs, and cats, keep that order throughout unless it really doesn’t make sense. If you have one
function takes (dog, cat, user) andthe other takes (user, cat, dog), it'll be hard to
use.

Try not to use variables that come from the module or globals. They should be fairly self-contained.

A foolish consistency is the hobgoblin of little minds. Consistency is good, but foolishly following
some idiotic mantra because everyone else does is bad style. Think for yourself.

Always, always have class Name (object) format or else you will be in big trouble.

Code Style

Give your code vertical space so people can read it. You will find some very bad programmers
who are able to write reasonable code, but who do not add any spaces. This is bad style in any
language because the human eye and brain use space and vertical alignment to scan and separate
visual elements. Not having space is the same as giving your code an awesome camouflage paint
job.

If you can’t read it out loud, it’s probably hard to read. If you are having a problem making
something easy to use, try reading it out loud. Not only does this force you to slow down and really
read it, but it also helps you find difficult passages and things to change for readability.

Try to do what other people are doing in Python until you find your own style.

Once you find your own style, do not be a jerk about it. Working with other people’s code is part
of being a programmer, and other people have really bad taste. Trust me, you will probably have
really bad taste too and not even realize it.

If you find someone who writes code in a style you like, try writing something that mimics their
style.

 

146

Exercise 44: Evaluating Your Game

Learn Python The Hard Way, Release 2.0

 

Good Comments

There are programmers who will tell you that your code should be readable enough that you do not
need comments. They’ll then tell you in their most official sounding voice that, “Ergo you should
never write comments.” Those programmers are either consultants who get paid more if other
people can’t use their code, or incompetents who tend to never work with other people. Ignore
them and write comments.

When you write comments, describe why you are doing what you are doing. The code already says
how, but why you did things the way you did is more important.

When you write doc comments for your functions , make the comments documentation for some-
one who will have to use your code. You do not have to go crazy, but a nice little sentence about
what someone does with that function helps a lot.

Finally, while comments are good, too many are bad, and you have to maintain them. Keep your
comments relatively short and to the point, and if you change a function, review the comment to
make sure it’s still correct.

Evaluate Your Game

I want you now to pretend you are me. Adopt a very stern look, print out your code, and take a red pen
and mark every mistake you find. Anything from this exercise and from other things you have known.
Once you are done marking your code up, I want you to fix everything you came up with. Then repeat
this a couple of times, looking for anything that could be better. Use all the tricks P’ve given you to break
your code down into the smallest tiniest little analysis you can.

The purpose of this exercise is to train your attention to detail on classes. Once you are done with this
bit of code, find someone else’s code and do the same thing. Go through a printed copy of some part of
it and point out all the mistakes and style errors you find. Then fix it and see if your fixes can be done
without breaking their program.

I want you to do nothing but evaluate and fix code for the week. Your own code and other people’s. It'll
be pretty hard work, but when you are done your brain will be wired tight like a boxer’s hands.

 

Good Comments 147

Learn Python The Hard Way, Release 2.0

 

 

148 Exercise 44: Evaluating Your Game

Exercise 45: Is-A, Has-A, Objects, and
Classes

An important concept that you have to understand is the difference between a Class and an Object.
The problem is, there is no real “difference” between a class and an object. They are actually the same
thing at different points in time. I will demonstrate by a Zen koan:

What is the difference between a Fish and a Salmon?

Did that question sort of confuse you? Really sit down and think about it for a minute. I mean, a Fish
and a Salmon are different but, wait, they are the same thing right? A Salmon is a kind of Fish, so I mean
it’s not different. But at the same time, becase a Salmon is a particular type of Fish and so it’s actually
different from all other Fish. That’s what makes it a Salmon and not a Halibut. So a Salmon and a Fish
are the same but different. Weird.

This question is confusing because most people do not think about real things this way, but they intuitively
understand them. You do not need to think about the difference between a Fish and a Salmon because
you know how they are related. You know a Salmon is a kind of Fish and that there are other kinds of
Fish without having to understand that.

Let’s take it one step further, let’s say you have a bucket full of 3 Salmon and because you are a nice
person, you have decided to name them Frank, Joe, and Mary. Now, think about this question:

What is the difference between Mary and a Salmon?

Again this is a weird question, but it’s a bit easier than the Fish vs. Salmon question. You know that Mary
is a Salmon, and so she’s not really different. She’s just a specific “instance” of a Salmon. Joe and Frank
are also instances of Salmon. But, what do I mean when I say instance? I mean they were created from
some other Salmon and now represent a real thing that has Salmon-like attributes.

Now for the mind bending idea: Fish is a Class, and Salmon is a Class, and Mary is an Object.
Think about that for a second. Alright let’s break it down real slow and see if you get it.

A Fish is a Class, meaning it’s not a real thing, but rather a word we attach to instances of things with
similar attributes. Got fins? Got gills? Lives in water? Alright it’s probably a Fish.

Someone with a Ph.D. then comes along and says, “No my young friend, this Fish is actually Salmo
salar, affectionately known as a Salmon.” This professor has just clarified the Fish further and made a

 

149

Learn Python The Hard Way, Release 2.0

 

new Class called “Salmon” that has more specific attributes. Longer nose, reddish flesh, big, lives in
the ocean or fresh water, tasty? Ok, probably a Salmon.

Finally, a cook comes along and tells the Ph.D., “No, you see this Salmon right here, I'll call her Mary
and I’m going to make a tasty fillet out of her with a nice sauce.” Now you have this instance of a Salmon
(which also is an instance of a Fish) named Mary turned into something real that is filling your belly. It
has become an Object.

There you have it: Mary is a kind of Salmon that is a kind of Fish. Object isa Class isaClass.

How This Looks In Code

This is a weird concept, but to be very honest you only have to worry about it when you make new
classes, and when you use a class. I will show you two tricks to help you figure out whether something is
aClass or Object.

First, you need to learn two catch phrases “is-a” and “has-a”. You use the phrase is-a when you talk about
objects and classes being related to each other by a class relationship. You use has-a when you talk about
objects and classes that are related only because they reference each other.

Now, go through this piece of code and replace each ##?? comment with a replacement comment that
says whether the next line represents an is—a or a has~a relationship, and what that relationship is. In
the beginning of the code, I’ve laid out a few examples, so you just have to write the remaining ones.

Remember, is-a is the relationship between Fish and Salmon, while has-a is the relationship between
Salmon and Gills.

## Animal is-a object (yes, sort of confusing) look at the extra credit
class Animal(object):
pass

## ?°?
class Dog(Animal):

def init__(self, name):
## ??

self.name = name

## ?°?
class Cat (Animal):

def init__(self, name):
## ??

self.name = name

## ??
class Person(object):

 

150 Exercise 45: Is-A, Has-A, Objects, and Classes

Learn Python The Hard Way, Release 2.0

 

2 def __ init__(self, name):

2B ## ??

24 self.name = name

25

26 ## Person has-a pet of some kind
21 self.pet = None

28

2 Of OP?

 

30 Class Employee (Person):

31

 

32 def __init__(self, name, salary):

3 ## 2? hmm what is this strange magic?
34 super (Employee, self) .__init__ (name)
35 #H ??

36 self.salary = salary

37

33H OP?

3 Class Fish(object):

40 pass

41

a ## 2?

4 Class Salmon(Fish):

44 pass

45

4 «4## 2°?

4 Class Halibut (Fish):

48 pass

49

50.

51 ## rover is-a Dog

5s. rover = Dog("Rover")
53

54 oH OP?

s satan = Cat ("Satan")
56

5s. ## ??

ss mary = Person("Mary")

59

6 86 fF OP?

6. mary.pet = satan
62

63 86 2?

6 frank = Employee("Frank", 120000)

65

 

6 ## 2?

o frank.pet = rover
68

0 8«60## 2?

 

How This Looks In Code 151

70

71

72

2B

75

76

Learn Python The Hard Way, Release 2.0

 

flipper = Fish()

## 2?
crouse = Salmon ()

## 2°?
harry = Halibut ()

About class Name(object)

Remember how I was yelling at you to always use class Name (object) andIcouldn’t tell you why?
Now I can tell you, because you just learned about the difference between a class and an object.
I couldn’t tell you until now because you would have just been confused and couldn’t learn to use the
technology.

What happened is Python’s original rendition of class was broken in many serious ways. By the time
they admitted the fault it was too late, and they had to support it. In order to fix the problem, they needed
some “new class” style so that the “old classes” would keep working but you could use the new more
correct version.

This is where “class is-a object” comes in. They decided that they would use the word “object”, lower-
cased, to be the “class” that you inherit from to make a class. Confusing right? A class inherits from the
class named object to make a class but it’s not an object really it’s a class, but do not forget to inherit
from object.

Exactly. The choice of one single word meant that I couldn’t teach you about this until now. Now you
can try to understand the concept of a class that is an object if you like.

However, I would suggest you do not. Just completely ignore the idea of old style vs. new style classes
and assume that Python always requires (object) when you make a class. Save your brain power for
something important.

Extra Credit

1. Research why Python added this strange ob ject class, and what that means.
2. Is it possible to use a Class like it’s an Ob ject?

3. Fill out the animals, fish, and people in this exercise with functions that make them do things. See
what happens when functions are in a “base class” like Animal vs. in say Dog.

4. Find other people’s code and work out all the is-a and has-a relationships.
5. Make some new relationships that are lists and dicts so you can also have “has-many” relationships.

6. Do you think there’s a such thing as a “is-many” relationship? Read about “multiple inheritance”,
then avoid it if you can.

 

152 Exercise 45: Is-A, Has-A, Objects, and Classes

Exercise 46: A Project Skeleton

This will be where you start learning how to setup a good project “skeleton” directory. This skeleton
directory will have all the basics you need to get a new project up and running. It will have your project
layout, automated tests, modules, and install scripts. When you go to make a new project, just copy this

directory to a new name and edit the files to get started.

Skeleton Contents: Linux/OSX

First, create the structure of your skeleton directory with these commands:

mkdir -p projects

cd projects/

mkdir skeleton

cd skeleton

mkdir bin NAME tests docs

MMM Ww

 

I use a directory named projects to store all the various things I’m working on. Inside that directory
I have my skeleton directory that I put the basis of my projects into. The directory NAME will be
renamed to whatever you are calling your project’s main module when you use the skeleton.

Next we need to setup some initial files:

S$ touch NAME/__init__.py
S touch tests/__init__.py

 

That creates empty Python module directories we can put our code in.

setup.py file we can use to install our project later if we want:

try:

from setuptools import setup
except ImportError:

from distutils.core import setup

config = {
‘"description': 'My Project',

Then we need to create a

 

153

Learn Python The Hard Way, Release 2.0

 

 

‘author': 'My Name',

‘url's "URL te get it at.",
"download_url': 'Where to download it.',
"author_email': 'My email.',

‘version’: “*Q.L',

‘install_requires': ['nose'],
"packages': ['NAME'],

"seripts’: [].,

"name': 'projectname'

setup (**config)

Edit this file so that it has your contact information and is ready to go for when you copy it.

 

Finally you will want a simple skeleton file for tests named test s/NAME_tests.py:

from nose.tools import *

import NAME

 

def setup():
print “SETUP!"

def teardown():
print "TEAR DOWN!"

 

def test_basic():
print "I RAN!"

Installing Python Packages

Make sure you have some packages installed that makes these things work. Here’s the problem though.
You are at a point where it’s difficult for me to help you do that and keep this book sane and clean. There
are sO many ways to install software on so many computers that I’d have to spend 10 pages walking you
through every step, and let me tell you I am a lazy guy.

Rather than tell you how to do it exactly, I’m going to tell you what you should install, and then tell you
to figure it out and get it working. This will be really good for you since it will open a whole world of
software you can use that other people have released to the world.

Next, install the following python packages:
1. pip from http://pypi.python.org/pypi/pip
2. distribute from http://pypi.python.org/pypi/distribute
3. nose from http://pypi.python.org/pypi/nose/
4. virtualenv from http://pypi.python.org/pypi/virtualenv

 

154 Exercise 46: A Project Skeleton

Learn Python The Hard Way, Release 2.0

 

Do not just download these packages and install them by hand. Instead see how other people recommend
you install these packages and use them for your particular system. The process will be different for most
versions of Linux, OSX, and definitely different for Windows.

I am warning you, this will be frustrating. In the business we call this “yak shaving”. Yak shaving is
any activity that is mind numblingly irritatingly boring and tedious that you have to do before you can
do something else that’s more fun. You want to create cool Python projects, but you can’t do that until
you setup a skeleton directory, but you can’t setup a skeleton directory until you install some packages,
but you can’t install packages until you install package installers, and you can’t install package installers
until you figure out how your system installs software in general, and so on.

Struggle through this anyway. Consider it your trial-by-annoyance to get into the programmer club. Every
programmer has to do these annoying tedious tasks before they can do something cool.

Testing Your Setup

After you get all that installed you should be able to do this:

S nosetests

 

Ran 1 test in 0.007s
OK

Pll explain what this nosetests thing is doing in the next exercise, but for now if you do not see that,
you probably got something wrong. Make sure you put__ init__.py files in your NAME and tests
directory and make sure you got tests/NAME_tests.py right.

 

 

Using The Skeleton

You are now done with most of your yak shaving. Whenever you want to start a new project, just do this:
1. Make a copy of your skeleton directory. Name it after your new project.

2. Rename (move) the NAME module to be the name of your project or whatever you want to call
your root module.

. Edit your setup.py to have all the information for your project.

 

. Rename tests/NAME_tests.py to also have your module name.

. Double check it’s all working using nosetests again.

nH nn HR W

. Start coding.

 

Testing Your Setup 155

Learn Python The Hard Way, Release 2.0

 

Required Quiz

This exercise doesn’t have extra credit but a quiz you should complete:

1.
2.

Read about how to use all of the things you installed.

Read about the setup.py file and all it has to offer. Warning, it is not a very well-written piece
of software, so it will be very strange to use.

. Make a project and start putting code into the module, then get the module working.

. Put a script in the bin directory that you can run. Read about how you can make a Python script

that’s runnable for your system.

. Mention the bin script you created in your setup.py so that it gets installed.

. Use your setup.py to install your own module and make sure it works, then use pip to uninstall

It.

 

156

Exercise 46: A Project Skeleton

Exercise 47: Automated Testing

Having to type commands into your game over and over to make sure it’s working is annoying. Wouldn’t
it be better to write little pieces of code that test your code? Then when you make a change, or add a new
thing to your program, you just “run your tests” and the tests make sure things are still working. These
automated tests won’t catch all your bugs, but they will cut down on the time you spend repeatedly typing
and running your code.

Every exercise after this one will not havea What You Should See section, but instead it will have
aWhat You Should Test section. You will be writing automated tests for all of your code starting
now, and this will hopefully make you an even better programmer.

I won’t try to explain why you should write automated tests. I will only say that, you are trying to
be a programmer, and programmers automate boring and tedious tasks. Testing a piece of software is
definitely boring and tedious, so you might as well write a little bit of code to do it for you.

That should be all the explanation you need because your reason for writing unit tests is to make your
brain stronger. You have gone through this book writing code to do things. Now you are going to take the
next leap and write code that knows about other code you have written. This process of writing a test that
runs some code you have written forces you to understand clearly what you have just written. It solidifies
in your brain exactly what it does and why it works and gives you a new level of attention to detail.

Writing A Test Case

We’re going to take a very simple piece of code and write one simple test. We’re going to base this little
test on a new project from your project skeleton.

First, make a ex47 project from your project skeleton. Make sure you do it right and rename the module
and get that first tests/ex47_tests.py test file going right. Also make sure nose runs this test file.
IMPORTANT make sure you also delete tests/skel_tests.pyc if it’s there.

Next, create a simple file ex47/game.py where you can put the code to test. This will be a very silly
little class that we want to test with this code in it:

class Room(object):

 

157

20

21

22

23

24

25

26

27

28

29

30

31

32

Learn Python The Hard Way, Release 2.0

 

def _ init__(self, name, description):
self.name = name
self.description = description
self.paths = {}

def go(self, direction):
return self.paths.get (direction, None)

def add_paths(self, paths):
self.paths.update (paths)

Once you have that file, change unit test skeleton to this:

from nose.tools import *
from ex47.game import Room

def

def

def

test_room():

gold = Room("GoldRoom",
"""This room has gold in it you can grab. There's a
door to the nerth, ™"")

assert_equal(gold.name, "GoldRoom")

assert_equal(gold.paths, {})

test_room_paths():

center = Room("Center", "Test room in the center.")
north = Room("North", "Test room in the north.")
south = Room("South", "Test room in the south.")

 

center.add_paths({'north': north, 'south': south})
assert_equal(center.go('north'), north)
assert_equal(center.go('south'), south)

test_map():

start = Room("Start", "You can go west and down a hole.")

west = Room("Trees", "There are trees here, you can go east.")
down = Room("Dungeon", "It's dark down here, you can go up.")

start.add_paths({'west': west, 'down': down})
west.add_paths({'east': start})
down.add_paths({'up': start})

assert_equal(start.go('west'), west)
assert_equal(start.go('west').go('east'), start)
assert_equal(start.go('down').go('up'), start)

This file imports the Room class you made in the ex47.game module so that you can do tests on it.
There are then a set of tests that are functions starting with test_. Inside each test case there’s a bit
of code that makes a Room or a set of Rooms, and then makes sure the rooms work the way you expect

 

158

Exercise 47: Automated Testing

Learn Python The Hard Way, Release 2.0

 

them to work. It tests out the basic room features, then the paths, then tries out a whole map.

The important functions here are assert_equal which makes sure that variables you have set or
paths you have built in a Room are actually what you think they are. If you get the wrong result, then
nosetests will print out an error message so you can go figure it out.

Testing Guidelines

Follow these general loose set of guidelines when making your tests:

1. Test files go in tests/ and are named BLAH_tests.py otherwise nosetests won’t run
them. This also keeps your tests from clashing with your other code.

2. Write one test file for each module you make.

3. Keep your test cases (functions) short, but do not worry if they are a bit messy. Test cases are
usually kind of messy.

4. Even though test cases are messy, try to keep them clean and remove any repetitive code you can.
Create helper functions that get rid of duplicate code. You will thank me later when you make a
change and then have to change your tests. Duplicated code will make changing your tests more
difficult.

5. Finally, do not get too attached to your tests. Sometimes, the best way to redesign something is to
just delete it, the tests, and start over.

What You Should See

~/projects/simplegame S$ nosetests

 

Ran 3 tests in 0.007s
OK

That’s what you should see if everything is working right. Try causing an error to see what that looks like
and then fix it.

Extra Credit

1. Go read about nosetests more, and also read about alternatives.

2. Learn about Python’s “doc tests” and see if you like them better.

 

Testing Guidelines 159

Learn Python The Hard Way, Release 2.0

 

3. Make your Room more advanced, and then use it to rebuild your game yet again but this time, unit
test as you go.

 

160 Exercise 47: Automated Testing

Exercise 48: Advanced User Input

Your game probably was coming along great, but I bet how you handled what the user typed was be-
coming tedious. Each room needed its own very exact set of phrases that only worked if your player
typed them perfectly. What you’d rather have is a device that lets users type phrases in various ways. For
example, we’d like to have all of these phrases work the same:

* open door

* open the door

¢ go THROUGH the door

¢ punch bear

¢ Punch The Bear in the FACE

It should be alright for a user to write something a lot like English for your game, and have your game
figure out what it means. To do this, we’re going to write a module that does just that. This module will
have a few classes that work together to handle use input and convert it into something your game can
work with reliably.

In a simple version of English the following elements:
¢ Words separated by spaces.
¢ Sentences composed of the words.
¢ Grammar that structures the sentences into meaning.

That means the best place to start is figuring out how to get words from the user and what kinds of words
those are.

Our Game Lexicon

In our game we have to create a Lexicon of words:
¢ Direction words: north, south, east, west, down, up, left, right, back.

¢ Verbs: go, stop, kill, eat.

 

161

Learn Python The Hard Way, Release 2.0

 

¢ Stop words: the, in, of, from, at, it
¢ Nouns: door, bear, princess, cabinet.
¢ Numbers: any string of 0 through 9 characters.

When we get to nouns, we have a slight problem since each room could have a different set of Nouns,
but let’s just pick this small set to work with for now and improve it later.

Breaking Up A Sentence

Once we have our lexicon of words we need a way to break up sentences so that we can figure out what
they are. In our case, we’ve defined a sentence as “words separated by spaces’, so we really just need to
do this:

stutt
words

raw_input('> ')
Stuff, split ()

That’s really all we’ll worry about for now, but this will work really well for quite a while.

Lexicon Tuples

Once we know how to break up a sentence into words, we just have to go through the list of words and
figure out what “type” they are. To do that we’re going to use a handy little Python structure called a
“tuple”. A tuple is nothing more than a list that you can’t modify. It’s created by putting data inside two
() with a comma, like a list:

first_word = ('direction', 'north')
second_word = ('verb', 'go')
sentence = [first_word, second_word]

This creates a pair of (TYPE, WORD) that lets you look at the word and do things with it.

This is just an example, but that’s basically the end result. You want to take raw input from the user, carve
it into words with split, then analyze those words to identify their type, and finally make a sentence
out of them.

Scanning Input

Now you are ready to write your scanner. This scanner will take a string of raw input from a user and
return a sentence that’s composed of a list of tuples with the (TOKEN, WORD) pairings. If a word isn’t
part of the lexicon then it should still return the WORD, but set the TOKEN to an error token. These
error tokens will tell the user they messed up.

Here’s where it gets fun. I’m not going to tell you how to do this. Instead I’m going to write a unit
test und you are going to write the scanner so that the unit test works.

 

162 Exercise 48: Advanced User Input

Learn Python The Hard Way, Release 2.0

 

Exceptions And Numbers

There is one tiny thing I will help you with first, and that’s converting numbers. In order to do this though,
we’re going to cheat and use exceptions. An exception is an error that you get from some function you
may have run. What happens is your function “raises” an exception when it encounters an error, then you
have to handle that exception. For example, if you type this into python:

~/projects/simplegame $ python
Python 2.6.5 (r2063, Apr 16 2010, :41)
[GCC 4.4.3] on linux2
Type "help", "copyright", "credits" or "license" for more information.
>>> int ("hell")
Traceback (most recent call last):
File "<stdin>", line 1, in <module>
ValueError: invalid literal for int() with base 10: 'hell'
Sa

 

 

That ValueError is an exception that the int () function threw because what you handed int () is
not a number. The int () function could have returned a value to tell you it had an error, but since it
only returns integers, it’d have a hard time doing that. It can’t return -1 since that’s a number. Instead of
trying to figure out what to return when there’s an error, the int () function raises the ValueError
exception and you deal with it.

 

You deal with an exception by using the try and except keywords:

def convert_number(s):
try:
return int(s)
except ValueError:
return None

You put the code you want to “try” inside the try block, and then you put the code to run for the error
inside the except. In this case, we want to “try” to call int () on something that might be a number.
If that has an error, then we “catch” it and return None.

In your scanner that you write, you should use this function to test if something is a number. You should
also do it as the last thing you check for before declaring that word an error word.

What You Should Test

Here are the files tests/lexicon_tests.py that you should use:

from nose.tools import *
from ex48 import lexicon

def test_directions():

 

What You Should Test 163

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

Learn Python The Hard Way, Release 2.0

 

def

def

def

def

def

assert_equal(lexicon.scan("north"), [('direction', 'north')])
result = lexicon.scan("north south east")
assert_equal(result, [('direction', 'north'),

('direction', 'south'),

('direction', 'east')])

test_verbs():
assert_equal(lexicon.scan("go"), [('verb', '‘go')])
result = lexicon.scan("go kill eat")
assert_equal(result, [('verb', 'go'),

('verb', 'kill'),

('verb', ‘eat')])

test_stops():
assert_equal(lexicon.scan("the"), [('stop', 'the')])
result = lexicon.scan("the in of")
assert_equal(result, [('stop', '‘'the'),

("stop", “*in'),

("step", ‘of')])

test_nouns():
assert_equal(lexicon.scan("bear"), [('noun', 'bear')])
result = lexicon.scan("bear princess")
assert_equal(result, [('noun', 'bear'),

('noun', 'princess')])

test_numbers():
assert_equal(lexicon.scan("1234"), [('number', 1234)])
result = lexicon.scan("3 91234")
assert_equal(result, [('number', 3),
('number', 91234)])

test_errors():
assert_equal(lexicon.scan("ASDFADFASDF"), [('error', 'ASDFADFASDF')])
result = lexicon.scan("bear IAS princess")
assert_equal(result, [('noun', ‘'bear'),
("errer!, "IAS"),
('noun', 'princess')])

Remember that you will want to make a new project with your skeleton, type in this test case (do not
copy-paste!) and write your scanner so that the test runs. Focus on the details and make sure everything
works right.

 

164

Exercise 48: Advanced User Input

Learn Python The Hard Way, Release 2.0

 

Design Hints

Focus on getting one test working at a time. Keep this simple and just put all the words in your lexicon
in lists that are in your lexicon.py module. Do not modify the input list of words, but instead make
your own new list with your lexicon tuples in it. Also, use the in keyword with these lexicon lists to
check if a word is in the lexicon.

Extra Credit

1. Improve the unit test to make sure you cover more of the lexicon.
2. Add to the lexicon and then update the unit test.

3. Make your scanner handles user input in any capitalization and case. Update the test to make sure
this actually works.

4. Find another way to convert the number.

5. My solution was 37 lines long. Is yours longer? Shorter?

 

Design Hints 165

Learn Python The Hard Way, Release 2.0

 

 

166 Exercise 48: Advanced User Input

Exercise 49: Making Sentences

What we should be able to get from our little game lexicon scanner is a list that looks like this:

>>> £rom ex48 import lexicon

>>> print lexicon.scan("go north")

[('verb', 'go'), ('direction', 'north')]

>>> print lexicon.scan("kill the princess")

[('verb', 'kill'), ('stop', 'the'), ('noun', 'princess') ]

>>> print lexicon.scan("eat the bear")

[('verb', 'eat'), ('stop', 'the'), ('noun', 'bear')]

>>> print lexicon.scan("open the door and smack the bear in the nose")
[('error', 'open'), ('stop', 'the'), ('noun', 'door'), ('error', ‘'and'),
("erron', "smack'), ('step', “the'), ('neun', "“bear"), ("step", ‘in’),
('stop', ‘'the'), ('error', 'nose')]

>>>

Now let us turn this into something the game can work with, which would be some kind of Sentence
class.

If you remember grade school, a sentence can be a simple structure like:
Subject Verb Object

Obviously it gets more complex than that, and you probably did many days of annoying sentence graphs
for English class. What we want is to turn the above lists of tuples into a nice Sentence object that has
subject, verb, and object.

Match And Peek

To do this we need four tools:
1. A way to loop through the list of tuples. That’s easy.
2. A way to “match” different types of tuples that we expect in our Subject Verb Object setup.
3. A way to “peek” at a potential tuple so we can make some decisions.

4. A way to “skip” things we do not care about, like stop words.

 

167

Learn Python The Hard Way, Release 2.0

 

We use the peek function to say look at the next element in our tuple list, and then match to take one off
and work with it. Let’s take a look at a first peek function:

def peek (word_list):
if word_list:
word = word_list [0]
return word[0]
else:
return None

Very easy. Now for the match function:

def match(word_list, expecting):
if word_list:
word = word_list.pop (0)

if word[0] == expecting:
return word
else:
return None
else:
return None

Again, very easy, and finally our skip function:

def skip(word_list, word_type):
while peek (word_list) == word_type:
match(word_list, word_type)

By now you should be able to figure out what these do. Make sure you understand them.

The Sentence Grammar

With our tools we can now begin to build Sentence objects from our list of tuples. What we do is a
process of:

1. Identify the next word with peek.

2. If that word fits in our grammar, we call a function to handle that part of the grammar, say
parse_subject.

3. If it doesn’t, we raise an error, which you will learn about in this lesson.
4. When we’re all done, we should have a Sentence object to work with in our game.

The best way to demonstrate this is to give you the code to read, but here’s where this exercise is different
from the previous one: You will write the test for the parser code I give you. Rather than giving you the
test so you can write the code, I will give you the code, and you have to write the test.

Here’s the code that I wrote for parsing simple sentences using the ex48 . lexicon module:

 

168 Exercise 49: Making Sentences

Learn Python The Hard Way, Release 2.0

 

 

class ParserError (Exception) :
pass

class Sentence (object):

def __init__(self, subject, verb, object):
# remember we take ('noun', 'princess') tuples and convert them
self.subject = subject [1]
self.verb = verb[1]
self.object = object[1]

def peek (word_list):
if word_list:
word = word_list [0]
return word[0]
else:
return None

def match(word_list, expecting):
if word_list:
word = word_list.pop (0)

if word[0] == expecting:
return word
else:
return None
else:
return None

def skip(word_list, word_type):
while peek(word_list) == word_type:
match(word_list, word_type)

def parse_verb(word_list):
skip (word _lAist, "step")

if peek(word_list) == 'verb':
return match(word_list, 'verb')

else:
raise ParseError ("Expected a verb next.")

 

 

 

 

def parse_object (word_list):

 

The Sentence Grammar 169

Learn Python The Hard Way, Release 2.0

 

skip(word_list, 'stop')
next = peek (word_list)

if next == 'noun':
return match(word_list, 'noun')
if next == 'direction':

return match(word_list, 'direction')
else:
raise ParseError ("Expected a noun or direction next.")

 

 

 

def parse_subject (word_list, subj):
verb = parse_verb(word_list)
obj = parse_object (word_list)
return Sentence(subj, verb, obj)
def parse_sentence(word_list):

skip(word_list, 'stop')

start = peek (word_list)

if start == 'noun':
subj = match(word_list, 'noun')
return parse_subject (word_list, subj)
elif start == 'verb':

# assume the subject is the player then
return parse_subject (word_list, ('noun', 'player'))
else:
raise ParserError("Must start with subject, object, or verb not: %s" % start)

 

A Word On Exceptions

You briefly learned about exceptions, but not how to raise them. This code demonstrates how to do that
with the ParserError at the top. Notice that it uses classes to give it the type of Exception. Also
notice the use of raise keyword to raise the exception.

In your tests, you will want to work with these exceptions, which I'll show you how to do.

What You Should Test

For Exercise 49 is write a complete test that confirms everything in this code is working. That includes
making exceptions happen by giving it bad sentences.

 

170 Exercise 49: Making Sentences

Learn Python The Hard Way, Release 2.0

 

Check for an exception by using the function assert_raises from the nose documentation. Learn
how to use this so you can write a test that is expected to fail, which is very important in testing. Learn
about this function (and others) by reading the nose documentation.

When you are done, you should know how this bit of code works, and how to write a test for other
people’s code even if they do not want you to. Trust me, it’s a very handy skill to have.

Extra Credit

1. Change the parse_ methods and try to put them into a class rather than be just methods. Which
design do you like better?

2. Make the parser more error resistant so that you can avoid annoying your users if they type words
your lexicon doesn’t understand.

3. Improve the grammar by handling more things like numbers.

4. Think about how you might use this Sentence class in your game to do more fun things with a
user’s input.

 

Extra Credit 171

Learn Python The Hard Way, Release 2.0

 

 

172 Exercise 49: Making Sentences

Exercise 50: Your First Website

These final three exercises will be very hard and you should take your time with them. In this first one
you'll build a simple web version of one of your games. Before you attempt this exercise you must have
completed Exercise 46 successfully and have a working pip installed such that you can install packages
and know how to make a skeleton project directory. If you don’t remember how to do this, go back to
Exercise 46 and do it all over again.

Installing lpthw.web

Before creating your first web application, you'll first need to install the “web framework” called
lpthw.web. The term “framework” generally means “some package that makes it easier for me to
do something”. In the world of web applications, people create “web frameworks” to compensate for the
difficult problems they’ve encountered when making their own sites. They share these common solutions
in the form of a package you can download to bootstrap your own projects.

In our case, we'll be using the lpthw. web framework, but there are many, many, many others you can
choose from. For now, learn lpthw.web then branch out to another one when you’re ready (or just
keep using lpthw.web since it’s good enough).

Using pip install lpthw.web:

S sudo pip install lpthw.web
[sudo] password for zedshaw:
Downloading/unpacking lpthw.web

Running setup.py egg_info for package lIpthw.web

Installing collected packages: lpthw.web
Running setup.py install for lpthw.web

Successfully installed lpthw.web
Cleaning up...

This will work on Linux and Mac OSX computers, but on Windows just drop the sudo part of the pip
install command and it should work. If not, go back to Exercise 46 and make sure you can do it reliably.

 

173

Learn Python The Hard Way, Release 2.0

 

Warning: Other Python programmers will warn you that lpthw.web is just a fork of another
web framework called web.py, and that web.py has too much “magic”. If they say this, point
out to them that Google App Engine originally used web. py and not a single Python programmer

complained that it had too much magic, because they all worked at Google. If it’s good enough for
Google, then it’s good enough for you to get started. Then, just get back to learning to code and ignore
their goal of indoctrination over education.

 

Make A Simple “Hello World” Project

Now you’re going to make an initial very simple “Hello World” web application and project directory
using lpthw. web. First, make your project directory:

cd projects

mkdir gothonweb

cd gothonweb

mkdir bin gothonweb tests docs templates
touch gothonweb/__init__.py

touch tests/__init__.py

TE SH AE a A.

You’ll be taking the game from Exercise 42 and making it into a web application, so that’s why you’re
calling it gothonweb. Before you do that, we need to create the most basic lpthw.web application
possible. Put the following code into bin/app.py:

import web

urls = (
‘/', index"

)

app = web.application(urls, globals())

class index:
def GET(self):
greeting = "Hello World"
return greeting

 

at name == ""— main__":

app.run()

Then run the application like this:

S python bin/app.py
http://0.0.0.0:8080/

Finally, use your web browser and go to the URL http: //localhost:8080/ and you should see
two things. First, in your browser you'll see Hello, world!. Second, you’ll see your terminal with

 

174 Exercise 50: Your First Website

Learn Python The Hard Way, Release 2.0

 

new output like this:

S python bin/app.py

http://0.0.0.0:8080/

127.0.0.1:59542 - - [13/Jun/2011 :43] "HTTP/1.1 GET /" - 200 OK

127.0.0.1:59542 - - [13/Jun/2011 :43] "HTTP/1.1 GET /favicon.ico" - 404 Not Found

Those are log messages that pt hw. web prints out so you can see that the server is working, and what
the browser is doing behind the scenes. The log messages help you debug and figure out when you have
problems. For example, it’s saying that your browser tried to get /favicon.ico but that file didn’t
exist so it returned 404 Not Found status code.

I haven’t explained the way any of this web stuff works yet, because I want to get you setup and ready to
roll so that I can explain it better in the next two exercises. To accomplish this, ll have you break your
Ipthw.web application in various ways and then restructure it so that you know how it’s setup.

What’s Going On?

Here’s what’s happening when your browser hits your application:

1. Your browser makes a network connection to your own computer, which is called localhost
and is a standard way of saying “whatever my own computer is called on the network”. It also uses
port 8080.

2. Once it connects, it makes an HTTP request to the bin/app.py application and asks for the /
URL, which is commonly the first URL on any website.

3. Inside bin/app.py you’ve got a list of URLs and what classes they match. The only one we have
is the ’/’, ‘index’ mapping. This means that whenever someone goes to / with a browser,
lpthw.web will find the class index and load it to handle the request.

 

4. Now that lpthw.web has found class index it calls the index .GET method on an instance
of that class to actually handle the request. This function runs, and simply returns a string for what
lpthw.web should send to the browser.

5. Finally, lpthw.web has handled the request and sends this response to the browser which is what
you are seeing.

Make sure you really understand this. Draw up a diagram of how this information flows from your
browser, to lpthw.web, then to index. GET and back to your browser.

 

Fixing Errors

First, delete line 11 where you assign the greeting variable, then hit refresh in your browser. You
should see an error page now that gives you lots of information on how your application just exploded.

 

What’s Going On? 175

Learn Python The Hard Way, Release 2.0

 

You know that the variable greeting is now missing, but lothw.web gives you this nice error page
to track down exactly where. Do each of the following with this page:

1. Look at each of the Local vars outputs (click on them) and see if you can follow what variables
it’s talking about and where they are.

2. Look at the Request Information section and see if it matches anything you’re already fa-
miliar with. This is information that your web browser is sending to your got honweb application.
You normally don’t even know that it’s sending this stuff, so now you get to see what it does.

3. Try breaking this simple application in other ways and explore what happens. Don’t forget to also
look at the logs being printed into your terminal as lpthw.web will put other stack traces and
information there too.

Create Basic Templates

You can break your Ipthw.web application, but did you notice that “Hello World” isn’t a very good HTML
page? This is a web application, and as such it needs a proper HTML response. To do that you will create
a simple template that says “Hello World” in a big green font.

The first step is to create a templates/index.html file that looks like this:

Sdef with (greeting)

<html>
<head>
<title>Gothons Of Planet Percal #25</title>
</head>
<body>

Sif greeting:

I just wanted to say <em style="color: green; font-size: 2em;">Sgreeting</em>.
Selse:

<em>Hello</em>, world!

</body>
</html>

If you know what HTML is then this should look fairly familiar. If not, research HTML and try writing a
few web pages by hand so you know how it works. This HTML file however is a template, which means
that lpthw.web will fill in “holes” in the text depending on variables you pass in to the template. Every
place you see $greeting will be a variable you’ll pass to the template that alters its contents.

To make your bin/app.py do this, you need to add some code to tell Lpt hw. web where to load the
template and to render it. Take that file and change it like this:

import web

 

176 Exercise 50: Your First Website

Learn Python The Hard Way, Release 2.0

 

urls = (
t/!, “Index*

app = web.application(urls, globals())

 

render = web.template.render('templates/')

class Index(object):
def GET(self):
greeting = "Hello World"
return render.index(greeting = greeting)

 

if _oname__ == "__main__":
app.run()

 

Pay close attention to the new render variable, and how I changed the last line of index.GET so it
returns render. index () passing in your greeting variable.

Once you have that in place, reload the web page in your browser and you should see a different message
in green. You should also be able to do a View Source on the page in your browser to see that it is
valid HTML.

This may have flown by you very fast, so let me explain how a template works:

1. In. your bin/app.py you’ve added a new variable render which is a
web.template. render object.

2. This render object knows how to load .htm1 files out of the templates/ directory because
you passed that to it as a parameter.

 

3. Later in your code, when the browser hits the index. GET like before, instead of just returning
the string greeting, you call render. index and pass the greeting to it as a variable.

4. This render.index method is kind of a magic function where the render object sees
that you’re asking for index, goes into the templates/ directory, looks for a page named
index.html, and then “renders” it, or converts it.

5. In the templates/index.html1 file you see the beginning definition that says this template
takes a greet ing parameter, just like a function. Also, just like Python this template is indenta-
tion sensitive, so make sure you get them right.

6. Finally, you have the HTML in templates/index.html that looks at the greeting vari-
able, and if it’s there, prints one message using the $greet ing, or a default message.

To get deeper into this, change the greeting variable and the HTML to see what effect it has. Also create
another template named templates/foo.html and render that using render. foo() instead of
render. index () like before. This will show you how the name of the function you call on render
is just matched toa . html fileintemplates/.

 

Create Basic Templates 177

Learn Python The Hard Way, Release 2.0

 

Extra Credit

1. Read the documentation at http://webpy.org/ which is the same as the 1pt hw. web project.
. Experiment with everything you can find there, including their example code.

. Read about HTMLS and CSS3 and make some other .html and .css files for practice.

KR WwW WN

. If you have a friend who knows Django and is willing to help you, then consider doing Ex 50, 51,
and 52 in Django instead to see what that’s like.

 

178 Exercise 50: Your First Website

Exercise 51: Getting Input From A
Browser

While it’s exciting to see the browser display “Hello World”, it’s even more exciting to let the user
submit text to your application from a form. In this exercise we’ll improve our starter web application
using forms and storing information about the user into their “session”.

How The Web Works

Time for some boring stuff. You need to understand a bit more about how the web works before you can
make a form. This description isn’t complete, but it’s accurate and will help you figure out what might
be going wrong with your application. Also, creating forms will be easier if you know what they do.

Pll start with a simple diagram that shows you the different parts of a web request and how the information
flows:

Your Browser

http://learnpythonthehardway.org/

 
  

     
   
 
   
 

Web App's
index.GET

  

My
Server

I’ve labeled the lines with letters so I can walk you through a regular request process:

 

179

Learn Python The Hard Way, Release 2.0

 

Ts

. Youtype inthe urlhttp://learnpythonthehardway.org/ into your browser and it sends

the request outon line (A) to your computer’s network interface.

. Your request goes out over the internet on line (B) and then to the remote computer on line

(C) where my server accepts the request.

. Once my computer accepts it, my web application gets iton line (D), and my Python code runs

the index. GET handler.

 

. The response comes out of my Python server when I return it, and goes back to your browser

over line (D) again.

. The server running this site takes the response off Line (D) then sends it back over the internet

online (C).

. The response from the server then comes off the internet on line (B), and your computer’s

network interface hands it to your browseron line (A).

Finally, your browser then displays the response.

In this description there are a few terms you should know so that you have a common vocabulary to work
with when talking about your web application:

Browser The software that you’re probably using every day. Most people don’t know what it really does,

they just call it “the internet”. Its job is to take addresses (like http://learnpythonthehardway.org)
you type into the URL bar, then use that information to make requests to the server at that address.

Address This is normally a URL (Uniform Resource Locator) like http://learnpythonthehardway.org/

and indicates where a browser should go. The first part http indicates the protocol you want
to use, in this case “Hyper-Text Transport Protocol”. You can also try ftp://ibiblio.org/ to see how
“File Transport Protocol” works. The learnpythonthehardway.org partis the “hostname”,
or a human readable address you can remember and which maps to a number called an IP address,
similar to a telephone number for a computer on the Internet. Finally, URLs can have a trailing
path like the /book/ part of http://learnpythonthehardway.org/book/ which indicates a file or
some resource on the server to retrieve with a request. There are many other parts, but those are
the main ones.

Connection Once a browser knows what protocol you want to use (http), what server you want to talk

to (learnpythonthehardway.org), and what resource on that server to get, it must make a connec-
tion. The browser simply asks your Operating System (OS) to open a “port” to the computer,
usually port 80. When it works the OS hands back to your program something that works like
a file, but is actually sending and receiving bytes over the network wires between your com-
puter and the other computer at “learnpythonthehardway.org”. This is also the same thing that
happens with http://localhost:8080/ but in this case you’re telling the browser to connect to your
own computer (localhost) and use port 8080 rather than the default of 80. You could also do
http://learnpythonthehardway.org:80/ and get the same result, except you’re explicitly saying to
use port 80 instead of letting it be that by default.

Request Your browser is connected using the address you gave. Now it needs to ask for the resource

it wants (or you want) on the remote server. If you gave /book/ at the end of the URL, then

 

180

Exercise 51: Getting Input From A Browser

Learn Python The Hard Way, Release 2.0

 

you want the file (resource) at /book/, and most servers will use the real file /book/index.html but
pretend it doesn’t exist. What the browser does to get this resource is send a request to the server.
I won’t get into exactly how it does this, but just understand that it has to send something to query
the server for the request. The interesting thing is that these “resources” don’t have to be files.
For instance, when the browser in your application asks for something, the server is returning
something your Python code generated.

Server The server is the computer at the end of a browser’s connection that knows how to answer your
browser’s requests for files/resources. Most web servers just send files, and that’s actually the
majority of traffic. But you’re actually building a server in Python that knows how to take requests
for resources, and then return strings that you craft using Python. When you do this crafting, you
are pretending to be a file to the browser, but really it’s just code. As you can see from Ex. 50, it
also doesn’t take much code to create a response.

Response This is the HTML (css, javascript, or images) your server wants to send back to the browser
as the answer to the browser’s request. In the case of files, it just reads them off the disk and sends
them to the browser, but it wraps the contents of the disk in a special “header” so the browser knows
what it’s getting. In the case of your application, you’re still sending the same thing, including the
header, but you generate that data on the fly with your Python code.

That is the fastest crash course in how a web browser accesses information on servers on the internet. It
should work well enough for you to understand this exercise, but if not, read about it as much as you can
until you get it. A really good way to do that is to take the diagram, and break different parts of the web
application you did in Exercise 50. If you can break your web application in predictable ways using the
diagram, you’ll start to understand how it works.

How Forms Work

The best way to play with forms is to write some code that accepts form data, and then see what you can
do. Take your bin/app. py file and make it look like this:

import web

urls = (
"/hello', 'Index'

app = web.application(urls, globals())

render = web.template.render('templates/')

 

class Index(object):
def GET(self):
form = web.input (name="Nobody")
greeting = "Hello, %s" % form.name

 

 

How Forms Work 181

Learn Python The Hard Way, Release 2.0

 

return render.index(greeting = greeting)

if _oname_| == "__main__":
app.run()

Restart it (hit CTRL-c and then run it again) to make sure it loads again, then with your browser go
to http: //localhost:8080/hello which should display, “I just wanted to say Hello, Nobody.”
Next, change the URL in your browser to http: //localhost:8080/hello?name=F rank and
you’ll see it say “Hello, Frank.” Finally, change the name=F rank part to be your name. Now it’s saying
hello to you.

Let’s break down the changes I made to your script.

1. Instead of just a string for greeting I’m now using web. input to get data from the browser.
This function takes a key=value set of defaults, parses the ?name=Frank part of the URL you
give it, and then returns a nice object for you to work with that represents those values.

2. I then construct the greeting from the new form.name attribute of the form object, which
should be very familiar to you by now.

3. Everything else about the file is the same as before.

You’re also not restricted to just one parameter on the URL. Change this example to give two vari-
ables like this: http: //localhost:8080/hello?name=Frankégreet=Hola. Then change
the code to get form.name and form. greet like this:

greeting = "Ss, %s" % (form.greet, form.name)

After that, try the URL. Next, leave out the £greet=Hola part so that you can see the error you get.
Since greet doesn’t have a default value in web. input (name="Nobody") then it is a required
field. Now go back and make it have a default in the web. input call to see how you fix this. Another

thing you can do is set its default to greet =None so that you can check if it exists and then give a better
error message, like this:

form = web.input (name="Nobody", greet=None)

if form.greet:

greeting = "%Ss, %s" % (form.greet, form.name)
return render.index(greeting = greeting)
else:

 

return "ERROR: greet is required."

Creating HTML Forms

Passing the parameters on the URL works, but it’s kind of ugly and not easy to use for regular people.
What you really want is a “POST form”, which is a special HTML file that has a <form> tag in it. This
form will collect information from the user, then send it to your web application just like you did above.

 

182 Exercise 51: Getting Input From A Browser

Learn Python The Hard Way, Release 2.0

 

Let’s make a quick one so you can see how it works. Here’s the new HTML file you need to create, in
templates/hello_form.html:

<html>
<head>
<title>Sample Web Form</title>
</head>
<body>

<h1l>Fill Out This Form</h1>

<form action="/hello" method="POST">
A Greeting: <input type="text" name="greet">
<br/>
Your Name: <input type="text" name="name">
<br/>
<input type="submit">

</form>

</body>
</html>

You should then change bin/app. py to look like this:

import web
urls = (
"/hello', 'Index'
app = web.application(urls, globals())

render = web.template.render('templates/')

 

class Index(object):
def GET(self):
return render.hello_form()

 

def POST(self):
form = web.input (name="Nobody", greet="Hello")
greeting = "%Ss, %s" % (form.greet, form.name)
return render.index(greeting = greeting)
if _oname__ == "__main__":
app.run()

Once you’ve got those written up, simply restart the web application again and hit it with your browser
like before.

This time you’ll get a form asking you for “A Greeting” and “Your Name”. When you hit the Submit

 

Creating HTML Forms 183

Learn Python The Hard Way, Release 2.0

 

button on the form, it will give you the same greeting you normally get, but this time look at the URL in
your browser. See how it’s http: //localhost:8080/hel11o even though you sent in parameters.

The part of the hello_form.html file that makes this work is the line with <form
action="/hello" method="POST">. This tells your browser to:

1. Collect data from the user using the form fields inside the form.

2. Send them to the server using a POST type of request, which is just another browser request that
“hides” the form fields.

3. Send that to the /hello URL (as shown in the action="/hello" part).

You can then see how the two <input> tags match the names of the variables in your new code. Also
notice that instead of just a GET method inside class index, I have another method POST.

 

How this new application works is:

1. The browser first hits the web application at /hello but it sends a GET, so our index.GET
function runs and returns the he 1 lo_form.

 

 

2. You fill out the form in the browser, and the browser does what the <form> says and sends the
data as a POST.

 

3. The web application then runs the index. POST method rather than the index .GET method to
handle this request.

4. This index.POST method then does what it normally does to send back the hello page like
before. There’s really nothing new in here, it’s just moved into a new function.

As an exercise, go into the templates/index.html file and add a link back to just /hello
so that you can keep filling out the form and seeing the results. Make sure you can explain
how this link works and how it’s letting you cycle between templates/index.html and
templates/hello_form. html and what’s being run inside this latest Python code.

 

Creating A Layout Template

When you work on your game in the next Exercise, you’ll need to make a bunch of little HTML pages.
Writing a full web page each time will quickly become tedious. Luckily you can create a “layout”
template, or a kind of shell that will wrap all your other pages with common headers and footers. Good
programmers try to reduce repetition, so layouts are essential for being a good programmer.

Change templates/index.html to be like this:
Sdef with (greeting)
Sif greeting:
I just wanted to say <em style="color: green; font-size: 2em;">Sgreeting</em>.

Selse:
<em>Hello</em>, world!

 

184 Exercise 51: Getting Input From A Browser

Learn Python The Hard Way, Release 2.0

 

Then change templates/hello_form.htm1 to be like this:

<h1>Fill Out This Form</h1>

<form action="/hello" method="POST">
A Greeting: <input type="text" name="greet">

<br/>
Your Name: <input type="text" name="name">
<br/>
<input type="submit">
</form>

All we’re doing is stripping out the “boilerplate” at the top and the bottom which is always on every page.
We’ll put that back into a single templates/layout.html file that handles it for us from now on.

Once you have those changes, create a templates/layout.html file with this in it:

Sdef with (content)

<html>
<head>
<title>Gothons From Planet Percal #25</title>
</head>
<body>

$:content

</body>
</html>

This file looks like a regular template, except that it’s going to be passed the contents of the other tem-
plates and used to wrap them. Anything you put in here doesn’t need to be in the other templates. You
should also pay attention to how $: content is written, since it’s a little different from the other tem-
plate variables.

The final step is to change the line that makes the render object to be this:

render = web.template.render('templates/', base="layout")

 

Which tells lpthw.web to use the templates/layout.html file as the base template for all the
other templates. Restart your application and then try to change the layout in interesting ways, but without
changing the other templates.

Writing Automated Tests For Forms

It’s easy to test a web application with your browser by just hitting refresh, but come on, we’re program-
mers here. Why do some repetitive task when we can write some code to test our application? What

 

Writing Automated Tests For Forms 185

Learn Python The Hard Way, Release 2.0

you’re going to do next is write a little test for your web application form based on what you learned in

Exercise 47. If

you don’t remember Exercise 47, read it again.

You need to do a bit of setup to make Python let you load your bin/app. py file for testing. When we

get to Exercise
thinks bin/ is

52 you'll change this, but for now create an empty bin/__init__.py file so Python
a directory.

I’ve also created a simple little function for lpthw.web that lets you assert things about your web
application’s response, aptly named assert_response. Create the file tests/tools.py with

these contents:

from nose.tools import *

import re

def assert_response(resp, contains=None, matches=None, headers=None, status="200"):

assert status in resp.status, "Expected response Sr not in Sr" % (status, resp.status)

if status == "200":
assert resp.data, "Response data is empty."

if contains:
assert contains in resp.data, "Response does not contain %r" % contains

if matches:

reg

= re.compile (matches)

assert reg.matches(contains), "Response does not match Sr" % matches

if headers:

ass

rt_equal(resp.headers, headers)

 

Once that’s in place you can write your automated test for the last version of the bin/app.py file you
created. Create a new file named test s/app_tests.py with this:

from nose.tools import *
from bin.app import app
from tests.tools import assert_response

def test_index():
# check that we get a 404 on the / URL

resp =

assert_

# test
resp =

assert_

# make
resp =

assert_

app. request ("/")
response(resp, status="404")

our first GET request to /hello
app. request ("/hello")
response (resp)

sure default values work for the form
app.request ("/hello", method="POST")
response (resp, contains="Nobody")

 

186

Exercise 51: Getting Input From A Browser

Learn Python The Hard Way, Release 2.0

 

# test that we get expected values

data = {'name': 'Zed', 'greet': 'Hola'}

resp = app.request("/hello", method="POST", data=data)
assert_response(resp, contains="Zed")

Finally, use nosetests to run this test setup and test your web application:

S nosetests

 

Ran 1 test in 0.059s
OK

What I’m doing here is I’m actually importing the whole application from the bin/app.py module,
then running it manually. The 1pthw.web framework has a very simple API for processing requests,
which looks like this:

app.request (localpart='/', method='GET', data=None, host='0.0.0.0:8080',
headers=None, https=False)

This means you can pass in the URL as the first parameter, then change the method of the request, as
wellas what form data you send, including the host and headers. This works without running an actual
web server so you can do tests with automated tests and also use your browser to test a running server.

To validate responses from this function, use the assert_response function from tests.tools
which has:

assert_response(resp, contains=None, matches=None, headers=None, status="200")

Pass in the response you get from calling app. request then add things you want checked. Use the
contains parameter to make sure that the response contains certain values. Use the st atus parameter
to check for certain responses. There’s actually quite a lot of information in this little function so it would
be good for you to study it.

In the tests/app_tests.py automated test I’m first making sure the / URL returns a “404 Not
Found” response, since it actually doesn’t exist. Then I’m checking that /he11o works with both a GET
and POST form. Following the test should be fairly simple, even if you might not totally know what’s
going on.

 

Take some time studying this latest application, especially how the automated testing works. Make sure
you understand how I imported the application from bin/app. py and ran it directly for the automated
test. This is an important trick that will lead to more learning.

 

Writing Automated Tests For Forms 187

Learn Python The Hard Way, Release 2.0

 

Extra Credit

. Read even more about HTML, and give the simple form a better layout. It helps to draw what you

want to do on paper and then implement it with HTML.

. This one is hard, but try to figure out how you’d do a file upload form so that you can upload an

image and save it to the disk.

. This is even more mind-numbing, but go find the HTTP RFC (which is the document that describes

how HTTP works) and read as much of it as you can. It is really boring, but comes in handy once
in a while.

. This will also be really difficult, but see if you can find someone to help you setup a web server

like Apache, Nginx, or thttpd. Try to serve a couple of your .html and .css files with it just to see if
you can. Don’t worry if you can’t, web servers kind of suck.

. Take a break after this and just try making as many different web applications as you can. You

should definitely read about sessions in web. py (which is the same as lpthw. web) so you can
understand how to keep state for a user.

 

188

Exercise 51: Getting Input From A Browser

Exercise 52: The Start Of Your Web
Game

We’re coming to the end of the book, and in this exercise I’m going to really challenge you. When you’re
done, you'll be a reasonably competent Python beginner. You’ll still need to go through a few more books
and write a couple more projects, but you’ll have the skills to complete them. The only thing in your way
will be time, motivation, and resources.

In this exercise, we won’t make a complete game, but instead we’ll make an “engine” that can run the
game from Exercise 42 in the browser. This will involve refactoring Exercise 42, mixing in the
structure from Exercise 47, adding automated tests, and finally creating a web engine that can run the
games.

This exercise will be huge, and I predict you could spend anywhere from a week to months on it before
moving on. It’s best to attack it in little chunks and do a bit a night, taking your time to make everything
work before moving on.

Refactoring The Exercise 42 Game

You’ve been altering the gothonweb project for two exercises and you’ll do it one more time in this
exercise. The skill you’re learning is called “refactoring”, or as I like to call it, “fixing stuff”. Refactoring
is a term programmers use to describe the process of taking old code, and changing it to have new features
or just to clean it up. You’ve been doing this without even knowing it, as it’s second nature to building
software.

What you’ll do in this part is take the ideas from Exercise 47 of a testable “map” of Rooms, and the
game from Exercise 42, and combine them together to create a new game structure. It will have the same
content, just “refactored” to have a better structure.

First step is to grab the code from ex47/game.py and copy it to gothonweb/map.py and copy
tests/ex47_tests.py filetotests/map_tests.py andrun nosetests again to make sure
it keeps working.

 

 

189

20

21

22:

23

24

25

26

27

28

29

30

31

32

33

34

35:

36

37

38

39

Learn Python The Hard Way, Release 2.0

 

Note: From now on I won’t show you the output of a test run, just assume that you should be doing it
and it’ll look like the above unless you have an error.

 

Once you have the code from Exercise 47 copied over, it’s time to refactor it to have the Exercise 42 map
in it. I’m going to start off by laying down the basic structure, and then you’ll have an assignment to
make the map. py file and the map_tests.py file complete.

First thing to do is lay out the basic structure of the map using the Room class as it is now:

class Room(object):

def _ init__(self, name, description):
self.name = name
self.description = description
self.paths = {}

def go(self, direction):
return self.paths.get (direction, None)

def add_paths(self, paths):
self.paths.update (paths)

central_corridor = Room("Central Corridor",

noe

The Gothons of Planet Percal #25 have invaded your ship and destroyed
your entire crew. You are the last surviving member and your last
mission is to get the neutron destruct bomb from the Weapons Armory,
put it in the bridge, and blow the ship up after getting into an
escape pod.

You're running down the central corridor to the Weapons Armory when
a Gothon jumps out, red scaly skin, dark grimy teeth, and evil clown costume
flowing around his hate filled body. He's blocking the door to the

Armory and about to pull a weapon to blast you.
we ")

laser_weapon_armory = Room("Laser Weapon Armory",

noe

Lucky for you they made you learn Gothon insults in the academy.
You tell the one Gothon joke you know:

Lbhe zbgure vf fb sng, jura fur fvgf nebhaq gur ubhfr, fur fvgf nebhag gur ubhfr.

The Gothon stops, tries not to laugh, then busts out laughing and can't move.
While he's laughing you run up and shoot him square in the head
putting him down, then jump through the Weapon Armory door.

You do a dive roll into the Weapon Armory, crouch and scan the room
for more Gothons that might be hiding. It's dead quiet, too quiet.

 

190 Exercise 52: The Start Of Your Web Game

41
42

43

45

46

47

48

49

50

SI

52

53

54

55

56

57

58

59

60

61

62

63

65

66

67

68

69

70

71

72

73

74

75

76

71

78

79

80

81

82

83

84

85

86

87

88

Learn Python The Hard Way, Release 2.0

 

You stand up and run to the far side of the room and find the
neutron bomb in its container. There's a keypad lock on the box
and you need the code to get the bomb out. If you get the code
wrong 10 times then the lock closes forever and you can't

get the bomb. The code is 3 digits.

wn *)

the_bridge = Room("The Bridge",

noe

The container clicks open and the seal breaks, letting gas out.
You grab the neutron bomb and run as fast as you can to the
bridge where you must place it in the right spot.

You burst onto the Bridge with the netron destruct bomb
under your arm and surprise 5 Gothons who are trying to
take control of the ship. Each of them has an even uglier
clown costume than the last. They haven't pulled their
weapons out yet, as they see the active bomb under your

arm and don't want to set it off.
we ")

escape_pod = Room("Escape Pod",

ng

You point your blaster at the bomb under your arm

and the Gothons put their hands up and start to sweat.

You inch backward to the door, open it, and then carefully
place the bomb on the floor, pointing your blaster at it.
You then jump back through the door, punch the close button
and blast the lock so the Gothons can't get out.

Now that the bomb is placed you run to the escape pod to
get eff this tin can,

You rush through the ship desperately trying to make it to

the escape pod before the whole ship explodes. It seems like
hardly any Gothons are on the ship, so your run is clear of
interference. You get to the chamber with the escape pods, and
now need to pick one to take. Some of them could be damaged
but you don't have time to look. There's 5 pods, which one

do you take?

on ")

 

the_end_winner = Room("The End",

moe

You jump into pod 2 and hit the eject button.
The pod easily slides out into space heading to

 

Refactoring The Exercise 42 Game 191

89

90

91

92

93

94

95

96

97

98

99

101

102

103

105

106

107

108

109

110

ll

112

113

114

115

116

117

118

119

120

121

122

123

124

125

126

127

128

Learn Python The Hard Way, Release 2.0

 

the planet below. As it flies to the planet, you look
back and see your ship implode then explode like a
bright star, taking out the Gothon ship at the same

time. You won!
we ny

 

the_end_loser = Room("The End",

noe

You jump into a random pod and hit the eject button.
The pod escapes out into the void of space, then
implodes as the hull ruptures, crushing your body
into jam jelly.

oon

)

escape_pod.add_paths ({
'2': the_end_winner,
'x': the _end_loser

})
generic_death = Room("death", "You died.")

the_bridge.add_paths ({
‘throw the bomb': generic_death,
"slowly place the bomb': escape_pod
})

laser_weapon_armory.add_paths ({
'0132': the_bridge,

'x': generic_death
})
central_corridor.add_paths ({
"shoot!': generic_death,
'dodge!': generic_death,
"tell a joke': laser_weapon_armory

})

START = central_corridor

You'll notice that there are a couple of problems with our Room class and this map:

1. We have to put the text that was in the if-else clauses that got printed before entering a room
as part of each room. This means you can’t shuffle the map around which would be nice. You’ll be
fixing that up in this exercise.

2. There are parts in the original game where we ran code that determined things like the bomb’s
keypad code, or the right pod. In this game we just pick some defaults and go with it, but later

 

192 Exercise 52: The Start Of Your Web Game

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34,

35

36

37

38

Learn Python The Hard Way, Release 2.0

 

3.

4.

you'll be given extra credit to make this work again.

I’ve just made a generic_death ending for all of the bad decisions, which you’ll have to finish
for me. You'll need to go back through and add in all the original endings and make sure they
work.

I’ve got a new kind of transition labeled "*" that will be used for a “catch-all” action in the engine.

Once you’ve got that basically written out, here’s the new automated test tests/map_test .py that
you should have to get yourself started:

from nose.tools import *
from gothonweb.map import *

def

def

def

def

test_room():

gold = Room("GoldRoom",
"""This room has gold in it you can grab. There's a
door to the north, """)

assert_equal(gold.name, "GoldRoom")

assert_equal(gold.paths, {})

test_room_paths():

center = Room("Center", "Test room in the center.")
north = Room("North", "Test room in the north.")
south = Room("South", "Test room in the south.")

 

center.add_paths({'north': north, 'south': south})
assert_equal(center.go('north'), north)
assert_equal(center.go('south'), south)

test_map():
start = Room("Start", "You can go west and down a hole.")
west = Room("Trees", "There are trees here, you can go east.")

down = Room("Dungeon", "It's dark down here, you can go up.")

start.add_paths({'west': west, 'down': down})
west.add_paths({'east': start})
down.add_paths({'up': start})

assert_equal(start.go('west'), west)
assert_equal(start.go('west').go('east'), start)
assert_equal(start.go('down').go('up'), start)

test_gothon_game_map():
assert_equal(START.go('shoot!'), generic_death)
assert_equal(START.go('dodge!'), generic_death)

 

room = START.go('tell a joke')
assert_equal(room, laser_weapon_armory)

 

Refactoring The Exercise 42 Game 193

20

21

22

23

Learn Python The Hard Way, Release 2.0

 

Your task in this part of the exercise is to complete the map, and make the automated test completely
validate the whole map. This includes fixing all the generic_death objects to be real endings. Make
sure this works really well and that your test is as complete as possible because we’ll be changing this
map later and you’ll use the tests to make sure it keeps working.

Sessions And Tracking Users

At a certain point in your web application you'll need to keep track of some information and associate
it with the user’s browser. The web (because of HTTP) is what we like to call “stateless”, which means
each request you make is independent of any other requests being made. If you request page A, put in
some data, and click a link to page B, all the data you sent to page A just disappears.

The solution to this is to create a little data store (usually in a database or on the disk) that uses a number
unique to each browser to keep track of what that browser was doing. In the little pt hw. web framework
it’s fairly easy, and there’s an example showing how it’s done:

import web
web.config.debug = False

urls = (
"“eoount", “eourt.",
"/reset", "reset"

)

app = web.application(urls, locals())

session = web.session.Session(app, web.session.DiskStore('sessions'), initializer={'count':

class count:
def GET(self):
session.count += 1
return str(session.count)

 

class reset:
def GET(self):
session.kill()
return ""

 

if _oname_— == "__main__":
app.run()

To make this work, you need to create a sessions/ directory where the application can put session
storage. Do that, run this application and go to /count. Hit refresh and watch the counter go up. Close
the browser and it forgets who you are, which is what we want for the game. There’s a way to make the
browser remember forever, but that makes testing and development harder. If you then go to /reset,
and back to /count you can see your counter reset because you’ve killed the session.

Take the time to understand this code so you can see how the session starts off with the count equal to

 

194 Exercise 52: The Start Of Your Web Game

Learn Python The Hard Way, Release 2.0

 

0. Also try looking at the files in sessions/ to see if you can open them up. Here’s a Python session
where I open up one and decode it:

>>> import pickle

>>> import base64

>>> base64.b6é4decode (open ("sessions/XXXXX") .read())
"(dp1\nS'count'\np2\nI1\nsS'ip'\np3\nV127.0.0.1\np4\nsS'session_id'\np5\nS'XxxX'\npé6\ns."
>>>

>>> x = base64.b64decode (open ("sessions/XXXXX") .read())

>>>
>>> pickle. loads (x)
fTeounts': 2, ‘ap’: uwI2Z700.0.1", “sesston_dd": "XXXXX'}

The sessions are really just dictionaries that get written to disk using pickle and base64 libraries.
There are probably as many ways to store and manage sessions as there are web frameworks, so it’s not
too important to know how these work. It does help if you need to debug the session or potentially clean
them out.

Creating An Engine

You should have your game map working and a good unit test for it. I now want to make a simple little
game engine that will run the rooms, collect input from the player, and keep track of where a play is in
the game. We’ll be using the sessions you just learned to make a simple game engine that will:

1. Start a new game for new users.

2. Present the room to the user.

3. Take input from the user.

4. Run their input through the game.

5. Display the results and keep going until they die.

To do this, you’re going to take the trusty bin/app.py you’ve been hacking on and create a fully
working, session based, game engine. The catch is I’m going to make a very simple one with basic
HTML files, and it’1l be up to you to complete it. Here’s the base engine:

import web
from gothonweb import map

urls = (

 

'/game', 'GameEngine',
‘/', ‘Index’,

app = web.application(urls, globals())

# little hack so that debug mode works with sessions

 

Creating An Engine 195

20

21

22

23

24

25

26

27

28

29

30

31

32

33

34

35

36

37

38

39

40

41

42

43

45

46

47

48

49

Learn Python The Hard Way, Release 2.0

 

if web.config.get('_session') is None:
store = web.session.DiskStore('sessions')
session = web.session.Session(app, store,
initializer={'room': None})
web.config._session = session
else:
session = web.config._session

 

render = web.template.render('templates/', base="layout")

class Index(object):
def GET(self):
# this is used to "setup" the session with starting values

 

session.room = map.START
web.seeother ("/game")

 

class GameEngine (object):

 

def GET(self):
if session.room:
return render.show_room(room=session. room)
else:
# why is there here? do you need it?
return render. you_died()

def POST(self):
form = web.input (action=None)

# there is a bug here, can you fix it?
if session.room and form.action:
session.room = session.room.go(form.action)

web.seeother ("/game")

if name == "__main__":
app.run()

There are even more new things in this script, but amazingly it’s an entire web based game engine in a
small file. The biggest “hack” in the script are the lines that bring the sessions back, which is needed so
that debug mode reloading works. Otherwise, each time you hit refresh the sessions will disappear and

the game won’t work.

Before you run bin/app.py you need to change your PYTHONPATH environment variable. Don’t know
what that is? I know, it’s kind of dumb you have to learn what this is to run even basic Python programs,

but that’s how Python people like things.

In your terminal, type:

 

196 Exercise 52: The Start Of Your Web Game

Learn Python The Hard Way, Release 2.0

 

export PYTHONPATH=SPYTHONPATH:.

On Windows do:

set PYTHONPATH=%PYTHONPATH?; .

You should only have to do it once per shell session, but if you get an import error, then you probably
need to do this or you did it wrong.

You should next delete templates/hello_form.html and templates/index.html
and create the two templates mentioned in the above code. Here’s a very simple
templates/show_room.html:

Sdef with (room)
<hl> Sroom.name </h1>

<pre>
Sroom.description
</pre>

Sif room.name == "death":
<p><a href="/">Play Again?</a></p>
Selse:
<p>
<form action="/game" method="POST">
— <input type="text" name="action"> <input type="SUBMIT">
</form>
</p>

That is the template to show a room as you travel through the game. Next you need one to
tell someone they died in the case that they got to the end of the map on accident, which is
templates/you_died-html:

<hl>You Died!</h1>

<p>Looks like you bit the dust.</p>
<p><a href="/">Play Again</a></p>

With those in place, you should now be able to do the following:

1. Get the test tests/app_tests.py working again so that you are testing the game. You won’t
be able to do much more than a few clicks in the game because of sessions, but you should be able
to do some basics.

2. Remove the sessions/*x files and make sure you’ve started over.
3. Run the python bin/app.py script and test out the game.

You should be able to refresh and fix the game like normal, and work with the game HTML and engine
until it does all the things you want it to do.

 

Creating An Engine 197

Learn Python The Hard Way, Release 2.0

 

Your Final Exam

Do you feel like this was a huge amount of information thrown at you all at once? Good, I want you to
have something to tinker with while you build your skills. To complete this exercise, I’m going to give
you a final set of exercises for you to complete on your own. You’ll notice that what you’ve written so
far isn’t very well built, it is just a first version of the code. Your task now is to make the game more
complete by doing these things:

1.

onnn

Fix all the bugs I mention in the code, and any that I didn’t mention. If you find new bugs, let me
know.

. Improve all of the automated tests so that you test more of the application and get to a point where

you use a test rather than your browser to check the application while you work.

. Make the HTML look better.

. Research logins and create a signup system for the application, so people can have logins and high

scores.

. Complete the game map, making it as large and feature complete as possible.
. Give people a “help” system that lets them ask what they can do at each room in the game.
. Add any other features you can think of to the game.

. Create several “maps” and let people choose a game they want to run. Your bin/app.py engine

should be able to run any map of rooms you give it, so you can support multiple games.

. Finally, use what you learned in Exercises 48 and 49 to create a better input processor. You have

most of the code necessary, you just need to improve the grammar and hook it up to your input
form and the GameEngine.

 

Good luck!

 

198

Exercise 52: The Start Of Your Web Game

Next Steps

You're not a programmer quite yet. I like to think of this book as giving you your “programming brown
belt”. You know enough to start another book on programming and handle it just fine. This book should
have given you the mental tools and attitude you need to go through most Python books and actually
learn something. It might even make it easy.

I recommend you continue with http://www.djangobook.com/ and start going through the 2nd Edition of
The Django Book. Even if you never plan on doing Python web programming, going through the book
will cement your skills in Python using an actual practical activity. It is also a better framework than the
lpthw.web you were using, but all of the concepts you’ve learned so far apply to the Django web
framework. Just take your time, ask questions, and you’ll get through it.

You could probably start hacking away at some programs right now, and if you have that itch, go ahead.
Just understand anything you write will probably suck. That’s alright though, I suck at every program-
ming language I first start using. Nobody writes pure perfect gold when they’re a beginner, and anyone
who tells you they did is a huge liar.

Finally, remember that this is something you have to do at least a couple hours a night for a while before
you can get good. If it helps, while you’re struggling to learn Python every night, ’'m hard at work
learning to play guitar. I work at it about 2 or 4 hours a day and still practice scales.

Everyone is a beginner at something.

 

199

Learn Python The Hard Way, Release 2.0

 

 

200 Next Steps

Advice From An Old Programmer

You’ve finished this book and have decided to continue with programming. Maybe it will be a career for
you, or maybe it will be a hobby. You’ll need some advice to make sure you continue on the right path,
and get the most enjoyment out of your newly chosen activity.

I’ve been programming for a very long time. So long that it’s incredibly boring to me. At the time that I
wrote this book, I knew about 20 programming languages and could learn new ones in about a day to a
week depending on how weird they were. Eventually though this just became boring and couldn’t hold
my interest anymore. This doesn’t mean I think programming is boring, or that you will think it’s boring,
only that J find it uninteresting at this point in my journey.

What I discovered after this journey of learning is that it’s not the languages that matter but what you do
with them. Actually, I always knew that, but I'd get distracted by the languages and forget it periodically.
Now I never forget it, and neither should you.

Which programming language you learn and use doesn’t matter. Do not get sucked into the religion
surrounding programming languages as that will only blind you to their true purpose of being your tool
for doing interesting things.

Programming as an intellectual activity is the only art form that allows you to create interactive art. You
can create projects that other people can play with, and you can talk to them indirectly. No other art form
is quite this interactive. Movies flow to the audience in one direction. Paintings do not move. Code goes
both ways.

Programming as a profession is only moderately interesting. It can be a good job, but you could make
about the same money and be happier running a fast food joint. You’re much better off using code as
your secret weapon in another profession.

People who can code in the world of technology companies are a dime a dozen and get no respect.
People who can code in biology, medicine, government, sociology, physics, history, and mathematics are
respected and can do amazing things to advance those disciplines.

Of course, all of this advice is pointless. If you liked learning to write software with this book, you
should try to use it to improve your life any way you can. Go out and explore this weird wonderful new
intellectual pursuit that barely anyone in the last 50 years has been able to explore. Might as well enjoy
it while you can.

Finally, Pll say that learning to create software changes you and makes you different. Not better or worse,

 

201

Learn Python The Hard Way, Release 2.0

 

just different. You may find that people treat you harshly because you can create software, maybe using
words like “nerd”. Maybe you’ll find that because you can dissect their logic that they hate arguing with
you. You may even find that simply knowing how a computer works makes you annoying and weird to
them.

To this I have one just piece of advice: they can go to hell. The world needs more weird people who
know how things work and who love to figure it all out. When they treat you like this, just remember
that this is your journey, not theirs. Being different is not a crime, and people who tell you it is are just
jealous that you’ve picked up a skill they never in their wildest dreams could acquire.

You can code. They cannot. That is pretty damn cool.

 

202 Advice From An Old Programmer

 STEVENS

 INSTITUTE of TECHNOLOGY

Text Mining with Python


2016



clipizzi@stevens.edu

SSE

 

Reasons for Text Mining w

85-90 percent of all
corporate data Is in some
kind of unstructured form
(e.g., text)
Unstructured corporate data
is doubling in size every 18
months

Tapping into these information sources is not an
option, but a need to stay
competitive


 

Text mining is a semi-automated process of extracting knowledge from
Unstructured data sources a.k.a. text data mining or knowledge
discovery in textual databases

El
STEVENS INSTITUTE of TECHNOLOGY | >

 

~S

Using Text Mining 

Benefits of text mining are obvious especially in text-
rich data environments

— e.g., law (court orders), academic research (research
articles), finance (quarterly reports), medicine
(discharge summaries), biology (molecular
interactions), technology (patent files), marketing
(customer comments}, etc.

Electronic communization records (e.g., Email)

— Spam filtering

— Email prioritization and categorization

— Automatic response generation

STEVENS INSTITUTE of TECHNOLOGY | 3

 

Challenges in Text Mining ie

Very high number of possible “dimensions”
All possible word and phrase types in the language
Unlike data mining:
records (= docs) are not structurally identical
records are not statistically indeoendent
Complex and subtle relationships between concepts in text
“Comcast merges with Time-Warner”
“Time-Warner is bought by Comcast”
Ambiguity and context sensitivity
automobile = car = vehicle = Toyota
Apple (the company) or apple (the fruit)

STEVENS INSTITUTE of TECHNOLOGY | 4

 

Text Mining Terminology 
Corpus Term dictionary
Terms Word frequency
Concepts Part-of-soeech tagging
stemming Morphology
Stop words Term-by-document matrix
Synonyms Occurrence matrix
Tokenizing Singular value decomposition

Latent semantic indexing

STEVENS INSTITUTE of TECHNOLOGY | 5

 

 

Two Mining Phases 

Text “preparation”. Beyond the typical data mining
cleaning, there are some semantic steps involved
here

Information Extraction. There are several ways To
extract information from clean data, based on the
goal of the search

STEVENS INSTITUTE of TECHNOLOGY |

 

Text “preparation” 

Basic data cleaning
“Tokenization”

stopwords removal
stemming/Lemmatization
Statistical Analysis
Additional Content Analysis

STEVENS INSTITUTE of TECHNOLOGY | 7

 

~S «

Tokenization e

The simplest way to represent a text is with a single
string, but if is difficult to process text in this format

Often, it is more convenient to work with a list of
tokens/elements

The task of converting a text from a single string to a
list of Tokens Is known as tokenization

There are tools/functions within libraries providing this
function: they take a string and returns a list of tokens

STEVENS INSTITUTE of TECHNOLOGY | g

 

stopwords removal S

It is A process To eliminate from the set of
words/tokens with no semantic value

There are words that have no semantic value in all
the contexts, such as articles and pronouns, and
those are part of any “standard” stopwords removal
Orocess

There are other words with intrinsic semantic value,
but irrelevant for the specific case. Those may be part
of the list of stopwords for that case

STEVENS INSTITUTE of TECHNOLOGY | 9

~)=h—l33slUui

Stemming/Lemmatization 

 

This is a step to eliminate words with the same semantic value

Stemming: is the process for reducing words to their stem, base or root form.
Examples are:

“fish” for ‘fishing", “fished” and "fisher’

“argu” for "argue", "argued", "argues", “arguing” and "argus”

Lemmatization: is the process of determining the lemma for a given word, where
“lemma” is the canonical form, dictionary form or citation form of a set of words.
For example, “run”, “runs”, “ran” and “running” are from the same canonical

form “run”. It may be based on an analysis of part of soeech for the words. Part

of speech are linguistic categories, such as noun and verb

A stemmer operates on a single word without knowledge of the context, and
therefore cannot discriminate between words which have different meanings
depending on part of soeech. They also may create “stem” with no dictionary
value (like “argu” above). However, stemmers are typically easier to implement
and run faster, and the reduced accuracy may not matter for some
applications

STEVENS INSTITUTE of TECHNOLOGY | 1

Natural Language ToolKit (NLTK) ¢

 

Is a Python a suite of libraries and programs for symbolic and
statistical natural language processing (NLP) - http://nitk.org/

Created by Steven Bird, Edward Loper, Ewan Klein

Provides:
Basic classes for representing data relevant to natural language processing.

Standard interfaces for performing tasks, such as tokenization, tagging, and
parsing.

Standard implementations of each task, which can be combined to solve
complex problems.

STEVENS INSTITUTE of TECHNOLOGY | 1

Modules lw

~)=h—l33slUui

fis

 

The NLTK modules include:

token: classes for representing and processing individual elements of text,
such as words and sentences

probability: classes for representing and processing probabilistic
information.

tree: classes for representing and processing hierarchical information over
text.

tagger: tagging each word with a part-of-soeech, a sense, etc

parser: building trees over text (includes chart, chunk and probabilistic
parsers)

classifier: classify text into categories (includes feature, featureSelection,
maxent, naivebayes

draw: visualize NLP structures and processes
corpus: access (tagged) corpus data

STEVENS INSTITUTE of TECHNOLOGY | 75

 

Accessing NLTK

Standard Python import command
>>> from nlitk.corpus import gutenberg
>>> gutenberg.items()

[‘austen-emma.txt’, ‘austen-persuasion.txt, ‘austen-sense. txt’,
‘bible-kjv.1xt, ‘blake-poems.ixt, ........ shakesoeare-hamlet.txt’,
‘shakespeare-macbeth.txt, 'whitman-leaves.txt’ ]

>>> from from nitk.corpus import stopwords
>>> stop = stopwords.words (‘english’)
>>> print [i fori in sentence.split() if | not in stop]

STEVENS INSTITUTE of TECHNOLOGY | 73

What you can do with NLTK

Tokenize and tag some text:

>>> import nitk
>>> sentence = """At eight o'clock on Thursday morning
--- Arthur didn't feel very good."""

>>> tokens = nltk.word_tokenize(sentence)

>>> tokens

{'At’, ‘eight', "o'clock", ‘'on',
‘Archusr,; cia , ot, eal;
>>> tagged = nltk.pos_tag(tokens)

STEVENS INSTITUTE of TECHNOLOGY | 14

 

Tokens and Types S

 

¢ The term word can be used in two different ways:
1. Torefer to an individual occurrence of a word
2. Toreferto an abstract vocabulary item
¢ For example, the sentence “my dog likes his dog”
contains five occurrences of words, but four
vocabulary items.
¢ To avoid confusion use more precise terminology:
1. Word token: an occurrence of a word
2. Word Type: a vocabulary item

STEVENS INSTITUTE of TECHNOLOGY | 75

 


Tokens and Types (continued) S

¢ In NLTK, tokens are constructed trom their
types using the Token constructor:
>>> from nlitk.token import *
>>> my_word_type = ‘dog’
‘dog’
>>> my_word_token =Token(my_word_type)
‘dog @|?]

¢ Token member functions include type and
loc

STEVENS INSTITUTE of TECHNOLOGY | 74

 

Tokenization S
 >>> from nitk.token import WSTokenizer
 >>> tokenizer = WSTokenizer(]
 >>> text_str = ‘Hello world. This is a test tile.’
 >>> tokenizer.tokenize(text_str]
[‘Hello'@ [Ow], ‘world.'@[1w], ‘This'@[2w],
Is @[3w], ‘a'@[4w], Test @[5w], ‘file..@[é6w]]

STEVENS INSTITUTE of TECHNOLOGY | 17

 

~~ )=3h= =3)shlUuxR

‘ “a

Texts as Lists of Words 

¢ Text:
“Lama Ph.D. student in Interdisciplinary Graduate Program in Informatics(IGPI),
University of lowa. My advisor is Professor Kang Zhao in Department of
Management Science. My research interests include analyzing social support
and social network based on social media data, such as online health
community and Weibo. | am also interested in building recommendation
system, text mining and association rules mining. “

>>> text='I am a Ph.D. student in Interdisciplinary Graduate Program in Informatics(IGPI), University of Iowa. My advisor is Pro
fessor Kang Zhao in Department of Management Science. My research interests include analyzing social support and social network
based on social media data, such as online health community and Weibo. I am also interested in building recommendation system, t

ext mining and association rules mining. '

>>> text=text.split(' ')

>>> text

['I', ‘am', ‘a', ‘'Ph.D.', ‘student', ‘in', ‘Interdisciplinary', ‘Graduate’, 'Program', ‘in', ‘Informatics(IGPI),', ‘University’,
‘of’, ‘Iowa.', ‘My’, ‘advisor’, ‘is‘', ‘Professor’, ‘Kang’, ‘Zhao', ‘in', ‘Department’, ‘of', ‘Management’, ‘Science.', ‘My’, ‘r

esearch', ‘interests', ‘include’, ‘analyzing’, ‘social’, ‘support’, ‘and', ‘social’, ‘network', ‘based’, ‘on', ‘social', ‘media’

, ‘data,', ‘such', ‘as', ‘online’, ‘health’, ‘community', ‘and', ‘Weibo.', ‘I', ‘am', ‘also’, ‘interested’, ‘in’, ‘building’, ‘r

ecommendation', ‘system,', ‘text', ‘minina', ‘and', ‘association’, ‘rules’, ‘minina.', '']

STEVENS INSTITUTE of TECHNOLOGY | 1

Nee ee

Searching TEXT (nltk)

>>> import nltk.corpus
>>> from nltk.text import Text

ee
STEVENS INSTITUTE of TECHNOLOGY | 19

search (concordance)



 


searching TEXT (nltk)


• Searching words appear in a similar range of
contexts (similar)



 Common context



STEVENS INSTITUTE of TECHNOLOGY | 2

 



Computing with Language: Simple &
Statistics

 

¢ frequency distribution

>>> import nltk

>>> fdistl = nltk.FreqDist(test)

>>> fdistl


>>> fdistl.most_common(50)

STEVENS INSTITUTE of TECHNOLOGY | >)

- ee A

Computing with Language: Simple &
Statistics

 

¢ Collocations and Bigrams

>>> nltk.bigrams(text)

<generator object bigrams at 0x10e87e2d0>

>>> Llist(nltk.bigrams(text) ) |

[('I', ‘am'), ('am', ‘a'), (‘a', ‘Ph.D.'), ('Ph.D.', ‘student'), (‘student', ‘in'), (‘in', ‘Interdiscipli

nary'), (‘Interdisciplinary', '‘Graduate'), ('Graduate', ‘Program'), ('Program', ‘in'), (‘in', ‘Informatic
|} s(IGPI),'), ('Informatics(IGPI),', ‘University'), ('University', ‘of'), ('of', 'Iowa.'), ('Iowa.', 'My'),

('My', ‘advisor'), ('‘advisor', ‘is'), ('is', ‘'Professor'), ('Professor', ‘Kang'), ('Kang', ‘Zhao'), (‘Zh

ao', ‘in'), (‘in', ‘'Department'), ('Department', ‘of'), ('of', ‘Management'), ('Management', ‘Science.'),
3 ('Science.', ‘My'), ('My', ‘research'), (‘research', ‘interests'), ('‘interests', ‘include'), ('include',
, ‘analyzing'), (‘analyzing', ‘social'), (‘social', ‘support'), (‘support', ‘and'), (‘and', ‘social'), ('s
| ocial', ‘network'), ('‘network', ‘based'), ('based', ‘on'), (‘on', ‘social'), ('social', ‘media'), (‘media
i ', ‘data,'), (‘data,', ‘such'), (‘such', ‘as'), ('as', ‘online'), ('online', ‘health'), (‘health', ‘commu
| nity'), ('community', ‘and'), ('‘and', ‘Weibo.'), (‘Weibo.', ‘I'), ('I', ‘am'), (‘am', ‘also'), (‘also', '
j interested'), (‘interested', ‘in'), ('in', ‘building'), ('building', ‘recommendation'), ('recommendation'

, ‘system,'), ('system,', ‘text'), ('text', ‘mining'), (‘mining', ‘and'), ('and', ‘association'), (‘assoc

iation', ‘rules'), (‘rules', ‘mining.')]

>>> List(nltk.trigrams(text) )

STEVENS INSTITUTE of TECHNOLOGY | 39

 

ee Mkt

Categorizing and Tagging Words 

¢ Divide a string into substrings
— nitk.tokenize
¢ Part-of-speech

 

 

 

— nitk.pos_tag
AT Article
NN Noun
VB Verb
IJ Adjective
IN Preposition
CD Number
END Sentence-ending punctuation

 

STEVENS INSTITUTE of TECHNOLOGY | 73

 

 

Categorizing and Tagging Words S

>>> text='I am a Ph.D. student in Interdisciplinary Graduate Program in Informatics(IGPI), University of Iowa. My
advisor is Professor Kang Zhao in Department of Management Science. My research interests include analyzing soci
al support and social network based on social media data, such as online health community and Weibo. I am also in
terested in building recommendation system, text mining and association rules mining. '
>>> tokens=nltk.word_tokenize(text)
>>> tokens
[‘I', ‘am', ‘a', ‘'Ph.D.', ‘student', ‘in', ‘Interdisciplinary', ‘'Graduate', ‘Program’, ‘in', ‘Informatics', ‘'(',
“‘IGPI', ')', ',', ‘University', ‘of', ‘Iowa', ‘'.', ‘My', ‘advisor’, ‘is', ‘Professor', ‘Kang’, ‘Zhao', ‘in', ‘Dep
artment', ‘of', ‘Management’, ‘Science’, '.', ‘My’, ‘research', ‘interests', ‘include', ‘analyzing', ‘social’, 's
upport', ‘and', ‘social', ‘network', ‘based', ‘on', ‘social', ‘media', ‘data’, ',', ‘such’, ‘as', ‘online', ‘heal
th', ‘community’, ‘and', ‘Weibo', '.', ‘I', ‘am', ‘also’, ‘interested’, ‘in', ‘building’, ‘recommendation', ‘syst
em', ‘',', ‘text', ‘mining', ‘and', ‘association’, ‘rules', ‘mining’, '.']
>>> tagged = nltk.pos_tag(tokens)
>>> tagged
[(°r*, “PRP*), (‘an*, “VeP*), (*a’, “pT*}, (*Ph.D.*, “NNP*), (*student*, ‘NN'), (*in*, *IN'), (*Interdisciplinary
‘, 'J3'), ('Graduate', '‘NNP'), ('Program', 'NNP'), (‘in', ‘IN'), (‘Informatics', '‘NNP'), ('(', ‘('), ('IGPI', ‘NN
Pt}, cs", a hae . rep, ('University', ‘NNP'), Caf". ‘IN'), ('Iowa', ‘NNP'), c*.4, 3, ('Ny*., 'PRPS'), (*
advisor', 'NN'), (‘is', ‘VBZ'), ('Professor', 'NNP'), ('Kang', ‘'NNP'), ('Zhao', ‘NNP'), (‘in', ‘IN'), (‘Departmen
t', 'NNP'), (‘of', ‘IN'), ('Management', '‘NNP'), ('Science', 'NNP'), (‘'.', '.'), ('My', '‘PRP$'), ('research', ‘NN
‘), ('interests', 'NNS'), ('include', ‘VBP'), ('analyzing', ‘VBG'), ('‘social', ‘JJ'), ('‘support', '‘NN'), (‘and',
‘cC'), (‘social', ‘33'), (‘network’, ‘NN'), (‘based', ‘VBN‘'), (‘on', ‘IN‘'), (‘social', ‘J3'), (‘media', ‘NNS‘'), (
‘data', 'NNS'), (',', ','), ('such', ‘JJ'), (‘'as', ‘IN'), (‘online', ‘JJ'), (‘health', 'NN'), ('‘community', '‘NN')
» (tand', ‘'CC'), ('Weibo', 'NNP'), ('.', ‘'.'), ('I', ‘'PRP'), ('am', ‘VBP'), (‘also', 'RB'), (‘interested', ‘'JJ'),
(‘in', ‘IN'), ('‘building', 'VBG'), ('recommendation', 'NN'), (‘system', 'NN'), (',', ','), ('text', 'NN'), (‘min
ing', 'NN'), (‘and', ‘'CC'), (‘association', 'NN'), ('rules', ‘NNS'), (‘mining', 'NN‘'), (‘.', ‘'.')]

ee
STEVENS INSTITUTE of TECHNOLOGY | 54

 

 

Introducing WordNet w

¢ A large lexical database, or “electronic dictionary”

¢ Covers most English nouns, verbs, adjectives,
adverbs

¢ Electronic format makes if amenable to automatic
manipulation

¢ Used in many applications (document retrieval and
sorting, machine translation....]

STEVENS INSTITUTE of TECHNOLOGY | 25

 

What is WORDNET

¢ Machine readable semantic dictionary interlinked
by semantic relations

¢ Developed by PRINCETON University

¢ Large lexical database for English language

¢ Language forms a scale free network with small

average shortest path having words as nodes and

concepts as links

source: http://wordnet.princeton.edu/

ME
STEVENS INSTITUTE of TECHNOLOGY | 2

What's so special about WordNet? ¢

 

¢ Traditional dictionaries are organized alphabetically,
so words that are grouped together (on the same
page) are unrelated

¢ WordNet Is organized by meaning, so words in close
proximity are related

STEVENS INSTITUTE of TECHNOLOGY | 37

 

Use of wordnet e

¢ Easily navigable

¢ Used as online dictionary for English

¢ Freely for public availability

— structure to show relations in the form of

* -noun, verb, adjective, adverb
¢ -synonymn
¢ -hypernym (ls a kind of ...)
¢ -hyponym (... is a kind of)
¢ -troponym (particular ways to ...)
* -meronym (parts of...)

source: http://wordnet.princeton.edu/
—E————
STEVENS INSTITUTE of TECHNOLOGY | 28

Basic Design of WordNet

 

WordNet entries are word-concept mappings

One concept can be expressed by many words
(synonymy):

— {car, auto, automobile}
— {close, shut}
¢ One word can express many concepts (polysemy):
— {club, stick}
— {club, nightclub}
— {club, playing card}

STEVENS INSTITUTE of TECHNOLOGY | 29

 

Basic Design of WordNet S

¢ WordNet's building blocks: sets of synonyms (synsets]

— {hit, beat}
— {big, large}
— {queue, line}
¢ Each synset expresses a distinct concept.

¢ Currently, WordNet contains appr. 117,000 synsets

ME
STEVENS INSTITUTE of TECHNOLOGY | 39

Basic Design of WordNet \@
WordNet stores, and allows one to retrieve,

--all concepts that a given word can express
--all words that express a given concept

STEVENS INSTITUTE of TECHNOLOGY | 3)

 

 

Graphic representation using Tree ¢
structure



STEVENS INSTITUTE of TECHNOLOGY | 35

~)=h—l33slUui

Graphic representation using Force 
Graph

 


STEVENS INSTITUTE of TECHNOLOGY | 33

 

Sentiment Analysis iw

Sentiment
— A thought, view, or attitude, especially one based
mainly on emotion instead of reason
Sentiment Analysis
— opinion mining
— use of natural language processing (NLP) and
computational techniques to automate the extraction
or classification of sentiment from typically
Unstructured text

° Can be applied to get product reviews, consumer attitudes,
trends

STEVENS INSTITUTE of TECHNOLOGY | 34

Sentiment Analysis: Approaches

Machine learning

— Naive Bayes

— Maximum Entropy Classifier
Support Vector Machine
Markov Blanket Classifier

Unsupervised methods
— Use lexicons

 

STEVENS INSTITUTE of TECHNOLOGY | 35

 

Limitation of NLTK

¢ Language limitations

¢ Corpora limitations

¢ Taxonomy limitations

¢ Minimal support for large sets of small text
¢ Limited predictive capabilities

¢ Limited associative capabilities

STEVENS INSTITUTE of TECHNOLOGY | 3

Text as network - Wordij ¢

 

WORD] is d package containing a set of tools for text
analysis

lt takes plain UTF-8/txt format files as inout

It creates Tiles to be analyzed by Wordi itself or by
external tools

One of the most information reach file is The network
of words

Network is based on co-occurrence of words within a
given window of words

STEVENS INSTITUTE of TECHNOLOGY | 27

~~ )=3h= =3)shlUuxR
iy

“a

Using Wordij 


• Source File: text file in UTF-8
format
• Drop List File: file with a list
of words that will be
dropped
• Drop words / pairs
appearing less often than:
words / pairs appearing less
often will be not included in
the output files
• Window size for extracting
word pairs: two words
equal to or less than
window size preceding and
after each word in the tex


STEVENS INSTITUTE of TECHNOLOGY | 3g

 

Wordij files

 

 STEVENS

w INSTITUTE of TECHNOLOGY

Text Parsing with Python

2016



clipizzi@stevens.edu

SSE

 

What is Text Parsing?

> Common programming task
¢ Extract or split a sequence of characters
¢ Applications:

Simple file parsing

Data extraction

Find and replace

Parsers — syntactic analysis
Natural Language Processing

S

STEVENS INSTITUTE of TECHNOLOGY | 9

 

Ss

Python Text Parsing Methods ie

¢ String Functions

Faster, easier to understand and maintain
If you can do, DO IT!
Different built-in functions

¢ Regular Expressions

concise way for complex patterns
amazingly powerful
wide variety of operations

when you go beyond simple, think about regular
expressions

STEVENS INSTITUTE of TECHNOLOGY | 3

 

Managing Strings

¢ We can get at any single
character in a string using an
index specified in square
brackets

¢ The index value must be an
integer and starts at zero

¢ The index value can be an
expression that is computed



STEVENS INSTITUTE of TECHNOLOGY | 4

 

A Character Too Far

¢ You will get a python
error if you attempt to
index beyond the end
of a string.

¢ So be careful when

constructing index
values and slices

>>> Zot = ‘abc’

>>> print zot[5]

Traceback (most recent call last):
File "<stdin>", line 1, in
<module>|IndexError: string index
out of range

>>>

STEVENS INSTITUTE of TECHNOLOGY | 5

 

 

Strings Have Length 

¢ There is a built-in function
len that gives us the

length of a string 

STEVENS INSTITUTE of TECHNOLOGY |


Looping Through Strings 
¢ Using a while
statement and an
iteration variable, and 
the len function, we 
can construct a loop 
to look at each of the 
letters in a string index = index + | 22

individually

STEVENS INSTITUTE of TECHNOLOGY | 7

 

Looping and Counting

¢ This is a simple loop
that loops through
each letter in a string
and counts the
number of times the
loop encounters the
‘a character.



word = ‘banana’
count = 0
for letter in word :
if letter == ‘a’:
count = count + |
orint count

STEVENS INSTITUTE of TECHNOLOGY | g

 

 

Slicing Strings 

¢ We can also look at any
continuous section of a

string using a colon 
operator 

¢ The second number is
one beyond the end of 
the slice - “up to but not 
including” 

¢ If the second number Is
beyond the end of the
String, it stops at the end

STEVENS INSTITUTE of TECHNOLOGY | 9

String Concatenation

When the +
operator is
applied to
strings, it means
"concatenation"

STEVENS INSTITUTE of TECHNOLOGY | 1

 

 Using in as an Operator

The in keyword can
also be used to check
to see if one string is
"in" another string
• The in expression is a
logical expression and
returns True or False
and can be used in an
if statement


String Comparison
if word == 'banana':
print 'All right, bananas.'
if word < 'banana':
print 'Your word,' + word + ', comes before banana.’
elif word > 'banana':
print 'Your word,' + word + ', comes after banana.’
else:
print 'All right, bananas.'

STEVENS INSTITUTE of TECHNOLOGY | 75

String Library

Python has a number of string
functions which are in the string
library

These functions are already
built into every string - we
invoke them by appending the
function to the string variable

These functions do not modify
the original string, instead they
return a new string that has
been altered

 

>>> greet = ‘Hello Bob’
>>> zap = greet.lower()
>>> print zap

hello bob

>>> print greet

Hello Bob

>>> print 'Hi There'.lower()
hi there

>>>

STEVENS INSTITUTE of TECHNOLOGY | 73

 

String Library str.capitalize()
str.center(width[, fillchar])
str.endswith(suffix[, start[, end]])
str.find(sub[, start[, end]])
str.lstrip([chars])
str.replace(old, new[, count])
str.lower()
str.rstrip([chars])
str.strip([chars])
str.upper()
STEVENS INSTITUTE of TECHNOLOGY | 14

 

Searching a String 

¢ We use the find() function to
search for a substring within
another string 

¢ find() finds the first
occurrence of the substring

¢ If the substring Is not found,
find() returns -1

¢ Remember that string
position starts at zero



STEVENS INSTITUTE of TECHNOLOGY | 75

search and Replace 

¢ The replace()
function Is like a
“search and replace”
operation in a word
processor

¢ It replaces all
occurrences of the
search string with the
replacement string

>>> greet = ‘Hello Bob’

>>> nstr = greet.replace(‘Bob','Jane’)
>>> print nstr

Hello Jane

>>> greet = ‘Hello Bob’

>>> nstr = greet.replace('o’,'xX’)

>>> print nstr

HellX BXb

>>>

STEVENS INSTITUTE of TECHNOLOGY | 74

 

Stripping Whitespace

¢ Sometimes we want to
take a string and
remove whitespace at
the beginning and/or
end

¢ Istrio() and rstrip() to the
left and right only

¢ stripo() Removes both
begin and ending
whitesoace

 


STEVENS INSTITUTE of TECHNOLOGY | 17

 

Example 

From stephen.marquard@uct.ac.za Sat Jan 5 :16 2008

>>> data = ‘From steohen.marquard@uct.ac.za Sat Jan 5 :16 2008’
>>> atpos = data.find('@')

>>> print atpos

21

>>> sopos = data.find(' ',atpos)

>>> print sopos

3]

>>> host = data[atpos+] : sopos]

>>> print host

UCT.aC.Za

STEVENS INSTITUTE of TECHNOLOGY | 1

 

Regular Expressions

In computing, a regular expression, also referred to as
"regex' or "regexp", provides a concise and flexible
means for matching strings of text, such as particular
characters, words, or patterns of characters. A regular
expression is written in a formal language that can be
interoreted by a regular expression processor.

htto://en.wikipedia.org/wiki/Regular_expression

ME
STEVENS INSTITUTE of TECHNOLOGY | 19

 

~S «

Understanding Regular Expressions i&

¢ Very powerful and quite cryptic
¢ Fun once you understand them

¢ Regular expressions are a language unto
themselves

¢ A language of "marker characters" - programming
with characters

¢ Itis kind of an “old school’ language - compact

STEVENS INSTITUTE of TECHNOLOGY | 2

eee A ||

 

Regular Expression Quick Guide iw

N

$
\s
\S

*
*¢
+
+¢

Matches the beginning of a line
Matches the end of the line
Matches any character
Matches whitespace
Matches any non-whitespace character
Repeats a character zero or more times
Repeats a character zero or more times (non-greedy)
Repeats a chracter one or more times
Repeats a character one or more times (non-greedy)

[aeiou] Matches a single character in the listed set
[AXYZ] Matches a single character not in the listed set
[a-z0-9] The set of characters can include a range

(

Indicates where string extraction is to start
Indicates where string extraction is to end

STEVENS INSTITUTE of TECHNOLOGY | >)

 

~S «

The Regular Expression Module lw

¢ Before you can use regular expressions in your
program, you must import the library using “import
re"

¢ You can use re.search() to see if a string matches a
regular expression similar to using the find() method
for strings

¢ You can use re.findall() extract portions of a string
that match your regular expression similar to a
combination of find() and slicing: var[5:10]

STEVENS INSTITUTE of TECHNOLOGY | 25

Using re.search() like find() 

hand = open('mbox-short.txt')
for line in hand:
line = line.rstrip()
if line.find('From:') >= 0:
print line


import re
hand = open('mbox-short.txt')
for line in hand:
line = line.rstrip()
if re.search('From:', line) :
print line
STEVENS INSTITUTE of TECHNOLOGY | 73

 

Using re.search() like startswith()

 

hand = open('mbox-short.txt')
for line in hand:
line = line.rstrip()
if line.startswith('From:') :
print line

import re
hand = open('mbox-short.txt')
for line in hand:
line = line.rstrip()
if re.search('^From:', line) :
print line

STEVENS INSTITUTE of TECHNOLOGY | 24


 

STEVENS INSTITUTE of TECHNOLOGY | 39

  

: Hello, Hello, everybody! 1:55 : What are you? All This is the very last class 2:00 Scott Guetens: doing well. 2:07 : So let's wait another few minutes. So a few sessions. It's 2:11 : 628, so let's start the home time at 6. 30, 2:20 : so in theory you have a 5 min. But considering that the number is kind of a a limited. 2:26 : If you want to take more time for your presentation, you can definitely do that. The more time doesn't mean an hour, but can be 15 min instead of 5 min. So it's definitely up to you. 2:37 : We have a I don't know exactly how many of you did that 2:51 : easy teams, but I have a 2:58 : one team that presented last week, and that it's raised by the way and let's see 3:03 : what is going to be today. 3:14 : So just to have a a a quick start. Yeah. Yeah. Yeah, Good. Good. I just had a question. If I was in a group. Do we all need to submit the report 3:18 Dean Manomat: individually or just one person. 3:27 : Yeah, I mean, you know, if you can submit that the same exact report, if you have a a presentation or the submit that and the python that. 3:29 Dean Manomat: Okay. I thought it was just one, so I didn't submit it on Sunday, but I can submit it after. 3:42 Dean Manomat: Thank you. 3:50 Scott Guetens: Sorry, Professor, Another question did. Were we supposed to submit the presentation as well along with the the report and the code 3:51 : you don't have to. 4:00 : It's generally what people do. But I mean, you don't have to. 4:03 Scott Guetens: Okay, thanks. 4:08 : I was. I was wondering if I could volunteer to go first by chance. 4:09 Scott Guetens: Yeah, okay, that's fine. I can. I can. 4:22 Scott Guetens: I can go if everyone's ready. 4:25 : Oh, okay, go ahead. 4:28 : That's what they've been? Asked Leona. Yeah. 4:34 Scott Guetens: Who Who is that? 4:38 Leona Chia: Yeah, you and me? We're in the same team man. 4:40 Scott Guetens: What this is, Scott. 4:44 Scott Guetens: Oh, I thought it was being talking. I'm sorry. That's okay. I was like, Wait what I don't remember. Work with you. Okay, that makes sense. 4:47 Scott Guetens: All right. 4:58 Scott Guetens: Is everyone able to see my screen? 5:01 : Yep. 5:04 Scott Guetens: Let me put this up. 5:06 Scott Guetens: and can everyone see the Powerpoint? 5:09 : Yep. 5:12 Scott Guetens: All right. So i'm gonna go ahead and get started. So 5:14 Scott Guetens: I 5:19 Scott Guetens: did my final project on the publicly available Federal aviation administration's digital obstacle file. 5:20 Scott Guetens: and I ran a data analysis on it with my Python script that I wrote. So the digital obstacle file is a a do f dot d at file that the faa puts out every 56 days, and it gets updated with obstacle data, and when I say obstacle. I'll, I'll explain what that means as I go forward. 5:31 Scott Guetens: so 5:55 Scott Guetens: the contents of my my project. My report here is that the first slide will be the project goals and the introduction to the data obstacle file. 5:57 Scott Guetens: I'll go into a little bit about my methodology for data analysis. 6:09 Scott Guetens: I'll show you some of my results that I came from that I came upon using my script, and then some conclusions that I came to in this project. 6:13 Scott Guetens: So the goals of my project. My goal was to improve the safety of the airspace for air traffic control systems and pilots. By analyzing this 6:26 Scott Guetens: data digital obstacle for obstacle file that the Faa puts out to kind of get some ideas about where these obstacles are are primarily located, some of the more common obstacles, and then the less common obstacles 6:37 Scott Guetens: what the highest obstacles! And when I say Hi, I mean height above ground level, so they they say agl, which is above ground level, and that height 6:55 Scott Guetens: I I I wanted to see, like, what are some of the highest obstacles that they record data on. I thought it was pretty interesting, given what's been on the news the last couple of months, what actually was the highest. But we'll get to that, and then I also wanted to see if there was a way I could build an interactive map 7:06 Scott Guetens: of all these obstacles. The conclusion I came to was was not really but i'll I'll get to that as well. 7:23 Scott Guetens: So I just also want to mention I I. So I analyzed this data set to ab same these insights and and to reach this goal. 7:30 Scott Guetens: all right. So some of the methodologies I used in my program for data analysis. 7:41 Scott Guetens: So some ways I manipulated the data. This data set consist of over 550,000 data points. So it was a lot of data to sort through. So some of the methodologies I use for grouping, categorizing and sorting the data. I calculated some summary statistics. 7:48 Scott Guetens: I created some visualizations. 8:07 Scott Guetens: I did some data, manipulations and processing, and I did try to do a correlation 8:11 Scott Guetens: analysis which didn't really come to much. You can see the result over here. I tried to do a correlation analysis between the latitude and longitude, and then the 8:17 Scott Guetens: the 8:29 Scott Guetens: above ground level. Basically how tall it is, how high the object the obstacle is. But I came to find that there was not really a correlation based upon where the obstacles located and the height of the object. 8:30 Scott Guetens: So 8:48 Scott Guetens: sorry my screen froze. 8:50 Scott Guetens: Can you hear me cool? Okay. Sorry. So here's my first set of results that I got. So some 8:54 Scott Guetens: things I calculated in this data were the largest item. The the tallest item above ground level was in in this data set that was recorded, which is mostly contained. 9:04 Scott Guetens: contains fa reported data through studies and things like that. So the the the highest item that was, there was a balloon that was recorded at 14,947 feet. 9:17 Scott Guetens: The lowest item was a fence was at 0 feet, but it was interesting that they're recording 9:28 Scott Guetens: offense. I also don't understand how it's 0 feet tall. 9:35 Scott Guetens: But some interesting data the average height above ground level was a 164.6 feet 9:39 Scott Guetens: so, and then the Median height above ground level. Of all the data was 86 feet and to the right here, you can see I made an interesting box plot of all of the different types of 9:48 Scott Guetens: data points, so I I took the obstacle name. Scott Guetens: which was in a specific field in the data. And I yeah Scott Guetens: calculated how high each one it or I, I, I use the above ground level data point to see, basically like, what is this spread the distribution of Scott Guetens: of how I and how low each individual object is? Scott Guetens: And you can see there's around 19 different types of objects that are reported. Some of them are pretty general terms, like one just says, Plant can't really tell exactly what type of plant that is, Hank. I'm not really sure. Make water tank. Scott Guetens: but there were only 20 used. And, like I said, there were we over 50,000 or over 500,000 data points so clearly they follow some sort of a methodology or system there. But anyway, you could see how broadly spread the data is in this box plot. Scott Guetens: and interestingly, and it's it's. It went so far as it it went above my Scott Guetens: above my Scott Guetens: graph here, but it recorded the balloon as as way higher than the you can kind of see this cluster of all of the other objects, and how high they are. So that was definitely an outlier that I struggled to deal with, and and I actually think it broke my plot a little bit. Scott Guetens: But let me on here. Scott Guetens: So some more results I came to. So the States, with the most obstacles, are some of the bigger States, Texas, California, Florida, Illinois, and Kansas over here on the left. Scott Guetens: and then you can see how many obstacles each of those locations have. I'd also like to point out that this data set also includes non some non-us data Scott Guetens: which was I? I didn't really wasn't really able to calculate in here with the States, because there are 2 different fields as you can see on the bottom, right city or state, but like over here you have the country, which is Puerto Rico Scott Guetens: and it those data points are calculated as well, because they, the Faa still cares about those for all surrounding United States flights. Scott Guetens: And then over here you see the States with the least amount of Scott Guetens: data points or of obstacles recorded. That's DC. Delaware, or Dial in Vermont and Hawaii. Scott Guetens: So then I broke it down to cities, and I analyze the the cities with the most at least so you can see Junction City. It occurred a lot in this Scott Guetens: in this data. I didn't know what Jackson City was, so I looked it up as I was running my report, and I realized that Junction City is just a really common name for a city. And so Scott Guetens: when I built this piece of the code. I actually found a bug Scott Guetens: where it was not so I should have had a check for what state the city was in. But Instead, I just got all of the data points for all of the town's name, Junction City. Scott Guetens: and that all added up to be greater than some of the bigger areas that you can see below that are actual, just single-name cities. Scott Guetens: So then you have Miami, San Diego Columbus Chicago. Pretty big cities makes a lot of sense. Scott Guetens: and then it is with the least. I just thought it was interesting to put a couple of them in there. There were a bunch of cities with one obstacle when I was running through it. But Scott Guetens: these were just the the top ones on the list that I saw. And then in the top right? You can see some really interesting data on the most common types of obstacles that exist. So you have the by far Scott Guetens: largest amount of obstacles are towers, and I the hour can mean a lot of things. And I I, in doing some research about the data set itself. I learned that the next line is also our but it's a transmission line specific tower, which is Scott Guetens: like a transmission line for, like, I think, cell phone towers or something. I was reading. Scott Guetens: But so the top 2 are really towers, all different kinds of towers, whatever that means generally. Scott Guetens: And then you have polls. I was really surprised to see windmills in there. There's a lot of windmills. I found that really interesting, because I didn't know there were still so many windmills around the country. But apparently that's something that general aviation cares a lot about. So there's 76,000 of them record in this data set Scott Guetens: buildings makes a lot of sense I actually would have expected that to be like the most common. Scott Guetens: But you could see there it only 58,000, Scott Guetens: and then the 5 least common that they recorded. You can see that there is only one gate wind indicator, whatever that means. Only one ship. I don't understand why it only recorded one ship Scott Guetens: like, I said, this is one of the limitations I'll get to that. But this is one of the limitations of the data set. This is not all encompassing. It's only what the faa has. It is as it's at its disposal in terms of the studies they conduct, and things like that. Scott Guetens: I just make sure i'm not going over time. Scott Guetens: and then Scott Guetens: heat cool system again. I'm not sure what that means. And then, Arch, I was picturing like the St. Louis arch things like that. Scott Guetens: And then on the bottom, right here you can see the 5 highest and 5 lowest obstacles that they had. The 5 highest are all balloons. So that's what I was referring to earlier, when I was Scott Guetens: discussing what was what has been in the news the past couple of weeks. I I imagine Scott Guetens: they've updated this data set because I don't know if I mentioned every 56 days they update it. Scott Guetens: and I imagine that they started really tracking these balloons after that news cycle Scott Guetens: going forward. This was just an interesting thing. I did where I I built a scatter plot of the latitude, longitude, decimal conversion, decimal, converted data points. Scott Guetens: and what it did was draw a pretty cool map of the United States. So I thought that was interesting. Not too much mathematical value here, but I thought it was a pretty cool thing that I did where you can really see in it : we can. Up to you. We we can see you, but we cannot hear you anymore. : Scott. : Scott. we lost You : got : okay, and let's wait a moment for Scott to come back. : I'm becoming a an expert on aviation. So : that's a sec. On the : presentation on the aviation data. Then I had the student master student doing her : on a so slowly but surely, and becoming an expert in aviation. : Okay. So : let's wait another few seconds, and then, if : Scott cannot join back. : we will go to the next. : In the meantime, Who? What? To be the next? : I mean, based on alphabetical order, I would say : umbrella would be the first. : Are you ready? : Okay, I mean, I'm: sorry that your first name a last name. You are with the first. : Okay, so : let's do that. So let's go ahead. April Amaral: Okay. let me. : Oh, hold on. : Scott. Scott Guetens: Hi, Professor! Sorry. My laptop totally froze. : Okay, I'll get it back. Do you want someone else to do the presentation while you will fix it up. Scott Guetens: So where where did it cut off? I I don't even know with the map of us. So Scott Guetens: yeah, I am reselling my laptop. Now it'll just be a minute. But Scott Guetens: yes, so yeah, someone else can go if they want. Well, way. I okay, sounds good. I've been looking glad. Sorry about that. : Yep. : So April Amaral: yeah, sorry. Let me share the screen. April Amaral: Okay, I'm: we're sharing. Okay. April Amaral: So I did my project on the official world golf ranking website. April Amaral: And okay. So here's just a little overview of what my project was. I'll give you a little background April Amaral: on the official world. Golf ranking website what it is, and how they April Amaral: do their rankings. April Amaral: And then i'll talk about my analysis of the data set, which was specifically for week 14, which is the weekend ending on April ninth, 2023, and then i'll talk about my conclusions from my analysis. April Amaral: Okay. So what is owdr it's a system of rating the performance levels of male professional golfers all around the world. April Amaral: The rankings is based on a player's position in individual tournaments or a rolling of 2 year period. April Amaral: The the website produces new rankings every week. April Amaral: and the calculate the rankings by dividing the toll points My number of events played in this average. Then whoever has the highest average that is April Amaral: at the top of the ranking, and it goes highest, lowest. April Amaral: So in my analysis I took this data set and I April Amaral: filtered it, and we only use and manipulated to only use specific columns April Amaral: of the data set. So I looked up the I'll i'll. I'll show you in a minute, when which columns I picked. But April Amaral: I used to find the top 10 players in the ranking the top. April Amaral: the bottom 10 players in the ranking the players who have increased their rank since the end of 2,022 the players of all who have decreased their rank, and who have not changed the rank since the end of 2,022. April Amaral: I also created a loop for a person to select a country. And the April Amaral: it will display the top 10 players from the selected country. April Amaral: I also created some charts for the top 10 countries with the most players. April Amaral: and I also have a chart for number of events played per percentage of players. So here we go. So here is April Amaral: what came up for the top and bottom, 10 for the ranking. April Amaral: So I use the columns of what their current rank is the rank of April Amaral: the end of 2022 which country. They're in the player's name. Their total points and number of events. April Amaral: And April Amaral: so here we have April Amaral: the rings that have changed since 2022 so the top here we have. April Amaral: who has increased their rank. April Amaral: Since the end of 2,022, we have about 994 April Amaral: players and then who's decreased their ranks in Santo, 2022. We have 7,399, April Amaral: and who has the same rank, is just this short list here. April Amaral: And April Amaral: okay, so like, I said, I created a loop for a person to select a country and the top 10 players from that country. So here I have an example of France and the United States. April Amaral: It gives all that information. The players name their points. April Amaral: I also have Italy and Japan. and I also another example for Chile. April Amaral: and then I have a example here If you selected a country that's not in the list, would say, no players found. and then you just hit exit to and the loop. April Amaral: And then, okay. So here is my chart for the top countries with the most players. April Amaral: and. as you can see, the United States has the most. April Amaral: And here is my hi chart for number of of events played by percentage of players. so April Amaral: based off of the data set. So the the highest ranking right it is by the most events played, so the highest would be probably in the 3.9%, April Amaral: 4.9% in the most are in the They've only played one to 9 events. April Amaral: Okay. And so what can I conclude from this? I think this would be useful for gambling? I personally don't April Amaral: gamble, but I feel like this Information wouldn't be useful for people that do that sort of thing, and and this would be helpful for upcoming sponsorships sponsors who wanna April Amaral: see who who is better April Amaral: playing, and also for a tournament participation as well. And this is also would be good for players to view their competition, even though you know golf is a individual sport. It's good to see how other people are playing. April Amaral: and that is it? That's all I got any questions. : Okay, I think it's a it's it's a good job. So are you a golf player, or I would Just I am. Well, I just picked it up. So i'm still be a beginner. Okay, all right, All right. Are you enjoying? I do enjoy it. Yes. : okay. : I mean, i'm a runner and don't do many other sports, and to me golf : it's kind of a question marker just to say with your chart. But I mean that that's me. : anyway. Okay, thanks a lot. So let's go back to Scott Guetens: Yep. I am ready to go. Hopefully. My laptop doesn't break again. Scott Guetens: I need to look a new laptop clearly Scott Guetens: let me share my screen. So I think Scott Guetens: I got to the point where I was on the the scatter plot just click through. Scott Guetens: So yeah, I was just pointing out that I I made this interesting scatter plot, like I said, not really much mathematical value here for analysis purposes, but I thought it was interesting how, when I plotted the data on like an xy axis Scott Guetens: of the latitude and longitude decimal values, that it basically drew a map of the us, because you can see how prominent these Scott Guetens: data points are throughout the Us. You could see MacOs. You could see Canada, Alaska pretty clearly. Scott Guetens: So I just thought that was pretty interesting. Scott Guetens: and in conclusion, so the goal of my study was to improve the safety of the airspace for air traffic control systems and pilots. Scott Guetens: And so I used various data analysis strategies to Scott Guetens: understand this data, such as summary statistics, data, visualization, manipulation, correlation, and categorization and a few limitations that I faced while and analyzing this data was the quality of the data as you saw in my box plot. Outliers were really a challenge for me for figuring out how to get them out of the box plot. It got it to a point where I wasn't able to to exclude it, which really skewed my data. Scott Guetens: That was just a the limitation of my ability in python, I think. But then the accuracy of the data was also a limitation. So one of the fields, in the and in the data file was the accuracy of each data point like how well the fa knows Scott Guetens: that that object is where they think it is. Scott Guetens: And that was I was trying to find a way to incorporate that into my analysis. But that started to get really complicated. But so that was more of a limitation of of my understanding of this really complex data file. Scott Guetens: And then, finally, the generalization, like I mentioned previously, this is an analysis just on this one fairly large data file, but it's obviously not all encompassing of of any other similar data sets Scott Guetens: so overall the Df. The it. Data set provides the valuable insights into the aerial obstacles monitored by the Faa, and can be used to enhance the safety of the airspace. Scott Guetens: This project was really interesting for me because I actually work on the Scott Guetens: air traffic control system that the air traffic controllers use the software system to direct all and route air traffic above 10,000 feet. So I thought this would be interesting to see Scott Guetens: like. Scott Guetens: why do we kind of distribute the airspace into different different heights. Scott Guetens: So I I use that. And I used my experience for my job to kind of build this data analysis, and and I found it really interesting if it's all right. If I have another quick minute. I want to show Scott Guetens: the interactive map that I built as well with my python program. I don't know if you guys are able to see this, it might take a second to load. Scott Guetens: Can you see that? : Not yet? Scott Guetens: Okay, let me know if it's visible. : So once you load that, I think, let let me give you a couple of feedback one is on when you show the data by State, and Cds : obviously states that are big are cities that are bigger. They have more objects. So it would be more of an indication to normalize the numbers by the size of the state of speed. : because at that point you may have a more of an index of sort of a risk that you have in that particular again city or a state. : The other thing that it would be interesting to do is to : find the sort of a a correlation between objects and accidents. : So, using another data set with accidents hopefully, they are jail located at the At the point. You can see that we have the objects that are more at risk. : or it can be more a risk for accidents, because not necessarily just a model of height. So it could be the shape. It could be all other things. So that's something I mean : I not criticizing what you do, but it will be, and what's next for the for the project that that you did. Scott Guetens: Yeah, I I definitely agree. It's interesting because one of the data points with I thought which I thought was a little strange to include, but it's it's pretty cool that they do is you can actually they They indicate the color of the object in the data set. They have, like they have like a group of of points for that, and that's one of the fields. Scott Guetens: and and like a marker so so that Scott Guetens: would go probably hand in hand with what you're talking about. Like if I can find like an an accident data set. I know the majority of accidents in in the error occur typically from like a birds like bird strikes. Scott Guetens: But you know, maybe there's still some correlation that that'd be interesting to look at, for sure. Scott Guetens: Yeah. : So but Scott Guetens: yeah, so so real quick. I just wanted to show I used the Api called folium. I don't know if you're familiar with it, to to try to plot all these data points. So I tried to do it with the full data set. It was not possible. The it was it. Basically what it does is it builds an an HTML file Scott Guetens: that calls in a bunch of like supporting mapping files. And I built this interactive Scott Guetens: data's data set. But I was only able. I had to kinda Scott Guetens: make the data set a little bit smaller. I had to manipulate it, so I I focused on everything higher than a 10, everything higher than Scott Guetens: 1,000 feet. Scott Guetens: So the vast majority of the data is is short, is lower than 10,000 feet, so you can see there's still quite a bit of of pieces of data. You can see. You know we talked about Texas. There's quite a piece, quite a few pieces in text, but just like you said it's a big state. Scott Guetens: you know big cities in that state. You can see like where a lot of the big ones are. Scott Guetens: But I just thought this was an interesting piece that I oh. spent a bit of time on that. I I thought it was pretty cool. I wanted to share : Good job. Okay. Scott Guetens: Thank you. : All right. So following the : are you okay? Presenting? Leona Chia: Yes, sorry. I was trying to click that. Yeah, I can let me Leona Chia: open Leona Chia: my presentation. Leona Chia: I don't know why I didn't do that earlier. My bad Leona Chia: see shared screen. Leona Chia: Okay, let me know. Once you guys can see that. : Yup, we can stay it. Dean Manomat: Yeah. Leona Chia: Okay. So I just you can see it still. : Yep. Yep. Leona Chia: Okay. I just use your template that you sent out as an example. So Leona Chia: fancy ones. Leona Chia: anyway. So we did our project. Me, Dean and Nina. Leona Chia: on public and media perception of mass shooting. Leona Chia: Let me give you some background. Leona Chia: Okay, so Leona Chia: obviously within the last month there was a lot of media attention on, like mass shooting it. It just kind of feels like it's been increasing a lot. We wanted to make sure that Leona Chia: it was real, I guess real. Or was it like sensationalized by the media? We didn't do we didn't correlate this to mental health or gun loss, As you can see, the assumptions were that, as you see in the screen, just because Leona Chia: the scope is quite big. Leona Chia: But anyway. so the project will was to investigate the public and media perception. So, using subjectivity instead of, you know, like I said, mental work on law, mental state or gun loss Leona Chia: to to a as the cause of mass shooting, or to find out the cause when analyzing the rate of number of mass shooting. We were limited to what's already out there need to obviously normalize some data while balancing Leona Chia: relevant view viewpoints on the topic. So there's a lot of Leona Chia: noise and or data in the media attention, obviously for mass shooting. So we had to deal with a lot of, I guess, combing through articles dealing with subjective discussion data also can be challenging. Leona Chia: So any one. Leona Chia: This is understanding. I think that's Dean. Dean Manomat: Yeah. So Dean Manomat: the problem to solve the problem to find the root cause of the mass shooting. We identified Eric. so we use, I believe, 5 different sources and reports from each stores Dean Manomat: to look through what they had to say about the mass shootings and what their reasoning is behind them. So. Dean Manomat: using the public forums we Dean Manomat: analyzed, we we use analysis and method, planning Dean Manomat: data, extraction, mythology Dean Manomat: developed visualizations which we will see later in the presentation. And then we evaluated it in terms of interpretations. Leona Chia: So in the data understanding again. So we have to collect data just to see the trends if it was actually growing or increasing. Or, again, based on media, was it just sensationalized? So we grab some data and a a Csv file. Leona Chia: and made a graph from that which I will show you way later, because all the attachment which is in the end. anyway. And then we have to understand some of the outliers so based on the excel that we have Leona Chia: the baseline of what was considered mass shooting. Was Leona Chia: it it changed? Basically, so Federal was started in at 4 as as the definition for Leona Chia: mass shooting. But then, in 2,013 they actually moved it to 3. So you can see a Leona Chia: quite a big spike from from those 2 dates. Leona Chia: and it. Leona Chia: anyway. So from the articles we grab Leona Chia: Conservative, leading Liberal leading, and then read it as kind of the neutral round, just to see where they're going, like where it Leona Chia: it was headed to for root cause. Obviously, with this topic, guns are going to be prevalent, and some other Leona Chia: words are going to be prevalent. So we kind of have to understand the context. So we had to use Bikerams just to get those Leona Chia: right. Leona Chia: And then here we have to prepare a few like I said this Csv file. We need grass from that. Leona Chia: Another thing that we had to do, because, like New York Times and Cnn, they're always ever changing. They're constantly updating, and some of them have policies to kinda Leona Chia: stop people from scraping data. So I what I did was on those I took Leona Chia: the content and just copy and paste and put into a txt file, and then some like fast. We were able to do that just like we did in the homework, and then red it. We had to do a little bit more. We actually had to be a developer level, and then we had to create an app Leona Chia: for it. And then after that, then we can scrape the comments from a certain post. Leona Chia: So Nina or Dean. Mina Shafik: No, I got it so Mina Shafik: like they said we extracted all the data through our graph shows the result of mass shootings. Mina Shafik: We went in looking at this neutral, so we didn't go in trying to face one side or the other. Mina Shafik: We wanted some of the things that we popped up. Is is there a greater new need for solution for a solution? Mina Shafik: So what were the keywords that people were saying, what are the causes? Was it mental health, gun, control, etc., etc.? Mina Shafik: So we use natural language pressing through python to take the sediment values of the discussion. Again, we look for common words and diagrams for every single one of the data sets Mina Shafik: we created data visualization. This is the sorry Mina Shafik: we have. Mina Shafik: My head's not there Today Leona Chia: We created a word clouds to run it to run it through. We also created the sentiment analysis, for every single one of the websites ran it through Mina Shafik: for any of the keywords what was believed to be the issue. Mina Shafik: and then we compared all the different opinions from each one. Mina Shafik: and what was similar about them, to try and fix and see what the issue that they're presenting. Again, we did not go in Mina Shafik: with into this project looking for a specific cause. You know Mina Shafik: we go to next slide. Leona Chia: I'm going to actually move to the Leona Chia: the work. We cloud the attachments that way, we can talk about it then. Leona Chia: so really quick, so obviously it has been increasing like I said, the trend line from 2,013. You can see the big spike afterwards. The trend line is going up, and just use a linear interpolation of those. And then again. Leona Chia: It looks like it's going to be normalizing because of Covid. You can see that spiked out from 2020 to 2021. Then it went back up, and it looks almost like it's normalizing. But, in fact, the average yearly number of M shooting actually has increased Mina Shafik: and sorry. One more thing, and actually something that we brought up during this point of the day is for a majority of that time people were Mina Shafik: quarantine people were isolated. Mina Shafik: so that could have been one of the causes. There is also less people out on the streets. If anybody here drives Mina Shafik: your commute to anywhere during Covid, if you were an essential worker was Mina Shafik: 10 times faster. Leona Chia: Alright, so this is for the more liberal leaning like whoever did cnn Leona Chia: and go. Dean Manomat: Yeah, I so I took a look at this word cloud for the Cnn article, and, as you can see, gone firearm violence. Those are like the biggest words, and the Cnn article is definitely a more liberal leaning article where they kind of focus more on Dean Manomat: like regulation and gun control kind of things being a solution for Dean Manomat: all these shootings. Leona Chia: you know. Mina Shafik: So. So I ran, for I ran the one for Fox, which we assumed was Mina Shafik: going to be about what it was. Mina Shafik: So again some of the biggest words were gone, Government antidepressant. But the surprising thing about the about Fox was it actually lean more into Mina Shafik: mental health, awareness for gun control? Mina Shafik: Not so much as to Mina Shafik: that, the guns are the violence itself. Again we assume that. Mina Shafik: And yeah it was it. It ran through exactly how we expected it. To Mina Shafik: none of the Mina Shafik: articles went against what we thought they were going to Leona Chia: Oops. Sorry. Leona Chia: and then the last one should be reddit. So this one was more neutral, and dean you wanna Dean Manomat: the way they kind of saw it was kind of from the point of view from it seemed like actual, normal people, almost where everyone had a different kind of opinion, because I guess Cnn and Fox are all Dean Manomat: news sources, and they have certain biases that they're going for. But Reddit seem to be more of like just everyone in the community putting out their thoughts and everything. So it kind of was a mix of the 2 Leona Chia: which wasn't surprising. So, anyway. So the next time we, if we would do this for future improvement, is definitely to expand the sources Leona Chia: using existing peer reviewed Leona Chia: and then obviously normalizing the metrics. So population versus total gun control, maybe in the State. Leona Chia: actually Leona Chia: evaluating and accept sorry, assessing mass shooting instances, not just the overall sense of what each Leona Chia: news channel. This outlook is, but more of a comparison between each one. Leona Chia: Yeah. Leona Chia: that's pretty much all we did. : Okay, sounds great. So what you did was basically : reading the news in a different way. So, reading the news, analyzing it. : we are more and more on reading the news by the titles. So that's exactly the opposite of that. So not just reading the news, but digging into it. : So what was your experience? So Leona Chia: for me? When I did it. I was honestly not surprised. Obviously, with, you know, like gun control and mental illness from from both sides. The thing that surprised me is how much they focus on certain details. So for me, you know. Leona Chia: i'm not trying to be polite political here, but a lot of the Times. Yes, I know gun control is a a a sensitive subject. But to me, a lot of times when we, when we speak to like liberal leaming Leona Chia: views. Leona Chia: Yes, gun controls the thing, but they also talk about mental illness. But in a lot of the articles that we read it was very few and far between of just having mental illness being one of the topics that they bring bring up. It was more towards it's, guns, guns, and more guns. Leona Chia: whereas when we looked at Leona Chia: the conservative leaning of views. Yes, they talk. They are very defensive about their guns, as we kinda all mostly know. But what surprised me is how much of the mental illness that they bring up. But when you read the title it definitely doesn't, say that. Mina Shafik: Yeah, I actually I was just saying I I agree with her. Mina Shafik: Didn't expect the more conservative side to be so much more focused on mental health Mina Shafik: and the Liberal side. I guess Mina Shafik: when we were going through it. We expected the liberal side to be more mental health. not cold. like mental health, and Mina Shafik: the Conservative side to be more. Mina Shafik: I guess. Gun awareness, if if you know. Mina Shafik: but it wasn't like that at all. The thing that surprised me the most was reddit. Mina Shafik: because I feel like Reddit is where everybody goes to Mina Shafik: voice their opinion. Mina Shafik: and I thought it was going to be very liberal. But it wasn't. : Yeah. I mean that the the news they have an agenda. So whatever is the agenda, but it's not necessarily the readers agenda : being able to dig into it. It's really essential. Now I think, for a second, and think in terms of the next generation of a chat : erez agmoni, when people would use a bots like that to go into the news and getting a a distilled version of the news 150 : that is exactly the opposite. So, unfortunately, I think that, like that will happen, meaning that we will be more and more manipulated by whoever is going to do this, somebody for us. : So my opinion is really important. So to create our own opinions. So, whatever it is, I mean. : I don't have any bias on that, but only if we create a deep rooted opinion, then we can really, I mean contribute to to the calls or the solving problems. So when problems with happen, and I mean I must. Shooting is a problem that needs to be fixed, because that's a real. So. : anyway. Do you wanna want something? Leona Chia: No, I just couldn't agree more. : Okay, all right. So we have 2 more, Kyle. : It is not here. Thomas : Thomas. Thomas Poklikuha: hey? Sorry about that. Yes, I I can present. Give me 1 s. : Yep. Thomas Poklikuha: Okay. Thomas Poklikuha: So what I did for my project is I wanted to analyze Thomas Poklikuha: the Nda draft combined data to Nba draft success as I grew up as an athlete, so I wanted to kind of dive into the field of athletics and see how Thomas Poklikuha: data drives drafts. Thomas Poklikuha: So the like. I said, the purpose of this was kinda to define how define and analyze how athleticism and different attributes affected Nba draft success. Thomas Poklikuha: So at the Combine, all of these players do a series of tests. and then they get graded Thomas Poklikuha: on. How Well, they do them. They bench, press, they sprint, they do agility, test, and all that. I wanted to see if I could build a correlation between these attributes, and how well they did getting drafted. Thomas Poklikuha: Meaning if you got drafted number one overall, that means you did a really really good job in the draft, and you can't score any Thomas Poklikuha: better than that. Thomas Poklikuha: I also wanted to see if there was any key factors that Thomas Poklikuha: overwhelmingly drove teams to draft you. If there was a handful of attributes. Thomas Poklikuha: If there was physical or anything that was an outlier of hey, you do really good in this one test. You're going to get drafted. Thomas Poklikuha: So, just to give you a little snippet of my code, I was able to. I tried to define athleticism Thomas Poklikuha: as fairly as I could for anyone who is an athlete. That's a really really hard thing to do. just because someone runs really fast or conventional out of weight doesn't necessarily mean that they're a good athlete. It's kind of an overall encompassing term. Thomas Poklikuha: So Thomas Poklikuha: just for the sake of this code I defined it as how high you could jump. Thomas Poklikuha: How much you could bench your agility and how fast you could sprint. And all of those data points were taken from the combine. Thomas Poklikuha: And then once I gave each player Thomas Poklikuha: a score. I could then Thomas Poklikuha: create comparisons and correlations from each attribute to draft success and Thomas Poklikuha: different draft ranges that had high athleticism. So, as you can see in this this first plot : if you were drafted one. We can also see your call the we, just if you : No, you didn't share the screen yet. Thomas Poklikuha: Oh, really. : Yep. Thomas Poklikuha: Oh, I'm sorry. : That's okay. Thomas Poklikuha: Oh, well, you go back and let me go back to slide, then, please. Yeah. Thomas Poklikuha: So these are the attributes that I wanted to generate coefficients from. Thomas Poklikuha: So they do, all of these at the combined. The sprint, agility, test, body, fat, bench, vertical. Thomas Poklikuha: All of this stuff they measure at the Nba Combine, and I wanted to generate a coefficients and see how each of these affected the your draft success. Thomas Poklikuha: So I already went over that slide. So the next slide is is how I defined athleticism. This was the score that I gave, which was also the vertical. Thomas Poklikuha: the bench. The agility, test and sprint Thomas Poklikuha: from from this score I can Thomas Poklikuha: give a a score to each player from 2,012 to 2,016, which is what the data set Thomas Poklikuha: tracked. So 5 years of of data. Thomas Poklikuha: And then I was able to plot the average athleticism by pick range. So Thomas Poklikuha: kind of. Logically it makes sense that the first 2 tenth pick Thomas Poklikuha: they have a higher athleticism score than the other picks. If you're a better athlete, it kind of makes sense that Thomas Poklikuha: you're gonna get picked high because you're Thomas Poklikuha: you have an easier time playing basketball and make it look easier. Thomas Poklikuha: As for the picks through a 11 through 40. There's no real change in that. It's kind of There's no Thomas Poklikuha: clear picture of it's a clear drop off or a clear increase. Thomas Poklikuha: And then also because I had the score for each player drafted in those 6 years, I could determine who the best athlete was Thomas Poklikuha: from my scoring system, and the best athlete was a man shepherd who you're an if you're a nick fan, you know who that is, but Thomas Poklikuha: he's good basketball player. Worst athlete is also easy to calculate. Once you have the score and to print all of the stats. Thomas Poklikuha: So I went ahead and did that something else that I did for all of the Thomas Poklikuha: attributes that are tested at the combined. I plotted them against the draft position, and I plotted them with the average across all of the years. So the red line here is the average across all of the years. Thomas Poklikuha: and this little text up here just tells you Thomas Poklikuha: how much is above average, and how much is below average. Thomas Poklikuha: So for all of these, all of the attributes that were tested in the combined. Thomas Poklikuha: I did this, and you can see where the number one draft pix are by this red dot. so you can see how they stack up against the field Thomas Poklikuha: bottom line is the summary. There's no clear indicator. So I I set out to try to find what factor matters the most in Thomas Poklikuha: draft success, and there's not one which Thomas Poklikuha: makes sense, because each year different teams have different needs for players. So Thomas Poklikuha: sometimes the worst team, the League drafts first, and they need a point guard who tends to be shorter and quicker Some other times. The worst teams in the League need centers who are tall and slower. So you're not going to get a perfect picture of. If you're really fast, you're going to be really successful. Thomas Poklikuha: But I was looking for a key key elements, and Thomas Poklikuha: it's it's useful. Thomas Poklikuha: but it doesn't have that clear and concise Thomas Poklikuha: attributes, or a couple of attributes that really indicate success in the future. Thomas Poklikuha: Some limitations of this data is like, I said. The team needs depending on what the team whose drafting you need is is gonna depend on what attributes they tend to draft height, verticals, speed, agility, all that stuff. Thomas Poklikuha: If I wanted to go further with my analysis, I could have Thomas Poklikuha: compared the results to actual actual Nba success. Thomas Poklikuha: How many years they played, how many all stars Thomas Poklikuha: they were voted on, how much money they made, or or how healthy they remain throughout their career. But overall, just looking at the combined data. I think I did a pretty good job of of visualizing the data Thomas Poklikuha: and drawing correlations between the attributes and the draft success. Thomas Poklikuha: So that is, concludes my presentation. I'm sorry that I started it late accidentally. Any questions I can answer them now. : One, not consideration, is not even a question. One thing that could have been done would be to class or all the athletes by the role that they could play. : because at the very end, as you were saying, By the end of your presentation it really depends what a team is looking for. So for certain positions, you have a more one characteristic than the other, and so on. : So it would be interesting to all of them by the role that they would play and see how the overall as the red this is, can play a role into being selected. 1: : What do you think? 1: Thomas Poklikuha: Yeah, I I agree. It's 1: Thomas Poklikuha: I saw about doing it 1: Thomas Poklikuha: in a way, but defining what a team needs 1: Thomas Poklikuha: is difficult. You can do it based off of position. But sometimes when players change positions it becomes difficult to track that. 1: : Yeah, yeah, yeah, yeah. 1: Thomas Poklikuha: So 1: Thomas Poklikuha: about doing that and clearing up and and making it cleaner. But like I said, once you get to that level and they start changing things it gets difficult to track. 1: : Yeah. Yeah, yeah, yeah. Okay, sounds good. Good job. 1: Thomas Poklikuha: Thank you. 1: : All right. I I think that we have only Kyle. That is not so. That is not 1: : in the session. So 1: that's basically it 1: : all that all. I really hope that you enjoyed the course that you enjoyed playing with the either 1: : for those of you who didn't quote the at all before. I hope that was not too bad for you. 1: : and if you have something to share about your experience in this course I would be super happy to hear, because I mean, your input is really valuable to me, because I will use it for the next semester 1: : or the following comments. 1: : and I think I will randomly pick someone. Eliza. 1: : Do you have any comments. 1: : comments wise? I mean, I like, I said, in a previous class, like I had never used Python in a very advanced way before. I definitely had my hand held the entire time when I had used it professionally in the past. 1: : I definitely My one comment was the jump from homework, 3 to homework 4 gave me a heart attack. But that's about the only negative thing I have to say. 1: : I think it was. I definitely think the course was very useful, definitely helped my overall coding ability for work, too. 1: : Sounds good. Okay? 1: : So that's basically the end of the class. So the end of the course, I will post the the final grades shortly, so we normally have a 72 h. So to post it, and I will, and a plan to stay in the that range. 1: : if you have any question, and I I still have a a couple of assignments to review before going to the final grade. One meaning I didn't check some of the quizzes. Actually, that we are controversial 1: : overall. You did great, I mean. The class was very good. 1: : I mentioned it few times, so i'm teaching 2, 6, 24 classes. This semester is most of the semester. So 1: : so this is the and the majority, if not all, of the Wn. Students, are professionals. 1: : while the Non wn that can be on campus or to be online our regular students. Most of the the majority of the cases nonprofessional. There is a big difference. There is a big difference in terms of size of the class. So the size of of a class like this one, it's more manage polar. It's more interactive. 1: My opinion at the very end is more effective 1: : when you have a class like 1: : 55 students, so that they have in the ws version of 6 and 2024. This semester, I think, are. I mean it. It's a broadcasting, basically 1: : but also there there is a big difference in t. So what is a at stake. When you are a professional, you are embarrassing your own time. You are taking time out of your personal life. Your job for this courts for spending time with me. 1: : and I grateful for that. So, and I definitely want to give you back as much as possible, and I really appreciate all the comments that you may have. 1: : This is not always the same when you have a regular students, so they are for use more on the grade. So for all of you, the grade that is important, but also the knowledge is important. You are going to use what you are learning in the program, in genital and in this course in for your job. 1: : So when you are a a professional, you basically have a 2 possible goals. Do better in the area where you currently are either in your company or switched into a different company or a take the opportunity for a career change. 1: : and both the cases there is quite a lot at stake for for you. So that means that that you may have a more 1: : issues at the very beginning because of the professional as a most of the time, some years of experience. But that means that that last time you were on, on, on the books learning something, I mean. That was several years ago, while for a a young kid coming out of the undergraduate program, they are more fresh. 1: : So I really understand that that could be more deep figure for you. So again I think I share my experience. So I was in industry for more than 25 years, and then I went back to her academy to get my Phd. 1: : And I start with my Phd. Age on 50 and and change. 1: : and that I mean it. It was not easy. What was I mean? It was fun, but was not an easy fan, let's say. 1: : and I really 1: : understand your point of view, and I I really appreciate the time you are You are spending in this program the time you spend on this code, so that you spend with me. 1: : So again. 1: : Thank you all. I appreciate you being in the class, and I really hope that that was useful for you. Whatever question you have, whatever I can be of any help, I would be happy to do so. 1: : So all the best enjoy the rest of your evening and the rest of your time, and I hope to see you some time soon. All the best. 1: : Thanks so much, Professor. Have a good night.
Who Killed the
Virtual Case File?

Written by: Harry Goldstein

Published: IEEE Spectrum, September 2005

 

Overview:

¢ September 2000: FBI IT begins Upgrade Project
o Virtual Case File (VCF) planned to replace the
Automated Case Support (ACS) system

¢ April 2005: Project failed spectacularly, and was
cancelled. FBI scrapped $105 million worth of
unusable code

 

 

FBI background - why was the
VCF so important?

- As of 2004, the FBI had 40-50 different investigative
databases and applications

¢ So, of course there were a lot of duplicate
information and functions

¢ The FBI’s work environment is paper-based

¢ Agents document every step and build case files

¢ Each form goes through approval chain

Automated Case Support (ACS)

¢ Most heavily used investigative application

¢ Stores forms related to investigations

¢ Debuted in 1995, and was considered antique
even then

¢ Cumbersome, inefficient, limited capabilities

¢ Complicated menus

¢ Allows basic searches

— “<y) “

The Trilogy (1)

September 2000: FBI Upgrade Project approved by
Congress

Project consisted of 3 parts:

¢ Information Presentation: provide new Pentium
PCs, scanners, printers and servers

¢ Transportation Network: provide secure local
and wide networks

¢ User Application: identify a way to replace the
FBI's 40-something investigative software
applications

¢ 3'4 component ultimately became the VCF

The Trilogy (2)

¢ June 2001: Applications International Corp.
(SAIC) awarded contract for software part

- Trilogy supposed to be delivered by the middle
of 2004

¢ But in 11/9 the inability if the FBI to share
information became public knowledge:
“The FBI's information systems were woefully
inadequate. The FBI lacked the ability to know
what it knew.” (11/9 commission report, 2004)

¢ Trilogy is shifted to high gear

VCEF - Goals

¢ Automate the FBI's paper based work
environment

¢ Allow agents and intelligence analysts to
share investigative information

¢ Replace the Automated Case Support system

eo ee ee eR

First problem - no general
direction
¢ The FBI didnt have a blueprint

o The FBI couldn't “make coherent or consistent
operational or technical decisions” about
creating policies and methods for sharing data,
and making tradeoffs between information
access and security (NRC report, 2004)

¢ The team began to “feel their way in the dark’.
Characterized investigative processes and
mapped them to the FBI’s databases

 

 

Defining the requirements (1)

¢ FBI’s team and SAIC’s engineers hashed out
what functions the VCF would perform

¢ Requirements document ended up being 800-
plus pages long

o Violated the first software planning rule: keep it
simple

¢ Included system layout and application logic

¢ Agents would bring web pages to sessions to
demonstrate how they wanted VCF to look

eo ee ee eR

 

 

Defining the requirements (2)

¢ The requirements were not sufficiently defined
in terms of completeness and correctness

¢ Later in the project, they required continuous
redefinitions

¢ This had a cascading effect on what had already
been designed and produced

4 a

More money

¢ January 2002: FBI requested an additional $70
million to accelerate Trilogy. Congress approved
$78 million

¢ SAIC agreed to deliver the initial version of VCF
in December 2003 (instead of June 2004)

¢ SAIC and the FBI were committed to creating an
entirely new system in 22 months

¢ FBI wanted to switch to VCF using flash cutover

Contracts problems

¢ Trilogy contracts were changed to reflect the
new deadlines

¢ Trilogy contracts didn't specify any formal
criteria for the FBI to accept of reject the
finished VCF software

¢ Trilogy contracts specified no formal project
schedule

o In particular, they specified no milestones

 

 

And yet more money

*December 2002: FBI asked Congress for additional
$137.9 million for the Trilogy
¢ The inspector general issued a report on the FBI’s IT
management
o “The lack of critical IT investment management
processes for Trilogy contributed to missed milestones
and led to uncertainties about costs, schedule, and
technical goals”
¢ Undisturbed by the findings, Congress approved
$123.2 million

¢ Total cost of Trilogy so far: $581 million

el a. et, = ee — ——————————————————

 

 

 

The writing

¢ Meanwhile, SAIC programmers were cranking
code, using a spiral development methodology

¢ Roughly 400 change requests of the system were
made by FBI from December 2002 to December
2003

¢ Some were cosmetic, but others required adding
new functions to the system

o Example: page crumb

eo ee ee eR

Changing the VCF

¢ Many changes of the program had to be made
by all 8 of SAIC’s development teams

¢ SAIC officials admit that in the rush to get the
program finished in time, they didn't ensure
that all the teams were making the changes the
same way

¢ The inconsistency meant that when one module
needed to communicate with another, error
sometimes occurred

“tile:

Delivering the VCF

¢ SAIC began testing the VCF in the fall of 2003

¢ 13 December 2003: SAIC delivered the VCF to
the FBI, only to have it declared DOA

¢ The FBI found 17 functional deficiencies it
wanted SAIC to fix before the system was
deployed

¢ Additional tests revealed 400 more deficiencies

 

 

 

VCF - The end

¢ June 2004: The FBI contracted an independent
reviewer to recommend what the FBI should do
with the VCF

¢ Delivered in January 2005, the report said:

o High level documents, including the concept of
operations, were neither incomplete,
inconsistent, and didn't map to user needs

o The software cannot be maintained without
difficulty

o It is therefore unfit to use

oe

VCE - After

¢ April 2005: FBI officially cancelled the VCF
project

¢ May 2005: FBI announced Sentinel. A 4-phase,
4-years project intended to do the VCF's job and
provide the FBI with a web based case
management system

¢ Sentinel’s estimated cost was unrevealed

¢ FBI's officials seem confident that the FBI can
handle the complicated project

=

 

 

 

SYSTEMS wa
ENGINEERING COMPLEX SYSTEMS
RESEARCH CENTER & ENTERPRISES

Natural Language Processing (NLP) for

Requirements Engineering

 

Extracting Formal Structures from Text

by
Dr. 
clipizzi@stevens.edu

October 2021

a Agenda
ENGINEERING

RESEARCH CENTER


 

October 2021 UNCLASSIFIED
The context

ENGINEERING

RESEARCH CENTER

 A requirement is a singular documented need—what a particular
product or service should be or how it should perform. It is a
statement that identifies a necessary attribute, capability,
characteristic, or quality of a system in order for it to have value
and utility to a user [Mitre]

 Requirements engineering (RE) is the process of defining,
documenting, and maintaining requirements in the engineering
design process. It is a common role in systems
engineering and software engineering [Wikipedia]

October 2021 UNCLASSIFIED

The context

ENGINEERING

RESEARCH CENTER

 Acomplete collection of requirements for a given system, can
provide an abstract representation of the system itself. The
resulting model is as accurate as the the process and the method
that is used to generate it, within a given range of time validity

 The collection of requirements has traditionally been a top-down
approach, requiring SMEs with a convergent vision

 SMEs may not be as available as needed, systems may change in

time

October 2021 UNCLASSIFIED

 The context

ENGINEERING

RESEARCH CENTER

 We are focusing on requirement engineering with a “reverse
engineering’ approach, extracting “requirements” from existing
material

 “Requirements” is a generic term and it may have different
meaning, depending on the context

 It could be an ERA model, if the focus is on data representation, it
could be a systemigram, if focus is on modeling a system, it could
be a causal chain

October 2021 UNCLASSIFIED 5

 Natural Language as source of Data

RESEARCH CENTER
 85-90 percent of all corporate data is in some kind of unstructured form, such
as text and multimedia /Gartner, 2019]

 Tapping into these information sources is a need to stay competitive


 Examples of application of Natural Language Processing: insurance (claim
processing); law (court orders); academic research (research articles); finance
(reports analysis); medicine (discharge summaries); technology (patent files);
marketing (customer comments)

Source: Tractica

 

October 2021 UNCLASSIFIED

Challenges in Natural Language Processing

ENGINEERING

RESEARCH CENTER

 Semantic ambiguity and context sensitivity
—automobile = car = vehicle = Toyota
—Apple (the company) or apple (the fruit)
 Syntactic/formal ambiguity
—Misspelling
— Different words for the same concept (e.g.: street; st.)
 Implicit knowledge

—We talk about things giving for granted common or specific knowledge

October 2021 UNCLASSIFIED

 Implementing NLP

RESEARCH CENTER

 Language is changing constantly, and NLP is following the changes, going from
processing based on predefined structures (taxonomies/ontologies, syntax) to
structures deducted from the text itself

Limitations of the traditional-deductive-
”symbolic” approach
• Today, language is more fragmented,
has less structure, has more jargons
• Different points of view may provide
different interpretations

Machine Learning/inductive approach
• Extracting a numerical structure from
text
• Different structures for different
points of view
• Different structures automatically
extracted over time

 Implementing NLP - limitations

RESEARCH CENTER

 Understanding Language is not “just” processing. Understanding is
a human characteristic, analyzed by philosophers as part of
Epistemology



 An accurate (by human standard) “understanding” can come only
from a model of human mind

 The current leading models in NLP/”NLU” are focused on the
algorithmic part, missing a real model representing how the
knowledge is created and used. It is basically representing the
brain, not the mind. The leading model for NLP (GPT-3 by Open-Al)
has 175 billion parameters, feeding a neural network providing
results as a black box

October 2021 UNCLASSIFIED 9



10

 


NLP-based tools and techniques for RE
 tools clustered by NLP4RE tasks and
then by RE phases

NLP techniques and their frequency of use

L. Zhao et al., "Natural Language Processing for Requirements Engineering: A Systematic Mapping Study," ACM Comput. Surv., vol. 54, no. 3
October 2021

UNCLASSIFIED

11

 State of the art — bottom line

RESEARCH CENTER

 We* considered 134 tools and approaches to apply NLP to
Requirement Engineering

 The main insights resulting from the analysis were that no
approach completely fulfilled the criteria of self extracting
requirements/structures

 Solutions leveraging on Machine Learning with a semi supervised
approach seems to be promising

*M. Vierlboeck, C. Lipizzi, and R. Nilchiani - "Natural Language Processing for Requirements Engineering & Structure Extraction: An Integrative Literature Review” — Under review

October 2021 UNCLASSIFIED 12

 Our Approach — 1* ingredient: the Network

RESEARCH CENTER

 The approach is based on a corpus representing the domain we
want to model

 The corpus does not have a formal structure connecting its
semantic elements

 Using approaches based on words/n-grams proximity and
applying techniques such as Word2Vec we create a semantic
network representing the corpus

 In the network, the nodes are words/n-grams and the edges are
calculated based on their proximity

October 2021 UNCLASSIFIED 13

Our Approach — 2" ingredient: Network Theory

RESEARCH CENTER


— We apply a partition/clustering method (based on Louvain Community
Detection), creating “topics”

— For the nodes in the cluster, we calculate a composite metric based on
degree centrality, page rank and betweenness centrality

— We pick the nodes with the highest values for the metric: those are the
candidates “subjects” in their clusters

October 2021 UNCLASSIFIED 14

 Our Approach — 3rd ingredient: The “Room” Theory*

 The “room theory” is a framework to address the relativity of the point of view by providing
a computational representation of the context

 The non computational theory was first released as “schema theory” by Sir Frederic Bartlett
(1886-1969) and revised for Al applications as “framework theory” by Marvin Minsky (mid
‘70)

 For instance, when we enter a physical room, we instantly know if it is a bedroom, a
bathroom, or a living room

 Rooms/schemata/frameworks are mental frameworks we use to organize remembered
information and represent an individuals/domain-specific view of the reality

 We create computational “rooms” by processing large corpora from the specific
domain/community generating numerical dataset (“embeddings table”). The table is a
representation of the words/ngrams, where each one of them is a n-dimensional vector and
we use it as a knowledge base for the context/point of view

*C. Lipizzi, D. Borrelli, and F. Capela, "The "Room Theory": a computational model to account subjectivity into Natural Language Processing

October 2021 UNCLASSIFIED 15

ie How the “room theory” works


“Room theory” enables the use of
context-subjectivity in the analysis of the
incoming documents

Context-subjectivity can be the point of
of view of a subject matter expert

The context-subjectivity in the analysis is
represented by a domain specific
numerical knowledge base, created from
a large domain specific & representative
corpus that is then transformed into a
numerical dataset (“embeddings table”)

1. A point of view for the comparison (the “room”). This is represented by the embeddings table
extracted from a large/representative corpus from the specific domain

2. Alist of “extended” keywords (using synonyms and misspellings) to be used for the analysis

(the ”benchmark”)


 Our Approach — putting things together

RESEARCH CENTER

 We prune the list of ngrams using the room theory

 We create ego networks for the “subjects”. The degrees of separation is function of
the size of the cluster

 The ego networks represent the semantic dependency between the nodes within
the topics

 The approach can be extended to inter-clusters relations to recreate the complete
formal representation

 Why all of this is relevant? The current ML-based models are limited to “similarity”
between semantical elements, but they do not consider more complex
relationships between them, such as semantic hierarchy

October 2021 UNCLASSIFIED 18

 Our Approach — putting things together

 The flow on top provides
adjustments based on
domain-specific knowledge
• The flow at the bottom is the
actual workflow on the system
documentation


How we use it so far



 We used it to determine the causal chain in the domain of technologies

 Each technology has “components”, that are other technologies required for the
first one. For example, cell. phones <- batteries, display, antennas, ...




 Our Approach — next steps

RESEARCH CENTER

With the proper funding, we will implement the following missing elements
 Upgrading the causal chain application/upgrading the overall approach:e Using the “room theory” to make the entities/nodes more relevant
 Implement the “inter-clusters” relations
 Implement a feedback mechanism to update the benchmarks
 Test it on multiple domains

 Extending it to applications where edges/relationships have a semantic value, such as for
Systemigrams and ERA

 We will create a bipartite/2-mode graph G = (EF, R, A) such that if e; is an entity and 7; is
a relationship, there is an edge aj = (e;,7;) € Aif and only if e; is associated to a
relationship 7;

 We will then extract a 1-mode graph with entities only. Two entities will be connected if
and only if the share the same relationship. The common relationship will be the label
in the Systemigrams or ERA

 The extraction of relationships will be done using the “room theory”


SYSTEMS CENTER FOR
ENGINEERING COMPLEX SYSTEMS

RESEARCH CENTER & ENTERPRISES

   

=) STEVENS

INSTITUTE of TECHNOLOGY
THE INNOVATION UNIVERSITY®

Thank you!

Dr. 
clipizzi@stevens.edu

You have a in notebook, you cannot really do that. 4:30 So the notebook is basically executed the step by step. 4:39 And I mean it. Presume that someone is doing it. 4:44 It is great when you do a presentation. It is great. 4:50 If you want to have it as a sort of showcase when you go for the job, and you want to show that you are doing coding in a proper way, and you use it, and that's fantastic. 4:51 I use it in 7 out of my projects. So when I go to a client or response, or and I say, Okay, I want to show you the code, and how the code is working with the results, step by step, it is great but what we do here is not. 5:05 Great, so please submit the the dog pie. An individual file. Now there's a Z file up, and now it, and not work with the notebook. 5:30 You could export the not Booker into a dot. 5:42 Pie they export is not a clean file. There are some control characters that are comments that are not comments. 5:49 So if you really want to use, if you are religiously, philosophy are against writing a dot pie, it is okay. 6:04 Use your notebook, export the notebook in the pie, but then you need to check it and edit it because you want to have something. 6:19 That is not fake comments or things that are not supposed to be in a file. 6:23 So that's the reason why I strongly encourage you to use any id you can use a pi Sharma. 6:38 You can use a you name it Eclipse. That's fine. 6:47 You can use a I don't know. Sometimes I use a text editor that that is called. 6:52 I don't even remember how it's call it sublime text. 7:03 That is a tech study or with just a little bit of a sugar on the indentation on the quotation. Things like that. 7:08 But minimal. So whatever you use, that's fine for me. 7:20 But what we want if the dog is a dot pie as it would be generated if you run it. 7:25 So that's something. Let's move on. 7:36 I really don't think that is a to I mean, we we can do that. 7:43 The. 7:52 Minimize this. Okay, let me share the screen again. 7:58 And let me go here. So what you were supposed to do? 8:07 It's basically just to write 8:14 You your name here. So for all the rest, there was not much to do, so it was just a way to become familiar with the that user interface that this program has this Pi Sharma or any other Id. 8:21 Be sure that you have python and python is running properly, and that's basically it. 8:42 I mean that very few points. There are 20 points for this assignment, just because it's very basic. 8:51 For this first assignment we are not enforcing in a street way the policy on a late submission, but to read one because we want to be sure that you do it in the proper way. 9:03 You still find on Python is working, you can run by charma, and that's the first reason. 9:13 The second reason is because is the first assignment. Things can go wrong. 9:26 The installation is not going. Okay. That's absolutely fine. 9:31 Also, I know, not really presenting anything more than last class, meaning meaning, I mean, the reason why I want you to submit before the beginning of the class is because I can present the solution of the program of the assignment. 9:37 But in this case anonymous, presenting it because it's pretty much the same man that you already had in that the Except site 0 0 pi, No. 3 screen. The that was on canvas because we we sub. I mean, we are giving us some slack. That doesn't mean that there is any change on the deadline for the submission of what next assignment. So next, assignment is you 60'clock next Wednesday, pdf, so not because you started late. You will start late. You will finish late the the next one. So that's deadline for next one is going to be next Wednesday. At 60'clock I think we can go at this point on something that I want to show you first, before we go to the slides. So let me go with that this shortcut video for a momenta Hi, Professor, I don't hear any audio for the video. Oh, okay, so let me fix that. I don't know why Okay, let me stop sharing for the cell phone that Alright, so let's do it in a different way. So. Okay, can you all go to the canvas? And I click on history of software Are you okay? With that Okay. Once you are on the canvas again, I click So go. Yeah, click on this one. And you will have a the video starting So I'll give you 3 min. In 3 min I will start talking again. Hopefully click on the video. Watch the video, and we will come back in 3 min. Purple. Yeah, we are watching a video on software engineering. Software development is a 3 min video and is on canvas, police, teleconnect and watch it Alright. So done with the video. Thank you for letting me know, and let me presume recording. Okay. So the video was a brief introduction to the key theory of software. So how it started, how evolved! Let me share the screen again. Let me go with the some slides. So so this first batch of light. So it's something that you will see again in a few classes with more details. But I really wanted to give you some of the key concepts that we are going to use while we will talk about developing software. Initially, software was not a discipline per se. So was a creating computers so and make them work so pretty much, they were fully integrated with the hardware. Then they realize that the hardware can stay. But if you change the software, then be hard to where I can do different things, and that that was a mid fifth, 19 fifties. Okay. And it began a sort of a separation on the 2. Initially, the first program that we are other additions, other engineers. There was no real discipline in information technology believe it or not. When I graduated in math there was no information technology school in the university where I was starting the university. I was starting was a univac city in Rome that in the international ranking is in the top 100. So it it. What we so E. Is not a bad university, but never nevertheless, I mean it was a beginning of eighties and 1,900 eightys. That means that that is not something that is ancient history, not Texas working working at least, but it's something that developed quite fast How we can evaluate the cost of software development. So, if you think you want to develop to build a region, you want to build a building for an airplane. So you basically at the cost of of development with software, I think, are a little bit different. So there are 3 main components. One is actual that the the actual developer. So right in the code, there is a component that is testing the code that so making sure that there are no errors. So, sometimes sometimes er errors are a very difficult to sport is highly recommended that some one who develop or the team who developed the software is not the team doing the testing, so they have a different goals and different men and mental attitudes so if you develop the code you want to Make the code working. If you are testing, you want a code to to fail. So the first case, you have a positive attitude. The the the second case, you have a negative attitude that you want to see where and how they call this failing. So developing, testing, testing. But then there is wide a lot more on maintenance. So software can change because a needs can change. You can have different marketing conditions, different conditions. Whatever is the use of the software that you wrote? So those 3 components are to be considered the when you develop software. So again your have the course of development, the cost of testing, and they also mainly some software has to be maintained for long time. So in a, in the government, in some banks we still have software that was wrote to a cobalt. We are talking about a language that is not used since good 40 years. So probably less 30 years. But that's it. So it was. I mean, when I started the coding I was coding in forth running cobalt so, and it was almost 40 years ago. So this is to say, can you imagine how much maintenance has been done to a piece of code that is a 30, and change a year old, and he's still running because he's doing a critical things? So there was a an interesting case when the Government gave money to people during the pandemic the money we are given that to the seeded sensor based on a program that was running on the local. Let's say, tax department. Those departments are seen using cobalt, but they had to do changes to a accommodate. I mean the the manager government was giving, and they had trouble finding people doing the coding with. So I mean, most of the cobalt developers are retired, or they're not around for any reason. So when you develop software, there are different things that you want to consider so let's start on how you do the internal process. So the thing that you normally do when you develop again a breeder building, or whatever is a, the, what's so called the waterfall approach. So you basically start with the requirements then you do the planning. You allocate the resources you start doing the developments. Then you do some testing, and then you release the entire program. That's okay. So basically, you do requirements design implementation. And then you release the the prop up to the user and you start the main. Most of the so-called legacy systems. So we were talking about cobalt programs those are some of the legacy are based on a waterfall. Obviously advantages. You can optimize the allocation of resources or the initial location resources. You can define the requirements upfront, but you don't have flexibility, meaning if something will go wrong, because there was a misunderstanding between the user and the developer, because the conditions change. Whatever is the reason, you cannot go back. So that's why, at the very end, only 16% of the systems developed software systems developed using the waterfall approach in 1,995. That was kind of the peak of this type of development. Only 16% was a really successful meaning. At the very end didn't really work. That we are software engineering started trying to apply an engineering approach to the development of software software engineering is to computer science pretty much like system engineering is to other developments. So software engineering is the system engineering of a software development We will talk a little bit more about software engineering. But let let me just end this short presentation with some basic practical principles. So you want to use a open source software. So using open source software, you have the possibility to adjust the software you are using, you had the possibility to count on a large community of developers that may have had the the same problem that you may have obviously not everything it's Super great. So if you buy a commercial software, you have an accountability that you don't have with the open source but on the other end, if you need to do any change to a commercial software, the only way that you can get it is to go to the developer. The owner of the software and saying, Can you change this in that? And then, if you are, I don't know where is on. Chances are you will get it if you are someone else. A a smaller company, I mean is just dropping your request in a box, hoping that sometimes the next release or the following release of the software will incorporate the the changes that you require user industry standards. So there are pretty much standards for most of the things that that you are doing. You do not want to invent a wheel, make the the graphical user interface separated from the actual program, they have different characteristics, most of the time. You require different skills in A, all the projects that I'm doing. I have different people. We're working on the back end and on the front end. When you work on the back end you you are more on a languages like Python. You were more on applying, or a translating algorithms and processes into code. When you are on the front end you are more on a tools. Flask or other there are on the the graphical aspects of the interface so different skills. Most of the time people doing one is not doing well. The second one with Python. There are some libraries to develop, a graphical user interface, but still at the quality. E. Is not great, so I'm not using those libraries in my projects, but I just develop a graphical user interface in a traditional way. So you basically have the user inputting some data or files. Then then the graphical user interface is taking the files, passing the files to the back end system. That is, processing it, and then is retarding a another file. That is the first one process, that what is the process and the graphical user interface? We present the results in the proper way. So that's the streamline that the pipeline of how those things can be. Now, if the backend is a a notebook, you cannot really do that. So. And that's another example. Why we want to have a dot. By. So you cannot really integrate the things as they should Alright, so let me stop sharing for a second, and let me check if there is a any question If not the, we will continue with another presentation. That is, adding some building blocks to the knowledge that we have so far to python of Python. And this is what we would do. Each class. I I will try to be brief, because I want you to do an in-class assignment in class assignments. At least the first assignments will be with no grading meaning is just for you to practice, and we will do. Group assignments, so I will open a breakout rooms. You will do your I will give you 15 min or so. You will write on the code in the team. Then one or more of the teams will present what they did or will share. I mean the process that they followed, and then I will present a solution, and then I will introduce the as assignment for next week, and that will be the end of the class Alright, so so like pretty much all the languages. Python as components like barriers, so functions. And then the mother Madigal, overrid also the subset, or overrid those, and you have a ways to represent a basic logic building blocks on of any process. The data, conditional sequences and loops So in Python there are many components that are built in and other components that you add the importing in your code function. So we will go to the functions in few classes. So variables can be numerica integer floating can be strings, lists, tuples. We will define all those things later on. But just to familiarize a a little bit dictionaries. So we will go into those different types. So again, you can assign values to variables. In this case you are assigning a to the variable X. The number 3, you wanna are assigning to so in this case you are assigning a number an integer. In this case you are assigning a string. This is a list that we will go back to that. But just to give first idea. So lists are in a square, brackets, and there are elements inside the list that are separated by comma. Those can be variable, so can be strings can be. Numbers can be lists, so you can have a list that is composed by this. So then you need to figure it out how to access it to the element. The elements of more interesting, some all, all the body of both, are reusable, like in this case. You had meet. I was Pam. If you say, meet equal sausage, then you are reassigning. You are replacing the previous value with the new by, so that's a an example. Not all the variables can be reusable, so we would go back to name so variable. So we mentioned that you can use pretty much any combination of let that's numbers under scores. Hmm! You cannot use what are called the reserve, the words so, and if right, else those are some of the recent words. So you want to be sure that you do not use those words for naming your variables. And now, important thing that we already mentioned last week is is to have to give to the variables names that are self-explanatory of the role that the variable is playing in your code. So if you are using a variable to measure the weight you want to call the variable weight, and so on. Comparison operators. So so you can have all the traditional overriders so greater than less than equal up equal 2 equal signs. So that's something that we definitely need to keep in mind. So if we do a conditional, we want to be sure that instead of using one equal sign, use 2 equal signs less than equal, and so on. Probably a little bit different from normala is a notable to that is exclamation. Mark equal some other program, some some other languages are using the less and greater together. But I mean confused version of 5 on a report. I I didn't try since age, so I don't know if it's in there, but everyone is using an exclamation market equal for the not equal as condition. Th. There is a a Crm that is called the Every logical process can be recreated, using a combination of sequences, conditionals, and loops. So if you have those 3 building blocks, you can create any process. Then I I mean that we'd buy don't you have a function? So that very high level, you don't want to go at that low level, but you could. So because of that. Pi don't like any other language. Is providing a way to do those 3 so sequential is that the most natural one? I don't know is starting from top to bottom, from left to right. So that's an example of a sequential step. So what you see on the left side is a diagram, that is a pseudocode. The for up the code that they you would eventually write. So you assign the value to to the variable X. You print the variable X, then you do X equal x plus 2. That's another example of a replacing the value or the previous value. You have a the 2 plus 2, meaning 4, you bring the new value, and you will get in this case for so that's the program. And that's the output again. This is a sequential conditional. Again on the left side. You have a a, a, a flow chat, conditional side, and are represented by roomboards. So in this case you assign, and what is inside the is a sort of a pseudo code. So you assign X. The variable the 5 to the variable X oops. Then you ask if X is less than 10, if yes, you will print smaller, then you ask, is greater than 5. If yes is bigger, and if no one or the 2 is satisfied, it will print. I mean. At any case we will print a finish when you do. If statements you use the double equal, cool, be careful. Is not one equal. We use the one E. Well, only to assign values to variables, not in the conditionals, and then at Yay. In the if statement you use the call on, and when there is a call on, in Python, you have the following line that is invented. So, if today is October thirtieth rent, a happy birthday job, if the condition is not satisfied, will not go into the indented part of the code but we'll go to the next one. We're Al, if it's else. If today is June 21, then a preinta happy with the loading Elsa meaning is not. October 30, is not June 21. You go here. There is no other. If you preint a good month. So again, else is a pretty much that she told us so. If The previous conditions are not satisfied. Then you go to the else condition. Each time there is a call or not, there is an indentation So again, be careful to use the double equal. Be careful using the the indentation. Those 2 are 2 or the most common errors. When you start coding in python, so you forget to use the tui wells. You just one, and you get an aerobics. And the second is, you do do the program in indentation, and the program is not doing what is super positive loops. That's the second, the third condition up. So again you have the flow charter. In this case you have you assigned 5 to the variable, and you ask if N is greater than 0? If yes, like. In this case, you print the number, and then you subtract one to the number to go back, and you will continue. Still the the value will be 0, and then you will print finish. So when you do loops, you need to be sure that there is a an exit condition. So if we don't have this N equal and minus one, it will be an infinite rule. So in the the program we stay there forever, because if they're going condition we'll never be unsatisfied. So it will be always a greater than 0. So again, be sure when you do an elite, Looper, whatever is the I mean in this case, we are using it for in a except site 0, we use the while. So those are both good options. Be sure that you have, and exit a condition. So for is another way by Donna is a dealing with the loops. So in this case you have a list of names, and you are a looping in this list. So for a name. There is a kind of a dummy variable in a list of names, so the first value that this name is getting when starting is the first name, so it will print the first name that is Frank. Then we'll go back to the beginning of the loop, and at that point name we take the second value that is married. We'll print Mary, and so on until the end of the list so that's another example how to use loops. We use the the the while through. This is another example. While countdown is less than 100. So you print. So you start assigning 25 to the value I mean, it will. There for a while. So you start with the value you subtract one till it's finished. That is something that is not exactly right here. So that's an infinite loop. If you start with the same value, and you ask why, that's different from 0. You subtract one, feel at the certain point will become 0, and will stop. So again be aware on the infinite loop in this case is always through, and we'll continue whatever. In this case there is another in loops. You can have a pieces of code, so you can have just doing something like in this case. Or in this case you'll print the name, or you can have a additional logic in it. So in this case you have numbers that will become a list in the range. 0 10. And for I this is the dummy vary, Ebola, that we will use in the loop, and only the looper. So in numbers, so initially, it will be 0. So print I that is 0. Then, if I is equal, 5, you break. So basically it will continue till springing 5, and then it will break. And this is pretty much the same, the reminder meaning, if dividing the number of by 10, we have a rest of 0. The reminder of 0, then it will print it. If it's not, we'll continue the so 2 things here de breaker. So in this case Breaker will take you out of the loop. In this case continue, that is, basically doing nothing and go to the next iteration. So break out of the loop, continue, skip, and and then go to the next That's another example. So again, you have a list, and then you start the loop you ask is the name? If the name is starting, that's one of the attributes, so that you can have a 2 strings. So name is a string at the first crown. That will be frank. If name starts with the M. Printer. Name all in the uppercase. Oh, the first round would not print anything meaning we'll skip the first one, then the second one will be Mary. Then start with Emma. Mary is starting with Emma again, keeping mind that the copy DNA is different from a small Emma, and then it will be printed and will continue. Eva will be stop, will be skipped, and Mohammed that will be printed We mentioned that so some of the common errors again call on means indentation. There is no indentation here. If you try to run this, you will get an effort. This case the indentation is good, but you need to have a double equal here. Not a single legal, the single equal is only to assign values to variables, and then you have all the links Alright, so we made it the pretty fast, so we will have a time. Let me stop sharing that. So what I'm going to do now is basically to introduce the in class except size And then I will create a breakout rooms. You will work on it. We give you at this point I can give you even a 20 min. Then I will take back the control. I will ask teams to present a day solutions, or just to discuss the issues that they may have had that, and then I will present a solution, and I will introduce the assignment for next week alright, so let me start sharing again and let me Go here. So that's the in-class exercise. Write a program, name the Exchange Dot. Dy that will mimic the conversion or Ann amount of us dollar. So into a currency of the users. Choice. We will work only on the integers, meaning this tool cannot be using real life, because I mean exchange rates are with decimal. If you use this amounts you will get errors. Using, what I'm going to introduce in a moment. Meaning is just an exercise, so can never be used in a a real cases. So you will ask how many dollar that you want to exchange. And you input your value name of the currency and this will be a string whatever you like. What is the exchange rate? Again? All of them both. The Us. Dollar and exchange rate has to be integer, and the output will will be the program. Writing, you can exchange $100. You will take this value from here for a, and this will be the multiplication between the Us. Dollar and the exchange rate. So you are going to leverage on what you did the in accept siz. 0, meaning this one. So you will have the same loop. You will have a Tim Miller, you will call them in a different way instead of first. Hmm. Name, or or you can leave whatever you like. Can you please mute your microphone, please? And then, obviously, you need to change a little bit because it's not as aation, but in this case it's a product. Also, you need to add the third request. That is the name of the of decar, and see. But then pretty much singular, so you can do a a save as and rename it, and work on the the new version, changing, editing what you need to change. So what you will do will will be to check if the input is nomadic and you will use the python. Building function is digit. So if the name just like we saw Just like we saw here. If name start with Emma. In this case would be if value is digit. But it's working oops. He's working pretty much. Using those building types I mean, you don't really need to go to that. But eventually, using the link, you can get all the building functions, including the oops, including other, is Dj. If not, you will start again, the looper, using the statement, continue again is, digit is not particularly smart, but is considering the as a numerical. The integers. So if you have a comma, because is a floating point, there are decimals, it will consider it a non numerical oh, and that's why this is just an exercise is not something that has anything practicing on the application. So you will start the loop. You prompt the user for the name of the currency from the user, for the exchange rate. Calculate the number, all the currency doing the multiplication, bring the result with separation that can be just print blank or using like we did the last time. The Backslash and backslash means that there is a special character, and that means new line. That means, before printing this Xyz Xyz. You pretty the blank line? Oh, is it just to give a nice answer to the user Okay. So I'm stopping sharing. I'm creating a Breakout rooms, so I will create that 9 breakout rooms pretty much with 9 participants each. So I will. Oh, let me do something for that! I need to publish. I think it's of any data. Okay. Already published the the text on the in classics, etc. And I'm creating the rooms. Please go to the rooms you have a about 20 min to work on. It. Then some one of you will present the results. Then I will talk on the results. Okay, my version. And then I will introduce the next exercise. And that could be the end. So I'm creating I'm opening all the rooms, so please go to the rooms. All right. So I'm resuming the recording. So you are all back? How was it? It was good. I I have one question, so we edited out the previous assignment that we did so. Should I, when we're submitting this, should should we make it into a new file, or we can just submit the the file that we edited there Alright! Oh, okay. So there is no submission here. The the in class assignments are just for practicing. You when we will approach the second half of the courts. I will use the in class assignments for extra points. So it's not going to be a lot of points but there would be some points for 2 reasons. One to incentivize you, to attend the classes, and the second is to give you the opportunity to get some additional points. To the the grade, that you will greet you. You will get, but for the time being there is no submission. Has just for practicing, and we will do the same for a few weeks. So it depends on. I mean how both they attendance, and your grades are going to be based on those 2 parameters. I will 100 move. I mean that the graded for those again it will be You cannot hear me. I I can hear you. I don't know about. I can hear you Okay, okay, so the points that you will get with the in class excel is, we'll be on top other regular points, meaning, if you do not do anything, you will not get anything. But you are not going to lose anything. So, apart from the opportunity to get the extra points. But we will go there in few weeks, and I will not re-explain everything. Alright! So volunteers who want to present that solution Again, there is no judgment. It's just to share the process that you follow the the results that you had, and that's it. There is no judgment, there is no great. You can go Anyone. You want to volunteer Okay. Sure I don't have. But I I I will, if you want me to Yeah, please. Okay. Alright, so can you see Yep. Okay. So I kind of base it off of what I did the first time around with using a little bit of that cheat sheet you gave us. And so I I only managed to write a few lines, but what I have is, you know, while true, I put documents input and then I would have the user enter the dollar amount. 1: And if dollar is digit it would be enter a new value, and then break, and else, and after that is kind of where I got stuck because I wanted to get to a point where I can have them name the currency. 1: And input the name, the name of the currency, and then sort of figure out what the exchange rate would be. 1: But that's sort of, I think. At that point the breakout room ended, and I got stuck on what to do. 1: Next. 1: Okay, so let that. Let me give you some comments on that. 1: So the the first one in Python, small letters and capital letters are different. 1: Wila is a Small. W. The the capital W. 1: In a while that, by the way, we will not be recognized as the statement, while so, and this is in line one. 1: So in line one you want to change. Yes. 1: And that's the first thing, and it so you will see one of the errors that went away. 1: The the second, when you have a column like in line 3. 1: The phone one has to be named. So you want Line 4 to be indented. 1: Further, more. 1: Like that 1: Okay. 1: And Al, so is okay in that position, because is the alternative to the if, and then I mean you are getting an error in line seeks, and because obviously that's nothing. 1: But those were the main issues 1: Okay. 1: So lines 7 is a no statement. So right? 1: Because you were a starting writing something, but then you stopped, and that's why you've got an error. 1: Yeah. 1: But that's fine. Okay, okay, alright, alright. 1: So anyone else. 1: I could share 1: Yeah, please. 1: So this is what I did while through, and then Usd to for the first input. 1: And then if it is a digit, then the current see that you want. 1: Then while true again for the exchange rate, and then a total, and then the final output. 1: So how many Us. Dollars you wanna exchange 100 convert to yen and 1, 1 4. 1: That's 1, 1, 4 0, 0. Yeah. 1: Okay, but you cannot. Hmm, leave right? So there is no done. 1: Option to leave the loop 1: But that that's okay. I mean, W was just I mean, you had a 20 min. 1: You couldn't do much 1: Okay, wha what's supposed to be different? 1: Well, if you look back at the Excel side 0, you will probably remember that there was a testing for a Dan and breaking the loop. 1: Okay. 1: So in this case a. I mean that, yeah, you have exit. 1: The, but is not a looper at this point? Right? 1: I mean a loop should continue so. There is no reason at this point to do a loop, or but then, if if successful, you just exit. 1: So you want to continue the loop or the like, the user would say, I'm done 1: Okay. 1: But it's okay. I mean, it is working. And that's fine. 1: You had 20 min, and that's absolutely great. For 20 min 1: Alright. Thank you. 1: Thank you. Okay. So let me share the screen and let me go to this. 1: This. So I'm going to show you 2 different solutions. 1: So this, this one is the basic one. So you have a while through print. 1: Nothing meaning. I'm leaving one blank line. 1: Then, that the first is dollar amount. How many Us. Dollars you want to exchange? 1: You type the value, or down to exit and checking, if done, you will print thanks for using the tool, and you will break, and at that point you will exit completely the program. 1: If it is not done, then you check. If these are value is passing. 1: The test is digita, if not, it's wrong. 1: Input, continue, you go back. If it is a number, then you ask for the second value. 1: That is the name you ask for the con, the conversion rate, that is a number checking. 1: If is digita, if not, the wrong input continue, you go back, and then if everything is okay, you calculate the converge and you print the value. 1: So if you run it 1: You have a 11, you know. Have any name 1: I hear about 33 1: And that's it. And then we've done. No, it is working. 1: Buddy reality. You have. I don't know. 1: $22. I want to convert in. 1: I don't know. Udos 1: And let's let's say the exchange rate is a 0 point 97 1: I'm getting an error out. So we already knew that the that is digit. 1: It is not considering the desk months, so meaning it is running, but it's running on the only with the in the jets. 1: So I'm maxing in from this one and I want to show you another way to do it. 1: So in Python we have the possibility to intercept the errors, meaning in this case what I'm doing same thing at the beginning. 1: So I'm asking the user to input the the number of bill left. 1: If Donna I exit, then I try to transform the value into a floating. 1: If the user, instead of typing a number, any number, is typing a string, I would get an error. 1: So try is basically intercepting the error meaning trying to do this operation. 1: If there is an aurora, and is the accept condition, print wrong, impot. 1: Please enter numbers only, and then continue, and then same thing, asking for the name, whatever the name is, then the the conversion rate, same thing. 1: Try, accept, and if there is an error, continue meaning, I go back. 1: Otherwise I do. The conversion value, and I print it with the try statement. 1: I could in theory intercept any kind of error, so I can specify the type of error that I got, and eventually do different things for different types of error. 1: If you don't specify anything meaning any error, you go to the accept statements. 1: So if I ran this one 1: So if I have a number of dollars, that is 22.5, he's okay. 1: Cut and see. 1: No, no, whatever. Exchange rate 2.3 2.4, and then I have the actual value 1: Yeah, I will definitely share the files. So if I go, if I do something like I have $22. 1: And want to convert that to something and exchange rate 1: It's not a number. At this point I will get an error 1: So again the option, the options you have to test. 1: If value that has been inputed by the user is an number on or not. 1: The options are either using is digit, but is digit as the limitation of testing only integers, meaning that if you have an integer, then it's okay. 1: It's if you have a floating point, number a number with decimal values, then it's not going to work. 1: So with the try, except you do the statement that that you want to do so again. 1: Keep in mind that the input is retarding a string, meaning that you need to transform the string into a number. 1: So we did that in this case. So we transform the floating or integer. 1: Whatever is the case? So. But if is not, and I'mbara that they use a type that is a string, you would get an error. 1: So you definitely want to have something where the is, digita. 1: But again is ditched. Has a nameations or a try extent again. 1: Keep in mind that when you have a column, not, then you need to do the Provera invitation 1: So that's big it I will published right away the the 2 solutions 1: Okay. And they are on canvas now, and you already have accept size one. 1: Let me share it, that 1: You still have some comments. So this exercise one excel size one. 1: It's really very similar, probably easier than the exercise that you just so in this case the conversion will be from sales. 1: Use too far and half so the user should be prompted for a input and just like sex size, 0 or the one that you did in class and will be, what is the temperature in sales use? 1: You want to convert, and the user will type something. The output that would be the value of the temperature in furniture would be, whatever it. 1: So in this case, it's like the equivalent of 15 degrees sets us is a 60 Fahrenheit. 1: So you do. The looper you do. You check that. 1: The input is a numerical suggestion is to use the try, acceptor. 1: But if you use the is digit is okay as well. 1: Keeping in mind that that that point. The the tool will not work with values, with the desk amounts so you calculate the number of degree using this formula so that the the the number of degrees in Fahrenheit that's all the temperature in Fahrenheit equal temperature in a 1: Celsius multiplied by 1.8 plus 32. 1: Then you print the results. So blank line. Either you print nothing meaning you will print a blank line, or you will use the new line notation like we did the few other times. 1: And that's basically it. So questions 1: Alright. Yeah. Good. Got it? 1: Do you want? Yeah, do you have a few minutes after lecture? 1: I I wanna figure out like the mistake that I made in in my career. 1: Thank you. 1: Absolutely. Yeah, yeah. I mean, if you want to share right now, that's absolutely fine. 1: If you want to share with the class, if not, the that's fine as well, we, we can just say 1: I had similar problems as previous people, so it'd be redundant.
tts STEVENS

le INSTITUTE of TECHNOLOGY

rT

F
=

Handling Data



clipizzi@stevens.edu

SSE

 

What Kind of Data? lw

Relational databases

Data warehouses

Transactional databases

Advanced DB and information repositories

¢ Object-oriented and object-relational databases
¢ Spatial databases

¢ Time-series data and temporal data

¢ Text databases and multimedia databases

¢ Heterogeneous and legacy databases

¢ Web

STEVENS INSTITUTE of TECHNOLOGY | 2

 

 

What is a Database? e

 Database:a collection of interrelated data stored
and organized in such a way that allows for easy
retrieval of information.

— Entities refers fo real or abstract things / object (e.g.:
students, Courses)

— Relationships (e.g.: Jack enrolled in EM 624 }

— More recently, also includes active components,
often called “business logic” or “domain logic”

Business Logic - determines how data Is
transformed or calculated, and how it is routed
to people or software (workflow).

ee
STEVENS INSTITUTE of TECHNOLOGY | 3

 

ots

Database Management System Y
(DBMS)

DBMS - is a software system designed to create,
manipulate, retrieve data in a database.

It is a piece of software that sits in front of a collection
of data and mediates applications accesses to the data
= E.g.a simple DBMS is your phone directory which holds

contact names and phone numbers of people you talk
to.

The primary goal of a DBMS is to simplify/enhance the
storing and accessing of data.


STEVENS INSTITUTE of TECHNOLOGY | 4

 

fi.
Describing Data: Data Models Y

¢ A data model is a collection of concepts for
describing data

¢ A schema is a description of a particular collection
of data, using a given data model

¢ The relational model of data is the most widely
used model today
— Main concept: relation, basically a table with rows
and columns.
— Every relation has a schema, which describes the
columns, or fields

ee
STEVENS INSTITUTE of TECHNOLOGY | 5

 

Levels of Data Abstraction

   

- Views describe how users see ‘e
the data
- Conceptual schema defines an er Views
logical structure
, ,
¢ Physical schema describes the

files and indexes used

  



° (sometimes called the
ANSI/SPARC model) 

STEVENS INSTITUTE of TECHNOLOGY | 6

 

cfs
Example: University Database ¥

¢ Conceptual schema:
— Students(sid: string, name: string,
login: string, age: integer, gpa:real)
— Courses(cid: string, cname:sitring,

credits:integer) Conceptual Schema

— Enrolled(sid:string, cid:string, {
° External Schema (View):
— Course_info(cid:string,enrollment:integer) co
¢ Physical schema: DB
— Relations stored as unordered files.
— Index on first column of Students.

STEVENS INSTITUTE of TECHNOLOGY | 7

 

Advantages of a DBMS Y

- Data independence

° Efficient data access

¢ Data integrity & security

¢ Data administration

¢ Concurrent access, crash recovery

° Reduced application development time

So why not use them always?
— Expensive/complicated to set up & maintain
— This cost & complexity must be offset by need

— General-purpose, not suited for soecial-purpose tasks
(e.g.: text search)

ee
STEVENS INSTITUTE of TECHNOLOGY | 8

 

fi.
Database Management System ¥

° Specialized software
- Available for PC’ s, workstations, mainframes, supercomputers
° Is expected to:

Allow users to create new databases (schema)
Give users the ability fo query/modity the data
Support the storage of very large amounts of data

Control access to data from many users at once
¢ Major vendors/products:

Oracle
IBM (DB2)
Microsoft (SQL Server, Access)

STEVENS INSTITUTE of TECHNOLOGY | 9

 

DBMS Components

° Storage manager:
— Stores on disk: data, metadata, indexes, logs
° Query processor:

— Parses queries, optimizes by selecting query plan,
executes the plan on the data
° Transaction manager:

— Logs database changes to support recovery after
system crashes

— Supports concurrent execution of transactions

STEVENS INSTITUTE of TECHNOLOGY | 10

 

cfs
Transactions — ACID Properties ¥

¢ Atomic — All of the work In a transaction completes
(commit) or none of if completes

¢ Consistent — A transaction transforms the database
from one consistent state to another consistent state.
Consistency is defined in terms of constraints

- Isolated -— The results of any changes made during a
transaction are not visible until the transaction has
committed

¢ Durable — The results of ag committed transaction
survive failures

STEVENS INSTITUTE of TECHNOLOGY | 11

 

Designing the Database

- Data model
— Entity
—  Arftribute
— Primary key
— Secondary keys

STEVENS INSTITUTE of TECHNOLOGY | 12

 

off
Entity-Relationship Modeling Y

- Database designers plan the database design in a
process called entity-relationship (ER) modeling
(Peter Chen 1976)

- ER diagrams consists of entities, attributes and
relationships
— Entity classes
— Instance
— Identifiers

STEVENS INSTITUTE of TECHNOLOGY | 13

Relationships Between Entities Sg



Entity Attribute Relationship
Weak Multivalued Weak
Entity Attribute Relationship

STEVENS INSTITUTE of TECHNOLOGY | 14

 

 

Entity-relationship diagram model Y

    

STEVENS INSTITUTE of TECHNOLOGY | 15

 

Normalization

¢ Normalization. Is the process of organizing the
fields and tables of a relational database to
minimize redundancy and dependency
— Minimum redundancy
— Maximum data integrity
— Best processing performance

¢ Normalized data occurs when attributes in the
table depend only on the primary key

STEVENS INSTITUTE of TECHNOLOGY | 16

 

 

Non-Normalized Relation Ye

FD Microsoft Access - [Non-Normalized Relation : Table]

1/1374 a side saat] 150 (15 5 |AAA Automotive e Main St. nae | | | |14 Wind -
1 |1759 |Hood an |15 | AAA Automotive) 123 Main St. (1/15/06 | |14 Wind St
|2273 | Head Light \75 _|17 |NAPA |178 Green Ave. | 1/15/06 | 114 Wind St
3451 | Grill 425 15 AAA Automotive 123 Main St. 1/15/06 | (14 Wind St
|2394 |Windshield 550 19 |All About Glass 145 Highway 1 1/31/06 | |110 Fifth Av
11125 | Windshield Wip|25 17 | NAPA |178 Green Ave 1/31/06 |2 1110 Fifth Av
11759 ‘Hood (225 |15 | AAA Automotive 123 Main St 1/31/06 _| |110 Fifth Av
| 1888 | Roof panel {650 (15 |AAA Automotive 123 Main St | 1/31/06 | 1110 Fifth Av
|1 |1374 |Left side panel (150 (15 AAA Automotive) 123 Main St 1431/06 | |110 Fifth Av
|1_ 1375 | Right side pane|150 |15 |AAA Automotive|123 Main St 1/3106 2s 2 110 Fifth Av _
1655 |Radialtires [175 |21 | Tire World 1153 Highway 12/2/06 |19 Mall Dr.
|1699_ | Chrome Hubcag/ 225 (29 | Chrome Center |197 Beulah Ave 2/2/06 |19 Mall Dr.
11991 |Gas cap 80 (17 |NAPA |178 Green Ave 2/2/06 |19 Mall Dr.
{1766 | Trunk lid 450 |15 |AAA Automotive 123 Main St | 2/9/06 |92 Star Ct.
|2395 | Rear windshield|550 (19 |All About Glass | 145 Highway 1 2/9/06 |92 Star Ct.
|2274 | Tail Light (65 (17 |NAPA |178 Green Ave | 2/9/06 |92 Star Ct.
|2497 |Rearbumper 495 15 AAA Automotive)123 Main St 2/9/06 |92 Star Ct.



 

STEVENS INSTITUTE of TECHNOLOGY |





 

STEVENS INSTITUTE of TECHNOLOGY |

Data Warehousing e

Data warehouses and Data
Data Warehouse Marts

 — Organized by business
 dimension or subject

— Multidimensional

— Historical

— Use online analytical

processing

(ETL stands for Extract, Transform,
Load; ODS for Operational Data
Storage}

 

Benefits for end users:
¢« Access data quickly and easily via Web browsers because they are
located in one place
* Consolidated view of organizational data

 

STEVENS INSTITUTE of TECHNOLOGY | 19

 

cfs
Relational Query Languages ¥

* Languages for describing queries on
a relational database
° Structured Query Language (SQL)
— Predominant application-level
query language


— Procedural

STEVENS INSTITUTE of TECHNOLOGY | 20

 

 

Relational Algebra 

* Domain: set of relations

° Basic operators: select, project, union, set
difference, Cartesian product

- Derived operators: set intersection, division, join

¢ Procedural: Relational expression specifies query
by describing an algorithm (the sequence in
which operators are applied) for determining the
result of an expression

STEVENS INSTITUTE of TECHNOLOGY | 21

 

SQL Characteristics

- Data stored in columns and tables
- Relationships represented by data
* Data Manipulation Language

* Data Definition Language

° Transactions

° Abstraction from physical layer

STEVENS INSTITUTE of TECHNOLOGY | 22

 

Example of SQL query: 
Operator

* Produce table containing subset of rows of argument
table satisfying condition
ocondition (relation)
¢ Example:
—Database: Person
Select: cHobby= ‘stamps’ (Person)

 
 
 


 

STEVENS INSTITUTE of TECHNOLOGY | 23

 

SQL Database Examples ¥

- Commercial
— IBM DB2
— Oracle RDMS
— Microsoft SQL Server
— Sybase SQL Anywhere
° Open Source (with commercial options)
— MySQL
— Ingres
° Significant portions of the world’ s economy use SQL
databases

STEVENS INSTITUTE of TECHNOLOGY | 24



The change of the $30B Database 
Market




 

STEVENS INSTITUTE of TECHNOLOGY | 25

ee
What Is Biggest Data Management .
Problem Coming Year?


STEVENS INSTITUTE of TECHNOLOGY | 26

 

 

ots
NoSQL lw

° Definition (from www.nosql-database.org):

— Next Generation Databases mostly addressing some of the points:
being non-relational, distributed, open-source and horizontal
scalable. The original intention has been modern web-scale
databases. The movement began early 2009 and is growing
rapidly. Often more characteristics apply as: schema-free, easy
replication support, simple API, eventually consistent / BASE (not
ACID)

° Products/Projects:

— htto://www.nosal-database.org/ lists 122 NoSQL Databases

— Cassandra

— CouchDB

— Hadoop & Hbase

— MongoDB

— Neod4j

— Etc.

ee
STEVENS INSTITUTE of TECHNOLOGY | 27

 

ots
BASE Transactions e

¢ Acronym contrived to be the opposite of ACID

— Basically Available: the system does guarantee the
availability of the data

— Soft state: the state of the system could change over time, so
even during times without input there may be changes going
on due fo ‘eventual consistency,’ thus the state of the system
is always ‘soft.’

— Eventually Consistent: The system will eventually become
consistent once if stops receiving Input
¢ Characteristics

— Weak consistency — stale data OK
Availability first

Best effort

Approximate answers OK

— Simpler and faster
SS

STEVENS INSTITUTE of TECHNOLOGY | 28

re i

Terminology: MongoDB example 

 

RDBMS MongoDB

Table Collection

Rows JSON Document
Index Index

Join Embedding & Linking

STEVENS INSTITUTE of TECHNOLOGY | 29

ee 6h llr

Document-oriented data model ©&

- MongoDB uses a document-oriented model using
collections

¢ Main characteristics:
— Schema-less

— Collections can be created on-the-fly when first
referenced

— Capped collections: Fixed size, older records
dropped after limit reached

— Collections store documents

STEVENS INSTITUTE of TECHNOLOGY | 30

 

Map Reduce Y

° Technique for indexing and searching large data volumes
° Two Phases, Map and Reduce
— Map
¢ Extract sets of Key-Value pairs from underlying data
¢ Potentially in Parallel on multiple machines
— Reduce
¢ Merge and sort sets of Key-Value pairs
¢ Results may be useful for other searches

° Map Reduce techniques differ across products

¢ Implemented by application developers, not by underlying
software

STEVENS INSTITUTE of TECHNOLOGY | 31

 overview

Hadoop is a framework for running applications on large clusters built of
commodity hardware. The Hadoop framework transparently provides
applications both reliability and data motion. Hadoop implements a
computational paradigm named Map/Reduce. In addition, it provides a
distributed file system (HDFS) that stores data on the compute nodes,
providing very high aggregate bandwidth across the cluster. Both
Map/Reduce and the distributed file system are designed so that node
failures are automatically handled by the framework

Hadoop's Distributed File System is designed to reliably store very large files across
machines in a large cluster. It is inspired by the Google File System. Hadoop
DFS stores each file as a sequence of blocks, all blocks in a file except the last
block are the same size. Blocks belonging to a file are replicated for fault
tolerance. The block size and replication factor are configurable per file. Files
in HDFS are "write once" and have strictly one writer at any time

[from: Hadoop wiki]

ee
STEVENS INSTITUTE of TECHNOLOGY | 32

 

ee 6h llr

Knowledge Discovery Process, in 

practice

 

Data
Preparation

 

Data Preparation
estimated to take 70-
80% of the time and
effort

STEVENS INSTITUTE of TECHNOLOGY | 33

Data Processing Flow

Analysis

 

 



¢ Types of Data Quality Problems:

Ambiguity
Uncertainty

Erroneous data values
Missing Values
Duplication

etc


STEVENS INSTITUTE of TECHNOLOGY | 34

 

Data Preparation 

¢ Data in the real world is dirty

— incomplete: lacking attribute values, lacking certain
attributes of interest, or containing only aggregate data

— noisy: containing errors or outliers
— inconsistent: containing discrepancies in codes or names
¢ No quality data => no quality mining results
— Quality decisions must be based on quality data
— Data warehouse needs consistent integration of quality data
— Assessment of quality reflects on confidence in results

STEVENS INSTITUTE of TECHNOLOGY | 35

 

Data Flaw and Data Quality



 

 

Two general ways to deal with DQ problems:
e Resolve them and then apply analysis on clean data
Classic Data Quality approach
e Account for them in the analysis on dirty data
E.g. put data into probabilistic DBMS
Often not considered as DQ

STEVENS INSTITUTE of TECHNOLOGY | 36

ee 6h llr

Major Tasks in Data Preprocessing ©

¢ Data cleaning
— Fillin missing values, smooth noisy data, identify or
remove outliers, and resolve inconsistencies
¢ Data integration
— Integration of multiple databases, data cubes, or files
¢ Data transformation
— Normalization and aggregation
° Data reduction
— Obtains reduced representation in volume but
produces the same or similar analytical results

STEVENS INSTITUTE of TECHNOLOGY | 37

Forms of data preprocessing



STEVENS INSTITUTE of TECHNOLOGY | 38

 STEVENS

INSTITUTE of TECHNOLOGY




Functions




clipizzi@stevens.edu

Richie Oyeleke
SSE
eee

 

What is a function? A Black box! S

 



STEVENS INSTITUTE of TECHNOLOGY |

 

Definition 

Function — refers to a collection of
program statements or block of code
that runs only when tt is called.

Aavantages:

= Hides the complexity or difficulty
involved in the execution of task

= Programs are easier to create —1.é.,
removes redundancy or repetition of

lines of codes

ee
STEVENS INSTITUTE of TECHNOLOGY | 3

Types of Functions &

BuilT-iIn Functions
User-defined Functions

Functions can also be imported
from external libraries

Functions take arguments as an input
and return values

STEVENS INSTITUTE of TECHNOLOGY |

 

Built-In Functions we

We've already used some built-in functions
range(n) —returns a list list from 0 — (n-1)
sum(x) — returns the sum of x

len(y) — returns the length of y

print (range(5)) [0, 1, 2, 3, 4]
print (sum(1, 2, 3, 4)) > 10

print (len([‘a’, ‘b’, ‘c’]) 3

STEVENS INSTITUTE of TECHNOLOGY | 5

 

Stored Steps - User-Defined Functions S

 


STEVENS INSTITUTE of TECHNOLOGY | 4

 

Structure of a function 

A complete program function consists of two
main parts, namely:

 Function definition — includes:
# Starting with the “det” keyword
= Function name / identifier
= Function Parameter(s)
= Body of the function

 Function call — fo initiate a function execution or
evokes It, if must include:

= Function name
=» Function arguments


STEVENS INSTITUTE of TECHNOLOGY | 5


User-Defined Functions 

 

As an example, say we want to calculate the area of a triangle, given its
height and width

def triangle_area(height, width):
area = (width/2.0) * height

return area

STEVENS INSTITUTE of TECHNOLOGY | g


User-Defined Functions 

 Say we want the square root of
a number
That’s a lot of work for
an approximate answer.
If only somebody else
had already done the
hard work for us...

STEVENS INSTITUTE of TECHNOLOGY | 9

 

Arguments 

¢ An argument is a value we pass into the function as
its INOUT When we call the function

« We use arguments so we can direct the function to
do different kinds of work when we call if at
different times

¢ We put the arguments in parenthesis after the
name of the function

big = max(‘Hello world’)



STEVENS INSTITUTE of TECHNOLOGY | g

Parameters

A paramefter Is a variable
which we use in the function
definition that is a “handle”
that allows the code in the
function to access the
arguments for a particular
function invocation.



>>> def greet(lang):
if lang == 'es':
print (‘Hola’)
elif lang == ‘tr:
print (‘Bonjour’)
else:
print (‘Hello’)

>>> greet(‘en’)
Hello

>>> greet(‘es')
Hola

>>> greet(‘fr’)
Bonjour

>>>

STEVENS INSTITUTE of TECHNOLOGY | 9

 

 

Return Values

¢ Often a function will take its arguments, do some
computation and return a value to be used as the
value of the function call in the calling expression.
The return keyword Is used for this



STEVENS INSTITUTE of TECHNOLOGY | 10

Return Value

A “fruitful” function is one
that produces a result (or
return value}

The return statement ends
the function execution and
“sends back” the result of
the function


STEVENS INSTITUTE of TECHNOLOGY | 11

 

Arguments, Parameters and Results 

 

>>> big = max('Hello world’) eararneter
>>> print (big)
'W'

STEVENS INSTITUTE of TECHNOLOGY | 12

Multiple Parameters / Arguments 

 

¢ We can define more than one def addtwo(a, b):
parameter in the function added=a+t+b
definition return added

¢ We simply add more X = addtwo(3, 5)
arguments when we call the print (x)
function

¢ We match the number and
order of arguments and
parameters Output: 8

STEVENS INSTITUTE of TECHNOLOGY | 13

eee | A

Default argument values 

def make_sandwich(meat, cheese = “cheddar”, bread = “rye”):

print (“I'll have a sandwich with ”, meat, “, “, cheese, “, and “, bread, “ bread.” )

3

make_sandwich(“ham” )
make_sandwich(“spam” )

make_sandwich(“spam”’, “cream cheese”)

I’ll have a sandwich with ham, cheddar, and rye bread.

I’ll have a sandwich with spam, cheddar, and rye bread.

I’ll have a sandwich with spam, cream cheese, and rye bread.

 

STEVENS INSTITUTE of TECHNOLOGY | 16

cf.
Varying number of arguments 

*args is a way to pass to a function as many arguments as we need, without
first soecifying it in the function definition

*args is not a reserved word

def printall(*args):
print (*args)

printall(1, 3, “Hi”)
printall(“Hello”, “Welcome”)

STEVENS INSTITUTE of TECHNOLOGY | 17

 

Summary: The use of functions S

¢ Organize your code into “paragraphs” - capture a
complete thought and “name it”

¢ Don’t repeat yourself - make if work once and then
reuse It

¢ If something gets too long or complex, break Up
logical chunks and put those chunks in functions

¢ Make a library of common parts of code that you
do over and over and/or share with others

STEVENS INSTITUTE of TECHNOLOGY | 14

 

 

Roles of Variables

¢ Most variables are local, only recognized in the
function that defined them

¢ Global variables are recognized everywhere, that
means they keep their values both inside the
function and in the rest of the program

STEVENS INSTITUTE of TECHNOLOGY | 17

 

 

tts ks
Local vs. Global ie

1 # Author: Cheryl Dugas

2

3 # local vs global


STEVENS INSTITUTE of TECHNOLOGY | 18

 

 
Functions and Scope - Local vs Global &

Variables that are defined within functions only exist there

If you assign a variable with the same name as one that exists outside the
function it will have the new value only inside the function:

def test_func(n):

X = nx*x*2
print x
print y 3
return None 16
° 4
xX = 3
y=4
print x 3
test_func(4)
print x

ee
STEVENS INSTITUTE of TECHNOLOGY | 21

Imported Functions Y

We can import libraries/oackages of functions

E.g., the math library has mathematical functions

import math
> 3.87298334621
print (math.sqrt(15))

STEVENS INSTITUTE of TECHNOLOGY | 22

 

Imported Functions

You can import the whole library
import math
print (mth.sqrt(10))
3.16227766017


You can import just one function
from math import sqrt


You can also import the library with a nickname
import math as mth
print (mth.sqrt(10))



STEVENS INSTITUTE of TECHNOLOGY | 23

 

 

Useful Links lw

httos://en.wikibooks.org/wiki/Python_Programming/Functions

htto://www.learnoython.org/en/Functions

htto://matplotlib.org/users/pyplot_ tutorial.pAtm|
htto://matplotliob.org/gallery.html#lines_ bars and_markers

STEVENS INSTITUTE of TECHNOLOGY | 24

STEVENS

INSTITUTE of TECHNOLOGY



Python Modules



clipizzi@stevens.edu

SSE

 

Python Package Links Y

Python Standard Library
hito://docs.python.org/2/libra
Built-in Functions
htto://docs.oython.org/2/library/functions.html

Python Package Index — over 39,500 packages
https://pypi.oython.org/pypi
Matplotlib plotting library
http://matplotliob.org/index.html
NumPy scientific (numerical) package
http://www.numpy.or
pandas Python Data Analysis Library
hitp://pandas.pydata.or

STEVENS INSTITUTE of TECHNOLOGY | 26

 

Built-in Functions

abs)

all()

any ()
ascii()
bin()

bool {)
bytearray()
bytes()
callable()
chr(}
classmethod()
compile( )
complex( )
delattr()

dict()
dir()}
divmod{ )
enumerate( )
eval()
exec()
filter()
float()
format( )
frozenset()
getattr( )
flobals()
hasattr()
hash()

Built-in Functions
help()

hex()

id()

input{)
int()
isinstance(}
issubclass(}
iter()

len()

list()
locals()
map()

max( )

memoryview( }

min( )
next()
object( )
oct()
open()
ord()
pow()
print()
property ()
range()
repr()
reversed()
round()
set()

cfs
Y

setattr()
slice()
sorted()
staticmethod()
str()

sum( }
super()
tuple()
type()
vars()

zip()
__import__()

STEVENS INSTITUTE of TECHNOLOGY | 27

 

 

PyLab 


 

STEVENS INSTITUTE of TECHNOLOGY | 28

 

matplotlib 

Most popular python library for plots
http://matplotlib.org/
Created by John D. Hunter (JDH)

 

Integrates well with IPython for interactive plot
development

The plots themselves are interactive: you Can zoom in on
a section of the plot and pan around.

STEVENS INSTITUTE of TECHNOLOGY | 29

Matplotlib — simple plotting library 

 

Matplotlib lets you make all the basic types of charts
Bar charts, pie charts, line charts, scatter plots, etc.

We use the pyplot set of functions from matplotlib and typically use a
nickname to make things easier:


 

import matplotlib.pyplot as plt 
x = range(100) =
y = [number**2 for number in x] 
plt.plot(x,y) 
plt.show()



STEVENS INSTITUTE of TECHNOLOGY | 30

 

Line Plot

Plot an array of values y against 1,2,3...

or an optional array of x values

plot(x, y, ‘cms’)

y = array of numbers (y-values) to be plotted

 

x = (optional) array of x axis labels

  
  
 

c = (optional) color; default = blue 
m = (optional) marker; default = none 

s = (optional) line style; default = solid

  
 

 

 

Option: plt.ylabel(‘line label’)


STEVENS INSTITUTE of TECHNOLOGY | 31

... options

Color

Blue b
Green g 
Red r
Cyan c
Magenta m
Yellow y
Black k
White w 



Marker

Point .
Pixel ,
Circle o
Square s
Diamond D
Thin diamond d
Cross x
Plus +
Star *
Hexagon H
Alt. Hexagon h
Pentagon p
Triangles ^,v,<,>
Vertical Line
Horizontal Line _

 

Line Style
Solid -
Dashed --
Dash-dot -.
Dotted :

STEVENS INSTITUTE of TECHNOLOGY | 32

 



Scatter Plot 
Plot (x, y) points from two
arrays of numbers: x and y 
scatter(x,y) 



some options: 

 

S=array_of_sizes __ (a size for each point)
S=n (one standard size}

C = ‘color’ (color, as letter or hex)
marker = ‘shape’ __(‘o'=circle, ‘s’=square)


STEVENS INSTITUTE of TECHNOLOGY | 33

 

 

Bar Chart lw

 

Create a bar chart using an
array of leff-edge positions and
one of bar heights.

bar ( x, y)

 

 

y = array of bar heights
x = array of left-edge positions, can be 0,1,2,..
Options include:

width =n (width of bar)

ee
STEVENS INSTITUTE of TECHNOLOGY | 34

 

Pie Chart 

Produces a pie chart from

an array of numbers
pie (y)

y = array of numbers

 

Options include:
labels = [array of label names]

ee
STEVENS INSTITUTE of TECHNOLOGY | 35

Histogram 

 

Creates a histogram trom
an array of values

plot ( y )

 

y = array of numbers (y-values)

Options include:

Ccumulative=True (creates Cumulative histogram]
bins =n (Number of bins, default = 10)

STEVENS INSTITUTE of TECHNOLOGY | 36

 

Multiple plots

subplot lets you have multiple charts in one

import matplotlib.pyplot as plt

x = [1,2,3,4,5]
y = [12,24,0,9,15]

plt.subplot(2,2,1)
plt.plot(x,y)

plt.subplot(2,2,2)
plt.scatter(x,y)

plt.subplot(2,2,3)
plt.pie(y)

plt.subplot(2,2,4)
plt.bar(x,y)

plt.show()



 

STEVENS INSTITUTE of TECHNOLOGY | 37

 

 

NumPy &

Numerical Python
http://www.numpy.org/
Created by Travis Oliphant

Mainly for scientific computing

 

BUT — advantages for data analysis
ndarray: Fast and efficient array object
Can handle large quantities of data more efficiently
ndarray can be read by other software
Some other basic python tasks (e.g. finding the average of
numbers in a list) can be done more easily with NumPy

Ee
STEVENS INSTITUTE of TECHNOLOGY | 32

 

offs
Numpy and arrays Y

>>> import numMpy as np
>>> a = np.arange(15).reshape(3, 5)
>>> a
array({[0, 1, 2, 3, 4],
[5, 6, 7, 8, 9],
[10, 11, 12, 13, 14]])
>>> a.shape
(3, 5)
>>> a.ndim
2
>>> a.dtype.name
'int64'
>>> a.itemsize
8
>>> a.size
15
>>> type(a)
<class 'numpy.ndarray'>
>>> b =np.array([6, 7, 8])
>>>b
array([6, 7, 8])
>>> type(b)
<class 'numpy.ndarray'>

STEVENS INSTITUTE of TECHNOLOGY | 39

pandas Y

 

Designed to make working with structured data fast
and easy ’

hnttp://oandas.oydata.org/
Created by Wes McKinney

 

Uses DataFrame, similar to R and matlab
DataFrame has both row and column labels

More later...

Ee
STEVENS INSTITUTE of TECHNOLOGY | 40

tts STEVENS

le INSTITUTE of TECHNOLOGY

he,

Python - Pandas



clipizzi@stevens.edu

SSE

 

 

 

pandas

Python Data
Analysis Library

providing high-performance, easy-to-use
data structures and data analysis tools
for the Python programming language.

 

STEVENS INSTITUTE of TECHNOLOGY | 2

 

es
Why Pandas? .

¢ pandas is built on top of NumPy and Is
intended to integrate well within a scientific
computing environment with many other 3rd
party libraries

¢ Flexibility of Python

¢ Working with Big data which excel struggles
with

Source: http://pandas.pydata.org/pandas-docs/stable/

ee
STEVENS INSTITUTE of TECHNOLOGY | 3

 

pandas S
Comparison with SQL

 
pandas    SQL
DataFrame table
column variable
row observation

groupby GROUP By

Source: http://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.htm

Ee
STEVENS INSTITUTE of TECHNOLOGY | 4

Pandas’ Data Structures lw

Two commonly used data
Structures in pandas are:

Series
DataFrame

STEVENS INSTITUTE of TECHNOLOGY | 5

 

 

Series

Series — 1 dimensional , array like
structure that holds identical data.

"For example, a collection of integer
numbers

so

Source: http://pandas.pydata.org/pandas-docs/stable/

Ee
STEVENS INSTITUTE of TECHNOLOGY | 4

SERIES

series can be
created as
follows:

pandas.Series(data,
index, dtype}

Data e.g., constants,
list, etc.

Index — same length as
data

Dtype — data type

>>> import pandas as pd
>>>s = pd.Series([2, 4, 6, 8], ['a’, 'b,
‘c', ‘d'])

>>> s
a 2
b 4
c 6
d 8
>>s['a']
2

>> s[[‘a’, ‘c']]
a 2
c 6


 

STEVENS INSTITUTE of TECHNOLOGY | 7

 

SERIES
From/To dictionary

>>> from pandas import *
>>> s = pd.Series([2, 4, 6, 8])
>>> index=['a', 'b', 'c’, ‘d’])
>>> s

a 2

b 4
c 6
d 8
>>> s['a’]

2

>>> s[fia’, ‘c’]]
a 2

c 6

 

 

>>> dictionary = dict(s)
fia’: 2, ‘c': 6, ‘d’: 8, ‘b’: 4}
>>> Series(dictionary }

 a 2

b 4
c 6
d 8

Index comes from
sorted dictionary keys

STEVENS INSTITUTE of TECHNOLOGY | 8

 

Working with SERIES 

series work with Numpy


 

STEVENS INSTITUTE of TECHNOLOGY | 9

ee 6h)
it

DataFrame 

DataFrame: is a 2-dimensional
data structure in table like
form.i.e., Consists of rows and
columns

=lf can hold heterogeneous data

STEVENS INSTITUTE of TECHNOLOGY | 10

a Oe

Creating a DataFrame 

 

A DataFrame can be created as follows:
pandas.DataFrame (data, index, columns,
dtype)

 data - constants, list, series, dict efc.

 Index — row labels

 columns — column labels

 dtype — the data for each column

ee
STEVENS INSTITUTE of TECHNOLOGY | 11

Creating Data Frames 

df = pd.DataFrame (np.random.randn (4,6),
index=['studentl1', 'student2', 'student3'
,' student4'], columns=list('ABCDEF' ) )
df

df pd.DataFrame(np.random.randn(4,6), index=(‘studentl’,student2’,'student3','student4'], columns=list( 'ABCDEF' ))
df

STEVENS INSTITUTE of TECHNOLOGY | 12

 

Data Loading S
¢ Text file data:

read_csv

read_table

¢ Structured data (JSON, XML, HTML)

Works well with existing libraries
¢ Excel

Depends on xlrd and openpyxl packages
¢ Database

pandas.io.sql module (read_frame}

STEVENS INSTITUTE of TECHNOLOGY | 13

 

Working with Excel files - Samples iw

 

>>> import pandas as pd

>>> df = pd.read_excel("excel-comp-data.xlsx'}

 

Import Excel files

>>> df["total"] = df[Jan"] + df["Feb"] + df['Mar']

 

Adding a column for totals

STEVENS INSTITUTE of TECHNOLOGY | 14

 

Group by &


>>> d = {‘one':[1,1,1,1,1], ‘two':[2,2,2,2,2], ‘letter':['a','a’,'b','b','c']} 
>>> df = pd.DataFrame(d) 

>>> one = df.groupby/'letter’) 
>>> one.sum() 


>>> letterone = df.groupby(['letter’,,one']).sum() 
>>> letterone 


ee
STEVENS INSTITUTE of TECHNOLOGY | 15

Descriptive statistics 

>>> df.mean()
one 2.263617

two -1.316694
three -1.975041

 

Also: Count, SUM,
median, min, max, abs,
orod,

std, var, skew, kurt,
quantile, cumsum,
Cumprod, Ccummax,
cummin

STEVENS INSTITUTE of TECHNOLOGY | 16

 

 

 

Plotting

>>> import numMpy as np

>>> import pandas as pd

>>> d = {'one' : np.random.rand(10), 'two' : np.random.rand(10)}
>>> df = pd.DataFrame(d)

>>> df.plot(style=['o','x'])

 

a Eee
STEVENS INSTITUTE of TECHNOLOGY | 17

 

Additional Functions S
¢ Data Aggregation

— GroupBy

— Pivot Tables

¢ Time Series

— Periods/Frequencies

— Time Series with Different Frequencies

— Downsampling/Upsampling

— Plotting with Time Series (auto-adjust scale)
¢ Advanced Analysis

— Decile and Quartile Analysis

— Signal Frontier Analysis

— Future Contract Rolling

— Rolling Correlations and Linear Regression

ee
STEVENS INSTITUTE of TECHNOLOGY | 18

Sample of functions lw

¢ data_structure = pd.read_csv('file_name.csv')
—- Read acsv file into a Pandas data structure
¢ print data_structure.info()
— Print raw information on the pandas data structure
¢ del data_structure [’column_name’]
— Delete the column “column_name”
¢ print data_structure.nead(n)
— Print the first “n” records in the data structure
¢ print data_structure [’column_name’].value_counts()

— Print the number of elements in the column “column _name” for its
different values

¢ data_structure2 = data_structure.groupby(’column_name’).sum()

— Grouping the the data by ’column_name’. This will keep only unique
values

* print data_structure.Count.idxmax()
— Print the element with the highest occurrence

¢ print data_structure.describe (}
— Print the descriptive statistics for the data structure

ee
STEVENS INSTITUTE of TECHNOLOGY | 19

 

STEVENS

INSTITUTE of TECHNOLOGY


Software Development





clipizzi@stevens.edu

SSE

 

 

~~

Beginning of software e

¢ Software separated from the hardware in 1950's
— emerged as a distinct technology
— became independent product

¢ Original programmers recruited trom the ranks of
hardware engineers and mathematicians
— used ad-hoc techniques from their former fields

STEVENS INSTITUTE of TECHNOLOGY | 2

 

ts

Inherent Problems with Software jw
Development

¢ Requirements are constantly changing

— The client might not know all the requirements in
advance

¢ Frequent changes are difficult to manage

— Identifying checkpoints for planning and cost
estimation Is difficult

¢ There is more than one software system

— New system must offen be backward compatible with
existing system (“legacy system”)

STEVENS INSTITUTE of TECHNOLOGY | 3

 

The Software Project Cycle w&

STEVENS INSTITUTE of TECHNOLOGY | 4

What are the costs of SW development?

¢ Roughly 60% are development costs, 40% are testing costs. For
custom software, evolution costs offen exceed development
costs

¢ Costs vary depending on the type of system being developed
and the requirements of system attributes such as performance
and system reliability

¢ Distribution of costs depends on the develooment model used

STEVENS INSTITUTE of TECHNOLOGY | 5

 

 

‘ on

Relative costs to fix errors 


Cost to fix an error increases as it is found later
in the development process

STEVENS INSTITUTE of TECHNOLOGY | 4

 

ks

Design Larger/Complex Programs 

¢ Building something larger requires good software

engineering

— Top-down: Start from requirements, then identify the
pieces to write, then write the pieces

— Bottom-up: Start building pieces you know, test them,
combine them, and keep going until you have your
program

— Debugging: Programming is the art of debugging a
blank sheet of paper

— Testing: Because nothing complicated and man-
made is flawless

— Maintenance: By far, the most expensive part of any
program

STEVENS INSTITUTE of TECHNOLOGY | 7

 

~~

Definitions >

¢ Software life cycle:

— Set of activities and their relationships to each other to
support the develooment of a software system

¢ Software develooment methodology:

— Acollection of techniques for building models applied
across the software life cycle

STEVENS INSTITUTE of TECHNOLOGY | 8

Software Life Cycle 

¢ The term “Lifecycle” is based on the metaphor of

the life of a person:

 

STEVENS INSTITUTE of TECHNOLOGY | 9

 

ER )Sh—hl3S lure

IEEE Std 1074: Standard for Software Life &
Cycle Activities


Software Development Activities
(example)

Requirements Analysis What is the problem?


 

STEVENS INSTITUTE of TECHNOLOGY | 11

 

Waterfall metaphor 

¢ Used in construction and manufacturing
— collect the requirements
— create a design

— follow the design during the entire construction
— transfer finished product to the user

— solve residual problems through maintenance
Intuitively appealing metaphor

— good design avoids the expensive late rework
— waterfall became the dominant paradigm

STEVENS INSTITUTE of TECHNOLOGY | 12

 

 

Waterfall model 

¢ Elaborate up-front
activities

¢ Most of the “legacy”
systems are still
largely based on the
waterfall

 

STEVENS INSTITUTE of TECHNOLOGY | 13

Advantages & Disadvantages wy

¢ Thorough requirements
definition

¢ Design proven

¢ Documentation emphasized

¢« Planning details

¢ Known quantities

Lack of flexibility for change

Less opportunity for
innovation

Test compressed

Customer only sees result at
end

Developer works from static
specification, not with
customer

Time lag between design and
results

STEVENS INSTITUTE of TECHNOLOGY | 14

 

 

Fact Checks ie

© In 1995
— 31% of all software projects were cancelled

— 53% were “challenged” (completed only with
great difficulty, with large cost or time overruns, or
substantially reduced functionality)

— only 16% could be called successful

¢ Obviously, the waterfall metaphor did not
solve the problems of software develooment

STEVENS INSTITUTE of TECHNOLOGY | 15

Band-Aid: Anticipation of changes S

 

¢ If changes can be anticipated at design
time, they can be controlled by a
parameterization, encapsulations, etc.
— waterfall model still can be used

¢ Experience confirms:

— many changes are not anticipated by the
original designers

— inability to change software quickly and reliably
means that business opportunities are lost

— only a band-aid solution

STEVENS INSTITUTE of TECHNOLOGY | 16

ER )Sh—hl3S lure

Band-Aid: Prototyping 

 

¢ Create a prototype to capture requirements

¢ Problem: volatility continues after prototype
has been completed

¢ Another band-aid

STEVENS INSTITUTE of TECHNOLOGY | 17

What is SW Engineering? 

¢ SW engineering is an engineering discipline
— concerned with all aspects of SW production starting from
the early stages of system specification through to the
maintenance of the system after it has started to be used
¢ all aspects of SW production:
— not only the technical processes
— but also deals with project management, development of
tools, methods and theories to support SW production

STEVENS INSTITUTE of TECHNOLOGY | 18

 

Difference between SWE and we
Computer Science

CS

— ... is concerned with theories and methods which establish
a basis for computers and SW systems while ...

© SWE...

— ... is concerned with the practical problems of producing
SW

¢ CS is as essential for SW engineers as ...
— ... Physics Is for electrical or mechanical engineers

STEVENS INSTITUTE of TECHNOLOGY | 19

 

Process Maturity 

¢ A software develooment process is mature
— if the development activities are well defined and

— If management has some control over the quality,
budget and schedule of the project

¢ Process maturity is described with
— a set of maturity levels and

— the associated measurements (metrics) fo manage
the process

¢ Assumption:

— With increasing maturity the risk of project failure
decreases

¢ CMM: Capability Maturity Model (SEI, Humphrey)

PEE eee
STEVENS INSTITUTE of TECHNOLOGY | 20

 

CMM levels 

1. Initial Level
also called ad hoc or chaotic
2. Repeatable Level
Process depends on individuals (“Champions'}
3. Defined Level
Process is institutionalized (sanctioned by management)
4. Managed Level

Activities are measured and provide feedback for resource
allocation (process itself does not change)

5. Optimizing Level

Process allows feedback of information to change process
itself

STEVENS INSTITUTE of TECHNOLOGY | 21

 

What does Process Maturity Measure? 

 

¢ The real indicator of process maturity is the level of
oredictability of project performance (quality, cost, schedule)

¢ Level 1: Random, unpredictable performance
¢ Level 2: Repeatable performance from project to project
¢ Level 3: Better performance on each successive project

¢ Level 4: Substantial improvement (order of magnitude) in one
dimension of project performance

¢ Level 5: Substantial improvements across all dimensions of
project performance

STEVENS INSTITUTE of TECHNOLOGY | 22

 

Pros and Cons of Process Maturity je

¢ Benefits:
— Increased control of projects
— Predictability of project cost and schedule

— Objective evaluations of changes in techniques, tools and
methodologies

— Predictability of the effect of a change on project cost or
schedule

¢ Problems:
— Need to watch a lot (“big brother’/“big sister”)

— Overhead to capture, store and analyze the required
information

¢ Agile Methodologies
— Deemphasize the importance of process maturity


STEVENS INSTITUTE of TECHNOLOGY | 23

What is Agile Process?


Product and developing
process respond to change
without breaking the system
Modules can be mixed,
reused, scaled, and
(re)configured as required
Close collaboration with the
user/Ccustomer is taken into
account fo facilitate change

STEVENS INSTITUTE of TECHNOLOGY | 24

a A

The Manifesto for Agile Software Development


“We are uncovering better ways of
developing software by doing it and
helping others do it. Through this work we
have come to value:

— Individuals and interactions over

processes and tools

— Working software over comprehensive

documentation

— Customer collaboration over contract

negotiation

— Responding to change over following

a plan
That is, while there is value in the items on
the right, we value the items on the left
more.”
agilemanifesto.org
Kent Beck et al

STEVENS INSTITUTE of TECHNOLOGY | 25

 

Agile Principles... ie
1.Customer satisfaction by rapid delivery of useful
software

2.Welcome changing requirements, even late in
development

3.Working software is delivered frequently (weeks
rather than months]

4.Working software is the principal measure of
progress

5.Sustainable develooment, able to maintain a
constant pace

6.Close, daily cooperation between business people
and developers

STEVENS INSTITUTE of TECHNOLOGY | 26

 

...Agile Principles S

7. Face-to-face conversation is the best form of
communication (co-location)

8. Projects are built around motivated individuals,
who should be trusted

9. Continuous attention to technical excellence and
good design

10. Simplicity—the art of maximizing the amount of
work not done—is essential

11. Self-organizing teams

12. Regular adaptation to changing circumstances

STEVENS INSTITUTE of TECHNOLOGY | 27

 

~— ts

Some Agile Methods 

¢ Extreme Programming (best known, 12 practices)
— Pair Programming
¢« Agile Unified Process
— Simplified version of Rational Unified Process
— Test Driven Development
¢ Scrum
— Daily Meeting
— Sprints
¢ Kanban
— Just-in-Time delivery

STEVENS INSTITUTE of TECHNOLOGY | 28

 

 

Scrum - Agile Methods



 

STEVENS INSTITUTE of TECHNOLOGY | 29

 

Architecture & Agile Development

¢ Even in an Agile project, you need foundations

— Architecture/Engineering is the foundation of your
software

¢ Agile development is suicidal before architecture is
clear!
— Use prototypes to quickly finalize architecture

STEVENS INSTITUTE of TECHNOLOGY | 30

ot
Multimedia Software Development Lifecycle ©&

 

¢ Multimedia software development is similar to any
other kind of software develooment
— Is complex
— Involves a large number of people

Takes a long time to develop

Has deadlines to meet

— Has budget limitations

Has user requirements

STEVENS INSTITUTE of TECHNOLOGY | 31

ot
Multimedia Software Development Lifecycle ©&

 

¢ Multimedia software have a life cycle
— Media production
¢ Involves producing graphics, audio, and video material
¢ Has a media production timeline
— Software production

¢ Involves putting various components and media
together

¢ Has a develooment timeline

STEVENS INSTITUTE of TECHNOLOGY | 32

 

 

 

 

 

Multimedia Software Development Lifecycle

  
 
 

 

STEVENS INSTITUTE of TECHNOLOGY | 33

Software development Trends... >



 

STEVENS INSTITUTE of TECHNOLOGY | 34

 

[~~ Sl

... software development Trends S


STEVENS INSTITUTE of TECHNOLOGY | 35

Practical Principles 

Use Open Source Software (as much as possible)
— Open and cost effective

— Media production

Use Python (as much as possible)

— Proven high-productivity language, very open, portable
Use Industry Standards

— Again, open & cost effective

Use a standard module interface

— Allows scripting

GUIl's are separate components

— Easy to simulate, allows automated testing
Communicate with HW through TCP/IP

— Open, easy to simulate & test

STEVENS INSTITUTE of TECHNOLOGY | 36

 

So recording started again. It's February 15th and is 631. 0:00 So let's start. I will share my screen. 0:08 And we'll start, as usual with the sort of road map that we have. 0:16 So that's the road map. We are right here February 15 next week. 0:23 We are not going to have class because it's President Day on Monday and Monday's classes are on Wednesday, meaning there would be no class for us. 0:29 So we will skip one class. Enjoy the break on 624, I will introduce exercise number four. 0:44 That will be a little bit more complex. So having a two weeks may be good for you. 0:56 Again, the assignments are at a growing level of complexity, so each assignment it's a little bit more complex than the previous one. 1:03 So don't fall behind. Keep current. 1:15 Stay, stay with the slides. 1:20 Review what you did and the solution that has been or will be published to make the comparison, 1:24 to understand what went wrong if something went wrong in your code. 1:34 See the differences between your submission and mine. 1:39 If your submission was doing good, but just to see probably another way for doing the same thing. 1:45 So next assignment would be 50 points as well. 1:53 And there will be the same approach of writing the code and at the same time and then writing a narrative by describing the results. 1:59 Something that is really relevant. Don't underestimate that part, because again, in M 624, 2:11 we are not doing a python because we are in computer science, but we are doing Python to do something. 2:22 So the doing something is basically the narrative, the conclusion that you can draw. 2:32 After getting the matrix and extracting the matrix from the data using Python. 2:40 So. Let me jump to the assignment. 2:49 So the assignment was the first one with files. 2:58 And some of you may have had a little bit of issues in reading the file. 3:04 So you need to make sure that there's the directory where both of my files and my programs are. 3:12 So you need to make sure that both files and program will set data files and program will stay in the same director. 3:25 If not, I mean, you can tell Python to open a find whatever it is that even if it is in the cloud. 3:35 But then things will become a little bit more complex because when you move from one platform to another one, 3:46 then the path that you defined in the name of the file may not work anymore. 3:52 So second consideration. 4:03 Make sure that you use the names for variables. 4:07 That is a mnemonic. That is something that is kind of representing the content of the variable. 4:14 So I use the name Gen Fab 2016 file for the handle of the file. 4:21 Um, you have yes questions? 4:30 No. Okay. Mute yourself. Um, you have been asked to keep the header on the file. 4:34 There are several ways for doing it. 4:46 The easiest way is to initialize the counter of the rows at minus one, meaning one that will be the second, meaning that you are keeping the head. 4:48 Another way could be to read the off the loop for the first line and then in the loop all the remaining lines. 5:04 So those are both reasonable, valuable solutions. 5:15 You can pick each one. This one is probably the easiest one. 5:23 Yeah. You will read something that you will not consider, but that's fine. 5:29 You would do anyway. So I read the five part one was the first file. 5:33 So I read the file. I mean, I opened the file assigning the pointer to the handler. 5:40 Then I initialize the counter. 5:48 That is one counter for the. That the number of lines in the file. 5:52 And then there are two more counters. 6:01 One for the subscriber. One for the customer. 6:05 And then I created a list of elements that I downloaded from the file is not the only way of handling the printing the last of the first rows, 6:10 but is a possibility. We will work with pandas today. 6:27 Pandas are a data structure. When you deal with a CSV file and in general, when you deal with files and in particular files containing numbers, 6:33 pandas are way much more efficient than variables of any kind, either release or dictionaries. 6:50 So but in this case because we didn't introduce yet pandas she we find a greater and less great. 7:03 So those are the initialization so that I did then I started the loop for on the file and I appended each line to the content this. 7:14 Then I added that one to the counter. 7:31 Again, I'm not sure if you already sold that, but if you want to add the number to a variable, 7:35 I mean adding meaning in place or meaning adding the value to existing value, 7:51 you can either do something right like a row counter one equal a row, counter one plus one, or you can use this notation that is more compact. 7:57 There is always a tradeoff between being compact and being more readable. 8:09 So in this case, it's not that much readable as having a row, counter one, equal row, counter one plus one, but is more compact. 8:15 But once you know it, you can use it. So row counter one plus equal one in this case, adding one to the existing bag. 8:25 Then I'm asking if the term subscriber is in the row. 8:36 If it is in the row, adding one to the subscriber counter. 8:40 If customer is in a row, adding one to the customer counter. 8:45 When you will finish the loop, you will have the number of lines to the number of subscribers and the number of customers. 8:51 So at that point, you are ready for whatever you have been required to do. 9:01 You also have in the variable a content, all the lines and you can print the last five lines. 9:08 So. So keep in mind that if you were being asked to print the first five lines in content, 9:20 you still have the first line that is the header meaning you shouldn't start from. 9:29 I mean, in this case, I'm actually printing the source, so I'm doing something different from one. 9:38 What is the right here? My apologies. But it is good to see that. 9:44 So instead of range zero five is one six because the first one is the header. 9:49 So it is not doing what is in the printing in that line 31, but the concept is the same. 9:59 Then I can calculate in the percentage of customers. I use the round by two. You don't have to, but when you use around you have a set number of decimal numbers to the result that will look nice. So I'm calculating the percentage sum and then I'm printing the results for the first file. So there are that much rows in the data file. Of those, that match is a customer that matches subscriber and that this is the percentage of a customers compared to the total. Pretty much the same thing for the second file. So same initialization because I've not been asked the. So this is actually something that you don't need there anyway. Yeah. I'm in. Okay. Sorry. So instead of doing. This for printing at the end and doing within the loop. So within the loop I'm printing the first five lines. So. If the counter is less than six printing the line. If not, I will skip it. Incrementing the all the counters. Culturally the percentage. Print in the results. Then part three. Uh, I in comparing roll counter one withdraw account to two and if it's the one is bigger than the second the that's the. The message that they would send. If no other message, then I'm asking if the percentage in the first file is greater than the percentage in second file. And the message. When I printed. That's what you have. So the lines. The number of rows. I mean that it is should be the summation of those two equal these one assuming because they are either customers or subscriber. But when you have a file from the award, don't assume anything because there could be errors. You couldn't have a blank. You can have a misspelled name and you will not get it. So don't consider the summation equal to the total as a proof of what you are doing is good because sometimes the numbers can be different and then you have all the rest. Okay. So questions so far. All right. So. Let's move on and let's talk we will talk about three different. Topics on the same larger topic that is handling data. Let me share the screen again. Let me go here and let's talk about the handling data. So this part is genetic. So it can be used in any language. It can even be used without a real language, but just in philosophical terms. So we work with data. So we mentioned last week that data could be text file, could be as we could be other. There are many different. Types of data that you can get, that you can have a relational database, so you can have data warehouses, you can have transactional databases like the list of transactions in a supermarket. You can have other forms of data. You can have data that you download from the web or that you scroll from the web from a web page. You can have a geographical data. You can have time series. So there are a multitude of data. Most of the data in that structured environment are in databases. So databases are the large integrated collection of data with some characteristics that are kind of the common denominator of the different databases that you can have. And in particular, all of them, they have what is called the database management system Dbmr. So there is this software. For the user to interact with the data is something that is making the handling the data easier than just working with CSP files. When you describe the data, there are several levels of data. So you have the physical data, meaning the zero one that are in whatever is. The device that you are using can be a hard disk. Whether it is the hard disk, it can be the cloud can be whatever. But the very end there is a collection on zero one. This collection on zero one is allocated in the physical device in different ways. When you delete the if and you actually do not delete the file, you just delete the pointer to the file. So you have this storage system, let's say is the equivalent of a disk that your data are not sequentially plays in one location or in another. Because if that was the case, when you delete the file and then there will be portions that will be not allocated because that you are deleting a file that is that big and you have a file, the new file is that big and you have some waste portion that not necessarily will be filled by the next file that that can be bigger. So what the system is doing is creating a content allocation table that is a sort of directory that is pointing to the system that want to access the data is pointing it to the pieces that are scattered on the physical device is like a map. So that the file, I don't know data the V is not contiguous in the space but is scattered in the physical device. When you delete the data, don't see as we finder you basically delete the entry to the file. So this map of the treasure will be deleted. So that's what you do when you delete a find. That means that eventually you can recreate the file later on. So don't trust the delete file and you return your device. The only way is to physically erase the file. The device, meaning writing a zero or whatever is a different character across the entire disk. That will require a hundred times the time that you would need to just erase the pointer. Table of Contents. The Table Allocation. The allocation table. But that's the only way. So you have the physical data on the device. You have this sort of table of content pointing to the location, basically associating the name of define with that where its pieces are in the disk. Then you have a more logical component. So what is this file representing? It is representing students. Is representing clients. So that's the schema, the conceptual schema that is on top of the physical schema. And then you have different views. So you have the data from the university, but you have different views because I cannot see the salaries of most of my colleagues. You cannot see some personal information of your colleagues. So whatever. But HRA can see both can see information about the students, information about the employees. So based on who you are, you have different view. So those are the levels of abstraction that you have when you deal with the fire. When you have a Dbmr database management system, you have a support somehow in doing your job, in interacting with the. Data you have. So is something that is helping you, retrieving the data, aggregating the data, making sure that if you have multiple people accessing the same data, there will be no conflict on that. It is a generic term, but there are different nuances of this concept of conflict. So a database management system the BMC as a. I mean in real world there are several products doing the job. Oracle was the leader. IBM with DB2 was another big name. Microsoft as a several of them. There are all of those are commercial products that are noncommercial products. Open source minus one PostgreSQL. So those two are the two most commonly used, but not the only ones. The main components of a deep VMs are a storage manager that is basically managing the table of content. There is a query process, so that is something helping the user to access information. Then you have a transaction manager working on the logs, on the changes on the database, and working on reducing the conflicts in terms of concurrent execution of the transactions. So those three are the three main components of the VMs. The job that the department is doing. It's generally referred to as a a.c.h ID atomic, meaning it's all or manner consistent, isolated, durable. So all of those are very important characteristics. Consider if you have a, uh, a power failure and you initiated a transaction, a power failure, or the transaction is being interrupted, what is going to happen? How much did you lose? So the is handling those situations with all or nothing, meaning that either the transaction has been completed or it's never been initiated. That's the logic. So you know how it works and you know what to expect. That's an example. Or. I mean, managing conflicts. You are reserving a seat in a concert. You want to be sure that no one will take that seat till you complete the transaction. When you design a database, you want to design the concept or model of it. So there are three main characteristics of a data model. Entity attributes and keys. So those are the main components. In the middle 1970s, Peter Chen created the entity relationship modeling. That is a way to handle entity relationships and keys. So in that very basic representation, you have a like in the chart, you have the entity that is represented with a rectangular shape, the object, and then you have the sort of Boyds that are the relationship and then you have the attributes so that that the oval referring to the entity. So in this case, you have two entities, shopper and item shopper buys item an item as three attributes. That is a type price and sorts. That's an example. Is a very basic example. In real life, things are quite complex and complicated because there are a lot of entities and other relationships and all those attributes. So that's an example of something that you can see in real life. Why you need such a mass, you need something like that because sometimes you really need to have a picture of the day that you have for several reasons. Because you want to change something and you want to see what the implications can be. Or because you have been audited and the auditor want to see what are the impacts of the transactions on, let's say, clients. And a logical schema is what is doing that. So there are federal laws requiring that the traceability of data and processes for all the company, that the data are publicly traded, meaning you need to have a representation of the data and the processes that your company is doing. It's a little bit more selective than that. Only the processes that are related to money, generation or management. But pretty much 99% of the processes are somehow impacting or impacted by money. Um, one thing that we do when we handle datasets is a normalization. So instead of giving a theoretical definition, I will use an example. I mean, the screenshot is from the Stone Age, but the concept is pretty much the same. So you have in this case, the Ebola. That can be excel in this case is access, but that doesn't really matter. So you have that in the same structure, in the same database or a spreadsheet. You have information about the order, information about the supplier, information about the class. And then that means that if, for example, the customer changes the address, then you need to go into each one of the rows where that customer is and change it. So it's definitely not efficient. The normalization it's aimed to address this point, creating separate tables for each one of the logical entities. So in that case, you had three logical entities, that is, order suppliers and customers. So if a customer is changing the address, you change the address only in the customer table, and that's it. You need to make sure. So we mention Keyser when we talked about entity, relationship model. You need to make sure that there is a way to connect the different tables. So you need to be sure that you have a customer number that can be addressed and that the customer number can be somehow linked to the other table. So you need to have a keys defining that single record to let you access that. When you are in a larger environment, you may not. I'm in the plane. A database may not serve you enough, so you may have a different databases for different functions of your endeavor, whatever it is a company, university or your own, eventually. And then you need to have something paging all, all of that and kind of extracting, massaging and presenting in a different way. So those megastructures are generally called the data warehouses where you have that single component. So that data databases and then you have a. A software that is taking pieces, is required, integrated them and presented to the user. So those are named the EPL that is for extractor transformer loader. And what they do is just that. So they take pieces from the different parts, different databases within the data warehouse. And they do eventually some cleaning of the data. They do some integration of the data, add some characteristics from the operations that you can do across the different databases and present the result to the user. So those are the episodes. We talked about the how to I mean, the DMZ didn't have the possibility to me. They give the possibility to the user to interact with the data. So to interact that you need the language. So Edgar called the, uh, created the relational query language or structure the query language escuela. That is the golden standard. The. For interacting with the relational database. It's a is a standard. But pretty much each vendor, they have a few differences, making them not 100% compatible. So there is a. A core functions that are across all the databases. Excuse me, but. Some of the vendors may have additional functions because obviously they say this is why my database, my dbmr is better than the others. So. Relational algebra is basically what is behind school. You can write a program in the actual escuela as all the logical structures that can. Define a language, you can have conditional sequences, loops. So all the basic functions are there. I generally in my programs. So when a user as well a generally. And the logic in Python. And then just cool the data after the processing from the database. Once I have the data, I do the remaining operations in Python. But the logic can be either on the Python side or on the execution side, because both have the possibility to do that. But because we fight on that, I have more libraries and more possibilities to do things in a more efficient way. I move the logic more on the Bible side than ask you outside. So I mean, we do have a course that is pretty much teaching as well. I created it a few years ago, is an undergraduate course, I think is a ISC to 25 and is data engineering that is very centered on that as well. So those are some of the names already mentioned, a few of them IBM, DB2, Oracle, Microsoft and then may as well a general use may as well in my applications. Well, I use either my to a lot of both that is not here. My name is pretty common. The reason why some people do not use my school are because my school has been bought by Oracle. Meaning yes, it is open source but is from someone who is selling databases as one of the main parts of the offer. Meaning at a certain point they can say, you know what, we will not dismiss my as well. And if you want to continue, you need to pay us money or the commercial version of it. So and that's why a lot of people is not using may as well but is using this there are more tools actually more add on to my as well and I'm enjoying my as you a little less than when it will become a commercial product I will do the migration. Not everything is in relational databases. So there is a portion of the data that is not international, that no relational is growing and is growing a lot because the universe is not relational and we are getting data from all over the sources and they can be in any format when you have a relational database that you need to define, whether it is like an Excel spreadsheet, that you need to define the header, you need to define the entities and the attributes, and then you will populate the database with the data. But if you are getting something you have no idea how is going to be formatted, then you cannot use a predefined structure. So when data is not structured, then you may have a problem. Most of the social media are using a dual approach. They have no relational database to get the information from, to collect the information from the user. Everything that we post and everything that they steal from us will go in a unstructured structure. And it is a no relation and no well, a database Facebook created. Cassandra that is no relational database. I don't use it. I use another product. We'll talk about that in a minute. But as backend, they have a relational database. So when you ask for your profile, profile of people, those are data that are in a relational database. So the frontend is not relational. The backend is more like. So again, the main reason for not having everything in a relational database is the lack of flexibility. So that's the main reason for that. When we go in. No excuse allowed non as well. There is no real definition that are known as well. It started around 2009 and is growing. A user MongoDB be among those. When I collect the tweets from the forum analyzing social phenomena and when I download the tweets, tweets are in a format that is the same font that is a gene similar, that is the same format that is in the structure of MongoDB, meaning dumping the tweets into MongoDB B is super easy. MongoDB is also an internal structure that can be mapped the 1 to 1 to an additional database, meaning that once I have my tweets on MongoDB and I want to do just like the example I was mentioning with Facebook, I want to create a relational database on some of the characteristics. At that point, the transition is easier. One of the main differences on relational databases is that they do not provide the same level of support than the relational are providing. There is no. Stress on the integrity of the transaction. So it's best effort is pretty much the same thing that you have when you access a web page. Is there or is not? You can access it or you cannot for any reason. So it's best, therefore. Meaning that all the logic that you have in relational database is now. To your code. So if you want to have integrity on the transaction, you need to take care of it. If you want to have any other of the features that a relational database may have, you need to do it yourself. So it is a tradeoff on one side. You don't care about the schema of the database, why you should do in a relational. But on the other side, you don't have all the functionalities that the relational database may provide. So this is a table comparing a relational database with MongoDB. There is a 1 to 1 correspondence between the two. MapReduce is another approach in relational database that has two steps. So map you basically upload your data and each day that is the logical data is defined as appear key value. And then when you do a user, you basically work on this bear to key values. To go to the subset of the dataset that you want to get is not as strict as a relational database where you can go directly on the individual record, but when you have a large quantity of data may be faster. Hadoop is one of the examples, so use of MapReduce approach. But generally speaking, Hadoop is providing support for distributed file systems, meaning you can have where systems is a generic term and can be stored as can be devices. So a few years ago we received a donation of several Mac Mini, so we couldn't do much with the Mac Mini. So because they were not that much powerful, we created the circle, the network of those Mac Mini, and we put an overall layer of Hadoop managing the individual communities as a resources. For the larger system that we created. That seems to be just like a nice pastime. But if you consider that we are in the process of having mega computers working on things like the GP that cannot run on your computer, but if you have a warden and a network of computers with somehow a logic approach, you know the master leave with something like Hadoop, then you can have a comparable computing power without having a single super powerful machine. So it will become more and more popular. When Hadoop was created, everyone was thinking Hadoop is the future. Reality is, not many people are. We know what Hadoop is, but most likely they're using it. Because when you have a way that all the data most of the time you have and in the cloud solution. So you have remote storage solutions. Meaning you have no idea what is the organization of data than Amazon or Google or Microsoft may have. So you just access the data. Most likely they have Hadoop, meaning Hadoop is really getting a good share of the market. But in terms of number of installation is not that big and I don't know my as well. So that's an interesting paradox, but that's the way it is. When we work with the town, one of the key parts of the process is the data preparation. When you work in an academic environment where when you are doing exercises in a course, most of the time the data you have is pretty clean, is predigested. But in reality, this is not the case. So when you have data that you take from real life, the data is dirty. So you need to clean it. The data preparation, I mean 70, 80% of time and effort is pretty much like that. So it depends on the type of data. So sometimes it's already pretty clean. Some of the times is now. So the data preparation is really essential because at the very end the quality of the data is in relative terms is the quality of the analysis. So if you have a I mean, if you consider models like Charter GP, again, those models are machine learning how machine learning is. But we will talk about that next week, next two weeks and next class. So those models, they have two components. They have the algorithm and the data. So the algorithms are basically discovering patterns in the data. And then there is another component matching your request with the partners. There has been this common meaning that if you don't have enough data, you don't have enough partners. If you don't have enough partners, the answers will be crappy. So a machine learning model, the quality of machine learning model is the quality of the data. So the quality and the quantity of the data. So we are not remembering many billion words that are in the chapter GPT. But pretty much it's all the data that is available in open source. It took them a week with the same energy that is required to run a few thousand CDs in the US. So that match, it's sort of brute force. But that's another story we talk next to us. So but overall, the quality of data is the quality of analysis, in particular when you have that data driven system. So like in machine learning. If you have not enough data or an or low quality of data, there is no way that you can do anything meaningful. So the data preparation, let me keep some of them up is basically on three main categories. The actual cleaning? Well, I mean, the cleaning means you have some mixing values. You can have outliers, you can have some values that seems to be wrong semantically. Uh, you may have inconsistencies. So all of those are part of the cleaning, then the integration. When you want to analyze a problem, you want to have as many different sources as possible because different sources will give you a better view of the topic that you are analyzing. Why our markets are so good in pinpointing us. Because they have information from Google and Facebook on the page that we see online. They have that information from the credit cards on what we spend. They have information from E-ZPass where we drive. They may have information from a physical vendor. So when we go in a store and there are cameras analyzing where we go and then the data is integrated with the social media if you have Facebook. Facebook is also the owner, May is also the owner of all the Instagram and WhatsApp, meaning that each one of them is giving a different view of you. And then you have a system collecting put in together, integrating those different aspects of information about you and creating a profile that at that point that is very strong because it is again the different aspects of your life. So the data integration is really essential and sometimes is really difficult because data can be different, forms can be with different semantic keywords. You can call something in a way or in the other you can call a street. The answer is the result or the full name or dot. So when you try to integrate the you need to consider that I mean that the street is the easy case, but that could be cases more complex. Transformation. We mentioned the normalization, but there is also the aggregation. If you have information about the number of times an event happened and then the duration of the event, well, if you had those two values separated, probably you cannot do much because let's say we are in the business of analyzing accidents, cut accidents. If you know that there are five accidents. And the driver is driving since ten years is something. But if it is ten accidents and the driver is driving, one year then is completely different. So the integration could be creating a new call on that with Accidents year. That's an example of transformation. So again, cleaning integration transformation. Okay. So that's the first part, the questions so far. All right, so let's move on. And let's go to the second part that is about the functions. So we are back to Python. So from the theory now we are in the language functions. Functions certainly are an important part of Python. Probably is the reason or are the single most important reason why Python is so popular. There are quite a lot of functions that you can get from outside, but several thousand other functions meaning in each domain. You may have multiple choices for a library user that can make your life easier. A typical example that they use. And I started working in artificial intelligence in 1985, 86. At that time we didn't have languages like Python, so we had Fortran, we had Lisp, but it was a language that was room based that was created for creating expert systems or group based systems. But when we had to create an algorithm, so one of the oldest, most commonly used algorithms is called the decision tree. So we had to build our own version in at least Fortran or one day rather of the decision tree algorithm. What I do now, I basically import a library and then from the library I call the function. So in two lines I am solving what would cost me probably a hundred lines. So not hundreds, but probably 50. Yes. So it it's a major difference. So functions can be functions that you define. Functions. So that are part of the standard Python functions that you import from external libraries. So for example, I was using before is the last one is imported from external libraries. Between functions. So range some length. So those are examples of functions. And those are built in in B.C. By then you can define a function. So to define a function, you use the reserved word, the F, then you name the function. In parentheses you have the arguments, meaning what you are passing to. The function is a function to calculate an equation. Then you need to pass the values or the variables for the equation, for the calculation, or you cannot. You can pass nothing and have the function doing something like in this case. So I'm passing nothing and the function is printing. Hi, Jim. So what I do is basically if the program is this function that can be anywhere in the code, I generally use them in the very beginning for clarity. And then I call the function with hello passing nothing. And when I call the function I enter here and then I will get I Jim then oops I continue my program, I print zip and then hello again. And we print again at the same values. Just like the conditional of when you create a function, you want to have a colony at the end and the content of the function is indented. So there's that syntax. So this is a function. When and how you use functions. You use functions when you have a sort of a recurring use of the same code. Like in this case, you define a function that is called the triangle area and you expect in the function to have height and width. And then what the function is doing is calculating the area. So with divided by two multiplied by height and then the function is returning the value. If you look at the other function, the first one that we saw, this one is not returning anything. Functions can have a written value or cannot. So in this case, there is no written function. So you call the function and the function is doing something, let's say, without returning. Anything in this case is returning the area. So let's see how is used. So is use the new pass. Does the with the say hi. And then you want to bring the area the triangle passing to the function. Those two values. If you want to change the values, you can use the same function. So that's the real reason why. One of the main reasons why you want to use the functions. 1: So instead of writing in times the same code you write once and then reuse it if you have a function that is doing its job just once. 1: You may consider not having a function, not for optimization purposes. 1: You can have it because a is more readable is pretty much like having blocks that are doing something in your code. 1: We don't use classes in that as a logical structuring python because I don't see the reason for that. 1: But when you use classes, the classes are like I mean, a few years ago a lot of people were building to object oriented programing. 1: Those classes are the equivalent of objects, so they contain functions and values. 1: So they are like single entities that can run in your program or can be called by another program to run independently from the original program. 1: So that's the class. 1: Most of the functions that we imported from outside are structured with the classes because you may want to use only a portion of the entire library. 1: Another example of this is the square root. So you calculate the square root. 1: You first ask if it's negative, but you can not have the square root of the negative number and the return will be none. 1: Then you say, if it's zero, then you return zero. 1: Then you define start and limit. And then you set the loop. 1: So you do the calculation for the square. 1: So if you want to print the square root of 15, you basically just pass the number to the function. 1: Then this 15 will take the place of this, this X and everything will be calculated as X as a value of 15. 1: Next time you call it with a different number than X will have a different body. 1: So that number is the argument that you are passing to the function. 1: So that's another example with no return of value. 1: So the function. Is generating a greeting for the user based on the language. 1: So if the language is. 1: Yes, a Spanish is. Hola. If it's French is bonjour. 1: Otherwise is hello. So if you call the function with the n, then you will get the allo. 1: Yes. Hola. F are you. 1: So that's an example of a passing values to the function, but the function having no written value. 1: Is doing something different based on the input value that is receiving. 1: The the town value again that can be there or cannot be there. 1: So this super basic stupid function is great and returning. 1: Hello. So basically when you print Greta Glenn, you will get the hello Glenn and whatever is the other you would get. 1: Hello. Obviously there is not much reason for creating a function, doing just one thing. 1: You just write hello. But that's an example. Just to explain how the written value may work. 1: You can have multiple written values and you need to handle that. 1: Um. That's an example of using the return value. 1: Uh, go here. 1: So in this case, it was just printing directly without greater value. 1: In this case, is returning the value of the greeting. 1: So if you pass in, you will return. 1: You will get hello as return value. And that's the printing and so on. 1: So you have that if you have a function like this one. 1: So the argument is hello world. 1: And this argument that will become the parameter for your function that will return w 1: the result this w will go into big so and then when you print big you would get w. 1: If you have more retail values, you will have multiple values instead of big. 1: That would be in May. That's how many return values you'll have, and those values will be separated by a comma. 1: So if it's written W and T, then it would it would be a mean instead of one that would be big comma. 1: Something else. Um. 1: You can pass multiple arguments so you can have the food eventually. 1: For us, the arguments. That's an example. 1: So you have the function make sandwich. You have meat, cheese, Ebola, cheddar bread, equal rye print. 1: The idea of a sandwich with meat, cheese and bread. 1: When you bring the. I mean, that's the function. 1: The function is printing those. If you pass ham, the only variable in the function that has no default value is meat. 1: And this ham will replace meat, meaning you will get. 1: I'll have a sandwich with ham, cheddar, brant and rye bread. 1: Right, Brett? Yes. Same sperm if you have more than one. 1: The second one, we replace the first value with the default value, meaning it will replace. 1: These are cheddar cheese. Equal cheddar. So the one it will print would be [INAUDIBLE] of sandwich with spam, cream cheese and rye bread. 1: So again, you can have defaults. Uh, you can have a variable number of arguments. 1: This is something that we don't use much because we tend to be a little bit confusing. 1: But with Python, you can do it. Again, 1: you use the function either when you have a piece of code that you want to reuse multiple times in your program and 1: or when you want to have your code structured in a way that is more with logic blocks in some other readability. 1: Generally speaking, I don't use functions if I don't re-use the same code. 1: But some other people again with more an object oriented view, they tend to think about objects that are either classes or functions. 1: Nothing wrong either way. I just feel more comfortable using functions to only when I have code that is going to be repeated. 1: Another point that is relevant. One of you started using a function for the previous assignment and that kind of got lost in that. 1: The concept of a local variable and global variables. 1: So variables within a function will say in the function like in the. 1: Unless they are defined as global. When that function is defined as global, then they will keep the value. 1: Let's have some examples. So you have this function where that is called foo and is getting two arguments x and y. 1: I define a as a global variable than a assign the value five to the variable a the value ten to the variable b. 1: I flip in this function x and y, and then I print the inside food and then I print A, B, X and Y. 1: So there's a function. So in the program I assign a1b, two x, 3y4. 1: Then I print that before calling the function and I have a, B, X and y and that's what I have. 1: I have one, two, three and four. Then I call the function passing it 25 and 50. 1: Meaning I'm going here. I am printing inside the function. 1: We are here. 1: And then in a has been defined as five and that's what you get to be is A 18, B is ten because it's been defined as a ten in the function X. 1: So we pass. X was three EPS in a Y was a four. 1: In this case. Uh. And so we passed the 25 and 50. 1: But we flip them within the function and you have 1525 when you are outside of the function, you print a because a was global. 1: It kept the value that was inside the function. 1: B b was not global meaning even if inside the function was then b is not global and we have the value to back and that's what you have. 1: And then x and Y. Same thing. They have the value that are outside of the function. 1: So again, local, global. 1: Keep in mind that that only functions so that they are explicitly defined as global will retain the value when they leave the function. 1: How many times they use that? Not many. But is it possible? 1: I mean, you want to have a function to have as much flexibility as possible. 1: You may not want to have a global variable like keeping up the volume, but sometimes in specific cases it may be useful. 1: That's another example. You can do all the operations you want. 1: Of course. Imported fudge. 1: So I mentioned before that there are several thousand functions in I don't know what's going on. 1: Can you please stop messing up with my screen? 1: Thank you. So. 1: Functions in this case. So you can import functions. 1: Let me stop shooting for a second and launch again. 1: All right. 1: Think, to disable that function because I mean, you can just not intentionally write on my slide and that is not going to be nice for the rest of us. 1: So Martha is one of the libraries that is available in in by done that. 1: So you the statement is an important name of the library and then within the library math there are a lot of sub functions and you can call up, 1: for example, the square root instead of writing our own function, the square root, so over 15 and you will have the result. 1: So that's an example. The other way to use it is a. 1: To import only the portion of the function that that you need. 1: So in this case, from math importing all the security. 1: Keep in mind that when you import a function, in reality you are importing in your program the entire library. 1: Meaning you are creating something that can be big. 1: So some of the functions are really big. Math is big, is not the biggest, but there are some that are really big. 1: And they point in your program. We say in memory using a lot of memory. 1: So if you have also quite a lot of data, then the combination of your program being a giant and the larger dataset, you may run out of memory. 1: So in those cases, keep in mind that you can import only the portions that you need and then the occupation of memory will be reduced. 1: Another way to import the libraries is to use nicknames. 1: So like in this case, I mean, this case, I'm not getting match because I'm just saving one character. 1: But some of the libraries, they have a longer name and then you use only two or three characters to call them. 1: So in this case, when you import a library and you add a nickname where the syntax is import name of the library as the name that you give. 1: Instead of using the original name, you can use the nickname same way as the original name. 1: And that's basically a little better about you. 1: What else with the links? Let's talk a little bit more about those modules, those packages. 1: So again, there are some built in functions. We know that we used some of them. 1: One of the most commonly used is a lot of it is for graphs, for creating graphs. 1: Graphs can be graphs like in Excel or pie charts, bar charts, histograms. 1: All of those can be created with Matlock. We will go back to visualizations in a few weeks, 1: but just keep in mind that Matlock is the starting point, let's say, for doing a visualization in Python. 1: In the past, Python was not great in visualization. 1: Now there are many other libraries doing a great job. 1: So that's an example of the application. 1: Again, Matlock Lib is a long name, so we generally use P as key to make it short. 1: So at that point you import a specialty and then when you use it, you just use PLC as an example plotting. 1: This is a line plot with all the options you want in types of marker, call or line scatterplot, the bar chart, the pie chart, the histograms. 1: You can have multiple plots on the same screen with subplots like this one. 1: So that's my plotline, though. Another super common is a numpy numerical python. 1: One of the advantages of numpy is that it is using another type of variable. 1: So is not. This is not dictionary but is called array. 1: Array are like lists but with some differences. 1: Only numbers can be on that think arrays as vectors. 1: So that's the best way to represent an array. 1: So in array is a vector vector as a dimension. 1: So if you have a vector in a two dimension, the vector we'd have two values that could be X and Y. 1: So it will be a point at that point, if you have a vector in a n dimensional space, 1: then in the variable of the array will be in square brackets and there would be n components for it. 1: We will use that a bit down the road. 1: And it's really the foundation of data analysis. 1: So the combination of new PI and pandas that you would see in a moment, it's what you really want. 1: Arrays can have multiple dimensions. 1: So like, I mean, it really depends. 1: Like matrixes, but can be dimensions. 1: And that's. Are probably the most commonly used libraries library in data science. 1: Not because it's doing a lot of data science, but. 1: So is is a way to represent data structures. 1: So you have the data, you want to have the data in a data structure that could be representational, like Excel, and then you want to use it. 1: Those structured data frames are similar to what is using MATLAB in our right, 1: if you are more familiar with that Wes McKinney created when he started working in a financial company in Wall Street, 1: and then he asked the company down the road to make it open source. 1: And they agreed, fortunately for us, and we are all using that. 1: One of the advantages of pandas is that the foundation is known by meaning. 1: Most of the functions are provided by pandas. 1: Are with the engine. 1: All we now know by. 1: All right. So let's talk a little bit more about those pandas. 1: So just to begin. Nothing to do with the animal even if obviously they play on the ambiguity is for a python data analysis 1: library again that it's relatively recent and is the most commonly used library for data science. 1: There are two types of structures. 1: One, there is a one dimensional structure. 1: Okay. Let's say here. One dimensional, two dimension. 1: So we will use them quite a lot. 1: There is kind of a correspondence 1 to 1 with either as well or Excel. 1: So DataFrame in Pandas is a table they call on is a variable row are observations and 1: you have a way to group by to merge somehow or to extract from existing structures. 1: So serious one dimensional data frames, two dimensional most of the time. 1: We call that a frame, even the one dimensional. 1: So those are the structures that we will use. 1: We'll be talking about in pandas. The variables would be pandas objects. 1: So we'd be no arrays would be no. 1: Uh. I don't know, dictionaries or lists. Uh. 1: That's an exemplar of one dimensional structure. So you import pandas. 1: That's another case when you use a nickname. 1: And then you create the PANDAS data structure called the S, and you create the structure from those numbers. 1: You create indexes for each one. And then when you print the structure, that's what you'll get. 1: Then you can address one element using the other as an index. 1: You can import from dictionaries. You can import. 1: I mean, because it's rooted in numpy. 1: You can do operations directly on the vector, on the entire structure, making it way much more efficient. 1: Down the road. We will talk about how to better use. 1: This approach instead of doing loops. So when you multiply the entire rule by another row, 1: it's way much more efficient that using numpy slash pandas instead of doing a loop multiplying each element, pet each element. 1: Python is not very efficient in loops is even. 1: Less efficient in loops. Within loops. 1: Nested loops, if you can avoid that. 1: And we will talk about that later on. That would be really helpful. 1: Anyway. So you will have several examples of that. 1: Just to give you an idea, I want to share with you for a moment that. 1: Something that Dylan may remember. That's a script that I wrote a few weeks ago for putting together the data on classes and. 1: Guilty from what they what they it's really a pain in the neck. 1: It's not very flexible is not giving much of the the any information that we want. 1: And that's what made me think so. 1: I was in charge for scheduling classes for a several semester. 1: Now my colleague VLAN is taking over, unfortunately for me, unfortunately for him. 1: But this life and the way I did it was using Excel. 1: But there was a gap between the information available on the work day and the information on my spreadsheet. 1: So each time I had to do something manually to clean the data, add somebody else, 1: and when you do it manually, you have no guarantee that you had consistency across the times. 1: You do the same operation using a program like the one you have on your screen, 1: you define the process and then you are sure that each time that you do it, the you basically apply the same rules. 1: Because there are parameters in play. 1: Instead of having the values of those parameters, they can be. 1: Because when you download the data from Workday, you have all the courses for the entire sequence. 1: So I'm interested to the courses for the school systems and enterprises. 1: So I want to extract the only those with a certain prefix. 1: That's an example. Or I want to focus on one semester. 1: So. I created a configuration file where all of those parameters are up there, 1: and then I read the file and my programmer will basically use the parameters to function. 1: So the reason why I'm showing that is because I'm using pandas. 1: So I'm using pandas to read the configuration file. 1: So let me see if I can show you what I'm talking about. 1: Just one sec. So that's the configuration file. 1: And so the configuration file as the name of the file that I'm reading. 1: The name of the file with the information about the faculty, the information on the suffix of courses that we offer to our corporate sponsors. 1: The name of the semester, the programs that we call over programs that are kind of a mix program. 1: So undergraduate and then I have a black list programs that they want to exclude anyway and whitelist courses that they want to include anyway. 1: And the number, the threshold of enrollment to be considered as a real course, that's an example. 1: Is a configuration file is something that you do to avoid to write in your program values that can change over time. 1: So at this point, I can use the same program for different semesters. 1: So without changing the program. 1: So I'm reading the configuration file into a pandas and then I extracting the name of the file with the courses, with the faculty and all the rest. 1: So that's a way to read that. I mean, it seems to be complex, but it's not that much. 1: Same thing for I mean, reading the configuration, reading the fine from what they read in the fine from faculty. 1: So for all of those, I'm using pandas. 1: So with pandas, you can read the as we you can read the Excel files text. 1: It's pretty much all the possible all the most commonly used sources and some of the less common. 1: You remember in last assignment that we had to do something to skip the header. 1: With pandas you can say head on zero and that header will be a out of the category. 1: You can transform a pandas into a list. 1: I mean, in this case, it's a relatively big. 1: Let's see. However, this one should be big enough. 1: So that's basically what you get from. From what day? 1: So you have all the courses for the entire university. 1: It's quite a lot and you are not interested much in the header and you need to select only courses with the thing with the certain graphics. 1: So anyway. That's an example of use. 1: So in this case, what I did was a cleaning of the input file, meaning eliminating those causes in the empanadas you have. 1: And in order in the conditions, the end is with the commercially ampersand, whatever you want to call this symbol here. 1: And the vertical is from that order to end an order and adding the conditions. 1: Then. Once they added the conditions, I'm adding. 1: Fisher So I want to know what is the program? 1: What is. 1: If it's an undergraduate and graduate or thinks that that an end in saving everything is going to be fine, that I will copy into my Excel spreadsheet. 1: So that's a typical example of real world use of pandas. 1: Could I have done that without pandas? Sure, I could have done it with a CSB. 1: But I mean, doing things like cleaning the input. 1: I knew I could do without pandas, but then I would need a lot of loops. 1: So right now it's pretty fast. 1: If I do with loops, it would take forever. 1: So anyway, that would be just an example. 1: So let me stop sharing at this point. 1: I hope I didn't scare you with that. 1: And, uh, at this point, it's eight or five. 1: We have good time for and in class exercise. 1: So let me introduce the in class exercise. 1: So let me share the screen again. They let me go here. 1: So the in class exercise that we do will be on using functions, of course. 1: So you take three numbers from the user, then you do a check if the numbers are numeric or not. 1: You will do it using a function that you create, and then you will print the factorial or the fourth of the three numbers, 1: and you will calculate the floating value of the solution or the linear equation that is a multiplied by x plus b equals zero, 1: where A and B are the two other numbers that you extracted that that you received from the user. 1: You will use your own functions both for the factorial and the linear equation. 1: Don't use external libraries, so you will use external libraries for the rest of your life. 1: But for this particular exercise, you are not going to use it. 1: Okay. So let me stop sharing. Let me publish. 1: Those are so. I published the solution for the previous. 1: Assignment. And then. 1: Okay. I published all the data, all the slides and in class exercise and let me create breakout rooms. 1: Okay. There are nine breakout rooms with three or four participants per room. 1: So it's eight way to give you, let's say, 20 minutes to start working on the assignment. 1: Then we will reconvene and we will share some of the results and your creations. 1: So the rooms are open. See you in about 20 minutes. 1: Okay. So I'm resuming the recording. 1: All the rooms are now closed because this. 1: And how was it? Any volunteer. 1: All right. So let me share the screen. 1: And let me goo here. Okay. 1: So the program was taking the three numbers from the user and then calculate the factory for the first one. 1: And then the solution, the linear equation created from the other two numbers. 1: Um, we have been required to have to create a function to determine if the number is numeric or not, 1: and then a function for the factorization and the function for the linear equation. 1: So the function for checking if numeric is pretty straightforward. 1: So we taking a value and then is trying to do a to calculate to transform the value into floating. 1: And if there is no error meaning it is okay and try it with a down at y if get an error with a thousand n. 1: So meaning is not numerical. 1: The factorization is a recursive function, so the factorial of zero is one always checking first if the input number is zero, 1: if yes with the other one, otherwise we will recursively call the factorial till well and the loops. 1: A linear equation. So again, you have a multiplied by X plus b equals zero, meaning x is equal mine or B divided by eight. 1: So and that's basically what you'll have. That's probably not very intuitive. 1: But instead of having each individual number ask the to the user, I created a loop on the three numbers and the three numbers will go into a list. 1: So the list will have three numbers that are the three numbers that the user will input. 1: So I initialize the counter all of the requests of numbers to zero one. 1: The counter is less or one two. I will ask it. 1: Please enter a number. And I just want to give the user a sense of how many numbers he included. 1: So if is the first time will be, 1: please enter a number that is a one or zero out of three and the second will be two out of three and three out of the three. 1: Then that's just a printer and then the input will be in a separate line just for a better visual. 1: Then I'm calling the function to determine if it's numeric. 1: So it's passing the number and getting over here. 1: If it will get an Y, then it will be numeric. 1: And then, if not, we'll continue. 1: And then we'll increase the counter to one. Not to ask the second one once it will finish in number list that you have, 1: you will have the three values and then I will print the factorial on the number. 1: That will be the first element of the of the lesser. 1: And I will pass this value to the factorial function and I will get the number. 1: Then for the linear equation, I'm placing the result in this variable here and then passing the entire list to the function. 1: I mean, I could have passed just the last two elements, but it's kind of easier this way. 1: And then using the first element, that is a meaning, the second element of the list and the third element of the list that it is a bit. 1: And I'm getting the value here and then I'm printing it. 1: So if I execute it. 1: And that the number 22. Three. 1: But before. So the factorial of 22 is this number the solution be being an equation from the sig on the two numbers? 1: Is this one in the process? I couldn't have probably around. 1: Instead of having the plain number would look nicer. 1: But that's what I have. Okay. 1: Questions? I have a question. 1: Yes. Good. In the second part where you're trying to find X for the linear equation, you rearrange the equation and put exit the left hand side. 1: That's how it works. Is that is that a way to put the equation as it is? 1: Because if the equations bigger, it might be difficult. 1: Do you know what what you want on the left hand side? Is it a way to put the equation as is? 1: Well, I mean, what you need is the solution. 1: If you look at the kind of a formula that a used is basically putting an X on the left side. 1: So if you look at that, the. 1: Right. Solution is X. 1: Yes. I'm saying. Suppose we have an equation that's a lot bigger than this. 1: Is that a way to improve the equation just as it is without bringing what you want to the left hand side? 1: Well, I mean, you can create a function for that. 1: But the very end you have a I mean, a linear equation. 1: If is linear, you have only two elements. 1: So you have one element that can be as big as you want. 1: But the very end is a coefficient is a number, and then you have a second element that is another number. 1: So for linear equations, f is one variable is like that. 1: Then F is a linear equation with multiple variables. 1: Then you need to change completely the approach in the way you write it. 1: I mean, yes, you can write in a different way, but I mean, it's it's a simple arithmetic of transformation that you need to do. 1: I mean, again, in line 33, that's basically placing X on the left side and all the rest on the right side. 1: But that's the way to solve the linear equation. 1: So again, that if you the linear equations can be complicated when you have multiple variables, at that point, 1: you have a system and you need to I mean, probably using new PI, you need to do the calculation, but that would be a different story. 1: So you will use pretty much linear algebra at that point. 1: I mean, in this case, it is kind of straightforward. 1: I don't know exactly how. Give you a better answer to your question. 1: Why do you think? Okay. 1: I think for a linear equation, it might be easier to do what we did, but I was just wondering if it was like a quadratic equation. 1: It would be harder to. Right. 1: A function like this? Yeah. I mean, do if it were a quadratic equation instead of a linear one, 1: if you have a quadratic equation instead of what you have in line 33, you should write a formula for it. 1: But don't expect to have the formula right as input. 1: So you need to know what the formula is. 1: So you have only the coefficient of the coefficients and then you apply the formula with the square root and all the rest. 1: Okay. All right. Thank you. Yeah. And I mean, the more complex is the equation and the more complex is going to be on line 33. 1: But that's basically where you solved the equation in a sense. 1: All right. Okay. So let me stay here for a second and let me spend a few minutes in. 1: How? So if you look at the example that I use here, as you can see, I have some functions. 1: So those are the functions that I'm using. 1: I'm using pandas, I'm using PI, and I'm using this function because there is a conflict between the versions of the functions that I'm using. 1: And I was getting a warning and I want to eliminate the warning using this library. 1: But that's I mean, is not functionally relevant. 1: You need to have those libraries in your system. 1: So to import those libraries, you want to go into settings. 1: You go into your projects. So in this case was a C, it was a C management and. 1: You have. Those are the libraries that they have in this project. 1: If you don't have a library, you need to add it. 1: So you go with plus. And then you do, I don't know, a new library. 1: I want to install a TensorFlow, for example. 1: So. You select the library you want to install, 1: you do install package and that will be installed at the point that when by Sharma will finish the indexing on the library that will appear here, 1: you may have every once in a while things like there's a small arrow here that is telling you that there is a newer version. 1: So you selected the you go here to upgrade and then. 1: All this is updating. And then at that point, I have the latest version. 1: Let me do the same thing for this one. But you can see here that is installing the package and then. 1: It's read. You have another way for doing it. 1: That is going to buy them packages. 1: And you can do the same thing. So you can do. For example, this one you have over here the description. 1: And then you do install and you have it. So that's basically how it works. 1: Importing libraries now. Let me now introduce the exercise for Exercise four is not that much different from what you did in Exercise three. 1: There is one relevant difference though. 1: So is it on the same CD bike? 1: So those are the files that we will use. 1: So let me close this. So those are the two files that you will use. 1: So one file is from November. 1: It is in 2021. And this is a little bit older. 1: So. If you compare the two files, you will see that the two fights that you have three, two duration, sometimes of time. 1: In this case, you don't have. The trip duration, but you have started an ending so you can calculate the duration from those two values. 1: So why they changed it? We don't know. But that's a matter of fact. 1: They changed it and we need to live with that. So in this something that is unfortunately pretty normal real life. 1: So you rely on data, they change the format. 1: I mean that what they think is happening all the time. 1: So they change the format and you need to adjust your code. 1: So that's pretty much the main difference compared to the previous assignment. 1: So you will have. You will read the fine. 1: You will create the function of print details and you will loop into the list of lists or the pandas say if you want to do it using pandas. 1: So this is explained as a list of lists and not as pandas, but you can do the way you want. 1: So you will look into the two files. 1: Those two files, meaning you have a big list that will be with each element will be one line. 1: And then if you want one element in one particular day, then you need to go into the list of list. 1: But we saw that last week. So for example, this is the the third element, the meaning the third row in the file, that's something like that. 1: If you want the data, that would be the first element for the fifth entry, but could be the second or whatever. 1: So you need to have two pointers. 1: The first pointer is the pointer to the record and the second pointer is the pointer to the element within that segment or whatever it is. 1: So these print details will loop into the. 1: Lisa and will collect the daily trip duration that you will get from Andy that started that. 1: So. I mean that in the first one you have the duration and the second one you don't. 1: So you want to calculate any way in duration, so you need to do it. 1: I mean, you need to subtract the the ended, the from the ended, the started to have the duration. 1: Then you print a blank line and then you print the average daily trip duration. 1: The five most popular starting and ending stations. 1: And the number of members and casual users. 1: Keeping in mind that even the user type changed. 1: So you have here that you have member and casual in the past. 1: So you had subscribers and customers or something at that thing. 1: Yeah. You have customers and subscribers. So why let in the new one? 1: You were members and casual. So that's another difference. 1: So there are two differences. Again, the file as different data structure, for example. 1: One is the redirection. The other note. So you need to handle that. 1: Then you will compare the results from the two files and the comparison will be based on the numbers that you created here. 1: Then you will print the end of file processing and you will write a and two page split. 1: A reporter describing based on the data, your findings you can have with some additions doesn't need to be two page, but cannot be two lines. 1: So that's basically it. Let me stop sharing. 1: Let me stop sharing this and. Let me make sure that I published the the in-class exercise. 1: I mean the solution. I'm publishing. 1: In the new assignment. And you should be good to go. 1: Okay. Questions? All right, so I'm stopping the recording.
Assignment is going to go. So anyway, the mid term, whatever is going to be the format will be in in 3 parts. 2:10 So one part will be questions like, there is a question here. 2:22 No, no, no, no! There is no on. Campus. So is all online. 2:32 M. 6, 24 is Ws meaning is online. 2:33 So if we will do in real-time we will be real-time. 2:40 But online but you will have a certain amount of time. 2:41 But it will be online. The reason why I'm doing that is because that's the way it is for the online version of the class meaning in the on campus. 2:48 The on campus version students have a 2 and an alpha for the mid term, and I want to make sure that the online and the on campus are compatible. 3:10 So the meter, the midterm is going to be 3 parts, one part will be. 3:27 Could you please mute yourself, Joel? One part is going to be questions, what is a software? 3:36 Not that generic, but question. So the second part will be code that you need to fix, and the third part is going to be a smaller pieces of code that you will write from Scratch. 3:46 Obviously the PC so code that you will write from scratch are not compatible with the assignments like exercise for exercise. 5. That you would see by the end of the class. 4:01 So 3 parts questions, code the to fix, evaluate force and then fix code to right from scratch. 4:17 So this is going to be the midterm. 4:29 Then we will all enjoy the the spring break, and then we will reconvene close to the end on the map. 4:32 All right. So questions so far. 4:44 Okay, so let me go into. 4:52 One topic that I don't like much to address, but unfortunately I have to. 4:58 So in the exercise, for we had more or less more than 10 students. 5:06 So I don't remember the exact number, but is a more than 10 students in 6 clusters who did very similar. 5:16 If not identical, assignments. I don't think a lot could mean that they are identical apart from the names of the variables. 5:27 So cheating all the changing, the names of the variables, or moving pieces. 5:42 One line up and on, while one line down. It's really a poor way of cheating. 5:51 So if you want to cheat cheat. Well, that's what I already said few times, and I'm repeating. 6:00 So I just want to let you know what is the process that we are following in a detective. 6:07 So from the syllabus cheating of any kind, including code sharing between students in individual assignments, as all the assignments that they have in this class, apart from the in-class assignment, is considered a cheating is plagiarism and may result in 0 6:15 grade. Normally, what I do I take the value for the assignment. 6:42 Let's say is 100% meaning is perfect. I divide that by the number of students involved in this sort of plgeries over the years, we kind of perfected the approach. 6:44 So let me explain how this has been done. 7:07 So we use a MOS. There is a tool that has been developed by Sanford University. 7:11 Most stands for a measure of software similarity, and it works with different languages, including python. 7:17 Moss measures the similarity between pace of files from a list we provide a list of assignments, so the example on the right it obviously not from these classes in Java I download. 7:27 I mean. I copied the plasm I copied by citing the source from a website in I mean from an article posted on Medium. 7:44 So we provide the whole list submissions, and then the tool is matching one-to-one, while the results have a decent, a good degree of reliability. 7:58 There could be miscalculations new to several factors. 8:19 I noticed that the most relevant one is the different lengths of the files. 8:21 So in the example on the right side, that you have in A, from the more similar to the less seen in era, you have the comparison between the 2 submissions. 8:35 So the first one means that there is a 96% similarity between those 2 when is different. 8:54 And it can happen like in this case. I consider the lowest number so I think it's fair again. 9:02 It's a tool may need to be adjusted. 9:14 So the way we do, we basically compare side by side the cases with the highest level of to do this, we use the details that moss is providing, and if it's something that is not clear, I open that 2 different windows in a plane editor and compared them side by 9:22 side it's a very time consuming approach, but unfortunately, we need to be fair in the grades that we provide. 9:50 Once we have that number, I develop a model in excel, that is, I mean, the one on the right is a screenshot of the model. So in the model we have the Max Square meaning, what is the school or other points that they would give if it was individually done, can be 100% meaning. It's a perfect solution can be 30%, whatever it is. So does this quarter that will be divided somehow than the number of students that involved in the case. The percentage of similarity. That is a combination of what moss is telling us. And our review after this stage, based on that, we calculate the penalty because there are some assignments that are intrinsically similar. Due to the the type of questions, so that I am in it. I added the the average similarity that is calculated from this table. So I transform the Pdf, all the HTML into a spreadsheet, and then I calculated the the average. Once I have the average, this will go here, and then the penalty is applied only if is higher than the average class. Similarity. Once they have dependent. It's just subtracting the score. The panelty, and I have the actual score. Some assignments may have more than one part. Each part is evaluated individually I have the weights for the different parts, and and and then I mean averaging a wedding. The tuna, I will get the. So that's basically the process. Now we take pleasure is very seriously, so we need to be fair. With all the students. If some of you are working together and someone else is struggling to do by himself or herself, then the second individual will be penalized, and that's not right. When you start losing points, because again open again, the penalty is points in that you are losing. So in of getting 100 you will get 59. So losing points at the very end may result in failing the course I'm not going to give any makeup assignment when you will lose points due to plagiarism, because I mean, it's your fault. You knew that, but you did it so again. We need to be fair. We need to play by the rules. There is nothing wrong again. It wasn't one of the first slides, and I want to repeat it. There is nothing wrong in using for a specific part of your code, not for the entire code. Obviously the sorts, like stack, overflow, or rather, but you need to side the except sorts. Page, URL, that you are using. If you don't, and someone else is using the same sorts. And that's someone else that is not citing the source as well. We cannot say if you use the same sorts, or you just walk together. So because what we see is that the code is identical at the certain level, and then for us there is plaggery. So, even if the process has been tested in previous semester, it could be wrong that, like any process, if it's wrong, step up and let us know, keeping in mind that this is not a negotiation, so before asking for a review make sure that you have a solid case. So we are spending time in detecting those cases. What the fairness, all of the class 100. Be respectful of the saveness of our time and the time, or the other students. So if a. Case will not be solved, we will send it to the Sevenths owner, Board. What is going to happen after that? It's out of our reach. Meaning. They may decide to expect the student, because there was the students, because there was a case of plagiarism, or reckon result not. You were wrong, keeping in mind that we have a trains of the old case and the trace is in the slides that I presented to you. So I really hope that this is clear. I want to stop the sharing for a moment, just to check. If you have any question on that again, all of these is not done to punish you in any way. But just to be fair, so there assignments are individual. You are supposed to do them by yourself, you can use outside the help from online resource. We do have a chat gpt now. So we don't know exactly how to deal with that from both the Su. And the faculty standpoint, keeping in mind that if you use a chat, Gp. Someone else can use it, and then there is the results may be very similar, and then you will be considered a in a cheating case just like that. But we cannot really detect if it's from Chat Gpt or not. But if others will do the same, then the outcome will be very similar, because yes, we will not generate exactly the same code, but will be very similar, and then the bladger Isma would be 2 keeping in mind that. Chad gpt is not processing your files, so there is no way that you can get something on a file that you provide to the. It's called the Nlm. Large language model, like Chat and Gpt down the road. We may develop a chat, gpt as a chutor. So I'm working on it to create those large language models with a given knowledge base that we can use for students. But I don't know if and when this is going to happen anyway. So that's basically it. Please be fair. Don Sheet. You are here to learn you are not here just to get a good grade, because there is a risk that you will not learn, and you will not get a good grade. So meaning you are wasting your time, your money, so let us help you as much as we can. If you have problems, check with us 200, and we can eventually have a keywords, not chat repeatedly, but human beings. Helping people in the class. But don't cheat, please. All right. So let me now go into the assignments. So the assignments had a certain level of complexity. So I was mentioning since the very beginning that the courts as a and that increasing level of complexity. So we will have a little bit of a break in the next assignment. So next assignment will be probably a little bit less complex than this one. But the trend is just like that, so there will be a growing level of a complexity if you already knew how to code the that could be good news for you, because now you will be more challenged than the initial ones. If not, don't underestimate the complexity meaning. Don't leave the time to do the assignment at the very last minute. But allocated the proper time for the assignment. One note on a technical aspects I'm using a new generation of Max with the apple Cdicon, meaning the architecture of the computer is not based on Intel, but is based on an Amd micro Profile. If you have a a python version. That was developed for the Intel may not work properly with the the Amd. The new apple Silicon Micro-processors. So go to the python, download the website and download the version. That is not the Intel version I'm telling it, because all the scripts that they had we're either getting warnings or not worrying at all. I tried 1 million different things so, and then I stepped into a case on the overflow. Saying, the problem it's a apple city. I could I try to ask to chat. Gpt, didn't work. The reason why didn't work is because the chat Gpt is trained on data up to 2,021 apple silicon was introduced. The after that meaning it has no information from 2,021 to today, including this one. But anyway, so be sure if you have a Mac latest generation, you use the proper version of a pipe. For the assignment, so the assignment was about just processing text. The Csv files Csv files related to let me go to the files. Related by the way, does the model that I was mentioning, and those are the formulas. The I'm using. So the files. Where are this one? And. And is other. So the main problem with those 2 files was the fact that in one you have the duration in the other. You have not in one. You have a user type that can be subscriber or a customer in the newest version on top of not having the duration. You don't have a user type, but you have a member casual and is an advisor either member or casual. So those are the 2 main differences. So in order to do that, you can use either unless or list. Will review with you body options. So this option is using lists. When you use a list, you actually use lists, lists meaning each record that you will read will be one element or the list. And then, if you want to access one element of one record, you need to address. You need to address, can you please mute yourself? You need to address the list into the list. Yeah. So like in this case. All right. So I'm importing the Csv library that I will use for reading the file, and I importing this daytime. That will help me. Calculating the during. So that's basically the function that will do. The printer. I did quite a lot of initializations, and then I started the I mean, the file is taking a list with duration. Start time and station, and if member or casual is a list of lists, and I will pass the first file first and then the second. So initializing elements start in the loophole, checking. If his member, I will add one if it's casual, I will add to the counter occasional. Then, if is none of those meaning is a blank, I will skip and continue. Then I'm checking. If the name now I'm for use on the name of the stations, and calculate checking. If the name is in already. Already in the dictionary, if it's yes, I will add one at the counter. We did the same thing when we calculated the occurrence of the names in a in class exercise. I'm not going to spend time on it, because we already did it. Then same thing for the end. So this is for the start. Insertion and the station, and then, when I finish, I will print the results. So that's a to tell the format for the data. That is a year, month, day, hour, minute seconds, so open the file in reading. This is not necessary, but I edit. Then I created the an empty list that will host the content of the first file. In this case, and. Then I started at the process of the file, and then I'm checking. If is a source subscriber, or a customer, if not, I will continue, and I, calculating a start, time and time using the function daytime that I was mentioning here, then the difference. Hi! Round it, just to have a a nicer view of it. And then I added the the element to the list that I will pass for processing, and then will continue as well. And I'm sorry. This was the second one. Stay on the first one the first one is the one with subscriber customers, and now members and casual meaning and I need to replace subscriber with member and customer with cashew so before appending to the file it could list the records on the file. I had to change. What is a inside of it? Same thing for the second file without this last portion, because it's not customer. I didn't use either cases. The duration, so the first one headed it Duration. But I didn't use it just to have the same process. I could have. Then I'm calling. Excuse me. The function for the first file for the second file closing not required. But I did it. The 2 files and annual processing, if I read it. That's the result. First, file, second file, and so on. So does the version with the list. It's about 120 lines. Let's see, we would pan. This is going to be shorter. It's about 80. So we are saving 40 lines again. It's not an exact count, because there are comments. So there are blank lines, but pretty much they both use the same number of blanks and comments, meaning the difference is compatible, so the only thing that that they imported here was a pandas. So the print details. It's very different from the previous one. The previous one was kind of a complex function, with a lot of initialization. With a lot of loops when you have pandas. Most of the time you will have way, much less loops. So with pandas you can do operations within the structure without doing the looph. If you want to do an average and you have a list, you need to calculate what is in it. 299 do the summation, and then divide by the number of elements with pandas. You just call the average function that is beating in a pandas from the foundation in. There's a odd number. So I calculated the I mean in this case, what I'm passing is not the entire file, but I'm passing only for each one of the 2 files I'm passing the second. So duration. The most popular star station, the most popular, the end station, the number of casual users, and the number of members so just that I'm not passing the old file, but I'm passing only the results. And then I calculating the I mean from the second, so I could have used just a second. So, but I wanted a nice, a presentation with how? What? Submits and seconds. 110, and that's why I did what they did. And in the main program, opening the file, read it into a panda structure, initializing a list that will contain the value that will be processed by the function. I could have used the one single data frame. I use the separate data frames just for a class. So mmm, start with this format, so in a pandas you have a 2 day time transforming one set of values into a daytime. But you need to specify the format. I could have done like I did here, using. Bari ebola for the format of the data. But I I didn't, but that's I mean it's not going to save much, because it's basically just 2 times. And I could have say a few characters, probably same thing for the end. Duration is the difference. And then I appended the the value into the empty spring list. Most frequent station. That's the beauty of Pandas. Instead of doing a loop, you just do count. And Max, and you have it. Then added the to the list. Same thing for the end station. Now on the first file. I want to count the number of members, but the members are in a column that is called user type, and not like the other one. Members, subscribers, and the name is not members, but is a subscribe member, but is a subscriber. So 200. I didn't do any transformation of the name, but I just counted the different thing using it in the same way. So this is for a subscriber. This is for customer than that pretty much similar thing for the second file, with the only difference being, instead of looking for the column user type. I'm looking for the column member. Casual. Instead of introducing subscribers and counting member, instead of counting a customer counting a cashual so, and then the 2 the lists to yeah, to the list, and then calling the function and process. That will process the results. And then and all the file processing, running. And you will have a the same exception. Results like check them. So! Again. It had the some level of complexity. Let me stop sharing and checking. If you have any question. All right. I just want to get. Feedback from Villa. Now, what do you think? So the solution that you presented in the second half. It's just now. It seems much more obvious to me than it did in the past. Because when you demonstrate how to write the date time, it's it's very, it's very clear. The one thing that because I was trying to work with pandas, the one thing that was unclear to me that I missed in my code was the Div. MoD function that you use? Could you explain actually, a little bit what that does? Thank you. Yeah. Sure. Yep. Okay. So you mean that? Let me start sharing again. I think it was in the the seventh line up top. In the beginning. Okay. Or in the eighteenth line. Yeah. Okay, yeah, I mean, that's something that that you don't need to do, meaning just a way to better represent the value. So if you don't do any of those, if you look at here when I run it, the version where the. With the with the list. I don't have hours, minutes, and seconds. I just have minutes. So what you could do. You have the number of cycles. So with Python. You have that function. This this mode that is dividing the value in this case, the number of seconds by 3,006 are calculating and it's placing in those 2 variables. You can do in a more basic way. So you can take the second. So divide the seconds by 3,600, and get the I mean, you understand? By 60, and you have minutes by 60, and you have minutes by 3,600, and you had the hours and then you can round it, and you have it so that's just a more compact way for doing it but it's not required. I mean the logic of the program wouldn't change at all. If you look at the the way I can, calculated the here. It was a this one. It's basically is divided by 60 and rounded to the second value. And that's the number of many. So you can do the same to calculate the minutes here right, and then divide by 3, 3, 3,600 to get the Howards. So again. It's a mine or a detail. The reason why, each time I'm picking on villain is working with me with us on the scheduling and the engine of the school. Since a few months, and because in the scheduling I developed a python program by python script to I mean, simplify somehow the whole process. I really want villain to be able to take over on a death and use it in the semester to come, and then it will be useful for many other things that you may want to do, or we may want. We may like him to do so just which like, why, I'm picking on. I'm sorry if I'm doing that, but I really want to be sure that from your standpoint you don't have a questions. No, I appreciate it. I'm more than happy to comment anytime you like. Okay, yeah, I mean, most of the time the questions that villain may have are questions that someone else may have. So I strongly encourage you all to ask me. Thinks, if something seems to be not particularly clear all right, so let me continue sharing and let me go now into the the slides. So we will cover a few things, and we will cover a little bit of introduction to Aa machine learning, and then we will talk about software development. Then we will do the in-class assignment. I really want to give you as much time as possible. Some of you ask me to give more time for practicing, and I would definitely love to do that all right. So. We do have a courts on AI machine learning for systems and enterprises that I created a couple of years ago, and I ran it each full with the proficiency in Pied on as a pre-review is a quotes that is a sort of an advanced version of another course that I created, that is, M. 6, 23. There is a knowledge, this callery, but that courts is not using coding, but it's just using a tool with a graphical user interface. And it's not particularly onto AI, but it's more on a daily science and algorithms that are used in a machine learning. But it's not specifically on machine learning. So what is artificial intelligence? There is no artificial intelligence, because there is no definition of a national intelligence. We cannot define national intelligence. We cannot really talk about their artificial version of it. So we don't understand how human mind is currently working, not knowing it. We, the only thing that would that we could do is consider it as a sort of black box, and place questions and see if they answers are similar to you right now. There is no generalized artificial intelligence so meaning. There is no system, including chat. Gpt. For sure. They can really be considered anarchifician intelligence. The heat sorry ourificial intelligence, it's a long one, right? So because there is no definition. People consider in the years artificial intelligence. Any system that somehow had some level. Of human behavior. So if you consider a the first robots have been developed fully mechanical, so other localis was one of the inventors related to not my own. By the way, and people told that was. Reproduction of a human being. So obviously it was a highly overrated going fast forward during World War 2. Alan Tuding was the one who created systems for a counter balance. The encryption for decreasing basically the encrypted messages from the Nazis, creating a system that is considered one of the first. On machine intelligence in a broad sense, he also created what we still call the to the the tutoring tests that I will explain in a minute. So I started working in artificial intelligence in 1986. It was in an arrangement company within. An information technology group. And I was working on what we're called the expert system sort of knowledge-based systems that are what we now call the symbolic part of artificial intelligence. At that time there was quite a lot of expectations. Big hikes on activation intelligence. But we failed. We failed for several reasons, let's say 3 main reasons. One was the lack of a computing power. So I mean, my expert systems run on either a computer called Excel Star. That was the one Steve Jobs used as a aspiration for the Max, and the micro operating system, and then a scale down version was running on a Mac, so that time I had a meeting that was the one that you probably saw in pictures. Square, the thing with the teeny, tiny monitor with flowoppy disk in it, so that's what we had. We use the languages so that we are either a lisp. That was a language or a developing expert system. So then and now defined the company, name the digital equipment that pulls. Then both by compar, that was bought by HP. And then disappeared. Initial version of it. They created a language that was called the Ops. 5. That was a rule-based language, and quite powerful, that not sure exactly why, but I could run on Max. So I developed this expert system that was in a wealth management 200 and that was for I mean, based on symbolic of what we now call a symbolic approach. So one problem was the lack of capacity, of I mean computing power. The second was the lack of power in terms of languages. So with Python you can call a library, and then you can use an algorithm directly from the library. 100 back in time we had to write the algorithm from scratch each time. So you can create your own libraries but at the very end it's going to physically copy that version into your code and do it again. So. It was not easy. The third element is the lack of data. So now everything is digitally, and we can leverage on the data to create models. So we couldn't do at that. So now we have probably I don't know 80% or all the information in the world that are in the digital format. Oh, no, they did, because he has some legacy, but he's close to that. At the time it was probably 8%. So meaning everything that is on machine learning, that is, hey, based on data. We we couldn't done it because we didn't have a the data to work. So that's and then the expectations we are. I mean we failed to deliver for those reasons. But we failed. And then what happened? I? What was called the winter of a young. So nothing happened that pretty much 20 years, 15 years! And then with the more powerful computers, with the more powerful languages, with more data than that, we have what we have now and then, we think that we have what we have now and then, we think that with the data, we can do that is not exactly true. So I mentioned that the tuling tests, the tuling tests is pretty much like that. You have a wall on one side of the wall. You have a human being, a questioner, a place in questions, on the other side us, something that can be another human being. Or can be a machine. The human is placing questions to whatever is on the other side. And it's placing questions over all possible kinds. If the answers would be satisfactory, and on the other side there is a a computer. Then the computer, the system passed the touring test. Obviously. From being rigorous, test, because it really depends on who is placing the questions. So if the questions are very basic, then answers can be easy. So I ask the chat. Gpt, if it could pass the tuding test, and the answer was very appropriate. By the way, it depends. Who is asking me questions, and that's exactly right. So right now there is no computer, no system that can pass a generalized tool test. So when we talk about the AI, we are basically in the all the automation, not everything that is automation is artificial intelligence. But what we do in artificial intelligence is automate somehow processes. So some of the automation is not particularly intelligent. So consider a tidy up in a industrial. Environment is just working on the boats. It's not doing anything else. Is a robot, is efficient, but it's not intelligent. So there could be robots that are not intelligence. Consider your that is cleaning, you know. Not very intent way. The floor. So those are robots with a little bit of intelligence, but not much. 250, within artificial intelligence. You can have systems, the focus on autonomy. Or now breathing down our TV show intelligence. There are 2 types overificial intelligence that is theificial intelligence that is based on symbol symbols can be rules like I was mentioning of the expert systems. It can be taxonomy. Taxonomies are, are a structure to represent to semantically represent a domain. Think about the animal kingdom. So you have mammals, and then you can dream down and have all the different animals. So that's an example of taxonomy. You can have taxonomies for pretty much everything with a semantic complexity, so you can have a taxonomy for an industry. 2 for a class of companies for one company. Things like that. So that's the a symbolic approach. The second approach that is based on a representation on knowledge that is kind of saying we were born with rules that are embedded into our brain. And we follow the rules. We learn. But there is a sort of a sorting board that we follow. So that's one approach. The second approach is what in the past that was considered a Mp. Dcism that is based on I know nothing. When I was born I knew nothing. When I was born. Then with experience, I learned, and I created my own knowledge. So that's the data driven approach. And that's the machine learning approach. So you start from the data and you get your knowledge from the data data. Science is helping both, because in ourificial intelligence you may have models 300 based on data that you want to. I mean the data that you want to clean, that you want to aggregate things like that, and even more on a machine. So data. Science provides a elements to artificial intelligence in general, but in particular in machine learning. So we mentioned the one of the 3 reasons why we now are receivingly talking about machine learning, artificial intelligence, and we couldn't do it in Mid eightys, because we have more data. So we have more data meaning, we can use it as a form of experience. And when you do, machine learning, machine learning is basically a model with 2 components, one, the algorithm and one the data one without the other. Wouldn't work. So in machine learning, that is great in terms of algorithm, what? But with the poor data, it getting poor performances in London, in China they have quite a lot of cameras, analyzing faces. So face recognition in those countries is working way much better than in Norway. We are the rules for privacy are more strict, so the algorithm are the same. But the Norwegian systems are appropriating results that not as good as the results in China and the results in London. So again, the same algorithm different data. If you consider we mentioned Chat Gpt that is based on a process we have the key algorithm is called transformats. The algorithm is based on artificial neural networks. There is a a way to multiplies. Series of Mavericks is one of the other. When you do the multiplication, you basically use weights in at each stage when you pass from, I mean one stage to the next one. Those are weights are called parameters, 2 to the next one. Those weights are called parameters, doing a similarity with the human mind. Those parameters would be similar to the new. So right now, Chat Gpt is based on a version of of transformats that is called Gpt. 2, while it is already available. A Gpt. Tree and is going to be available. Gpt. For a shortly Gpt. 2 has a half a 1 billion parameters. Gpt. 3 as a wide 1.5 1.6 billion parameters. The human brain as close to 2 billion neurons. So in terms of complexity, they can be seen in in terms of efficiency is a completely different story. So the human brain is based on knowledge. The knowledge is created through text in a broad sense, meaning what we hear, what we read, but is also based on a visual us, because I mean, we see things, and for what we see, we create knowledge, we have the ability to memorize, a sound sound we generate additional knowledge, and then smell the touch. So all of those are generating the knowledge. Then we have the ability to develop a I mean to present the results as a conversation. So we have a language element chart, Gpt, and only text, and only the language model. 1: So is more limited, is working on that getting the text, analyzing the text. 1: The detecting. 1: Then taking your question and matching your question with the pattern of patterns that are the closest, and then adding a layer of language modeling to present the results in a conversational way. 1: So chat gpt, it's basically like Google. 1: But with the ability to compile the results in sentences and present them as a compensation, so, and he's considering also the elements. 1: So each time you create a conversation is consisting in the pattern of your requests. 1: The other elements that you provide in previous requests. In the same session. 1: I mean, it's an interesting model like, keep in mind that to create it open. 1: AI span, 1.5 million dollar in energy energy at that was the equivalent of running 1,200 or a number close to that medium size. 1: Cds in the Us. Because of it's a lot of processing. 1: So we cannot create a chat. Gpt at Stevens. 1: So that's for sure, unless something will change in a dramatic. 1: So the 2 that I was mentioning before we definitely underperform compared to those giant modules. Anyway. 1: So some of the directions of AI. Natural language processing. 1: So I mentioned the chat. Gpt is a good example. 1: A robotics is becoming bigger and bigger. Robotics is not just playing robotics, but is also augmented. 1: The capabilities for humans that can be a physical or can be launched in logical would be like Gpt. 1: Physical like can be. There are devices that are physical, like can be. There are devices that are that some people in warehouses are wearing. Uhhhhhhhhhhh! 1: H! Ahhhhhhhhhh! Ahhhhhhhhhh! Ahhhhhhhhh! 1: As an example. You can think of what the defense industry can do. 1: Just think about. I don't mind. So we Gadnara is one of the, I would say, is the leader in a market analysis, in technologies and. 1: Among other things, is doing what is called the hype cycle. 1: So the hype, cycle, it's basically representation of technologies in a multi-dimensional view. 1: So you have when you launch a technology, that technology is generating a lot of hype. 1: So they hype is growing. I think, about Chat gpt in one week. 1: Reached the 1 million users. So there is a growing expectation. 1: Then after that, either. Then after that either will will fall into the I mean the expectations will be not met in the proper way, and we will go down, or can become prime time, and being used as a regular prime. 1: Time, and being used as a a regular technology like we use cellular phones. 1: Or so you have a innovation. Take over expectation this illusion. Mentor. 1: Eventually an adventure or productivity, so you can place in this chart the technologies that are running right now. 1: There is a a sort of life cycle, and there is a time factor. 1: So when you have technologies that are tagged as a more than 10 years, that means who knows? 1: In technology, 10 years, it's a, it's a very long time. 1: Everything can happen in technology in 10 years. So you have a quantum computing. 1: You have a generalized artificial intelligence autonomous vehicles that are, I mean, seen, as may happen, may not but it's kind of interesting. 1: One thing that is growing this edge. AI. That is an interesting application. 1: So you think for a second that when you have, I don't know your Alexa box. 1: The intelligence is not in the box, but is in the server, so when you have quite a lot of chat, Gpt, you place a question, but the answer is not in your computer, but is in Dave Server. 1: So when you have molded that are very big, very complex, you cannot run them on your computer. 1: And you rely on a server with intelligence. But this is kind of generating a delay that is due to the communication. 1: So because we have now more computing power at our disposal in our hands, like the cellular phones, like the computer I'm using for my zoom corner. 1: We can have more of our intelligence. The peripherical level that can be at the very end of the chain meaning on our computer, on our phone, or in the points of agregation. 1: But that's something that already happened in the entertainment industry. 1: Netflix, they have a content distribution network that is quite interesting. 1: Meaning not everything. 100 and streaming from God only knows a server far, far away most of the time they have a equivalent of warehouses where content, that is more used at that level when you have a very large condominium, so they may have their own netflix 1: server, I mean there should be bigger and there are some other conditions. 1: But that's the reality in artificial intelligence. 1: You can do something similar, so you can have some of the technology. 1: That is not all in one place, but kind of a distributed in the old shape. 1: Obviously you don't need to have a system that is oversighing the distribution of intelligence. 1: But that's one technology, that what mentioned. But that's another view. 1: That's emerging technical so I strongly encourage you to go through the slides and see what is there the chat on the right is kind of interesting. 1: Everybody is talking about the AI machine learning. So that's the number of companies public companies citing, mentioning AI machine learning in the earning code. 1: So from 0 to 10 years ago to thousands right now pretty much. 1: Everybody is talking about. I will use a yeah for something I'm already using. 1: AI. Another interesting bottom, right? How AI is perceived. 1: So right now we are still in the positive, less apocalyptic of AI. 1: AI can help, hey? I is good. 1: Who knows? I mean that there are controversial opinions. 1: So back in time there was a, you know mask that was against it, and then he invested in open AI, creating a chat Gpt, but that's another story. 1: So applications. Amazon is using some forms of a machine learning machine learning can work side by side with both software engineer and system engineering software engineering. 1: It's kind of the system engineering for computer science, meaning when you have a cell phone level or complexity, you can not just code. 1: You need to consider the old thing. You need to consider the system where your piece of software is running. 1: Going back to Chat Gpt, I mentioned that transformers as the core of the but in reality is a process. 1: So the process is pretty much like they collected a notion of pretty much everything that was available as open source. 1: Then you had the humans going through a sample of it and doing the cleaning because there could be some inappropriate content that you want to eliminate and then it's also doing the the humans in the Loop are also doing tagging of the content in a I mean sampling and 1: tagging. Then that samples tagging means you probably did the about image recognition. 1: So you have a 1 million images. And then you said, Okay, this is a cat. 1: This is a dog. This is a a rhino, and so on. 1: So you need to tag things at the very beginning to have the algorithm using the criteria that is extracting from your behavior to classify future elements that you are passing to the system. 1: So the process has a again, a starting point that is human on a subset of the system, tagging it for content. 1: Then they have a system that is taking another portion of the data, and using the criteria is a tagging in automatic way. 1: The humans go back and assign a score to that with the score. 1: They have another piece of system that is based on recurrent neural network. 1: That is a looking at the results, and then it's working on a larger piece of data and is a refining the results of the first round based on the I mean to maximize the score assigned by the humans at the very end. 1: This system has been a trained to classify to tag, and then to classify, and then it will work on the entire data set. 1: So at that point you have what I initially called the patterns, meaning those tagging of elements. 1: 2, but again there is a human in the loop. When you have everything that is tagged, then you apply the transformats, match the requests with the partners, but only after the human. 1: Did quite a lot of it. And this is designing a system. 1: So it's in between software engineering and system engineering. So you will check this slides with details. 1: Some of the applications of a AI in a broad sense, so speech, recognition, that we know that is working up to a certain point in banking industry, monitoring fruits in military industry we have drones. 1: We have a fully unmanned drones, but that's another story. 1: That we can probably considering is 745. 1: I wish I had a story in another moment. So that's basically it. 1: Let me check if there are questions, and if not, I will move to the next set of slides. 1: I will go relatively fast on them. You will have those lights, anyway, and we have eventually courses on quite a lot of courses on that topic. 1: So the topic is a software development and software engineering. So few classes ago, we mentioned that software started as a an integral part of the hardware. 1: So when Alan Turing was used to right programs, it was actually moving switches in a board. 1: So it was a flexible. But the software was actually hardware. 1: In a sense, so later on they became a separate discipline with some specific characteristics. 1: So when you develop software, you are developing something that by definition can be changed when you develop a breed, the bridges there. So you can do minimal changes. 1: But not much in software. When you develop a system, you can change it. 1: You can adapt, you can make it evolve, and that's a major difference. 1: So one of the issues. This slide is a slide that is probably the most commonly used slide in software engineering. 1: I saw it the first time, probably 30 years ago, and since seeing this night around, and I kept it for historical reasons, it kind of give the sense of how much it can be lost in translation, when the information is passed between the different actors in the software, developed so you have the client, you had 1: the formalitation on the needs in a contract. You have the specs you have. 1: The developers writing the code you have the debuggers fixing things in a certain way. 1: So each one, it's adding, Can you please mute yourself? 1: Hello, okay. So each time at each step you are losing something. 1: So, and that's probably the main reason why so many software projects failed when you develop software, you have a portion. 1: That is the actual development, a portion of the disturbance and a portion, and it is meant. 1: Maintenance is a sort of lifecycle, the sooner you will detect the errors, the lower will be the cost to fix it. 1: So again! 1: Life, cycle. Reality is some software never die, though we are seen using software growth 50 years ago in a cobola. 1: That is running in the government that is running in banks, and I mean maintaining it is becoming more and more complex. 1: Because people retired, people died, and no one really knows anymore. 1: I didn't do how to fix it. There are some cases they can eventually share with you. 1: So there is a standard for the software life cycle and developed by the iteration. 1: The typical approach that was used at the very beginning was the waterfall approach. 1: That is what we normally use when we do an engineering project building or a car or a bridge. 1: So we define the requirements. We design the system based on the requirements. 1: We start the coding, we do the testing. We pass the system to the client, and we start them. 1: Maintenance. So that's basically what is called the waterfall approach. 1: One. The advantages are pretty clear. You can plan in the proper way. 1: You can allocate resources in the proper way. You can do the requirements. 1: Definition in the bad way possible, but everything is frozen in time, meaning when you start the process you can never stop it, meaning, if at a certain point utilizes something is wrong. 1: Your loss. So there is a lack of flexibility. That's a single most important issue issue. 1: It's so important that only 16% of the developments in 1,995 that caused the peak of this approach worth success. 1: So then some ways to address the issue. So doing more parametric approach to add flexibility. 1: Do a little bit of prototyping by the very end. 1: There was no structure, the approach of it. And that's basically when people start thinking about software engineering. 1: So we need to have an engineering approach to the software development computer. 1: Science is essential to software engineering as a physics is essential for mechanical or electrical engineering. 1: So that's a way to say it. They are different. I mean. 1: Trust me. A lot of people still like confusing computer science with software engineering is, I mean, those people are in industry and in academy. 1: We do have people in our university not really understanding what one is doing compared to the other one. 1: We are hiring a new faculty for software engineering at the school of systems and Enterprises. 1: One of the questions that they always ask, What are the difference for you between computer science and software engineering? 1: And I generally hear all kinds of answers. So I generally see software. 1: I was a a member of the software engineer, partner, member of the Software Engineering Institute for a couple of years. 1: A 1 million years ago. The way I see software engineering is like system engineering for computers. 1: Science. So you need computer science. But computer science is coding is a optimizing the performance of your code. 1: Software engineering is thinking about your code as part of a mobile complex problem or environment and dealing with all the components but most of them all the time. 1: When you see an add, they are looking for a developer and they are asking for a software engineer or a vice versa. 1: So there is not much clarity in some people, but the difference is pretty clear. Actually. 1: So this software engineering institute that gives a based is hosted by the company man on university in Pittsburgh, created the capability maturity model. Cmm. 1: M that is a defining the phases of the development and then monitor it. 1: But there are 5 levels from the initial level. There is a dock codak. 1: There is no real engineering approach. Up to the level 5, where everything is reasonable and all the processes and data are fully traceable. 1: There are very few, if none companies in Level 5, but the idea is to monitor each development, each change and a trace it, and make it more robust and independent by the other processes. 1: But in order to do that, you really need to, those are more details on the different levels, you really need to have an approach of continuous monitoring like the brother big system, meaning there is an overhead that is really significant that can be justified when you have a very large project but would not be 1: justified when you have smaller projects. So that's why people started thinking about agile middle. 1: The agile processor. It's really rooted in a prototyping, interacting, revising intermediate results, refining the results till the launch of the project. 1: Is definitely not as rigorous as a Cmm. 1: But is a kind of trade-off, so I give you guidelines. 1: But they're not measuring every step that you are to. 1: You are taking so interaction collaboration, responding to changes. 1: So all of those are essential portion of the agile software development. 1: There are several methods labeled as a Gile scram is one of the most popular and is based on the same overall approach that is stated by the manifesto. 1: So I would stop here. I would probably go all the way to the end. 1: Some practical principles. User as much open source software as possible is a opener is cost effective. 1: Obviously there is no number. You can call it. Something is not working, meaning. 1: You need to rely on the community, but on some urban sales the community is huge. 1: So think about python. You have a problem in python. 1: You go to stack, overflow, or other, and you will get an answer in. 1: So on the other side, the you want to create your own version or one function. 1: You just take the function. Most likely the function is written in 5, on, and you can change the code to make the function work exactly as you want if you are the commercial software. 1: You cannot do that. The only thing you can do is a file, a request for change to the provider to the end dollars, and then the vendor. 1: Will take your requests, will place the request in a queue, and then eventually will apply in one of the following releases, based on what other are asking, I, Don is a the most popular language, Hmm. 1: Is open, is portable. So we know that it's portable up to a certain point. 1: I was mentioning the issues I had moving from Intel to Amd. 1: But it's portable, anyway. 1: The libraries, the libraries in Python is what is really making the difference. 1: Using this standard. Do not reinvent the wheel. 1: Use, a standard modules for interfaces. 1: Again, use libraries? Probably not pandas, not Python. 1: But there are other languages for that develop the graphical user interface as a separate from the back end portion. There are different. 1: They follow different life. Cycles, and you want to be sure that people developing one as different skills as people on the other side. 1: So that's basically it. 1: I just want to open, but not run. Now, this presentation. 1: Yeah, you will have it in your canvas. 1: 2. Is it one of the most famous cases of a failure in software development? 1: Is related to FBI developing the virtual case file. 1: So before this project, the Api didn't have a shared file system case file system meaning, if you have a case, you need to grow to the person who was handling the case and asked the person because there was no way to share information. 1: You can imagine how bad this could have been. 1: So they decided to create this common repository of cases, and they started investing several 1 million dollar, of cases. And they started investing 7 million dollar, then became 1 billion dollar. 1: And then it failed without the reason is pretty much what was in the slide with the 3 in a different positions, in different statuses, was a kind of lost in translation. 1: So there was no one taking the full responsibility of the system. 1: There was a not enough exchange of information between the different actors. 1: And let's basically what's generated. The failure. So go through the slides. 1: 200 and the failure so go through the slides. This lights are at the original slides in from 2,005 from a I triple E spectrum, and I mean that it's kind of interesting to see what's going on. 1: There are couple of other links that they will post in a moment on canvas. 1: One is some details on the capability, maturity, model. 1: If you're interested in the other, will be on some cases of, and it can be interesting to watch it just few minutes. 1: Okay, so it's about 80'clock. So it's I really want you to work for 1520 min at least. 1: On an in-class exit size. That will be something like this. 1: So you have a you want to build a system to analyze passwords, and to either take the password as a valid one or reject it. 1: So those are the conditions for checking the password. 1: At least one letter between all those small letters, one number one of those one letter as capital letters, one. 1: Those 3 special characters. Minimum, length, 6. Maximum 12. 1: And you want to create a function doing it. So you want to create a function that will take a list of passwords, analyze them and retire. 1: Or a printer. The password that passed the the criteria that was stated from one to 6. 1: So the input will be something like that, and the output will be the following password has been accepted, that the only one of those satisfying all the great idea is the first one, and that's what you will. 1: Okay. So I will stop sharing. At this point. I will create breakout rooms, and then I will post everything. 1: So we have a 9 breakout rooms, each 1, 3, 4 participants. 1: I will create the rooms I'm opening all the rooms. 1: So you have about 1520 min to work on it. 1: I would be here anyway. In in the meantime I will post the recording and make sure that the material will be available to you. 1: So see you in 1520 min. I'm opening the rooms. 1: Please join them. 1: All right. So all the rooms are closed. I'm assuming the recording questions. 1: Issues volunteers. 1: All right, so let me share the screen and let me go. 1: Here to present. 1: Then class assignment, so again, obviously is more structured, is more commented that you could have done in 15 min. 1: So. But that's the very having time it should be done. 1: So you have the definition, a little bit of the requirements. I'm using a function that is called stringa, that I will show you in a moment how this can work. 1: The function validating the password is taking one password at the time. 1: So checking, if is outside of the range in terms of length. 1: If this is the case, we and then I'm using within the function a list with the 4 remaining conditions. 1: So let me go back here. 1: So, if you don't consider the length, there are 4 conditions. 1: So those 4 conditions. In the meantime, let me copy this because I wouldn't use it in a moment. 1: So this list that will contain the check of those 4 conditions. 1: Are initialized an Eas Eastern, the the values are initialized to N. 1: Then, if the element? 1: So what I'm doing is transforming the single word into a list. 1: When you transform a string into a list, the elements of the list will be the word, the characters in the world. 1: So I'm checking. If this character is in the list of a small letter, so that I define the in the main program 100. 1: If this condition is past, is changing the first check. 1: Here from N. To y. Then the second check is on. 1: If so, this was the first. Now the second is the numbers checking. 1: If if if the element is there, then I will change the sake on the to y third condition, the capital letters. 1: If the element is there, we will change into why again. 1: And if the element is in the special character, we change to Y again, and then it will. 1: If An is in the okay list, meaning, there is one condition that is not right. 1: We will exit the loop, and we return, and otherwise we don't know why, so we'll continue till either will check all the characters and or will get an, and would go out first. 1: So now the list can be created as a plain list like this one. 1: That's a legitimate way of doing it or using the function that it was mentioning this a string you can get. 1: The list of small letters, list, screen, Ascii lowercase, same thing for the capital uppercase, and the number. 1: So either way you will have in those 3 variables the list of lower cases, upper cases and strings, and a special character is just 3 doesn't want to call the the function. 1: So then I'm printing. Enter the list of passwords, and then we will loop into the at least of passwords, and that will I mean that's a way to do the loop. 1: You can write the loop in a more explicit way, but it will be the same, and then. 1: The following, I mean, when you have a you do the loop in a list of password. So I mean over here. 1: I'm splitting. I'm sorry I'm splitting the list in an individual password, keeping in mind that the password. 1: Sorry about that are separated by comma. So over here I'm creating the list of passwords. 1: Then I'm looping into the into the passwords, passing it to the function, and then, if there, the return is why I will print the password. 1: That is why I will print the password. That is okay. 1: Otherwise I will. Oh, if I, if I run it! 1: Passing from the value. 1: And then, and that's what I have. 1: Okay. So before we go to the assignment, I want to spend another a few minutes in introducing tools that you may need for the as the assignment. 1: So one, it's pretty straightforward is that? 1: Calculating the correlation, correlation is one of the single, most important part of the pre-processing of a file so quoted means. 1: You analyze the variables in your data set, and you see if they are correlated, meaning either one is growing, the other is growing. 1: And that's the positive correlation one is growing, the other is decreasing. Over. 1: So that's some. And this is the negative correlation. 1: If you want to do this analysis because you will get insights from that. 1: So if you see that the 3 I don't know the more experience you have, the more you are paid. 1: That's an insight that you have on the data. 1: Or there are other cases. But again, that is probably the single most important part of the pre-processing that you do on the data. 1: So I'm using pandas for printing a seaborna to print it in a more nicer way. 1: This script was not working on the python version. 1: That was designed for Intel, so I had to change the python interpreter to make it run the one that you have on a on camp, that you will have on campus is the latest version, it's pretty much the same as the one before the only thing that changed in my case 1: is the interpretter so importing those functions? 1: Renamed the file into a pandas data structure. 1: Calculating the correlation matrix and representing it as a a beach. 1: Ual, and then saving into a file. If I run it. 1: The file is relatively big. That's what you have. 1: So you have. I don't know. The more blue is more positive correlation, the more greyish is more negative correlation. 1: So you have more salary when you are older. 1: As a Nfl player, and when you have more experience, so more experience and pro bowler are the 2 with the highest level of correlation. 1: In this case. So and that's one. The second one. 1: That they want to show to you is this explorer explorer it's a very powerful script, not because of the script itself, but because all these Y data profiling library that is doing quite a lot of things. 1: So if I ran it. 1: It will go into the file generating a bunch of subdrivals. 1: So while it is generating, let me go. 1: Let me go! Here! 1: Now it's finished. So that is being generated. Now I'm opening it so is an HTML. 1: Reporter with the everything you want to know about the file. 1: So you have an overview. You have a number of variables. 1: Observation, missing call missing cells and things like that. 1: 32. You have a menu. You can jump directly to correlation. 1: For example, and you have the correlation. You can jump on missing values. 1: And eventually you can have a dates or a different representation. 1: And on the variables you have the distribution that in some cases can be useful like in this one. 1: You have an analysis of this on the distribution, this Q. 1: And a can give you insights. So in this case the age it's a around 24, and there are very few that are more than 30, and that's makes sense. 1: So anyway, that's basically the 2 scripts that I will share in in canvas in a moment. 1: Let me go now back to the assignment. 1: So the assignment is on that. 1: Think I have it in a better version. 1: Yeah, okay, so there are 2 paths, one general questions. 1: This will be pretty much similar in terms of a structure to what you will get in the mid term. 1: So what is a jailed software? What is waterfall approach? 1: Why is not successful? The fine machine learning. And then you had the code. 1: So the coding is based on this. 1: Data set here, that is, on Covid. The data set has been downloaded from the Cdc website is not the latest. 1: Eventually you can download the latest and do the same work on a newer version, if you like, and so if you go here, you see that there are age groups. 1: There are age groups, and you have conditions, and you have a that's so, you basically have for all the age groups. 1: What are the number of deaths for each condition? 1: So basically, what you want to do is to create a graph. 2 graphs. 1: One is a Bach chart, and one is a pie chart. 1: I mean, those are not the real example. It's from a completely different excessive but you want to have something similar you want to have a butch. 1: You want to have a pie chart eventually, in one single representation, we saw an example last week when I introduced. 1: So you want to create a list of counters so the very end what you want to do using a function getting the different parts you want to. 1: Calculate the I mean a Comorbidd means. 1: All of those are cases of covid. 1: 19. But most of the people Guy, just because of Covid, but because we are a mobility, is a leading to fatal outcome. 1: So we want to analyze the commodity in the Co-OP condition. 1: So I mean causing the deaths so, and in particular, we want to analyze the commodities in the population of less than 35 years of age. 1: So meaning 0, 24 and 2534. 1: So I mean, I use the in the example. At least you could do with Python, with a Panandas is up to you, but the very end that you want to have a representation that to the presentations that is one pie chart and one. 1: Giving insights on on the cover for that population. 1: As usual, you will generate a couple of page with an interpretation interpretation. 1: Again, please pay. Attention is not describing the process, but this, describing the findings. 1: So if you describe the process, we will take points off, because this is not what we are asking. 1: We are asking an interpretation of the analysis, so you will submit your program. And then Nara Div. 1: As a dog. Pdf, and the part one meaning the answers to those questions can be either in a separate dog Pdf, file, or added as comments to your pytor file. 1: So that's basically it sorry for the longer it's 8, 44. 1: I will publish everything that we use today, and if you have any question, let me know. 1: There's a question. Yes, interpretation. Okay? So let's say that you will have a in this case in particular, you will have several comorbidities that can be more relevant. 1: And driving to the course of that. Does that happen? 1: The interpretation would be in the population that we are analyzing most frequent comorbidity is that one. 1: Because you can do a correlation. You can analyze what the correlation can be between the different groups. 1: Eventually, or you can just stay in that group and describe the finding. 1: So when you have a okay, I mean that I posted the 2 videos on a data storytelling check those that's exactly what you want to do. 1: You want to extract a story out of the data. So the data just the way they are. 1: They are not saying much. So if you describe a process, and then I give your report to someone who doesn't really know what you are doing it not getting much insights. 1: So think about that. That someone, a doctor, a medical doctor, asked that you, as the other scientist, can you analyze the file and let me know what what is the most common Com or Bdd, or what what are the most common commodities that for this group? 1: Of individuals. So you want to create something that will be useful for the end user, not as a an explanation over the process. 1: We did something similar to an explanation of the process for testing, but actually that was not even an explanation of the process, but was just describing how to test all the options. 1: All the branches in your code. So in this case, what we are asking is not, that is not the testing is not the explanation of the process, but it's really your findings. 1: So think about that. You are writing an article for the New York Times. 1: That's the target. So you want to write something that people are knowing nothing about. 1: Python, about a data science, about visualization that can get, can read, think about a visualization that can get, can read, think about. 1: I don't know someone who is not an engineer. 1: He is not a mathematician, he is not a mathematician, he's nothing to do. 1: He is not a software developer, but just a reader. 1: 250 general paper, so that's the goal. 1: I will keep sharing some examples. But again, don't do just a description of the process. 1: But do something that can be published on the new. 1: Your times, I mean, not necessarily. We ended up the way, but that kind of the the one that you are going to write. 1: All right. Okay, so that's the end of the class. 1:
Agile Tools for Real-World Data

 

O’REILLY” Wes McKinney

9

Data/Python

 

Python for Data Analysis “The scientific and data

Looking for complete instructions on manipulating, processing, analysis communities
cleaning, and crunching structured data in Python? This hands-on have been waitin 1g ‘for
book is packed with practical case studies that show you how to ;
effectively solve a broad set of data analysis problems, using several this book he Or years:
Python libraries—including NumPy, pandas, matplotlib, and [Python. ] 4

oaded with concrete
Written by Wes McKinney, the main author of the pandas library, ; :
Python for Data Analysis also serves as a practical, modern practical advice, yet
introduction to scientific computing in Python for data-intensive _
applications. It’s ideal for analysts new to Python and for Python ft ull of insight about
programmers new to scientific computing. how all the bieces it

m Use the IPython interactive shell as your primary together. It should

development environment become a canonical

m Learn basic and advanced NumPy (Numerical Python) features h , /
m Get started with data analysis tools in the pandas library reference for technica

m Use high-performance tools to load, clean, transform, computing in Python
merge, and reshape data ,
m Create scatter plots and static or interactive visualizations jor years fo come.
with matplotlib —Fernando Pérez
m Apply the pandas groupby facility to slice, dice, and research scientist at UC Berkeley,

summarize datasets

one of the originators of Python
m= Work with time series data in many different forms

m Learn how to solve problems in web analytics, social
sciences, finance, and economics, through detailed examples

Wes McKinney is the main author of pandas, the popular open
source Python library for data analysis. Wes is an active speaker and
participant in the Python and open source communities. He worked
as a quantitative analyst at AQR Capital Management and Python
consultant before founding DataPad, a data analytics company, in
2013. He graduated from MIT with an S.B. in Mathematics.

 

Twitter: @oreillymedia
facebook.com/oreilly

O’REILLY*

oreilly.com

US $39.99 CAN $41.99
ISBN: 978-1-449-31979-3

LL iii
! MUI

81449"31979

 

 

 

 

 

 

 

 

 

 

Python for Data Analysis

Wes McKinney

O’REILLY*

Beijing - Cambridge - Farnham - K6ln + Sebastopol - Tokyo

Python for Data Analysis
by Wes McKinney

Copyright © 2013 Wes McKinney. All rights reserved.
Printed in the United States of America.

Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.

O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions
are also available for most titles (http://my.safaribooksonline.com). For more information, contact our
corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.

Editors: Julie Steele and Meghan Blanchette Indexer: BIM Publishing Services
Production Editor: Melanie Yarbrough Cover Designer: Karen Montgomery
Copyeditor: Teresa Exley Interior Designer: David Futato
Proofreader: BIM Publishing Services Illustrator: Rebecca Demarest

October 2012: First Edition.

Revision History for the First Edition:
2012-10-05 First release
2013-05-17 Second release

See http://oreilly.com/catalog/errata.csp ?isbn=978 1449319793 for release details.

Nutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of
O’Reilly Media, Inc. Python for Data Analysis, the cover image of a golden-tailed tree shrew, and related
trade dress are trademarks of O’Reilly Media, Inc.

Many of the designations used by manufacturers and sellers to distinguish their products are claimed as
trademarks. Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a
trademark claim, the designations have been printed in caps or initial caps.

While every precaution has been taken in the preparation of this book, the publisher and author assume
no responsibility for errors or omissions, or for damages resulting from the use of the information con-
tained herein.

ISBN: 978-1-449-31979-3
[LS]
1368653545

 

Preface

1.

Table of Contents

Preliminaries .......... ccc ccc cece ccc ccc ceccecceccescecceees

What Is This Book About?
Why Python for Data Analysis?
Python as Glue
Solving the “Two-Language” Problem
Why Not Python?
Essential Python Libraries
NumPy
pandas
matplotlib
IPython
SciPy
Installation and Setup
Windows
Apple OS X
GNU/Linux
Python 2 and Python 3
Integrated Development Environments (IDEs)
Community and Conferences
Navigating This Book
Code Examples
Data for Examples
Import Conventions
Jargon
Acknowledgements

Introductory Examples ..............ceeeceeeceeeceeeeeeeeeee

1.usa.gov data from bit.ly
Counting Time Zones in Pure Python

NDDUNBBWWNNN HE

 

Counting Time Zones with pandas
MovieLens 1M Data Set

Measuring rating disagreement
US Baby Names 1880-2010

Analyzing Naming Trends
Conclusions and The Path Ahead

IPython: An Interactive Computing and Development Environment

IPython Basics
Tab Completion
Introspection
The %run Command
Executing Code from the Clipboard
Keyboard Shortcuts
Exceptions and Tracebacks
Magic Commands
Qt-based Rich GUI Console
Matplotlib Integration and Pylab Mode
Using the Command History
Searching and Reusing the Command History
Input and Output Variables
Logging the Input and Output
Interacting with the Operating System
Shell Commands and Aliases
Directory Bookmark System
Software Development Tools
Interactive Debugger
Timing Code: %time and %timeit
Basic Profiling: %prun and %run -p
Profiling a Function Line-by-Line
IPython HTML Notebook
Tips for Productive Code Development Using IPython
Reloading Module Dependencies
Code Design Tips
Advanced [Python Features
Making Your Own Classes IPython-friendly
Profiles and Configuration
Credits

NumPy Basics: Arrays and Vectorized Computation .................

The NumPy ndarray: A Multidimensional Array Object
Creating ndarrays
Data Types for ndarrays

 

iv | Table of Contents

Operations between Arrays and Scalars
Basic Indexing and Slicing
Boolean Indexing
Fancy Indexing
Transposing Arrays and Swapping Axes
Universal Functions: Fast Element-wise Array Functions
Data Processing Using Arrays
Expressing Conditional Logic as Array Operations
Mathematical and Statistical Methods
Methods for Boolean Arrays
Sorting
Unique and Other Set Logic
File Input and Output with Arrays
Storing Arrays on Disk in Binary Format
Saving and Loading Text Files
Linear Algebra
Random Number Generation
Example: Random Walks
Simulating Many Random Walks at Once

Getting Started with pandas ................e sees eee eeeeee

Introduction to pandas Data Structures
Series
DataFrame
Index Objects
Essential Functionality
Reindexing
Dropping entries from an axis
Indexing, selection, and filtering
Arithmetic and data alignment
Function application and mapping
Sorting and ranking
Axis indexes with duplicate values
Summarizing and Computing Descriptive Statistics
Correlation and Covariance
Unique Values, Value Counts, and Membership
Handling Missing Data
Filtering Out Missing Data
Filling in Missing Data
Hierarchical Indexing
Reordering and Sorting Levels
Summary Statistics by Level
Using a DataFrame’s Columns

102
103
103
104
105
106
108
109

en tr 111

112
112
115
120
122
122
125
125
128
132
133
136
137
139
141
142
143
145
147
149
150
150

 

Table of Contents | v

Other pandas Topics 151

Integer Indexing 151
Panel Data 152

6. Data Loading, Storage, and File Formats ...............ecceeceeceeceeeeees 155
Reading and Writing Data in Text Format 155
Reading Text Files in Pieces 160
Writing Data Out to Text Format 162
Manually Working with Delimited Formats 163
JSON Data 165
XML and HTML: Web Scraping 166
Binary Data Formats 171
Using HDF5 Format 171
Reading Microsoft Excel Files 172
Interacting with HTML and Web APIs 173
Interacting with Databases 174
Storing and Loading Data in MongoDB 176

7. Data Wrangling: Clean, Transform, Merge, Reshape ..............seseeeeeee 177
Combining and Merging Data Sets 177
Database-style DataFrame Merges 178
Merging on Index 182
Concatenating Along an Axis 185
Combining Data with Overlap 188
Reshaping and Pivoting 189
Reshaping with Hierarchical Indexing 190
Pivoting “long” to “wide” Format 192
Data Transformation 194
Removing Duplicates 194
Transforming Data Using a Function or Mapping 195
Replacing Values 196
Renaming Axis Indexes 197
Discretization and Binning 199
Detecting and Filtering Outliers 201
Permutation and Random Sampling 202
Computing Indicator/Dummy Variables 203
String Manipulation 205
String Object Methods 206
Regular expressions 207
Vectorized string functions in pandas 210
Example: USDA Food Database 212

 

vi | Table of Contents

8. Plotting and Visualization .............. cece cece eee e cece ceeeeneens 219

A Brief matplotlib API Primer 219
Figures and Subplots 220
Colors, Markers, and Line Styles 224
Ticks, Labels, and Legends 225
Annotations and Drawing on a Subplot 228
Saving Plots to File 231
matplotlib Configuration 231

Plotting Functions in pandas 232
Line Plots 232
Bar Plots 235
Histograms and Density Plots 238
Scatter Plots 239

Plotting Maps: Visualizing Haiti Earthquake Crisis Data 241

Python Visualization Tool Ecosystem 247
Chaco 248
mayavi 249
Other Packages 249
The Future of Visualization Tools? 249

9. Data Aggregation and Group Operations .................cceeeceeeeeee eens 251

GroupBy Mechanics 252
Iterating Over Groups 255
Selecting a Column or Subset of Columns 256
Grouping with Dicts and Series 257
Grouping with Functions 258
Grouping by Index Levels 259

Data Aggregation 259
Column-wise and Multiple Function Application 262
Returning Aggregated Data in “unindexed” Form 264

Group-wise Operations and Transformations 264
Apply: General split-apply-combine 266
Quantile and Bucket Analysis 268
Example: Filling Missing Values with Group-specific Values 270
Example: Random Sampling and Permutation 271
Example: Group Weighted Average and Correlation 273
Example: Group-wise Linear Regression 274

Pivot Tables and Cross-Tabulation 275
Cross-Tabulations: Crosstab 277

Example: 2012 Federal Election Commission Database 278
Donation Statistics by Occupation and Employer 280
Bucketing Donation Amounts 283
Donation Statistics by State 285

 

Table of Contents | vii

10. Time Series 0... ccc cece eee c cence eee e eee e eee eeeeee seen seen eeeeees
Date and Time Data Types and Tools
Converting between string and datetime
Time Series Basics
Indexing, Selection, Subsetting
Time Series with Duplicate Indices
Date Ranges, Frequencies, and Shifting
Generating Date Ranges
Frequencies and Date Offsets
Shifting (Leading and Lagging) Data
Time Zone Handling
Localization and Conversion
Operations with Time Zone-aware Timestamp Objects
Operations between Different Time Zones
Periods and Period Arithmetic
Period Frequency Conversion
Quarterly Period Frequencies
Converting Timestamps to Periods (and Back)
Creating a PeriodIndex from Arrays
Resampling and Frequency Conversion
Downsampling
Upsampling and Interpolation
Resampling with Periods
Time Series Plotting
Moving Window Functions
Exponentially-weighted functions
Binary Moving Window Functions
User-Defined Moving Window Functions
Performance and Memory Usage Notes

11. Financial and Economic Data Applications ................. cess eeeeeeeee eee
Data Munging Topics
Time Series and Cross-Section Alignment
Operations with Time Series of Different Frequencies
Time of Day and “as of” Data Selection
Splicing Together Data Sources
Return Indexes and Cumulative Returns
Group Transforms and Analysis
Group Factor Exposures
Decile and Quartile Analysis
More Example Applications
Signal Frontier Analysis
Future Contract Rolling

290
29).
293
294
296
297
298
299
301
303
304
305
306
307
308
309
311
312
312
314
316
318
319
320
324
324
326
327

329
329
330
332
334
336
338
340
342
343
345
345
347

 

viii | Table of Contents

Rolling Correlation and Linear Regression 350

12, Advanced NOMPY cscs .s isis sac ce sms swe mana swe sme na ome ow soa tm sma ams ae sea 353
ndarray Object Internals 353
NumPy dtype Hierarchy 354
Advanced Array Manipulation 355
Reshaping Arrays 355

C versus Fortran Order 356
Concatenating and Splitting Arrays 357
Repeating Elements: Tile and Repeat 360
Fancy Indexing Equivalents: Take and Put 361
Broadcasting 362
Broadcasting Over Other Axes 364
Setting Array Values by Broadcasting 367
Advanced ufunc Usage 367
ufunc Instance Methods 368
Custom ufuncs 370
Structured and Record Arrays 370
Nested dtypes and Multidimensional Fields 371
Why Use Structured Arrays? 372
Structured Array Manipulations: numpy.lib.recfunctions 372
More About Sorting 373
Indirect Sorts: argsort and lexsort 374
Alternate Sort Algorithms 375
numpy.searchsorted: Finding elements in a Sorted Array 376
NumPy Matrix Class 377
Advanced Array Input and Output 379
Memory-mapped Files 379
HDF5 and Other Array Storage Options 380
Performance Tips 380
The Importance of Contiguous Memory 381
Other Speed Options: Cython, f2py, C 382
Appendix: Python Language Essentials ................ceecceeceeeceeeceeeeeeees 385
T=) 433

 

Table of Contents | ix


 

Preface

The scientific Python ecosystem of open source libraries has grown substantially over
the last 10 years. By late 2011, I had long felt that the lack of centralized learning
resources for data analysis and statistical applications was a stumbling block for new
Python programmers engaged in such work. Key projects for data analysis (especially
NumPy, [Python, matplotlib, and pandas) had also matured enough that a book written
about them would likely not go out-of-date very quickly. Thus, I mustered the nerve
to embark on this writing project. This is the book that I wish existed when I started
using Python for data analysis in 2007. I hope you find it useful and are able to apply
these tools productively in your work.

Conventions Used in This Book

The following typographical conventions are used in this book:

Italic
Indicates new terms, URLs, email addresses, filenames, and file extensions.
Constant width
Used for program listings, as well as within paragraphs to refer to program elements
such as variable or function names, databases, data types, environment variables,
statements, and keywords.
Constant width bold
Shows commands or other text that should be typed literally by the user.
Constant width italic
Shows text that should be replaced with user-supplied values or by values deter-
mined by context.

This icon signifies a tip, suggestion, or general note.

 

 

 

xi

This icon indicates a warning or caution.

Using Code Examples

This book is here to help you get your job done. In general, you may use the code in
this book in your programs and documentation. You do not need to contact us for
permission unless you’re reproducing a significant portion of the code. For example,
writing a program that uses several chunks of code from this book does not require
permission. Selling or distributing a CD-ROM of examples from O’Reilly books does
require permission. Answering a question by citing this book and quoting example
code does not require permission. Incorporating a significant amount of example code
from this book into your product’s documentation does require permission.

 

We appreciate, but do not require, attribution. An attribution usually includes the title,
author, publisher, and ISBN. For example: “Python for Data Analysis by William Wes-
ley McKinney (O’Reilly). Copyright 2012 William McKinney, 978-1-449-31979-3.”

If you feel your use of code examples falls outside fair use or the permission given above,
feel free to contact us at permissions@oreilly.com.

Safari® Books Online
Ss f Safari Books Online (www.safaribooksonline.com) is an on-demand digital
altar! library that delivers expert content in both book and video form from the

world’s leading authors in technology and business.

Technology professionals, software developers, web designers, and business and cre-
ative professionals use Safari Books Online as their primary resource for research,
problem solving, learning, and certification training.

Safari Books Online offers a range of product mixes and pricing programs for organi-
zations, government agencies, and individuals. Subscribers have access to thousands
of books, training videos, and prepublication manuscripts in one fully searchable da-
tabase from publishers like O’Reilly Media, Prentice Hall Professional, Addison-Wesley
Professional, Microsoft Press, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John
Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT
Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, Course Tech-
nology, and dozens more. For more information about Safari Books Online, please visit
us online.

 

xii | Preface

How to Contact Us
Please address comments and questions concerning this book to the publisher:

O’Reilly Media, Inc.

1005 Gravenstein Highway North

Sebastopol, CA 95472

800-998-9938 (in the United States or Canada)
707-829-0515 (international or local)
707-829-0104 (fax)

We have a web page for this book, where we list errata, examples, and any additional
information. You can access this page at http://oreil.ly/python_for_data_analysis.

To comment or ask technical questions about this book, send email to
bookquestions@oreilly.com.

For more information about our books, courses, conferences, and news, see our website
at http://www.oreilly.com.

Find us on Facebook: http://facebook.com/oreilly
Follow us on Twitter: http://twitter.com/oreillymedia

Watch us on YouTube: http://www.youtube.com/oreillymedia

 

Preface | xiii


CHAPTER 1
Preliminaries

 

What Is This Book About?

This book is concerned with the nuts and bolts of manipulating, processing, cleaning,
and crunching data in Python. It is also a practical, modern introduction to scientific
computing in Python, tailored for data-intensive applications. This is a book about the
parts of the Python language and libraries you’ll need to effectively solve a broad set of
data analysis problems. This book is not an exposition on analytical methods using
Python as the implementation language.

When I say “data”, what am I referring to exactly? The primary focus is on structured
data, a deliberately vague term that encompasses many different common forms of
data, such as

¢ Multidimensional arrays (matrices)

¢ Tabular or spreadsheet-like data in which each column may be a different type
(string, numeric, date, or otherwise). This includes most kinds of data commonly
stored in relational databases or tab- or comma-delimited text files

¢ Multiple tables of data interrelated by key columns (what would be primary or
foreign keys for a SQL user)

e Evenly or unevenly spaced time series

This is by no means a complete list. Even though it may not always be obvious, a large
percentage of data sets can be transformed into a structured form that is more suitable
for analysis and modeling. If not, it may be possible to extract features from a data set
into a structured form. As an example, a collection of news articles could be processed
into a word frequency table which could then be used to perform sentiment analysis.

Most users of spreadsheet programs like Microsoft Excel, perhaps the most widely used
data analysis tool in the world, will not be strangers to these kinds of data.

 

Why Python for Data Analysis?

For many people (myself among them), the Python language is easy to fall in love with.
Since its first appearance in 1991, Python has become one of the most popular dynamic,
programming languages, along with Perl, Ruby, and others. Python and Ruby have
become especially popular in recent years for building websites using their numerous
web frameworks, like Rails (Ruby) and Django (Python). Such languages are often
called scripting languages as they can be used to write quick-and-dirty small programs,
or scripts. I don’t like the term “scripting language” as it carries a connotation that they
cannot be used for building mission-critical software. Among interpreted languages
Python is distinguished by its large and active scientific computing community. Adop-
tion of Python for scientific computing in both industry applications and academic
research has increased significantly since the early 2000s.

For data analysis and interactive, exploratory computing and data visualization, Python
will inevitably draw comparisons with the many other domain-specific open source
and commercial programming languages and tools in wide use, such as R, MATLAB,
SAS, Stata, and others. In recent years, Python’s improved library support (primarily
pandas) has made it a strong alternative for data manipulation tasks. Combined with
Python’s strength in general purpose programming, it is an excellent choice as a single
language for building data-centric applications.

Python as Glue

Part of Python’s success as a scientific computing platform is the ease of integrating C,
C++, and FORTRAN code. Most modern computing environments share a similar set
of legacy FORTRAN and C libraries for doing linear algebra, optimization, integration,
fast fourier transforms, and other such algorithms. The same story has held true for
many companies and national labs that have used Python to glue together 30 years’
worth of legacy software.

Most programs consist of small portions of code where most of the time is spent, with
large amounts of “glue code” that doesn’t run often. In many cases, the execution time
of the glue code is insignificant; effort is most fruitfully invested in optimizing the
computational bottlenecks, sometimes by moving the code to a lower-level language
like C.

In the last few years, the Cython project (http://cython.org) has become one of the
preferred ways of both creating fast compiled extensions for Python and also interfacing
with C and C++ code.

Solving the “Two-Language” Problem

In many organizations, it is common to research, prototype, and test new ideas using
a more domain-specific computing language like MATLAB or R then later port those

 

2 | Chapter 1: Preliminaries

ideas to be part of a larger production system written in, say, Java, C#, or C++. What
people are increasingly finding is that Python is a suitable language not only for doing
research and prototyping but also building the production systems, too. I believe that
more and more companies will go down this path as there are often significant organ-
izational benefits to having both scientists and technologists using the same set of pro-
grammatic tools.

Why Not Python?

While Python is an excellent environment for building computationally-intensive sci-
entific applications and building most kinds of general purpose systems, there are a
number of uses for which Python may be less suitable.

As Python is an interpreted programming language, in general most Python code will
run substantially slower than code written in a compiled language like Java or C++. As
programmer time is typically more valuable than CPU time, many are happy to make
this tradeoff. However, in an application with very low latency requirements (for ex-
ample, a high frequency trading system), the time spent programming in a lower-level,
lower-productivity language like C++ to achieve the maximum possible performance
might be time well spent.

Python is not an ideal language for highly concurrent, multithreaded applications, par-
ticularly applications with many CPU-bound threads. The reason for this is that it has
what is known as the global interpreter lock (GIL), a mechanism which prevents the
interpreter from executing more than one Python bytecode instruction at a time. The
technical reasons for why the GIL exists are beyond the scope of this book, but as of
this writing it does not seem likely that the GIL will disappear anytime soon. While it
is true that in many big data processing applications, a cluster of computers may be
required to process a data set in a reasonable amount of time, there are still situations
where a single-process, multithreaded system is desirable.

This is not to say that Python cannot execute truly multithreaded, parallel code; that
code just cannot be executed in a single Python process. As an example, the Cython
project features easy integration with OpenMP, a C framework for parallel computing,
in order to to parallelize loops and thus significantly speed up numerical algorithms.

Essential Python Libraries

For those who are less familiar with the scientific Python ecosystem and the libraries
used throughout the book, I present the following overview of each library.

 

Essential Python Libraries | 3

NumPy

NumPy, short for Numerical Python, is the foundational package for scientific com-
puting in Python. The majority of this book will be based on NumPy and libraries built
on top of NumPy. It provides, among other things:

¢ A fast and efficient multidimensional array object ndarray

¢ Functions for performing element-wise computations with arrays or mathematical
operations between arrays

* Tools for reading and writing array-based data sets to disk
* Linear algebra operations, Fourier transform, and random number generation

* Tools for integrating C, C++, and Fortran code to Python

Beyond the fast array-processing capabilities that NumPy adds to Python, one of its
primary purposes with regards to data analysis is as the primary container for data to
be passed between algorithms. For numerical data, NumPy arrays are a much more
efficient way of storing and manipulating data than the other built-in Python data
structures. Also, libraries written in a lower-level language, such as C or Fortran, can
operate on the data stored in a NumPy array without copying any data.

pandas

pandas provides rich data structures and functions designed to make working with
structured data fast, easy, and expressive. It is, as you will see, one of the critical in-
gredients enabling Python to be a powerful and productive data analysis environment.
The primary object in pandas that will be used in this book is the DataFrame, a two-
dimensional tabular, column-oriented data structure with both row and column labels:

>>> frame
total_bill tip sex smoker day time size

1 16.99 1.01 Female No Sun Dinner 2
2 10.34 1.66 Male No Sun Dinner 3
3 21.01 3.5 Male No Sun Dinner 3
4 23.68 3.31 Male No Sun Dinner 2
5 24.59 3.61 Female No Sun Dinner 4
6 25.29 4.71 Male No Sun Dinner 4
7 8.77 2 Male No Sun Dinner 2
8 26.88 3.12 Male No Sun Dinner 4
9 15.04 1.96 Male No Sun Dinner 2
10 14.78 3.23 Male No Sun Dinner 2

pandas combines the high performance array-computing features of NumPy with the
flexible data manipulation capabilities of spreadsheets and relational databases (such
as SQL). It provides sophisticated indexing functionality to make it easy to reshape,
slice and dice, perform aggregations, and select subsets of data. pandas is the primary
tool that we will use in this book.

 

4 | Chapter 1: Preliminaries

For financial users, pandas features rich, high-performance time series functionality
and tools well-suited for working with financial data. In fact, I initially designed pandas
as an ideal tool for financial data analysis applications.

For users of the R language for statistical computing, the DataFrame name will be
familiar, as the object was named after the similar R data. frame object. They are not
the same, however; the functionality provided by data. frame in R is essentially a strict
subset of that provided by the pandas DataFrame. While this is a book about Python, I
will occasionally draw comparisons with R as it is one of the most widely-used open
source data analysis environments and will be familiar to many readers.

The pandas name itself is derived from panel data, an econometrics term for multidi-
mensional structured data sets, and Python data analysis itself.

matplotlib

matplotlib is the most popular Python library for producing plots and other 2D data
visualizations. It was originally created by John D. Hunter (JDH) and is now maintained
by a large team of developers. It is well-suited for creating plots suitable for publication.
It integrates well with [Python (see below), thus providing a comfortable interactive
environment for plotting and exploring data. The plots are also interactive; you can
zoom in on a section of the plot and pan around the plot using the toolbar in the plot
window.

IPython

IPython is the component in the standard scientific Python toolset that ties everything
together. It provides a robust and productive environment for interactive and explor-
atory computing. It is an enhanced Python shell designed to accelerate the writing,
testing, and debugging of Python code. It is particularly useful for interactively working
with data and visualizing data with matplotlib. [Python is usually involved with the
majority of my Python work, including running, debugging, and testing code.

Aside from the standard terminal-based IPython shell, the project also provides
¢ A Mathematica-like HTML notebook for connecting to [Python through a web
browser (more on this later).
¢ A Qt framework-based GUI console with inline plotting, multiline editing, and
syntax highlighting
e An infrastructure for interactive parallel and distributed computing

I will devote a chapter to [Python and how to get the most out of its features. I strongly
recommend using it while working through this book.

 

Essential Python Libraries | 5

SciPy

SciPy is a collection of packages addressing a number of different standard problem
domains in scientific computing. Here is a sampling of the packages included:

* scipy.integrate: numerical integration routines and differential equation solvers

¢ scipy.linalg: linear algebra routines and matrix decompositions extending be-
yond those provided in numpy. linalg.

* scipy.optimize: function optimizers (minimizers) and root finding algorithms
* scipy.signal: signal processing tools
* scipy.sparse: sparse matrices and sparse linear system solvers

¢ scipy.special: wrapper around SPECFUN, a Fortran library implementing many
common mathematical functions, such as the gamma function

* scipy.stats: standard continuous and discrete probability distributions (density
functions, samplers, continuous distribution functions), various statistical tests,
and more descriptive statistics

* scipy.weave: tool for using inline C++ code to accelerate array computations

Together NumPy and SciPy form a reasonably complete computational replacement
for much of MATLAB along with some of its add-on toolboxes.

Installation and Setup

Since everyone uses Python for different applications, there is no single solution for
setting up Python and required add-on packages. Many readers will not have a complete
scientific Python environment suitable for following along with this book, so here I will
give detailed instructions to get set up on each operating system. I recommend using
one of the following base Python distributions:

¢ Enthought Python Distribution: a scientific-oriented Python distribution from En-
thought (http://www.enthought.com). This includes EPDFree, a free base scientific
distribution (with NumPy, SciPy, matplotlib, Chaco, and [Python) and EPD Full,
a comprehensive suite of more than 100 scientific packages across many domains.
EPD Full is free for academic use but has an annual subscription for non-academic
users.

¢ Python(x,y) (http://pythonxy.googlecode.com): A free scientific-oriented Python
distribution for Windows.

I will be using EPDFree for the installation guides, though you are welcome to take
another approach depending on your needs. At the time of this writing, EPD includes
Python 2.7, though this might change at some point in the future. After installing, you
will have the following packages installed and importable:

 

6 | Chapter1: Preliminaries

* Scientific Python base: NumPy, SciPy, matplotlib, and IPython. These are all in-
cluded in EPDFree.

e [Python Notebook dependencies: tornado and pyzmq. These are included in EPD-
Free.

¢ pandas (version 0.8.2 or higher).

At some point while reading you may wish to install one or more of the following
packages: statsmodels, PyTables, PyQt (or equivalently, PySide), xlrd, Ixml, basemap,
pymongo, and requests. These are used in various examples. Installing these optional
libraries is not necessary, and I would would suggest waiting until you need them. For
example, installing PyQt or PyTables from source on OS X or Linux can be rather
arduous. For now, it’s most important to get up and running with the bare minimum:
EPDFree and pandas.

For information on each Python package and links to binary installers or other help,
see the Python Package Index (PyPI, http://pypi.python.org). This is also an excellent
resource for finding new Python packages.

a,
‘ To avoid confusion and to keep things simple, I am avoiding discussion
of more complex environment management tools like pip and virtua-
via° lenv. There are many excellent guides available for these tools on the

* Internet.

Some users may be interested in alternate Python implementations, such
~~) as IronPython, Jython, or PyPy. To make use of the tools presented in

this book, it is (currently) necessary to use the standard C-based Python
interpreter, known as CPython.

 

S
«ed

 

Windows

To get started on Windows, download the EPDFree installer from http://www.en
thought.com, which should be an MSI installer named like epd_free-7.3-1-win-
x86.msi. Run the installer and accept the default installation location C:\Python27. If
you had previously installed Python in this location, you may want to delete it manually
first (or using Add/Remove Programs).

Next, you need to verify that Python has been successfully added to the system path
and that there are no conflicts with any prior-installed Python versions. First, open a
command prompt by going to the Start Menu and starting the Command Prompt ap-
plication, also known as cmd.exe. Try starting the Python interpreter by typing
python. You should see a message that matches the version of EPDFree you installed:
C:\Users\Wes>python
Python 2.7.3 |EPD_free 7.3-1 (32-bit)| (default, Apr 12 2012, :37) on win32

Type "credits", "demo" or "enthought" for more information.
>>>

 

Installation and Setup | 7

If you see a message for a different version of EPD or it doesn’t work at all, you will
need to clean up your Windows environment variables. On Windows 7 you can start
typing “environment variables” in the programs search field and select Edit environ
ment variables for your account. On Windows XP, you will have to go to Control
Panel > System > Advanced > Environment Variables. On the window that pops up,
you are looking for the Path variable. It needs to contain the following two directory
paths, separated by semicolons:

C:\Python27;C:\Python27\Scripts

If you installed other versions of Python, be sure to delete any other Python-related
directories from both the system and user Path variables. After making a path alterna-
tion, you have to restart the command prompt for the changes to take effect.

Once you can launch Python successfully from the command prompt, you need to

install pandas. The easiest way is to download the appropriate binary installer from

http://pypi.python.org/pypi/pandas. For EPDFree, this should be pandas-0.9.0.win32-

py2.7.exe. After you run this, let’s launch [Python and check that things are installed

correctly by importing pandas and making a simple matplotlib plot:
C:\Users\Wes>ipython --pylab

Python 2.7.3 |EPD free 7.3-1 (32-bit) |
Type "copyright", "credits" or "license" for more information.

IPython 0.12.1 -- An enhanced Interactive Python.

2 -> Introduction and overview of IPython's features.
#quickref -> Quick reference.

help -> Python's own help system.

object? -> Details about ‘object’, use 'object??' for extra details.

Welcome to pylab, a matplotlib-based Python environment [backend: WXAgg].
For more information, type 'help(pylab)'.

In [1]: import pandas
In [2]: plot(arange(10))

If successful, there should be no error messages and a plot window will appear. You
can also check that the Python HTML notebook can be successfully run by typing:

$ ipython notebook --pylab=inline

If you use the [Python notebook application on Windows and normally
ta) use Internet Explorer, you will likely need to install and run Mozilla

Firefox or Google Chrome instead.

 

EPDFree on Windows contains only 32-bit executables. If you want or need a 64-bit
setup on Windows, using EPD Full is the most painless way to accomplish that. If you
would rather install from scratch and not pay for an EPD subscription, Christoph
Gohlke at the University of California, Irvine, publishes unofficial binary installers for

 

8 | Chapter1: Preliminaries

all of the book’s necessary packages (hitp://www.lfd.uci.edu/~gohlke/pythonlibs/) for 32-
and 64-bit Windows.

Apple 0S X

To get started on OS X, you must first install Xcode, which includes Apple’s suite of
software development tools. The necessary component for our purposes is the gcc C
and C++ compiler suite. The Xcode installer can be found on the OS X install DVD
that came with your computer or downloaded from Apple directly.

Once you’ve installed Xcode, launch the terminal (Terminal.app) by navigating to
Applications > Utilities. Type gcc and press enter. You should hopefully see some-
thing like:

$ gcc
1686-apple-darwin10-gcc-4.2.1: no input files

Now you need to install EPDFree. Download the installer which should be a disk image
named something like epd_free-7.3-1-macosx-i386.dmg. Double-click the .dmg file to
mount it, then double-click the .mpkg file inside to run the installer.

When the installer runs, it automatically appends the EPDFree executable path to
your .bash_profile file. This is located at /Users/your_uname/.bash_profile:

# Setting PATH for EPD_ free-7.3-1
PATH="/Library/Frameworks/Python. framework/Versions/Current/bin:${PATH}"
export PATH

Should you encounter any problems in the following steps, you’ll want to inspect
your .bash_profile and potentially add the above directory to your path.

Now, it’s time to install pandas. Execute this command in the terminal:

$ sudo easy_install pandas

Searching for pandas

Reading http://pypi.python.org/simple/pandas/

Reading http://pandas.pydata.org

Reading http://pandas.sourceforge.net

Best match: pandas 0.9.0

Downloading http://pypi.python.org/packages/source/p/pandas/pandas-0.9.0.zip
Processing pandas-0.9.0.zip

Writing /tmp/easy_install-H5m1X6/pandas-0.9.0/setup.cfg

Running pandas-0.9.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-H5m1X6/
pandas-0.9.0/egg-dist-tmp-RhLGoz

Adding pandas 0.9.0 to easy-install.pth file

Installed /Library/Frameworks/Python. framework/Versions/7.3/lib/python2.7/
site-packages/pandas-0.9.0-py2.7-macosx-10.5-1386.egg

Processing dependencies for pandas

Finished processing dependencies for pandas

To verify everything is working, launch IPython in Pylab mode and test importing pan-
das then making a plot interactively:

 

Installation and Setup | 9

$ ipython --pylab

 ~/VirtualBox VMs/WindowsXP $ ipython

Python 2.7.3 |EPD free 7.3-1 (32-bit)| (default, Apr 12 2012, :34)
Type "copyright", "credits" or "license" for more information.

IPython 0.12.1 -- An enhanced Interactive Python.

? -> Introduction and overview of IPython's features.
#quickref -> Quick reference.
help -> Python's own help system.

object? -> Details about ‘object’, use 'object??' for extra details.

Welcome to pylab, a matplotlib-based Python environment [backend: WXAgg].
For more information, type ‘help(pylab)'.

In [1]: import pandas
In [2]: plot(arange(10))

If this succeeds, a plot window with a straight line should pop up.

GNU/Linux

a a,

    
 

Some, but not all, Linux distributions include sufficiently up-to-date
versions of all the required Python packages and can be installed using
< 4)8° the built-in package management tool like apt. I detail setup using EPD-
* Free as it's easily reproducible across distributions.

 

Linux details will vary a bit depending on your Linux flavor, but here I give details for
Debian-based GNU/Linux systems like Ubuntu and Mint. Setup is similar to OS X with
the exception of how EPDFree is installed. The installer is a shell script that must be
executed in the terminal. Depending on whether you have a 32-bit or 64-bit system,
you will either need to install the x86 (32-bit) or x86_64 (64-bit) installer. You will then
have a file named something similar to epd_free-7.3-1-rh5-x86_64.sh. To install it,
execute this script with bash:

$ bash epd_free-7.3-1-rh5-x86_64.sh

After accepting the license, you will be presented with a choice of where to put the
EPDFree files. I recommend installing the files in your home directory, say /home/wesm/
epd (substituting your own username for wesm).

Once the installer has finished, you need to add EPDFree’s bin directory to your
$PATH variable. If you are using the bash shell (the default in Ubuntu, for example), this
means adding the following path addition in your .bashrc:

export PATH=/home/wesm/epd/bin: $PATH
Obviously, substitute the installation directory you used for /home/wesm/epd/. After

doing this you can either start a new terminal process or execute your .bashrc again
with source ~/.bashrc.

 

10 | Chapter 1: Preliminaries

You need a C compiler such as gcc to move forward; many Linux distributions include
gcc, but others may not. On Debian systems, you can install gcc by executing:

sudo apt-get install gcc

If you type gcc on the command line it should say something like:

$ gcc
gcc: no input files

Now, time to install pandas:

$ easy_ install pandas

If you installed EPDFree as root, you may need to add sudo to the command and enter
the sudo or root password. To verify things are working, perform the same checks as
in the OS X section.

Python 2 and Python 3

The Python community is currently undergoing a drawn-out transition from the Python
2 series of interpreters to the Python 3 series. Until the appearance of Python 3.0, all
Python code was backwards compatible. The community decided that in order to move
the language forward, certain backwards incompatible changes were necessary.

I am writing this book with Python 2.7 as its basis, as the majority of the scientific
Python community has not yet transitioned to Python 3. The good news is that, with
a few exceptions, you should have no trouble following along with the book if you
happen to be using Python 3.2.

Integrated Development Environments (IDEs)

When asked about my standard development environment, I almost always say “IPy-
thon plus a text editor”. I typically write a program and iteratively test and debug each
piece of it in IPython. It is also useful to be able to play around with data interactively
and visually verify that a particular set of data manipulations are doing the right thing.
Libraries like pandas and NumPy are designed to be easy-to-use in the shell.

However, some will still prefer to work in an IDE instead of a text editor. They do
provide many nice “code intelligence” features like completion or quickly pulling up
the documentation associated with functions and classes. Here are some that you can
explore:

¢ Eclipse with PyDev Plugin

¢ Python Tools for Visual Studio (for Windows users)

e PyCharm

¢ Spyder

* Komodo IDE

 

Installation and Setup | 11

Community and Conferences

Outside of an Internet search, the scientific Python mailing lists are generally helpful
and responsive to questions. Some ones to take a look at are:

* pydata: a Google Group list for questions related to Python for data analysis and
pandas

* pystatsmodels: for statsmodels or pandas-related questions

* numpy-discussion: for NumPy-related questions

* scipy-user: for general SciPy or scientific Python questions

I deliberately did not post URLs for these in case they change. They can be easily located
via Internet search.

Each year many conferences are held all over the world for Python programmers. PyCon
and EuroPython are the two main general Python conferences in the United States and
Europe, respectively. SciPy and EuroSciPy are scientific-oriented Python conferences
where you will likely find many “birds of a feather” if you become more involved with
using Python for data analysis after reading this book.

Navigating This Book

If you have never programmed in Python before, you may actually want to start at the
end of the book, where I have placed a condensed tutorial on Python syntax, language
features, and built-in data structures like tuples, lists, and dicts. These things are con-
sidered prerequisite knowledge for the remainder of the book.

The book starts by introducing you to the [Python environment. Next, I give a short
introduction to the key features of NumPy, leaving more advanced NumPy use for
another chapter at the end of the book. Then, I introduce pandas and devote the rest
of the book to data analysis topics applying pandas, NumPy, and matplotlib (for vis-
ualization). I have structured the material in the most incremental way possible, though
there is occasionally some minor cross-over between chapters.

Data files and related material for each chapter are hosted as a git repository on GitHub:
http: //github.com/pydata/pydata-book

I encourage you to download the data and use it to replicate the book’s code examples

and experiment with the tools presented in each chapter. I will happily accept contri-

butions, scripts, [Python notebooks, or any other materials you wish to contribute to
the book's repository for all to enjoy.

 

12 | Chapter 1: Preliminaries

Code Examples
Most of the code examples in the book are shown with input and output as it would
appear executed in the IPython shell.

In [5]: code
Out[5]: output

At times, for clarity, multiple code examples will be shown side by side. These should
be read left to right and executed separately.

In [5]: code In [6]: code2
Out[5]: output Out[6]: output2
Data for Examples

Data sets for the examples in each chapter are hosted in a repository on GitHub: http:
//github.com/pydata/pydata-book. You can download this data either by using the git
revision control command-line program or by downloading a zip file of the repository
from the website.

I have made every effort to ensure that it contains everything necessary to reproduce
the examples, but I may have made some mistakes or omissions. If so, please send me
an e-mail: wesmckinn@gmail.com.

Import Conventions

The Python community has adopted a number of naming conventions for commonly-
used modules:
import numpy as np

import pandas as pd
import matplotlib.pyplot as plt

This means that when you see np.arange, this is a reference to the arange function in
NumPy. This is done as it’s considered bad practice in Python software development
to import everything (from numpy import *) from a large package like NumPy.

Jargon

Pll use some terms common both to programming and data science that you may not
be familiar with. Thus, here are some brief definitions:

Munge/Munging/Wrangling
Describes the overall process of manipulating unstructured and/or messy data into
a structured or clean form. The word has snuck its way into the jargon of many
modern day data hackers. Munge rhymes with “lunge”.

 

Navigating This Book | 13

Pseudocode
A description of an algorithm or process that takes a code-like form while likely
not being actual valid source code.

Syntactic sugar
Programming syntax which does not add new features, but makes something more
convenient or easier to type.

Acknowledgements

It would have been difficult for me to write this book without the support of a large
number of people.

On the O’Reilly staff, I’m very grateful for my editors Meghan Blanchette and Julie
Steele who guided me through the process. Mike Loukides also worked with me in the
proposal stages and helped make the book a reality.

I received a wealth of technical review from a large cast of characters. In particular,
Martin Blais and Hugh Brown were incredibly helpful in improving the book’s exam-
ples, clarity, and organization from cover to cover. James Long, Drew Conway, Fer-
nando Pérez, Brian Granger, Thomas Kluyver, Adam Klein, Josh Klein, Chang She, and
Stéfan van der Walt each reviewed one or more chapters, providing pointed feedback
from many different perspectives.

I got many great ideas for examples and data sets from friends and colleagues in the
data community, among them: Mike Dewar, Jeff Hammerbacher, James Johndrow,
Kristian Lum, Adam Klein, Hilary Mason, Chang She, and Ashley Williams.

Iam of course indebted to the many leaders in the open source scientific Python com-
munity who’ve built the foundation for my development work and gave encouragement
while I was writing this book: the [Python core team (Fernando Pérez, Brian Granger,
Min Ragan-Kelly, Thomas Kluyver, and others), John Hunter, Skipper Seabold, Travis
Oliphant, Peter Wang, Eric Jones, Robert Kern, Josef Perktold, Francesc Alted, Chris
Fonnesbeck, and too many others to mention. Several other people provided a great
deal of support, ideas, and encouragement along the way: Drew Conway, Sean Taylor,
Giuseppe Paleologo, Jared Lander, David Epstein, John Krowas, Joshua Bloom, Den
Pilsworth, John Myles-White, and many others I’ve forgotten.

I'd also like to thank a number of people from my formative years. First, my former
AQR colleagues who’ve cheered me on in my pandas work over the years: Alex Reyf-
man, Michael Wong, Tim Sargen, Oktay Kurbanov, Matthew Tschantz, Roni Israelov,
Michael Katz, Chris Uga, Prasad Ramanan, Ted Square, and Hoon Kim. Lastly, my
academic advisors Haynes Miller (MIT) and Mike West (Duke).

On the personal side, Casey Dinkin provided invaluable day-to-day support during the
writing process, tolerating my highs and lows as I hacked together the final draft on

 

14 | Chapter 1: Preliminaries

top of an already overcommitted schedule. Lastly, my parents, Bill and Kim, taught me
to always follow my dreams and to never settle for less.

 

Acknowledgements | 15


CHAPTER 2
Introductory Examples

 

This book teaches you the Python tools to work productively with data. While readers
may have many different end goals for their work, the tasks required generally fall into
a number of different broad groups:

Interacting with the outside world
Reading and writing with a variety of file formats and databases.

Preparation
Cleaning, munging, combining, normalizing, reshaping, slicing and dicing, and
transforming data for analysis.

Transformation
Applying mathematical and statistical operations to groups of data sets to derive
new data sets. For example, aggregating a large table by group variables.

Modeling and computation
Connecting your data to statistical models, machine learning algorithms, or other
computational tools

Presentation
Creating interactive or static graphical visualizations or textual summaries

In this chapter I will show you a few data sets and some things we can do with them.
These examples are just intended to pique your interest and thus will only be explained
at a high level. Don’t worry if you have no experience with any of these tools; they will
be discussed in great detail throughout the rest of the book. In the code examples you'll
see input and output prompts like In [15]:; these are from the IPython shell.

Va,
oe To follow along with these examples, you should run [Python in Pylab
43 mode by running ipython --pylab at the command prompt.

 

 

 

7

1.usa.gov data from bit.ly

In 2011, URL shortening service bit.ly partnered with the United States government
website usa. gov to provide a feed of anonymous data gathered from users who shorten
links ending with .gov or .mil. As of this writing, in addition to providing a live feed,
hourly snapshots are available as downloadable text files.!

In the case of the hourly snapshots, each line in each file contains a common form of
web data known as JSON, which stands for JavaScript Object Notation. For example,
if we read just the first line of a file you may see something like

In [15]: path = 'cho2/usagov_bitly_data2012-03-16-1331923249.txt'

In [16]: open(path) .readline()

Out[16]: '{ "a": "Mozilla\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\/535.11
(KHTML, like Gecko) Chrome\\/17.0.963.78 Safari\\/535.11", "c": "US", "nk": 1,
"tz": "America\\/New York", "gr": "MA", "g": "A6qOVH", “h": "wfLOtf", "1":
“orofrog", "al": "en-US,en;q=0.8", "hh": "1.usa.gov", "r":

"http: \\/\\/www. facebook. com\\/1\\/7AQEFzjSi\\/1.usa.gov\\/wfLotf", “u":
"http: \\/\\/www.ncbi.nlm.nih.gov\\/pubmed\\/22415991", "t": 1331923247, "hc":
1331822918, "cy": "Danvers", "11": [ 42.576698, -70.954903 ] }\n'

Python has numerous built-in and 3rd party modules for converting a JSON string into
a Python dictionary object. Here Ill use the json module and its loads function invoked
on each line in the sample file I downloaded:

import json
path = 'chO2/usagov_bitly_data2012-03-16-1331923249.txt'
records = [json.loads(line) for line in open(path) ]

If you’ve never programmed in Python before, the last expression here is called a list
comprehension, which is a concise way of applying an operation (like json. loads) to a
collection of strings or other objects. Conveniently, iterating over an open file handle
gives you a sequence of its lines. The resulting object records is now a list of Python
dicts:

In [18]: records[o]

Out [18]:

{u'a': u'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like
Gecko) Chrome/17.0.963.78 Safari/535.11',
u'al': u'en-US,en;q=0.8',

u'c': u'US',

u'cy': u'Danvers',

u'g': u'A6qgOVH',

u'gr': u'MA',

u'h': u'wfLdtf',

u'hc': 1331822918,

u'hh': u'1.usa.gov',

u'l': u'orofrog',

u'll': [42.576698, -70.954903],

1. http://;www.usa.gov/About/developer-resources/1usagov.shtml

 

18 | Chapter 2: Introductory Examples

u'nk': 1,

u'r’: u'http://www. facebook.com/1/7AQEFzjSi/1.usa.gov/wfLOtf' ,
u't': 1331923247,

u'tz': u'America/New_York',

u'u': u'http://www.ncbi.nlm.nih.gov/pubmed/22415991' }

Note that Python indices start at 0 and not 1 like some other languages (like R). It’s
now easy to access individual values within records by passing a string for the key you
wish to access:

In [19]: records[o]['tz']
Out[19]: u'America/New_York'

The u here in front of the quotation stands for unicode, a standard form of string en-
coding. Note that [Python shows the time zone string object representation here rather
than its print equivalent:

In [20]: print records[o]['tz']
America/New_York

Counting Time Zones in Pure Python

Suppose we were interested in the most often-occurring time zones in the data set (the
tz field). There are many ways we could do this. First, let’s extract a list of time zones
again using a list comprehension:

In [25]: time_zones = [rec['tz'] for rec in records]

KeyError Traceback (most recent call last)
/home/wesm/book_scripts/whetting/<ipython> in <module>()

----> 1 time_zones = [rec['tz'] for rec in records]

KeyError: 'tz'

Oops! Turns out that not all of the records have a time zone field. This is easy to handle
as we can add the check if 'tz' in rec at the end of the list comprehension:

In [26]: time_zones = [rec['tz'] for rec in records if 'tz' in rec]

In [27]: time_zones[:10]

Out [27]:

[u'America/New York",
u'America/Denver' ,
u'America/New_York',
u'America/Sao Paulo’,
u'America/New_York',
u'America/New_York',
u'Europe/Warsaw' ,

u'' )

u'' J

u'']

Just looking at the first 10 time zones we see that some of them are unknown (empty).

You can filter these out also but I'll leave them in for now. Now, to produce counts by

 

1.usa.gov data from bit.ly | 19

time zone I'll show two approaches: the harder way (using just the Python standard
library) and the easier way (using pandas). One way to do the counting is to use a dict
to store counts while we iterate through the time zones:

def get_counts(sequence) :
counts = {}
for x in sequence:
if x in counts:
counts[x] += 1
else:
counts[x] = 1
return counts

If you know a bit more about the Python standard library, you might prefer to write
the same thing more briefly:

from collections import defaultdict

def get_counts2(sequence) :
counts = defaultdict(int) # values will initialize to 0
for x in sequence:
counts[x] += 1
return counts

I put this logic in a function just to make it more reusable. To use it on the time zones,
just pass the time_zones list:

In [31]: counts = get_counts(time_zones)

In [32]: counts['America/New_York' ]
Out[32]: 1254

In [33]: len(time_zones)
Out[33]: 3440

If we wanted the top 10 time zones and their counts, we have to do a little bit of dic-
tionary acrobatics:

def top _counts(count_dict, n=10):
value_key_pairs = [(count, tz) for tz, count in count_dict.items()]
value_key_pairs.sort()
return value_key_pairs[-n:]

We have then:

In [35]: top_counts(counts)
Out[35]:
[(33, u'America/Sao Paulo'),
(35, u'Europe/Madrid'),
(36, u'Pacific/Honolulu'),
(37, u'Asia/Tokyo'),
(74, u'Europe/London'),
(191, u'America/Denver'),
(382, u'America/Los Angeles’),
(400, u'America/Chicago'),

 

20 | Chapter 2: Introductory Examples

(521, u''),
(1251, u'America/New_York')]

If you search the Python standard library, you may find the collections .Counter class,
which makes this task a lot easier:

In [49]: from collections import Counter
In [50]: counts = Counter(time_zones)

In [51]: counts.most_common(10)

Out[51]:

[(u'America/New_York', 1251),
(u"', 521),
(u'America/Chicago', 400),
(u'America/Los Angeles’, 382),
(u'America/Denver', 191),
(u'Europe/London', 74),
(u'Asia/Tokyo', 37),
(u'Pacific/Honolulu', 36),
(u'Europe/Madrid', 35),
(u'America/Sao Paulo', 33)]

Counting Time Zones with pandas

The main pandas data structure is the DataFrame, which you can think of as repre-
senting a table or spreadsheet of data. Creating a DataFrame from the original set of
records is simple:

In [289]: from pandas import DataFrame, Series
In [290]: import pandas as pd; import numpy as np
In [291]: frame = DataFrame(records)

In [292]: frame

Out [292]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 3560 entries, 0 to 3559
Data columns:

_heartbeat_ 120 non-null values
a 3440 non-null values
al 3094 non-null values
c 2919 non-null values
cy 2919 non-null values
g 3440 non-null values
gr 2919 non-null values
h 3440 non-null values
hc 3440 non-null values
hh 3440 non-null values
kw 93 non-null values

1 3440 non-null values
1l 2919 non-null values
nk 3440 non-null values
E 3440 non-null values

 

1.usa.gov data from bit.ly | 21

t 3440 non-null values
tz 3440 non-null values
u 3440 non-null values
dtypes: float64(4), object(14)

In [293]: frame['tz'][:10]

Out [293]:

0 America/New_York
1 America/Denver
2 America/New_York
3 America/Sao_Paulo
4 America/New_York
5 America/New_York
6 Europe/Warsaw
7

8

9

Name: tz

The output shown for the frame is the summary view, shown for large DataFrame ob-
jects. The Series object returned by frame['tz'] has a method value_counts that gives
us what we’re looking for:

In [294]: tz_counts = frame['tz'].value_counts()

In [295]: tz_counts[:10]

Out[295]:
America/New_York 1251
521
America/Chicago 400
America/Los_Angeles 382
America/Denver 191
Europe/London 74
Asia/Tokyo 37
Pacific/Honolulu 36
Europe/Madrid 35
America/Sao_Paulo 33

Then, we might want to make a plot of this data using plotting library, matplotlib. You
can do a bit of munging to fill in a substitute value for unknown and missing time zone
data in the records. The fillna function can replace missing (NA) values and unknown
(empty strings) values can be replaced by boolean array indexing:

In [296]: clean_tz = frame['tz'].fillna('Missing' )
In [297]: clean_tz[clean_tz == ''] = 'Unknown'
In [298]: tz_counts = clean_tz.value_counts()

In [299]: tz_counts[:10]

Out[299]:

America/New_York 1251
Unknown 521
America/Chicago 400
America/Los_Angeles 382

 

22 | Chapter 2: Introductory Examples

America/Denver 191

Missing 120
Europe/London 74
Asia/Tokyo 37
Pacific/Honolulu 36
Europe/Madrid 35

Making a horizontal bar plot can be accomplished using the plot method on the
counts objects:

In [301]: tz_counts[:10].plot(kind='barh', rot=0)
See Figure 2-1 for the resulting figure. We'll explore more tools for working with this

kind of data. For example, the a field contains information about the browser, device,
or application used to perform the URL shortening:

In [302]: frame['a'][1]
Out [302]: u'GoogleMaps/RochesterNY'

In [303]: frame['a'][50]
Out[303]: u'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2'

In [304]: frame['a'][51]
Out[304]: u'Mozilla/5.0 (Linux; U; Android 2.2.2; en-us; LG-P925/V10e Build/FRG83G)
AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1'

 

Europe/Madrid'
Pacific/Honolulu
Asia/Tokyo}
Europe/London
Missing
America/Denver
America/Los_Angeles
America/Chicago
Unknown
America/New_York

0 200 400 600 800 1000 1200 1400

 

 

 

 

 

Figure 2-1. Top time zones in the 1.usa.gov sample data

Parsing all of the interesting information in these “agent” strings may seem like a
daunting task. Luckily, once you have mastered Python’s built-in string functions and
regular expression capabilities, it is really not so bad. For example, we could split off
the first token in the string (corresponding roughly to the browser capability) and make
another summary of the user behavior:

In [305]: results = Series([x.split()[0] for x in frame.a.dropna()])

In [306]: results[:5]

Out [306]:

0 Mozilla/5.0
1 GoogleMaps/RochesterNY

 

1.usa.gov data from bit.ly | 23

2 Mozilla/4.0

3 Mozilla/5.0

4 Mozilla/5.0

In [307]: results.value_counts()[:8]
Out [307]:

Mozilla/5.0 2594
Mozilla/4.0 601
GoogleMaps/RochesterNY 121
Opera/9.80 34
TEST_INTERNET_AGENT 24
GoogleProducer 21
Mozilla/6.0 5
BlackBerry8520/5.0.0.681 4

Now, suppose you wanted to decompose the top time zones into Windows and non-
Windows users. As a simplification, let’s say that a user is on Windows if the string
‘Windows' is in the agent string. Since some of the agents are missing, I’ll exclude these
from the data:

In [308]: cframe = frame[frame.a.notnull() ]

We want to then compute a value whether each row is Windows or not:

In [309]: operating system = np.where(cframe['a'].str.contains('Windows'),
satay 62 ‘Windows', 'Not Windows')

In [310]: operating system[:5]
Out [310]:
Windows
Not Windows
Windows
Not Windows
Windows

S2SRPWNPF OC

ame: a

Then, you can group the data by its time zone column and this new list of operating
systems:

In [311]: by_tz_os = cframe.groupby(['tz', operating system])
The group counts, analogous to the value_counts function above, can be computed
using size. This result is then reshaped into a table with unstack:

In [312]: agg counts = by_tz_os.size().unstack().fillna(o)

In [313]: agg counts[:10]

Out [313]:
a Not Windows Windows
tz

245 276
Africa/Cairo 0 3
Africa/Casablanca 0 1
Africa/Ceuta 0 2
Africa/Johannesburg 0 1
Africa/Lusaka 0 1

 

24 | Chapter 2: Introductory Examples

America/Anchorage
America/Argentina/Buenos Aires
America/Argentina/Cordoba
America/Argentina/Mendoza

oOoRSs
rFPROR

Finally, let’s select the top overall time zones. To do so, I construct an indirect index
array from the row counts in agg counts:

# Use to sort in ascending order
In [314]: indexer = agg counts.sum(1).argsort()

In [315]: indexer[:10]

Out [315]:
tz

24
Africa/Cairo 20
Africa/Casablanca 21
Africa/Ceuta 92
Africa/Johannesburg 87
Africa/Lusaka 53
America/Anchorage 54
America/Argentina/Buenos Aires 57
America/Argentina/Cordoba 26
America/Argentina/Mendoza 55

I then use take to select the rows in that order, then slice off the last 10 rows:

In [316]: count_subset = agg counts.take(indexer)[-10: ]

In [317]: count_subset

Out [317]:
a Not Windows Windows
tz
America/Sao_Paulo 13 20
Europe/Madrid 16 19
Pacific/Honolulu 0 36
Asia/Tokyo D 35
Europe/London 43 31
America/Denver 132 59
America/Los_ Angeles 130 252
America/Chicago 115 285
245 276
America/New_York 339 912

Then, as shown in the preceding code block, this can be plotted in a bar plot; I'll make
it a stacked bar plot by passing stacked=True (see Figure 2-2) :

In [319]: count_subset.plot(kind="barh', stacked=True)
The plot doesn’t make it easy to see the relative percentage of Windows users in the

smaller groups, but the rows can easily be normalized to sum to 1 then plotted again
(see Figure 2-3):

In [321]: normed_subset = count_subset.div(count_subset.sum(1), axis=0)

In [322]: normed_subset.plot(kind='barh', stacked=True)

 

1.usa.gov data from bit.ly | 25

 

America/New_York

America/Chicago
America/Los_Angeles

America/Denver

(Zz

1

Europe/London
Asia/Tokyo|
Pacific/Honolulu

a
MH Not Windows
America/S20 Paulo iy GH Windows

1 1 1 1 re 1
200 400 600 800 1000 1200 1400

Europe/Madrid

 

 

 

 

o

 

 

 

Figure 2-2. Top time zones by Windows and non-Windows users

 

  
 

America/New_York a
H Not Windows
GH Windows

America/Chicago
America/Los_Angeles

America/Denver

tz

Europe/London
Asia/Tokyo|
Pacific/Honolulu

Europe/Madrid

 

 

America/Sao_Paulo

 

°
°
°
N
°
B
°
a
°
©
BH
°

 

 

 

Figure 2-3. Percentage Windows and non-Windows users in top-occurring time zones

All of the methods employed here will be examined in great detail throughout the rest
of the book.

MovieLens 1M Data Set

GroupLens Research (http://www.grouplens.org/node/73) provides a number of collec-
tions of movie ratings data collected from users of MovieLens in the late 1990s and

 

26 | Chapter 2: Introductory Examples

early 2000s. The data provide movie ratings, movie metadata (genres and year), and
demographic data about the users (age, zip code, gender, and occupation). Such data
is often of interest in the development of recommendation systems based on machine
learning algorithms. While I will not be exploring machine learning techniques in great
detail in this book, I will show you how to slice and dice data sets like these into the
exact form you need.

The MovieLens 1M data set contains 1 million ratings collected from 6000 users on
4000 movies. It’s spread across 3 tables: ratings, user information, and movie infor-
mation. After extracting the data from the zip file, each table can be loaded into a pandas
DataFrame object using pandas.read_table:

import pandas as pd
unames = ['user_id', 'gender', ‘age’, ‘occupation’, ‘zip']

users = pd.read_table('ml-1m/users.dat', sep='::', header=None,
names=unames )

rnames = ['user_id', 'movie id', ‘rating’, ‘timestamp’ ]
ratings = pd.read_table('ml-1m/ratings.dat', sep='::', header=None,
names=rnames )

mnames = ['movie_id', 'title', 'genres']
movies = pd.read_table('ml-1m/movies.dat', sep='::', header=None,
names=mnames )

You can verify that everything succeeded by looking at the first few rows of each Da-
taFrame with Python's slice syntax:

In [334]: users[:5]

Out [334]:

user_id gender age occupation zip
0 1 F 1 10 48067
1 2 M 56 16 70072
2 3 M 25 15 55117
3 4 M 45 7 02460
4 5 M 25 20 55455

In [335]: ratings[:5]

Out [335]:

user_id movie_id rating timestamp
0 1 1193 5 978300760
1 1 661 3 978302109
2 1 914 3 978301968
3 1 3408 4 978300275
4 1 2355 5 978824291
In [336]: movies[:5]
Out [336]:

movie_id title genres
0 1 Toy Story (1995) Animation|Children's|Comedy
1 2 Jumanji (1995) Adventure|Children's|Fantasy
2 3 Grumpier Old Men (1995) Comedy | Romance
3 4 Waiting to Exhale (1995) Comedy | Drama

 

MovieLens 1M DataSet | 27

4 5 Father of the Bride Part II (1995) Comedy

In [337]: ratings

Out [337]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 1000209 entries, 0 to 1000208
Data columns:

user_id 1000209 non-null values
movie_id 1000209 non-null values
rating 1000209 non-null values

timestamp 1000209 non-null values
dtypes: int64(4)

Note that ages and occupations are coded as integers indicating groups described in
the data set’s README file. Analyzing the data spread across three tables is not a simple
task; for example, suppose you wanted to compute mean ratings for a particular movie
by sex and age. As you will see, this is much easier to do with all of the data merged
together into a single table. Using pandas’s merge function, we first merge ratings with
users then merging that result with the movies data. pandas infers which columns to
use as the merge (or join) keys based on overlapping names:

In [338]: data = pd.merge(pd.merge(ratings, users), movies)

In [339]: data

Out [339]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 1000209 entries, 0 to 1000208
Data columns:

user_id 1000209 non-null values
movie_id 1000209 non-null values
rating 1000209 non-null values
timestamp 1000209 non-null values
gender 1000209 non-null values
age 1000209 non-null values
occupation 1000209 non-null values
zip 1000209 non-null values
title 1000209 non-null values
genres 1000209 non-null values

dtypes: int64(6), object(4)

In [340]: data.ix[o]

Out [340]:

user_id 1
movie_id 1
rating 5
timestamp 978824268
gender F
age 1
occupation 10
zip 48067
title Toy Story (1995)
genres Animation | Children's | Comedy
Name: 0

 

28 | Chapter2: Introductory Examples

In this form, aggregating the ratings grouped by one or more user or movie attributes
is straightforward once you build some familiarity with pandas. To get mean movie
ratings for each film grouped by gender, we can use the pivot_table method:

In [341]: mean_ratings = data.pivot_table('rating', rows='title',
ere ; cols='gender', aggfunc='mean' )

In [342]: mean_ratings[:5]

Out [342]:

gender F M
title

$1,000,000 Duck (1971) 3.375000 2.761905
"Night Mother (1986) 3.388889 3.352941
'Til There Was You (1997) 2.675676 2.733333
"burbs, The (1989) 2.793478 2.962085

..-And Justice for All (1979) 3.828571 3.689024

This produced another DataFrame containing mean ratings with movie totals as row
labels and gender as column labels. First, I’m going to filter down to movies that re-
ceived at least 250 ratings (a completely arbitrary number); to do this, I group the data
by title and use size() to get a Series of group sizes for each title:

In [343]: ratings by title = data.groupby('title').size()

In [344]: ratings by title[:10]

Out [344]:

title

$1,000,000 Duck (1971) 37
"Night Mother (1986) 70
"Til There Was You (1997) 52
"burbs, The (1989) 303
...And Justice for All (1979) 199
1-900 (1994) 2
10 Things I Hate About You (1999) 700
101 Dalmatians (1961) 565
101 Dalmatians (1996) 364
12 Angry Men (1957) 616

In [345]: active titles = ratings by title.index[ratings by title >= 250]

In [346]: active titles

Out [346]:

Index(['burbs, The (1989), 10 Things I Hate About You (1999),
101 Dalmatians (1961), ..., Young Sherlock Holmes (1985),
Zero Effect (1998), eXistenZ (1999)], dtype=object)

The index of titles receiving at least 250 ratings can then be used to select rows from
mean_ratings above:

In [347]: mean_ratings = mean_ratings.ix[active titles]

In [348]: mean_ratings

Out [348]:

<class 'pandas.core. frame.DataFrame' >

Index: 1216 entries, ‘burbs, The (1989) to eXistenZ (1999)

 

MovieLens 1M DataSet | 29

Data columns:

F 1216 non-null values
M 1216 non-null values
dtypes: float64(2)

To see the top films among female viewers, we can sort by the F column in descending
order:

In [350]: top female ratings = mean_ratings.sort_index(by='F', ascending=False)

In [351]: top female ratings[:10]

Out [351]:

gender E M
Close Shave, A (1995) 4.644444 4.473795
Wrong Trousers, The (1993) 4.588235 4.478261
Sunset Blvd. (a.k.a. Sunset Boulevard) (1950) 4.572650 4.464589
Wallace & Gromit: The Best of Aardman Animation (1996) 4.563107 4.385075
Schindler's List (1993) 4.562602 4.491415
Shawshank Redemption, The (1994) 4.539075 4.560625
Grand Day Out, A (1992) 4.537879 4.293255
To Kill a Mockingbird (1962) 4.536667 4.372611
Creature Comforts (1990) 4.513889 4.272277
Usual Suspects, The (1995) 4.513317 4.518248

Measuring rating disagreement

Suppose you wanted to find the movies that are most divisive between male and female
viewers. One way is to add a column to mean_ratings containing the difference in
means, then sort by that:

In [352]: mean_ratings['diff'] = mean_ratings['M'] - mean_ratings['F']

Sorting by 'diff' gives us the movies with the greatest rating difference and which were
preferred by women:

In [353]: sorted _by diff = mean_ratings.sort_index(by='diff' )

In [354]: sorted by diff[:15]

Out [354]:

gender F M diff
Dirty Dancing (1987) 3.790378 2.959596 -0.830782
Jumpin' Jack Flash (1986) 3.254717 2.578358 -0.676359
Grease (1978) 3.975265 3.367041 -0.608224
Little Women (1994) 3.870588 3.321739 -0.548849
Steel Magnolias (1989) 3.901734 3.365957 -0.535777
Anastasia (1997) 3.800000 3.281609 -0.518391
Rocky Horror Picture Show, The (1975) 3.673016 3.160131 -0.512885
Color Purple, The (1985) 4.158192 3.659341 -0.498851
Age of Innocence, The (1993) 3.827068 3.339506 -0.487561
Free Willy (1993) 2.921348 2.438776 -0.482573
French Kiss (1995) 3.535714 3.056962 -0.478752
Little Shop of Horrors, The (1960) 3.650000 3.179688 -0.470312
Guys and Dolls (1955) 4.051724 3.583333 -0.468391
Mary Poppins (1964) 4.197740 3.730594 -0.467147
Patch Adams (1998) 3.473282 3.008746 -0.464536

 

30 | Chapter2: Introductory Examples

Reversing the order of the rows and again slicing off the top 15 rows, we get the movies
preferred by men that women didn’t rate as highly:

# Reverse order of rows, take first 15 rows
In [355]: sorted by diff[::-1][:15]

Out[355]:

gender F M diff
Good, The Bad and The Ugly, The (1966) 3.494949 4.221300 0.726351
Kentucky Fried Movie, The (1977) 2.878788 3.555147 0.676359
Dumb & Dumber (1994) 2.697987 3.336595 0.638608
Longest Day, The (1962) 3.411765 4.031447 0.619682
Cable Guy, The (1996) 2.250000 2.863787 0.613787
Evil Dead II (Dead By Dawn) (1987) 3.297297 3.909283 0.611985
Hidden, The (1987) 3.137931 3.745098 0.607167
Rocky III (1982) 2.361702 2.943503 0.581801
Caddyshack (1980) 3.396135 3.969737 0.573602
For a Few Dollars More (1965) 3.409091 3.953795 0.544704
Porky's (1981) 2.296875 2.836364 0.539489
Animal House (1978) 3.628906 4.167192 0.538286
Exorcist, The (1973) 3.537634 4.067239 0.529605
Fright Night (1985) 2.973684 3.500000 0.526316
Barb Wire (1996) 1.585366 2.100386 0.515020

Suppose instead you wanted the movies that elicited the most disagreement among
viewers, independent of gender. Disagreement can be measured by the variance or
standard deviation of the ratings:

# Standard deviation of rating grouped by title
In [356]: rating std_by title = data.groupby('title')['rating'].std()

# Filter down to active titles
In [357]: rating std_by title = rating std_by title.ix[active titles]

# Order Series by value in descending order
In [358]: rating _std_by title.order(ascending=False) | :10]

Out [358]:

title

Dumb & Dumber (1994) 1.321333
Blair Witch Project, The (1999) 1.316368
Natural Born Killers (1994) 1.307198
Tank Girl (1995) 1.277695
Rocky Horror Picture Show, The (1975) 1.260177
Eyes Wide Shut (1999) 1.259624
Evita (1996) 1.253631
Billy Madison (1995) 1.249970
Fear and Loathing in Las Vegas (1998) 1.246408
Bicentennial Man (1999) 1.245533

Name: rating

You may have noticed that movie genres are given as a pipe-separated (|) string. If you
wanted to do some analysis by genre, more work would be required to transform the
genre information into a more usable form. I will revisit this data later in the book to
illustrate such a transformation.

 

MovieLens 1M DataSet | 31

US Baby Names 1880-2010

The United States Social Security Administration (SSA) has made available data on the
frequency of baby names from 1880 through the present. Hadley Wickham, an author
of several popular R packages, has often made use of this data set in illustrating data
manipulation in R.

In [4]: names.head(10)

Out [4]:

name sex births year
0 Mary F 7065 1880
1 Anna F 2604 1880
2 Emma F 2003 1880
3 Elizabeth F 1939 1880
4 Minnie F 1746 1880
5 Margaret F 1578 1880
6 Ida F 1472 1880
7 Alice F 1414 1880
8 Bertha F 1320 1880
9 Sarah F 1288 1880

There are many things you might want to do with the data set:

¢ Visualize the proportion of babies given a particular name (your own, or another
name) over time.

¢ Determine the relative rank of a name.

¢ Determine the most popular names in each year or the names with largest increases
or decreases.

e Analyze trends in names: vowels, consonants, length, overall diversity, changes in
spelling, first and last letters

e Analyze external sources of trends: biblical names, celebrities, demographic
changes

Using the tools we’ve looked at so far, most of these kinds of analyses are very straight-
forward, so I will walk you through many of them. I encourage you to download and
explore the data yourself. If you find an interesting pattern in the data, I would love to
hear about it.

As of this writing, the US Social Security Administration makes available data files, one
per year, containing the total number of births for each sex/name combination. The
raw archive of these files can be obtained here:

http: //www.ssa.gov/oact/babynames/limits .html

In the event that this page has been moved by the time you’re reading this, it can most
likely be located again by Internet search. After downloading the “National data” file
names.zip and unzipping it, you will have a directory containing a series of files like
yob1880.txt. I use the UNIX head command to look at the first 10 lines of one of the
files (on Windows, you can use the more command or open it in a text editor):

 

32 | Chapter2: Introductory Examples

In [367]: !head -n 10 names/yob1880.txt
Mary,F, 7065
Anna,F, 2604
Emma , F , 2003
Elizabeth, F,1939
Minnie,F,1746
Margaret,F,1578
Ida, F,1472
Alice,F,1414
Bertha, F,1320
Sarah, F,1288

As this is a nicely comma-separated form, it can be loaded into a DataFrame with
pandas.read_csv:

In [368]: import pandas as pd
In [369]: names1880 = pd.read_csv('names/yob1880.txt', names=['name', 'sex', ‘births'])

In [370]: names1880

Out [370]:

<class 'pandas.core. frame.DataFrame' >
Int64Index: 2000 entries, 0 to 1999
Data columns:

name 2000 non-null values

sex 2000 non-null values
births 2000 non-null values
dtypes: int64(1), object(2)

These files only contain names with at least 5 occurrences in each year, so for simplic-
ity’s sake we can use the sum of the births column by sex as the total number of births
in that year:

In [371]: names1880.groupby('sex').births.sum()

Out [371]:

sex

F 90993
M 110493

Name: births

Since the data set is split into files by year, one of the first things to do is to assemble
all of the data into a single DataFrame and further to add a year field. This is easy to
do using pandas. concat:

# 2010 is the last available year right now
years = range(1880, 2011)

pieces = []
columns = ['name', ‘sex', ‘births']

for year in years:
path = 'names/yob%d.txt' % year
frame = pd.read_csv(path, names=columns)

frame['year'] = year
pieces .append(frame)

 

US Baby Names 1880-2010 | 33

# Concatenate everything into a single DataFrame
names = pd.concat(pieces, ignore _index=True)

There are a couple things to note here. First, remember that concat glues the DataFrame
objects together row-wise by default. Secondly, you have to pass ignore _index=True
because we’re not interested in preserving the original row numbers returned from
read_csv. So we now have a very large DataFrame containing all of the names data:

Now the names DataFrame looks like:

In [373]: names

Out [373]:

<class 'pandas.core. frame.DataFrame' >
Int64Index: 1690784 entries, 0 to 1690783
Data columns:

name 1690784 non-null values
sex 1690784 non-null values
births 1690784 non-null values
year 1690784 non-null values

dtypes: int64(2), object(2)

With this data in hand, we can already start aggregating the data at the year and sex
level using groupby or pivot_table, see Figure 2-4:

In [374]: total_births = names.pivot_table('births', rows='year',
ere ; cols='sex', aggfunc=sum)

In [375]: total_births.tail()
Out [375]:

sex F M

year

2006 1896468 2050234

2007 1916888 2069242

2008 1883645 2032310

2009 1827643 1973359

2010 1759010 1898382

In [376]: total_births.plot(title='Total births by sex and year')

Next, let’s insert a column prop with the fraction of babies given each name relative to
the total number of births. A prop value of 0.02 would indicate that 2 out of every 100
babies was given a particular name. Thus, we group the data by year and sex, then add
the new column to each group:

def add_prop(group):

# Integer division floors
births = group.births.astype(float)

group['prop'] = births / births.sum()
return group
names = names.groupby(['year', 'sex']).apply(add_prop)

 

34 | Chapter2: Introductory Examples

 

 

se00000 : Total births by sex and year

2000000
1500000
1000000;

500000

 

 

 

a a |
ao 1900 1920 1940 1960 1980 2000 2020
year

 

 

Figure 2-4. Total births by sex and year

    
 

Vs

4

; Remember that because births is of integer type, we have to cast either

the numerator or denominator to floating point to compute a fraction
“12° (unless you are using Python 3!).

 

The resulting complete data set now has the following columns:

In [378]: names

Out [378]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 1690784 entries, 0 to 1690783
Data columns:

name 1690784 non-null values
sex 1690784 non-null values
births 1690784 non-null values
year 1690784 non-null values
prop 1690784 non-null values

dtypes: float64(1), int64(2), object(2)

When performing a group operation like this, it's often valuable to do a sanity check,
like verifying that the prop column sums to 1 within all the groups. Since this is floating
point data, use np.allclose to check that the group sums are sufficiently close to (but
perhaps not exactly equal to) 1:

In [379]: np.allclose(names.groupby(['year', 'sex']).prop.sum(), 1)
Out[379]: True

Now that this is done, I’m going to extract a subset of the data to facilitate further
analysis: the top 1000 names for each sex/year combination. This is yet another group
operation:

def get_top1000(group) :
return group.sort_index(by='births', ascending=False)[:1000]

 

US Baby Names 1880-2010 | 35

 

grouped = names.groupby(['year', 'sex'])
top1000 = grouped.apply(get_top1000)

i}

If you prefer a do-it-yourself approach, you could also do:

pieces = []
for year, group in names.groupby(['year', 'sex']):

pieces. append(group.sort_index(by="births', ascending=False) | :1000])
top1000 = pd.concat(pieces, ignore _index=True)

The resulting data set is now quite a bit smaller:

In [382]: top1000

Out [382]:

<class 'pandas.core. frame.DataFrame' >
Int64Index: 261877 entries, 0 to 261876
Data columns:

name 261877 non-null values
sex 261877 non-null values
births 261877 non-null values
year 261877 non-null values
prop 261877 non-null values

dtypes: float64(1), int64(2), object(2)

We'll use this Top 1,000 data set in the following investigations into the data.

Analyzing Naming Trends

With the full data set and Top 1,000 data set in hand, we can start analyzing various
naming trends of interest. Splitting the Top 1,000 names into the boy and girl portions
is easy to do first:

In [383]: boys = top1000[top1000.sex == 'M']
In [384]: girls = top1000[top1000.sex == 'F']

Simple time series, like the number of Johns or Marys for each year can be plotted but
require a bit of munging to be a bit more useful. Let’s form a pivot table of the total
number of births by year and name:

In [385]: total_births = top1000.pivot_table('births', rows='year', cols='name',
: aggfunc=sum)

Now, this can be plotted for a handful of names using DataFrame’s plot method:

In [386]: total_births

Out [386]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 131 entries, 1880 to 2010
Columns: 6865 entries, Aaden to Zuri
dtypes: float64(6865)

In [387]: subset = total_births[['John', ‘Harry’, 'Mary', 'Marilyn']]

In [388]: subset.plot(subplots=True, figsize=(12, 10), grid=False,
evarey of title="Number of births per year")

 

36 | Chapter2: Introductory Examples

See Figure 2-5 for the result. On looking at this, you might conclude that these names
have grown out of favor with the American population. But the story is actually more
complicated than that, as will be explored in the next section.

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

 

Number of births per year
we98
80
70000! — John
60000}
50000}
40000
30000}
20000}
10000
year

10000
B00) — Harry
6000)
4000
2000)

° year
80000;
70000} —_
60000} Mary||
50000}
40000
30000}
20000}
10000

o year
12000
10000) — Marilyn
8000)
6000)
4000
2000
es) soe so °° we oe age aw

year

 

 

 

Figure 2-5. A few boy and girl names over time

Measuring the increase in naming diversity

One explanation for the decrease in plots above is that fewer parents are choosing
common names for their children. This hypothesis can be explored and confirmed in
the data. One measure is the proportion of births represented by the top 1000 most
popular names, which I aggregate and plot by year and sex:

n [390]: table = top1000.pivot_table('prop', rows='year',
awn et cols='sex', aggfunc=sum)

In [3 91]: table.plot(title='Sum of table1000.prop by year and sex',
awmast yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10))

See Figure 2-6 for this plot. So you can see that, indeed, there appears to be increasing
name diversity (decreasing total proportion in the top 1,000). Another interesting met-
ric is the number of distinct names, taken in order of popularity from highest to lowest,
in the top 50% of births. This number is a bit more tricky to compute. Let’s consider
just the boy names from 2010:

n [392]: df = boys[boys.year == 2010]

n [393]: df

Out [393]:

<class 'pandas.core. frame.DataFrame' >
Int64Index: 1000 entries, 260877 to 261876
Data columns:

 

US Baby Names 1880-2010 | 37

name 1000 non-null values

sex 1000 non-null values
births 1000 non-null values
year 1000 non-null values
prop 1000 non-null values

dtypes: float64(1), int64(2), object(2)

 

Sum of table1000.prop by year and sex

 

 

OR 80 1890 1900 1910 1920 1930 1940 1950 1960 1970 1980 1990 2000 2010
year

 

 

 

Figure 2-6. Proportion of births represented in top 1000 names by sex

After sorting prop in descending order, we want to know how many of the most popular
names it takes to reach 50%. You could write a for loop to do this, but a vectorized
NumPy way is a bit more clever. Taking the cumulative sum, cumsum, of prop then calling
the method searchsorted returns the position in the cumulative sum at which 0.5 would
need to be inserted to keep it in sorted order:

In [394]: prop_cumsum = df.sort_index(by='prop', ascending=False) .prop.cumsum()

In [395]: prop_cumsum|[ :10]

Out [395]:

260877 0.011523
260878 0.020934
260879 0.029959
260880 0.038930
260881 0.047817
260882 0.056579
260883 0.065155
260884 0.073414
260885 0.081528
260886 0.089621

In [396]: prop_cumsum.searchsorted(0.5)
Out[396]: 116

 

38 | Chapter2: Introductory Examples

Since arrays are zero-indexed, adding 1 to this result gives you a result of 117. By con-
trast, in 1900 this number was much smaller:

In [397]: df = boys[boys.year == 1900]
In [398]: in1900 = df.sort_index(by='prop', ascending=False) .prop.cumsum()

In [399]: in1900.searchsorted(0.5) + 1
Out [399]: 25

It should now be fairly straightforward to apply this operation to each year/sex com-
bination; groupby those fields and apply a function returning the count for each group:

def get_quantile_count(group, q=0.5):
group = group.sort_index(by='prop', ascending=False)
return group.prop.cumsum().searchsorted(q) + 1

diversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)
diversity = diversity.unstack('sex')

This resulting DataFrame diversity now has two time series, one for each sex, indexed
by year. This can be inspected in [Python and plotted as before (see Figure 2-7):

In [401]: diversity.head()
Out[ 401]:

sex FM

year

1880 38 14

1881 38 14

1882 38 15

1883 39 15

1884 39 16

In [402]: diversity.plot(title="Number of popular names in top 50%")

 

Number of popular names in top 50%

250

 

 

 

 

 

tao 1900 1920 1940 1960 1980 2000 2020
year

 

 

 

Figure 2-7. Plot of diversity metric by year

 

US Baby Names 1880-2010 | 39

As you can see, girl names have always been more diverse than boy names, and they
have only become more so over time. Further analysis of what exactly is driving the
diversity, like the increase of alternate spellings, is left to the reader.

The “Last letter” Revolution

In 2007, a baby name researcher Laura Wattenberg pointed out on her website (http:
//www.babynamewizard.com) that the distribution of boy names by final letter has
changed significantly over the last 100 years. To see this, I first aggregate all of the births
in the full data set by year, sex, and final letter:

# extract last letter from name column

get_last_letter = lambda x: x[-1]

last_letters = names.name.map(get_last_letter)
last_letters.name = 'last_letter'

table = names.pivot_table('births', rows=last_letters,
cols=['sex', 'year'], aggfunc=sum)

Then, I select out three representative years spanning the history and print the first few
rows:

In [404]: subtable = table.reindex(columns=[1910, 1960, 2010], level='year')

In [405]: subtable.head()

Out[405]:

sex F M

year 1910 1960 2010 1910 1960 2010
last_letter

a 108376 691247 670605 977 5204 28438
b NaN 694 450 411 3912 38859
c 5 49 946 482 15476 23125
d 6750 3729 2607 22111 262112 44398
e 133569 435013 313833 28655 178823 129012

Next, normalize the table by total births to compute a new table containing proportion
of total births for each sex ending in each letter:

In [406]: subtable.sum()

Out [406]:

sex year

F 1910 396416
1960 2022062
2010 1759010

M 1910 194198
1960 2132588
2010 1898382

In [407]: letter_prop = subtable / subtable.sum().astype(float)

With the letter proportions now in hand, I can make bar plots for each sex broken
down by year. See Figure 2-8:

import matplotlib.pyplot as plt

 

40 | Chapter 2: Introductory Examples

fig, axes = plt.subplots(2, 1, figsize=(10, 8))
letter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')
letter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female',

legend=False)

 

 

 

 

 

 

 

 

abcdefg@dhijktimnopqrstuvwxydaz
last_letter
Female

 

 

 

en |

abcde f G@hijktmnopaqrstuvwx yz

last_letter

 

Figure 2-8. Proportion of boy and girl names ending in each letter

As you can see, boy names ending in “n” have experienced significant growth since the
1960s. Going back to the full table created above, I again normalize by year and sex
and select a subset of letters for the boy names, finally transposing to make each column
a time series:

In [410]: letter_prop = table / table.sum().astype(float)

In [411]: dny_ts = letter_prop.ix[['d', 'n', 'y'], 'M'].T

In [412]: dny_ts.head()
Out [412]:

year
1880
1881
1882
1883
1884

d

0.083055
0.083247
0.085340
0.084066
0.086120

0.153213
0.153214
0.149560
0.151646
0.149915

y

0.075760
0.077451
0.077537
0.079144
0.080405

With this DataFrame of time series in hand, I can make a plot of the trends over time
again with its plot method (see Figure 2-9):

In [414]: dny_ts.plot()

 

US Baby Names 1880-2010 | 41

 

 

 

 

 

 

1 1 1 1 L 1
0.09380 1900 1920 1940 1960 1980 2000 2020

ear

 

 

 

Figure 2-9. Proportion of boys born with names ending in d/n/y over time

Boy names that became girl names (and vice versa)

Another fun trend is looking at boy names that were more popular with one sex earlier
in the sample but have “changed sexes” in the present. One example is the name Lesley
or Leslie. Going back to the top1000 dataset, I compute a list of names occurring in the
dataset starting with 'les1':

In [415]:
In [416]:
In [417]:

In [418]:
Out [418]:

all_names = top1000.name.unique()
mask = np.array(['lesl' in x.lower() for x in all_names])
lesley like = all_names[mask]

lesley like
array([Leslie, Lesley, Leslee, Lesli, Lesly], dtype=object)

From there, we can filter down to just those names and sum births grouped by name
to see the relative frequencies:

In [419]: filtered = top1000[top1000.name.isin(lesley like) ]

In [420]: filtered.groupby('name').births.sum()

Out [420]:

name

Leslee 1082
Lesley 35022
Lesli 929
Leslie 370429
Lesly 10067

Name: births

Next, let’s aggregate by sex and year and normalize within year:

 

42 | Chapter2: Introductory Examples

In [421]: table

In [422]: table

In [423]: table.

Out [423]:

sex
year
2006
2007
2008
2009
2010

F

1.
1.
1.
1.
1.

M

NaN
NaN
NaN
NaN
NaN

= filtered.pivot_table('births', rows='year',
cols='sex', aggfunc='sum')

= table.div(table.sum(1), axis=0)

tail()

Lastly, it’s now easy to make a plot of the breakdown by sex over time (Figure 2-10):
In [425]: table.plot(style={'M': 'k-', 'F': 'k--'})

 

 

 

 

 

 

O80

 

1920

1940

year

1960 1980 2000 2020

 

 

Figure 2-10. Proportion of male/female Lesley-like names over time

Conclusions and The Path Ahead

The examples in this chapter are rather simple, but they’re here to give you a bit of a
flavor of what sorts of things you can expect in the upcoming chapters. The focus of
this book is on tools as opposed to presenting more sophisticated analytical methods.
Mastering the techniques in this book will enable you to implement your own analyses
(assuming you know what you want to do!) in short order.

 

Conclusions and The Path Ahead | 43


CHAPTER 3
IPython: An Interactive Computing and
Development Environment

 

Act without doing; work without effort. Think of the small as large and the few as many.
Confront the difficult while it is still easy; accomplish the great task by a series of small
acts.

—Laozi

People often ask me, “What is your Python development environment?” My answer is
almost always the same, “IPython and a text editor”. You may choose to substitute an
Integrated Development Environment (IDE) for a text editor in order to take advantage
of more advanced graphical tools and code completion capabilities. Even if so, I strongly
recommend making [Python an important part of your workflow. Some IDEs even
provide [Python integration, so it’s possible to get the best of both worlds.

The [Python project began in 2001 as Fernando Pérez’s side project to make a better
interactive Python interpreter. In the subsequent 11 years it has grown into what’s
widely considered one of the most important tools in the modern scientific Python
computing stack. While it does not provide any computational or data analytical tools
by itself, IPython is designed from the ground up to maximize your productivity in both
interactive computing and software development. It encourages an execute-explore
workflow instead of the typical edit-compile-run workflow of many other programming
languages. It also provides very tight integration with the operating system’s shell and
file system. Since much of data analysis coding involves exploration, trial and error,
and iteration, [Python will, in almost all cases, help you get the job done faster.

Of course, the [Python project now encompasses a great deal more than just an en-
hanced, interactive Python shell. It also includes a rich GUI console with inline plotting,
a web-based interactive notebook format, and a lightweight, fast parallel computing
engine. And, as with so many other tools designed for and by programmers, it is highly
customizable. I’ll discuss some of these features later in the chapter.

 

45

Since IPython has interactivity at its core, some of the features in this chapter are dif-
ficult to fully illustrate without a live console. If this is your first time learning about
IPython, I recommend that you follow along with the examples to get a feel for how
things work. As with any keyboard-driven console-like environment, developing mus-
cle-memory for the common commands is part of the learning curve.

 

 

Va,
oe Many parts of this chapter (for example: profiling and debugging) can
43 be safely omitted on a first reading as they are not necessary for under-
“s §|a* standing the rest of the book. This chapter is intended to provide a

standalone, rich overview of the functionality provided by IPython.

|Python Basics

You can launch [Python on the command line just like launching the regular Python
interpreter except with the ipython command:

$ ipython
Python 2.7.2 (default, May 27 2012, :12)
Type "copyright", "credits" or "license" for more information.

IPython 0.12 -- An enhanced Interactive Python.

2 -> Introduction and overview of IPython's features.
*quickref -> Quick reference.

help -> Python's own help system.

object? -> Details about ‘object’, use 'object??' for extra details.

In [1]: a=5

In [2]: a
Out[2]: 5

You can execute arbitrary Python statements by typing them in and pressing
<return>. When typing just a variable into [Python, it renders a string representation
of the object:

In [541]: import numpy as np
In [542]: data = {i : randn() for i in range(7)}

In [543]: data

Out [543]:

{0: 0.6900018528091594,
1: 1.0015434424937888,
2: -0.5030873913603446,
3: -0.6222742250596455,
4: -0.9211686080130108,
5: -0.726213492660829,
6: 0.2228955458351768}

 

46 | Chapter3: IPython: An Interactive Computing and Development Environment

Many kinds of Python objects are formatted to be more readable, or pretty-printed,
which is distinct from normal printing with print. If you printed a dict like the above
in the standard Python interpreter, it would be much less readable:

>>> from numpy.random import randn

>>> data = {i : randn() for i in range(7)}

>>> print data

{0: -1.5948255432744511, 1: 0.10569006472787983, 2: 1.972367135977295,

3: 0.15455217573074576, 4: -0.24058577449429575, 5: -1.2904897053651216,

6: 0.3308507317325902}

IPython also provides facilities to make it easy to execute arbitrary blocks of code (via
somewhat glorified copy-and-pasting) and whole Python scripts. These will be dis-
cussed shortly.

Tab Completion

On the surface, the [Python shell looks like a cosmetically slightly-different interactive
Python interpreter. Users of Mathematica may find the enumerated input and output
prompts familiar. One of the major improvements over the standard Python shell is
tab completion, a feature common to most interactive data analysis environments.
While entering expressions in the shell, pressing <Tab> will search the namespace for
any variables (objects, functions, etc.) matching the characters you have typed so far:

In [1]: an_apple = 27
In [2]: an_example = 42

In [3]: an<Tab>
an_apple and an_example any

In this example, note that [Python displayed both the two variables I defined as well as
the Python keyword and and built-in function any. Naturally, you can also complete
methods and attributes on any object after typing a period:

In [3]: b = [1, 2, 3]

In [4]: b.<Tab>
b.append b.extend b.insert b.remove  b.sort
b.count b. index b.pop b.reverse

The same goes for modules:

In [1]: import datetime

In [2]: datetime.<Tab>

datetime.date datetime .MAXYEAR datetime. timedelta
datetime. datetime datetime .MINYEAR datetime.tzinfo
datetime.datetime_CAPI datetime.time

 

[Python Basics | 47

Note that IPython by default hides methods and attributes starting with
underscores, such as magic methods and internal “private” methods
#ia° and attributes, in order to avoid cluttering the display (and confusing
new Python users!). These, too, can be tab-completed but you must first
type an underscore to see them. If you prefer to always see such methods
in tab completion, you can change this setting in the [Python configu-
ration.

  

 

Tab completion works in many contexts outside of searching the interactive namespace
and completing object or module attributes. When typing anything that looks like a file
path (even in a Python string), pressing <Tab> will complete anything on your com-
puter’s file system matching what you’ve typed:

In [3]: book_scripts/<Tab>

book_scripts/cprof_example.py book_scripts/ipython_script_test.py
book_scripts/ipython_bug.py book_scripts/prof_mod.py

In [3]: path = 'book_scripts/<Tab>

book_scripts/cprof_example.py book_scripts/ipython_script_test.py
book_scripts/ipython_bug.py book_scripts/prof_mod.py

Combined with the %run command (see later section), this functionality will undoubt-
edly save you many keystrokes.

Another area where tab completion saves time is in the completion of function keyword
arguments (including the = sign!).

Introspection

Using a question mark (?) before or after a variable will display some general informa-
tion about the object:

In [545]: b?

Type: list

String Form:[1, 2, 3]

Length: 3

Docstring:

list() -> new empty list

list(iterable) -> new list initialized from iterable's items

This is referred to as object introspection. If the object is a function or instance method,
the docstring, if defined, will also be shown. Suppose we’d written the following func-
tion:

def add_numbers(a, b):

Add two numbers together

Returns

the_sum : type of arguments

 

48 | Chapter3: IPython: An Interactive Computing and Development Environment

return a + b

Then using ? shows us the docstring:
In [547]: add_numbers?

Type: function

String Form:<function add_numbers at Ox5fad848>

File: book_scripts/<ipython-input-546-5473012eeb65>
Definition: add_numbers(a, b)

Docstring:

Add two numbers together

Returns

the_sum : type of arguments

Using ?? will also show the function’s source code if possible:

In [548]: add_numbers??

Type: function

String Form:<function add_numbers at 0x5fad848>

File: book_scripts/<ipython-input-546-5473012eeb65>
Definition: add_numbers(a, b)

Source:

def add_numbers(a, b):

Add two numbers together
Returns

the_sum : type of arguments

return a + b

? has a final usage, which is for searching the [Python namespace in a manner similar
to the standard UNIX or Windows command line. A number of characters combined
with the wildcard (*) will show all names matching the wildcard expression. For ex-
ample, we could get a list of all functions in the top level NumPy namespace containing
load:

In [549]: np.*load*?

np. load

np. loads

np. loadtxt
np.pkgload

The %run Command

Any file can be run as a Python program inside the environment of your IPython session
using the %run command. Suppose you had the following simple script stored in ipy
thon_script_test.py:

def f(x, y, z):
return (x + y) /z

a=5

 

[Python Basics | 49

b=6
c= 7.5

result = f(a, b, c)

This can be executed by passing the file name to %run:

In [550]: %run ipython_script_test.py

The script is run in an empty namespace (with no imports or other variables defined)
so that the behavior should be identical to running the program on the command line
using python script.py. All of the variables (imports, functions, and globals) defined
in the file (up until an exception, if any, is raised) will then be accessible in the [Python

shell:

In [551]: c
Out[551]: 7.5

In [552]: result
Out[552]: 1.4666666666666666

If a Python script expects command line arguments (to be found in sys.argv), these
can be passed after the file path as though run on the command line.

Vs
:
sO Should you wish to give a script access to variables already defined in
“ Ss the interactive [Python namespace, use %run -i instead of plain %run.

 

 

Interrupting running code

Pressing <Ctrl-C> while any code is running, whether a script through %run or a long-
running command, will cause a KeyboardInterrupt to be raised. This will cause nearly
all Python programs to stop immediately except in very exceptional cases.

When a piece of Python code has called into some compiled extension
— ta) modules, pressing <Ctr1-C> will not cause the program execution to stop
immediately in all cases. In such cases, you will have to either wait until

control is returned to the Python interpreter, or, in more dire circum-
stances, forcibly terminate the Python process via the OS task manager.

 

Executing Code from the Clipboard

A quick-and-dirty way to execute code in [Python is via pasting from the clipboard.
This might seem fairly crude, but in practice it is very useful. For example, while de-
veloping a complex or time-consuming application, you may wish to execute a script
piece by piece, pausing at each stage to examine the currently loaded data and results.
Or, you might find a code snippet on the Internet that you want to run and play around
with, but you’d rather not create a new .py file for it.

 

50 | Chapter3: IPython: An Interactive Computing and Development Environment

Code snippets can be pasted from the clipboard in many cases by pressing <Ctr1l-Shift-
V>. Note that it is not completely robust as this mode of pasting mimics typing each
line into IPython, and line breaks are treated as <return>. This means that if you paste
code with an indented block and there is a blank line, [Python will think that the in-
dented block is over. Once the next line in the block is executed, an IndentationEr
ror will be raised. For example the following code:

x= 5

y=7

if x>5:

xt=1

y =8
will not work if simply pasted:
In [1]: x =5
In [2]: y=7

In [3]: if x > 5:
wcoie 8 x t= 1

In [4]: y=8
IndentationError: unexpected indent

If you want to paste code into IPython, try the paste and %cpaste
magic functions.

As the error message suggests, we should instead use the %paste and %cpaste magic
functions. %paste takes whatever text is in the clipboard and executes it as a single block

in the shell:
In [6]: %paste

x=5

y=7

if x>5:
x t= 1
y=8

## -- End pasted text --

Depending on your platform and how you installed Python, there’s a
~—tSs small chance that %paste will not work. Packaged distributions like
EPDFree (as described in in the intro) should not be a problem.

 

*cpaste is similar, except that it gives you a special prompt for pasting code into:

In [7]: %cpaste

Pasting code; enter '--' alone on the line to stop or use Ctrl-D.
2x = 5

7y=7

:if x > 5:

 

[Python Basics | 51

With the %cpaste block, you have the freedom to paste as much code as you like before
executing it. You might decide to use %cpaste in order to look at the pasted code before
executing it. If you accidentally paste the wrong code, you can break out of the
%cpaste prompt by pressing <Ctr1-C>.

Later, I’ll introduce the IPython HTML Notebook which brings a new level of sophis-
tication for developing analyses block-by-block in a browser-based notebook format
with executable code cells.

[Python interaction with editors and IDEs

Some text editors, such as Emacs and vim, have 3rd party extensions enabling blocks
of code to be sent directly from the editor to a running [Python shell. Refer to the
IPython website or do an Internet search to find out more.

Some IDEs, such as the PyDev plugin for Eclipse and Python Tools for Visual Studio
from Microsoft (and possibly others), have integration with the [Python terminal ap-
plication. If you want to work in an IDE but don’t want to give up the IPython console
features, this may be a good option for you.

Keyboard Shortcuts

IPython has many keyboard shortcuts for navigating the prompt (which will be familiar
to users of the Emacs text editor or the UNIX bash shell) and interacting with the shell’s
command history (see later section). Table 3-1 summarizes some of the most commonly
used shortcuts. See Figure 3-1 for an illustration of a few of these, such as cursor move-
ment.

 

C-b Cf
+H
In [27]: a_variable In [27]: a_vari Ck
Ca Ce In [27]: Cu

 

 

 

Figure 3-1. Illustration of some of IPython’s keyboard shortcuts

 

52 | Chapter3: IPython: An Interactive Computing and Development Environment

Table 3-1.

Command

Standard IPython Keyboard Shortcuts

Description

Ctrl1-p or up-arrow Search backward in command history for commands starting with currently-entered text

Ctrl-nordown-arrow — Search forward in command history for commands starting with currently-entered text

Ctrl-r Readline-style reverse history search (partial matching)
Ctr1-Shift-v Paste text from clipboard

Ctrl-c Interrupt currently-executing code

Ctrl-a Move cursor to beginning of line

Ctrl-e Move cursor to end of line

Ctrl-k Delete text from cursor until end of line

Ctrl-u Discard all text on current line

Ctrl-f Move cursor forward one character

Ctrl-b Move cursor back one character

Ctrl-1 Clear screen

Exceptions and Tracebacks

If an exception is raised while %run-ing a script or executing any statement, [Python will
by default print a full call stack trace (traceback) with a few lines of context around the
position at each point in the stack.

In [553]: %run cho3/ipython_bug.py

AssertionError Traceback (most recent call last)
/home/wesm/code/ipython/IPython/utils/py3compat.pyc in execfile(fname, *where)
176 else:
177 filename = fname
--> 178 __builtin_.execfile(filename, *where)
book_scripts/ch03/ipython_bug.py in <module>()
13 throws_an_exception()
14
---> 15 calling things()
book_scripts/ch03/ipython_bug.py in calling things()

--->

11 def calling things():

12 works_fine()
13 throws_an_exception()
14

15 calling things()

book_scripts/ch03/ipython_bug.py in throws_an_exception()
7 a=5
8 b=6

----> 9 assert(a + b == 10)
10

11 def calling things():

AssertionError:

 

[Python Basics | 53

Having additional context by itself is a big advantage over the standard Python inter-
preter (which does not provide any additional context). The amount of context shown
can be controlled using the %xmode magic command, from minimal (same as the stan-
dard Python interpreter) to verbose (which inlines function argument values and more).
As you will see later in the chapter, you can step into the stack (using the %debug or
%pdb magics) after an error has occurred for interactive post-mortem debugging.

Magic Commands

IPython has many special commands, known as “magic” commands, which are de-
signed to facilitate common tasks and enable you to easily control the behavior of the
IPython system. A magic command is any command prefixed by the the percent symbol
%. For example, you can check the execution time of any Python statement, such as a
matrix multiplication, using the %timeit magic function (which will be discussed in
more detail later):

In [554]: a = np.random.randn(100, 100)

In [555]: %timeit np.dot(a, a)
10000 loops, best of 3: 69.1 us per loop

Magic commands can be viewed as command line programs to be run within the IPy-
thon system. Many of them have additional “command line” options, which can all be
viewed (as you might expect) using ?:

In [1]: %reset?
Resets the namespace by removing all names defined by the user.

Parameters

-f : force reset without asking for confirmation.

-s : 'Soft' reset: Only clears your namespace, leaving history intact.
References to objects may be kept. By default (without this option),
we do a ‘hard’ reset, giving you a new session and removing all
references to objects from the current session.

Examples

In [8]: 'a' in _ip.user_ns
Out[8]: True

In [9]: %reset -f

In [1]: 'a' in _ip.user_ns
Out[1]: False

 

54 | Chapter3: IPython: An Interactive Computing and Development Environment

Magic functions can be used by default without the percent sign, as long as no variable
is defined with the same name as the magic function in question. This feature is called
automagic and can be enabled or disabled using %automagic.

Since IPython’s documentation is easily accessible from within the system, I encourage
you to explore all of the special commands available by typing %quickref or %magic. I
will highlight a few more of the most critical ones for being productive in interactive
computing and Python development in [Python.

Table 3-2. Frequently-used IPython Magic Commands

Command Description

*quickref Display the IPython Quick Reference Card

magic Display detailed documentation for all of the available magic commands
*debug Enter the interactive debugger at the bottom of the last exception traceback
ahist Print command input (and optionally output) history

“pdb Automatically enter debugger after any exception

zpaste Execute pre-formatted Python code from clipboard

*cpaste Open a special prompt for manually pasting Python code to be executed
mreset Delete all variables / names defined in interactive namespace

“page OBJECT Pretty print the object and display it through a pager

“run script. py
“prun statement
stime statement

%timeit statement

Run a Python script inside IPython
Execute statement with cProfile and report the profiler output
Report the execution time of single statement

Run a statement multiple times to compute an emsemble average execution time. Useful for

timing code with very short execution time
Awho, zwho_ls, %whos _ Displayvariablesdefinedininteractive namespace, with varying levels ofinformation/ verbosity

%xdel variable Delete a variable and attempt to clear any references to the object in the IPython internals

 

Qt-based Rich GUI Console

The IPython team has developed a Qt framework-based GUI console, designed to wed
the features of the terminal-only applications with the features provided by a rich text
widget, like embedded images, multiline editing, and syntax highlighting. If you have
either PyQt or PySide installed, the application can be launched with inline plotting by
running this on the command line:

ipython qtconsole --pylab=inline
The Qt console can launch multiple IPython processes in tabs, enabling you to switch

between tasks. It can also share a process with the [Python HTML Notebook applica-
tion, which I'll highlight later.

 

IPython Basics | 55

 

a IPython
File Edit View Kernel Magic Window Help

For more information, type 'help(pylab)'.

In [1]: img = plt.imread('book_scripts/ch03/stinkbug.png' )
In [2]: imshow(img)
Cut[2]: <matplotlib.image.AxesImage at Ox42ece50>
0
50
100
150
200
250
300

350
Qo 100 200 300 400

In [3]: plot (randn(1000) .cumsum() )
cut[3]: [<matplotlib.lines.Line2D at 0x45406d0>]

 

 

 

 

Figure 3-2. IPython Qt Console

Matplotlib Integration and Pylab Mode

Part of why IPython is so widely used in scientific computing is that it is designed as a
companion to libraries like matplotlib and other GUI toolkits. Don’t worry if you have
never used matplotlib before; it will be discussed in much more detail later in this book.
If you create a matplotlib plot window in the regular Python shell, you'll be sad to find
that the GUI event loop “takes control” of the Python session until the plot window is
closed. That won’t work for interactive data analysis and visualization, so IPython has

 

56 | Chapter3: IPython: An Interactive Computing and Development Environment

implemented special handling for each GUI framework so that it will work seamlessly

with the shell.

The typical way to launch [Python with matplotlib integration is by adding the --

pylab flag (two dashes).
$ ipython --pylab

This will cause several things to happen. First [Python will launch with the default GUI
backend integration enabled so that matplotlib plot windows can be created with no
issues. Secondly, most of NumPy and matplotlib will be imported into the top level
interactive namespace to produce an interactive computing environment reminiscent
of MATLAB and other domain-specific scientific computing environments. It’s possi-
ble to do this setup by hand by using %gui, too (try running %gui? to find out how).

 

=. svn: python wos
File Edt View Bookmarks [Settings Help
Hane: Adj Close, Length: 252

fin (7): spy_close, plot?
|Outl 7]; <motplotlib, axes, AxesSubplot at 64496491G>

in (8): pit. flouret
jOutl eo: Gmoteletl ib, flgure.Figure ot 8x489ee18>

Hin (9: spy_close,plot()
Qut(9); <metplotlib. axes, Axessubplot ot BxdBaSada>

 CiB1: exit

[Lf:58 “/Drepbes (oouk/ovn $ Lpythan —pyl ab=q

Puthon 2.7.2 1EPD 7. 1-2 ‘(e4-bstI (default, ty 3 2611. 15: 7k Si)

Type “copyright”. “credits’ or “License” for more information.

BRuthon 8.13.dey -- An enhenced Interactive Python.

> intreauict on at and overview of TPuthan S features.

Julohrer = Quick refer

> Python's aun nneip sustem.

pevects 3 Details about ‘object’. use ‘object??’ for extra deta

|Welcome to pylab, a matplotiib-based Python environnent (backend:

For nore information, tune ‘help(pulab)’. Ww
wa

090; plot¢np. random, randn¢ 166) . cumsun()) n

poeta: Cématplotlib.lines.Lino2D at Gx3fdsfdB>] ay oC +" BvY&a

o C2): pit. flouret)

ut(2): <motplotlib. fiqure.Figure at Bx3fesdsa>

fin £30; from pondas. io. data import get_data_yahoo

m 0: spy_close = get_data_yshoot SPY’ JL'Adj Close']

nm CS): spy_close.plot()
ut[S): <motplotlib, axes, AxesSubplot at Gx3Fff2d9@>

nm €&):
pn (6):
fn (62: O

 

 

Figure 2

 

 

 

Figure 3-3. Pylab mode: IPython with matplotlib windows

 

IPython Basics | 57

Using the Command History

IPython maintains a small on-disk database containing the text of each command that
you execute. This serves various purposes:

¢ Searching, completing, and executing previously-executed commands with mini-
mal typing
¢ Persisting the command history between sessions.

¢ Logging the input/output history to a file

Searching and Reusing the Command History

Being able to search and execute previous commands is, for many people, the most
useful feature. Since [Python encourages an iterative, interactive code development
workflow, you may often find yourself repeating the same commands, such as a %run
command or some other code snippet. Suppose you had run:

In[7]: %run first/second/third/data_script.py

and then explored the results of the script (assuming it ran successfully), only to find
that you made an incorrect calculation. After figuring out the problem and modifying
data_script.py, you can start typing a few letters of the run command then press either
the <Ctr1-P> key combination or the <up arrow> key. This will search the command
history for the first prior command matching the letters you typed. Pressing either
<Ctr1-P> or <up arrow> multiple times will continue to search through the history. If
you pass over the command you wish to execute, fear not. You can move forward
through the command history by pressing either <Ctr1-N> or <down arrow>. After doing
this a few times you may start pressing these keys without thinking!

Using <Ctr1-R> gives you the same partial incremental searching capability provided
by the readline used in UNIX-style shells, such as the bash shell. On Windows, read
line functionality is emulated by IPython. To use this, press <Ctr1-R> then type a few
characters contained in the input line you want to search for:

In [1]: a_command = foo(x, y, z)
(reverse-i-search)*com': a_command = foo(x, y, Z)

Pressing <Ctr1-R> will cycle through the history for each line matching the characters
you ve typed.

Input and Output Variables

Forgetting to assign the result of a function call to a variable can be very annoying.
Fortunately, [Python stores references to both the input (the text that you type) and
output (the object that is returned) in special variables. The previous two outputs are
stored in the _ (one underscore) and __ (two underscores) variables, respectively:

 

58 | Chapter3: IPython: An Interactive Computing and Development Environment

In [556]: 2 ** 27
Out[556]: 134217728

In [557]: _

Out[557]: 134217728
Input variables are stored in variables named like _ixX, where X is the input line number.
For each such input variables there is a corresponding output variable _X. So after input
line 27, say, there will be two new variables _27 (for the output) and _i27 for the input.

In [26]: foo = 'bar'

In [27]: foo
Out[27]: 'bar'

In [28]: _i27
Out[28]: u'foo'

In [29]: _27

Out[29]: 'bar'
Since the input variables are strings, that can be executed again using the Python
exec keyword:

In [30]: exec _i27

Several magic functions allow you to work with the input and output history. %hist is
capable of printing all or part of the input history, with or without line numbers.
*reset is for clearing the interactive namespace and optionally the input and output
caches. The %xdel magic function is intended for removing all references to a particu-
lar object from the [Python machinery. See the documentation for both of these magics
for more details.

 

When working with very large data sets, keep in mind that IPython’s
— ta) input and output history causes any object referenced there to not be
garbage collected (freeing up the memory), even if you delete the vari-
ables from the interactive namespace using the del keyword. In such
cases, careful usage of %xdel and %reset can help you avoid running into
memory problems.

 

 

 

Logging the Input and Output

IPython is capable of logging the entire console session including input and output.
Logging is turned on by typing %logstart:

In [3]: %logstart

Activating auto-logging. Current session state plus future input saved.
Filename : ipython_log.py

Mode : rotate

Output logging : False

Raw input log : False

 

Using the Command History | 59

Timestamping : False
State : active

IPython logging can be enabled at any time and it will record your entire session (in-
cluding previous commands). Thus, if you are working on something and you decide
you want to save everything you did, you can simply enable logging. See the docstring
of %logstart for more options (including changing the output file path), as well as the
companion functions %logoff, %logon, %logstate, and %logstop.

Interacting with the Operating System

Another important feature of [Python is that it provides very strong integration with
the operating system shell. This means, among other things, that you can perform most
standard command line actions as you would in the Windows or UNIX (Linux, OS X)
shell without having to exit IPython. This includes executing shell commands, changing
directories, and storing the results of a command in a Python object (list or string).
There are also simple shell command aliasing and directory bookmarking features.

See Table 3-3 for a summary of magic functions and syntax for calling shell commands.
I'll briefly visit these features in the next few sections.

Table 3-3. IPython system-related commands

Command Description
!cmd Execute cmd in the system shell
output = !cmd args Run cmd and store the stdout in output

walias alias_name cmd _ Define analias for a system (shell) command

*bookmark Utilize IPython’s directory bookmarking system

wcd directory Change system working directory to passed directory

%pwd Return the current system working directory

wpushd directory Place current directory on stack and change to target directory
*popd Change to directory popped off the top of the stack

‘dirs Return a list containing the current directory stack

wdhist Print the history of visited directories

env Return the system environment variables as a dict

Shell Commands and Aliases

Starting a line in [Python with an exclamation point !, or bang, tells [Python to execute
everything after the bang in the system shell. This means that you can delete files (using
rm or del, depending on your OS), change directories, or execute any other process. It’s
even possible to start processes that take control away from IPython, even another
Python interpreter:

 

60 | Chapter3: Python: An Interactive Computing and Development Environment

In [2]: !python

Python 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul 3 2011, :51)
[GCC 4.1.2 20080704 (Red Hat 4.1.2-44)] on linux2

Type "packages", "demo" or "enthought" for more information.

>>>

The console output of a shell command can be stored in a variable by assigning the !-
escaped expression to a variable. For example, on my Linux-based machine connected
to the Internet via ethernet, I can get my IP address as a Python variable:

In [1]: ip_info = !ifconfig etho | grep "inet "

In [2]: ip_info[0].strip()
Out[2]: ‘inet addr:192.168.1.137 Bcast:192.168.1.255 Mask:255.255.255.0°

The returned Python object ip_info is actually a custom list type containing various
versions of the console output.

IPython can also substitute in Python values defined in the current environment when
using !. To do this, preface the variable name by the dollar sign $:

In [3]: foo = 'test*'

In [4]: !1s $foo
test4.py test.py test.xml

The %alias magic function can define custom shortcuts for shell commands. Asa simple
example:
In [1]: %alias 11 ls -1

In [2]: 11 /usr

total 332

drwxr-xr-x 2 root root 69632 2012-01-29  bin/
drwxr-xr-x 2 root root 4096 2010-08-23  games/
drwxr-xr-x 123 root root 20480 2011-12-26  include/
drwxr-xr-x 265 root root 126976 2012-01-29  lib/
drwxr-xr-x 44 root root 69632 2011-12-26  1ib32/
lrwxrwxrwx 1 root root 3 2010-08-23  lib64 -> lib/
drwxr-xr-x 15 root root 4096 2011-10-13  local/
drwxr-xr-x 2 root root 12288 2012-01-12  sbin/
drwxr-xr-x 387 root root 12288 2011-11-04  share/
drwxrwsr-x 24 root src 4096 2011-07-17  src/

Multiple commands can be executed just as on the command line by separating them
with semicolons:
In [558]: %alias test_alias (cd ch08; 1s; cd ..)

In [559]: test_alias
macrodata.csv spx.csv tips.csv

You'll notice that [Python “forgets” any aliases you define interactively as soon as the
session is closed. To create permanent aliases, you will need to use the configuration
system. See later in the chapter.

 

Interacting with the Operating System | 61

Directory Bookmark System

IPython has a simple directory bookmarking system to enable you to save aliases for
common directories so that you can jump around very easily. For example, I’m an avid
user of Dropbox, so I can define a bookmark to make it easy to change directories to
my Dropbox:

In [6]: %bookmark db /home/wesm/Dropbox/

Once I’ve done this, when I use the %cd magic, I can use any bookmarks I’ve defined

In [7]: cd db
(bookmark:db) -> /home/wesm/Dropbox/
/home/wesm/Dropbox

If a bookmark name conflicts with a directory name in your current working directory,
you can use the -b flag to override and use the bookmark location. Using the -1 option
with %bookmark lists all of your bookmarks:

In [8]: %bookmark -1

Current bookmarks:
db -> /home/wesm/Dropbox/

Bookmarks, unlike aliases, are automatically persisted between IPython sessions.

Software Development Tools

In addition to being a comfortable environment for interactive computing and data
exploration, Python is well suited as a software development environment. In data
analysis applications, it’s important first to have correct code. Fortunately, [Python has
closely integrated and enhanced the built-in Python pdb debugger. Secondly you want
your code to be fast. For this [Python has easy-to-use code timing and profiling tools.
1 will give an overview of these tools in detail here.

Interactive Debugger

IPython’s debugger enhances pdb with tab completion, syntax highlighting, and context
for each line in exception tracebacks. One of the best times to debug code is right after
an error has occurred. The %debug command, when entered immediately after an ex-
ception, invokes the “post-mortem” debugger and drops you into the stack frame where
the exception was raised:

In [2]: run cho3/ipython_bug.py

AssertionError Traceback (most recent call last)
/home/wesm/book_scripts/cho3/ipython_bug.py in <module>()

13 throws_an_exception()

14

---> 15 calling things()

/home/wesm/book_scripts/cho3/ipython_bug.py in calling things()

 

62 | Chapter3: IPython: An Interactive Computing and Development Environment

11 def calling things():

12 works _fine()
---> 13 throws_an_exception()
14

15 calling things()

/home/wesm/book_scripts/ch03/ipython_bug.py in throws_an_exception()

7 a=5
8 b = 6

----> 9 assert(a + b == 10)
10

11 def calling things():
AssertionError:

In [3]: %debug
> /home/wesm/book_scripts/ch03/ipython_bug.py(9)throws_an_exception()

8 b = 6
----> 9 assert(a + b == 10)
10

ipdb>

Once inside the debugger, you can execute arbitrary Python code and explore all of the
objects and data (which have been “kept alive” by the interpreter) inside each stack
frame. By default you start in the lowest level, where the error occurred. By pressing
u (up) and d (down), you can switch between the levels of the stack trace:

ipdb> u

> /home/wesm/book_scripts/ch03/ipython_bug.py(13)calling things()
12 works _fine()

---> 13 throws_an_exception()
14

Executing the %pdb command makes it so that [Python automatically invokes the de-
bugger after any exception, a mode that many users will find especially useful.

It’s also easy to use the debugger to help develop code, especially when you wish to set
breakpoints or step through the execution of a function or script to examine the state
at each stage. There are several ways to accomplish this. The first is by using %run with
the -d flag, which invokes the debugger before executing any code in the passed script.
You must immediately press s (step) to enter the script:

In [5]: run -d cho3/ipython_bug.py

Breakpoint 1 at /home/wesm/book_scripts/cho3/ipython_bug.py:1
NOTE: Enter 'c' at the ipdb> prompt to start your script.

> <string>(1)<module>()

ipdb> s
--Call--
> /home/wesm/book_scripts/ch03/ipython_bug.py(1)<module>()
1---> 1 def works fine():
2 a=5
3 b=6

 

Software Development Tools | 63

After this point, it’s up to you how you want to work your way through the file. For
example, in the above exception, we could set a breakpoint right before calling the
works fine method and run the script until we reach the breakpoint by pressing c
(continue):

ipdb> b 12

ipdb> c

> /home/wesm/book_scripts/ch03/ipython_bug.py(12)calling things()

11 def calling things():

2--> 12 works _fine()
13 throws_an_exception()

At this point, you can step into works fine() or execute works fine() by pressing n
(next) to advance to the next line:

ipdb> n
> /home/wesm/book_scripts/ch03/ipython_bug.py(13)calling things()
2 12 works _fine()
---> 13 throws_an_exception()
14

Then, we could step into throws_an_exception and advance to the line where the error
occurs and look at the variables in the scope. Note that debugger commands take
precedence over variable names; in such cases preface the variables with ! to examine
their contents.

ipdb> s

--Call--

> /home/wesm/book_scripts/ch03/ipython_bug.py(6)throws_an_exception()
5

----> 6 def throws_an_exception():
7 a=5

ipdb> n

> /home/wesm/book_scripts/ch03/ipython_bug.py(7)throws_an_exception()
6 def throws _an_exception():

“> 700 a5
8 b = 6

ipdb> n

> /home/wesm/book_scripts/ch03/ipython_bug.py(8)throws_an_exception()
7 a=5

----> 8 b = 6
9 assert(a + b == 10)

ipdb> n

> /home/wesm/book_scripts/ch03/ipython_bug.py(9)throws_an_exception()
8 b = 6

----> 9 assert(a + b == 10)
10

ipdb> !a

5

ipdb> !b

6

 

64 | Chapter3: IPython: An Interactive Computing and Development Environment

Becoming proficient in the interactive debugger is largely a matter of practice and ex-
perience. See Table 3-4 for a full catalogue of the debugger commands. If you are used
to an IDE, you might find the terminal-driven debugger to be a bit bewildering at first,
but that will improve in time. Most of the Python IDEs have excellent GUI debuggers,
but it is usually a significant productivity gain to remain in [Python for your debugging.

Table 3-4. (I)Python debugger commands

Command

h(elp)

help command
c(ontinue)
q(uit)

b(reak) number

b path/to/file.py:number
s(tep)

n(ext)

u(p) / d(own)
a(rgs)

debug statement
1(ist) statement

w(here)

Action

Display command list

Show documentation for command

Resume program execution

Exit debugger without executing any more code

Set breakpoint at number in current file

Set breakpoint at line number in specified file

Step into function call

Execute current line and advance to next line at current level
Move up/down in function call stack

Show arguments for current function

Invoke statement statement in new (recursive) debugger
Show current position and context at current level of stack

Print full stack trace with context at current position

Other ways to make use of the debugger

There are a couple of other useful ways to invoke the debugger. The first is by using a
special set_trace function (named after pdb.set_trace), which is basically a “poor
man’s breakpoint”. Here are two small recipes you might want to put somewhere for
your general use (potentially adding them to your IPython profile as I do):

def set_trace():

from IPython.core.debugger import Pdb
Pdb(color_scheme='Linux').set_trace(sys._getframe().f_back)

def debug(f, *args, **kwargs):
from IPython.core.debugger import Pdb
pdb = Pdb(color_scheme='Linux' )
return pdb.runcall(f, *args, **kwargs)

The first function, set_trace, is very simple. Put set_trace() anywhere in your code
that you want to stop and take a look around (for example, right before an exception
occurs):

In [7]: run cho3/ipython_bug.py

> /home/wesm/book_scripts/ch03/ipython_bug.py(16)calling things()
15 set_trace()

 

Software Development Tools | 65

---> 16 throws_an_exception()
17

Pressing c (continue) will cause the code to resume normally with no harm done.

The debug function above enables you to invoke the interactive debugger easily on an
arbitrary function call. Suppose we had written a function like
def f(x, y, z=1):
tmp =xty
return tmp / z
and we wished to step through its logic. Ordinarily using f would look like (1, 2,
z=3). To instead step into f, pass f as the first argument to debug followed by the po-
sitional and keyword arguments to be passed to f:
In [6]: debug(f, 1, 2, z=3)
> <ipython-input>(2)F()
1 def f(x, y, z):
aesay 2 tmp =xty
3 return tmp / z

ipdb>
I find that these two simple recipes save me a lot of time on a day-to-day basis.

Lastly, the debugger can be used in conjunction with %run. By running a script with
%run -d, you will be dropped directly into the debugger, ready to set any breakpoints
and start the script:

In [1]: %run -d cho3/ipython_bug.py

Breakpoint 1 at /home/wesm/book_scripts/cho3/ipython_bug.py:1
NOTE: Enter 'c' at the ipdb> prompt to start your script.

> <string>(1)<module>()

ipdb>

Adding -b with a line number starts the debugger with a breakpoint set already:

In [2]: %run -d -b2 ch03/ipython_bug.py

Breakpoint 1 at /home/wesm/book_scripts/cho3/ipython_bug.py:2
NOTE: Enter 'c' at the ipdb> prompt to start your script.

> <string>(1)<module>()

ipdb> c

> /home/wesm/book_scripts/ch03/ipython_bug.py(2)works_fine()
1 def works fine():

1---> 2 a=5
3 b=6

ipdb>

 

66 | Chapter3: IPython: An Interactive Computing and Development Environment

Timing Code: %time and %timeit

For larger-scale or longer-running data analysis applications, you may wish to measure
the execution time of various components or of individual statements or function calls.
You may want a report of which functions are taking up the most time in a complex
process. Fortunately, [Python enables you to get this information very easily while you
are developing and testing your code.

Timing code by hand using the built-in time module and its functions time.clock and
time.time is often tedious and repetitive, as you must write the same uninteresting
boilerplate code:

import time

start = time.time()

for i in range(iterations):

# some code to run here
elapsed per = (time.time() - start) / iterations

Since this is such a common operation, [Python has two magic functions %time and
%timeit to automate this process for you. %time runs a statement once, reporting the
total execution time. Suppose we had a large list of strings and we wanted to compare
different methods of selecting all strings starting with a particular prefix. Here is a
simple list of 700,000 strings and two identical methods of selecting only the ones that
start with 'foo':

# a very large list of strings

strings = ['foo', 'foobar', 'baz', ‘qux',

"python', 'Guido Van Rossum'] * 100000

method1 = [x for x in strings if x.startswith('foo')]
method2 = [x for x in strings if x[:3] == 'foo']

It looks like they should be about the same performance-wise, right? We can check for
sure using %time:
In [561]: %time methoda = [x for x in strings if x.startswith('foo')]

CPU times: user 0.19 s, sys: 0.00 s, total: 0.19 s
Wall time: 0.19 s

In [562]: %time method2 = [x for x in strings if x[:3] == 'foo']
CPU times: user 0.09 s, sys: 0.00 s, total: 0.09 s
Wall time: 0.09 s

The Wall time is the main number of interest. So, it looks like the first method takes
more than twice as long, but it’s not a very precise measurement. If you try %time-ing
those statements multiple times yourself, you’ll find that the results are somewhat
variable. To get a more precise measurement, use the %timeit magic function. Given
an arbitrary statement, it has a heuristic to run a statement multiple times to produce
a fairly accurate average runtime.

In [563]: %timeit [x for x in strings if x.startswith('foo')]
10 loops, best of 3: 159 ms per loop

 

Software Development Tools | 67

In [564]: %timeit [x for x in strings if x[:3] == 'foo']

10 loops, best of 3: 59.3 ms per loop
This seemingly innocuous example illustrates that it is worth understanding the per-
formance characteristics of the Python standard library, NumPy, pandas, and other
libraries used in this book. In larger-scale data analysis applications, those milliseconds
will start to add up!

%timeit is especially useful for analyzing statements and functions with very short ex-
ecution times, even at the level of microseconds (1le-6 seconds) or nanoseconds (le-9
seconds). These may seem like insignificant amounts of time, but of course a 20 mi-
crosecond function invoked 1 million times takes 15 seconds longer than a 5 micro-
second function. In the above example, we could very directly compare the two string
operations to understand their performance characteristics:

In [565]: x = 'foobar'

In [566]: y = 'foo'

In [567]: %timeit x.startswith(y)
1000000 loops, best of 3: 267 ns per loop

In [568]: %timeit x[:3] == y
10000000 loops, best of 3: 147 ns per loop

Basic Profiling: %prun and %run -p

Profiling code is closely related to timing code, except it is concerned with determining
where time is spent. The main Python profiling tool is the cProfile module, which is
not specific to [Python at all. cProfile executes a program or any arbitrary block of
code while keeping track of how much time is spent in each function.

A common way to use cProfile is on the command line, running an entire program
and outputting the aggregated time per function. Suppose we had a simple script which
does some linear algebra in a loop (computing the maximum absolute eigenvalues of
a series of 100 x 100 matrices):

import numpy as np
from numpy.linalg import eigvals

def run_experiment (niter=100):
K = 100
results = []
for _ in xrange(niter):
mat = np.random.randn(K, K)
max_eigenvalue = np.abs(eigvals(mat) ).max()
results. append(max_eigenvalue)
return results
some_results = run_experiment()
print ‘Largest one we saw: %s' % np.max(some_results)

 

68 | Chapter3: Python: An Interactive Computing and Development Environment

Don’t worry if you are not familiar with NumPy. You can run this script through
cProfile by running the following in the command line:

python -m cProfile cprof_example.py
If you try that, you'll find that the results are outputted sorted by function name. This

makes it a bit hard to get an idea of where the most time is spent, so it’s very common
to specify a sort order using the -s flag:

$ python -m cProfile -s cumulative cprof_example.py
Largest one we saw: 11.923204422
15116 function calls (14927 primitive calls) in 0.720 seconds

Ordered by: cumulative time

ncalls tottime percall cumtime percall filename: lineno(function)
1 0.001 0.001 0.721 0.721 cprof_example.py:1(<module>)

100 0.003 0.000 0.586 0.006 linalg.py:702(eigvals)
200 0.572 + 0.003 0.572 0.003 {numpy.linalg.lapack_lite.dgeev}
1 0.002 0.002 0.075 0.075 _init__.py:106(<module>)
100 0.059 0.001 0.059 0.001 {method 'randn')
1 0.000 0.000 0.044 0.044 add_newdocs.py:9(<module>)
2 0.001 0.001 0.037 0.019 _ init__.py:1(<module>)
2 0.003 0.002 0.030 0.015 _ init__.py:2(<module>)
1 0.000 0.000 0.030 0.030 type_check.py:3(<module>)
1 0.001 0.001 0.021 0.021 _init__.py:15(<module>)
1 0.013 0.013 0.013 0.013 numeric.py:1(<module>)
1 0.000 0.000 0.009 0.009 _ init__.py:6(<module>)
1 0.001 0.001 0.008 0.008 _ init__.py:45(<module>)
262 0.005 0.000 0.007 0.000 function_base.py:3178(add_newdoc)
100 0.003 0.000 0.005 0.000 linalg.py:162(_assertFinite)

Only the first 15 rows of the output are shown. It’s easiest to read by scanning down
the cumtime column to see how much total time was spent inside each function. Note
that if a function calls some other function, the clock does not stop running. cProfile
records the start and end time of each function call and uses that to produce the timing.

In addition to the above command-line usage, cProfile can also be used programmat-
ically to profile arbitrary blocks of code without having to run a new process. [Python
has a convenient interface to this capability using the %prun command and the -p option
to %run. %prun takes the same “command line options” as cProfile but will profile an
arbitrary Python statement instead of a whole . py file:

In [4]: %prun -1 7 -s cumulative run_experiment()
4203 function calls in 0.643 seconds

Ordered by: cumulative time
List reduced from 32 to 7 due to restriction <7>

ncalls tottime percall cumtime percall filename: lineno(function)
1 0.000 0.000 0.643 0.643 <string>:1(<module>)
1 0.001 0.001 0.643 0.643 cprof_example.py:4(run_experiment)
100 0.003 0.000 0.583 0.006 linalg.py:702(eigvals)

 

Software Development Tools | 69

200 0.569 +=0.003 0.569 0.003 {numpy.linalg.lapack_lite.dgeev}

100 0.058 0.001 0.058 0.001 {method 'randn'}

100 0.003 0.000 0.005 0.000 linalg.py:162(_assertFinite)

200 0.002 0.000 0.002 0.000 {method 'all' of 'numpy.ndarray' objects}

Similarly, calling %run -p -s cumulative cprof_example.py has the same effect as the
command-line approach above, except you never have to leave [Python.

Profiling a Function Line-by-Line

In some cases the information you obtain from %prun (or another cProfile-based profile
method) may not tell the whole story about a function’s execution time, or it may be
so complex that the results, aggregated by function name, are hard to interpret. For
this case, there is a small library called line_profiler (obtainable via PyPI or one of the
package management tools). It contains an [Python extension enabling a new magic
function %lprun that computes a line-by-line-profiling of one or more functions. You
can enable this extension by modifying your [Python configuration (see the [Python
documentation or the section on configuration later in this chapter) to include the
following line:

# A list of dotted module names of IPython extensions to load.
c.TerminalIPythonApp.extensions = ['line_profiler']

line profiler can be used programmatically (see the full documentation), but it is
perhaps most powerful when used interactively in IPython. Suppose you had a module
prof_mod with the following code doing some NumPy array operations:

from numpy.random import randn

def add_and_sum(x, y):
added =x + y
summed = added.sum(axis=1)
return summed

def call_function():
x = randn(1000, 1000)
y = randn(1000, 1000)
return add_and_sum(x, y)

If we wanted to understand the performance of the add_and_sum function, %prun gives
us the following:

In [569]: %run prof_mod
In [570]: x = randn(3000, 3000)
In [571]: y = randn(3000, 3000)

In [572]: %prun add_and_sum(x, y)
4 function calls in 0.049 seconds
Ordered by: internal time
ncalls tottime percall cumtime percall filename: lineno(function)
1 0.036 0.036 0.046 0.046 prof_mod.py:3(add_and_sum)

 

70 | Chapter3: Python: An Interactive Computing and Development Environment

1 0.009 0.009 0.009 0.009 {method 'sum' of 'numpy.ndarray' objects}
1 0.003 0.003 0.049 0.049 <string>:1(<module>)
1 0.000 0.000 0.000 0.000 {method ‘disable’ of ' lsprof.Profiler' objects}

This is not especially enlightening. With the line_profiler [Python extension activa-
ted, anew command %lprun is available. The only difference in usage is that we must
instruct %lprun which function or functions we wish to profile. The general syntax is:

’lprun -f funci -f func2 statement_to_profile

In this case, we want to profile add_and_sum, so we run:

In [573]: %lprun -f add_and_sum add_and_sum(x, y)
Timer unit: 1e-06 s

File: book_scripts/prof_mod.py

Function: add_and_sum at line 3

Total time: 0.045936 s

Line # Hits Time Per Hit % Time Line Contents
3 def add_and_sum(x, y):
4 1 36510 36510.0 79.5 added = x + y
5 1 9425 9425.0 20.5 summed = added.sum(axis=1)
6 1 1 1.0 0.0 return summed

You'll probably agree this is much easier to interpret. In this case we profiled the same
function we used in the statement. Looking at the module code above, we could call
call_ function and profile that as well as add_and_sum, thus getting a full picture of the
performance of the code:

In [574]: %lprun -f add_and_sum -f call_function call_function()

Timer unit: 1e-06 s

File: book_scripts/prof_mod.py

Function: add_and_sum at line 3
Total time: 0.005526 s

Line # Hits Time Per Hit % Time Line Contents
3 def add_and_sum(x, y):
4 1 4375 4375.0 79.2 added =x + y
5 1 1149 1149.0 20.8 summed = added.sum(axis=1)
6 1 2 2.0 0.0 return summed

File: book_scripts/prof_mod.py
Function: call_function at line 8
Total time: 0.121016 s

Line # Hits Time Per Hit % Time Line Contents
8 def call_function():
9 1 57169 57169.0 47.2 x = randn(1000, 1000)
10 1 58304 58304.0 48.2 y = randn(1000, 1000)
11 1 5543 5543.0 4.6 return add_and_sum(x, y)

As a general rule of thumb, I tend to prefer %prun (cProfile) for “macro” profiling and
%lprun (line_profiler) for “micro” profiling. It’s worthwhile to have a good under-
standing of both tools.

 

Software Development Tools | 71

The reason that you have to specify explicitly the names of the functions
you want to profile with %lprun is that the overhead of “tracing” the
~ 418° execution time of each line is significant. Tracing functions that are not
* of interest would potentially significantly alter the profile results.

  

 

[Python HTML Notebook

Starting in 2011, the [Python team, led by Brian Granger, built a web technology-based
interactive computational document format that is commonly known as the [Python
Notebook. It has grown into a wonderful tool for interactive computing and an ideal
medium for reproducible research and teaching. I’ve used it while writing most of the
examples in the book; I encourage you to make use of it, too.

It has a JSON-based . ipynb document format that enables easy sharing of code, output,
and figures. Recently in Python conferences, a popular approach for demonstrations
has been to use the notebook and post the .ipynb files online afterward for everyone
to play with.

The notebook application runs as a lightweight server process on the command line.
It can be started by running:
$ ipython notebook --pylab=inline
[NotebookApp] Using existing profile dir: u'/home/wesm/.config/ipython/profile default’
NotebookApp] Serving notebooks from /home/wesm/book_scripts

[
[NotebookApp] The IPython Notebook is running at: http://127.0.0.1:8888/
[NotebookApp] Use Control-C to stop this server and shut down all kernels.

On most platforms, your primary web browser will automatically open up to the note-
book dashboard. In some cases you may have to navigate to the listed URL. From there,
you can create a new notebook and start exploring.

Since you use the notebook inside a web browser, the server process can run anywhere.
You can even securely connect to notebooks running on cloud service providers like
Amazon EC2. As of this writing, a new project NotebookCloud (http://notebookcloud
.appspot.com) makes it easy to launch notebooks on EC2.

Tips for Productive Code Development Using IPython

Writing code in a way that makes it easy to develop, debug, and ultimately use inter-
actively may be a paradigm shift for many users. There are procedural details like code
reloading that may require some adjustment as well as coding style concerns.

As such, most of this section is more of an art than a science and will require some
experimentation on your part to determine a way to write your Python code that is
effective and productive for you. Ultimately you want to structure your code in a way
that makes it easy to use iteratively and to be able to explore the results of running a
program or function as effortlessly as possible. I have found software designed with

 

72 | Chapter3: Python: An Interactive Computing and Development Environment

 

BD S@ O 127.00.1:8888/sa0esdoe-d135-447b-269d-ade4gecsi77tyy, m * 23 Lt) & aw
1 Ply]: Notebook NotebookEx Last saved: Jul 26 1:06 PM

| File Elites Viewoo Insert Callees Kernalecs Help |

 

6 « 6 A t + iz 2 > = | Code v}

 

import numpy as np
import pandas as pd
print 'Hello world!'

Hello world!

tips = pd. read_csv('book_scripts/ch08/tips. csv')
tips. head()

[Tita bile [sex [smoker] daytime [sie
efies=_[roi|remae|ie — [Sin|nnar|2
afio3e_[1ee|vae [no [Sun|inmer|3
[2101 |a50|vale [nv [Sin | Dever >
s|zsee_[aai|vae [no _[Sun| inner?

3.61] Female} No Sun | Dinner} 4

 

In [4]: img = plt.imread('book_scripts/ch03/stinkbug. png' ]
figure(figsize=(4, 4))|
plt. imshow( img)

Out[4]: <matplotlib.image.AxesImage at O0x7f465d3428510>

0

50

 

 

 

 

 

 

 

 

Figure 3-4. [Python Notebook

IPython in mind to be easier to work with than code intended only to be run as as
standalone command-line application. This becomes especially important when some-
thing goes wrong and you have to diagnose an error in code that you or someone else
might have written months or years beforehand.

 

Tips for Productive Code Development Using IPython | 73

 

Reloading Module Dependencies

In Python, when you type import some_lib, the code in some_lib is executed and all the
variables, functions, and imports defined within are stored in the newly created
some_lib module namespace. The next time you type import some_lib, you will get a
reference to the existing module namespace. The potential difficulty in interactive code
development in [Python comes when you, say, *run a script that depends on some other
module where you may have made changes. Suppose I had the following code in
test_script.py:

import some_lib

x= 5

y = [1, 2, 3, 4]

result = some_lib.get_answer(x, y)
If you were to execute %run test_script.py then modify some_lib.py, the next time you
execute %run test_script.py you will still get the old version of some_lib because of
Python’s “load-once” module system. This behavior differs from some other data anal-
ysis environments, like MATLAB, which automatically propagate code changes.! To
cope with this, you have a couple of options. The first way is to use Python's built-in
reload function, altering test_script.py to look like the following:

import some_lib

reload(some_lib)

x= 5
y = [1, 2, 3, 4]
result = some_lib.get_answer(x, y)
This guarantees that you will get a fresh copy of some lib every time you run
test_script.py. Obviously, if the dependencies go deeper, it might be a bit tricky to be
inserting usages of reload all over the place. For this problem, [Python has a special
dreload function (not a magic function) for “deep” (recursive) reloading of modules. If
I were to run import some lib then type dreload(some_lib), it will attempt to reload
some_lib as well as all of its dependencies. This will not work in all cases, unfortunately,
but when it does it beats having to restart IPython.

Code Design Tips

There’s no simple recipe for this, but here are some high-level principles I have found
effective in my own work.

1. Since a module or package may be imported in many different places in a particular program, Python
caches a module’s code the first time it is imported rather than executing the code in the module every
time. Otherwise, modularity and good code organization could potentially cause inefficiency in an
application.

 

74 | Chapter3: IPython: An Interactive Computing and Development Environment

Keep relevant objects and data alive

It’s not unusual to see a program written for the command line with a structure some-
what like the following trivial example:

from my_functions import g

def f(x, y):
return g(x + y)

def main():
x = 6
y=7.5
result = x + y

if _name_ == ''_main_':

Do you see what might be wrong with this program if we were to run it in IPython?
After it’s done, none of the results or objects defined in the main function will be ac-
cessible in the [Python shell. A better way is to have whatever code is in main execute
directly in the module’s global namespace (orintheif name == '_ main_': block,
if you want the module to also be importable). That way, when you %run the code,
you'll be able to look at all of the variables defined in main. It’s less meaningful in this
simple example, but in this book we’ll be looking at some complex data analysis prob-
lems involving large data sets that you will want to be able to play with in IPython.

Flat is better than nested

Deeply nested code makes me think about the many layers of an onion. When testing
or debugging a function, how many layers of the onion must you peel back in order to
reach the code of interest? The idea that “flat is better than nested” is a part of the Zen
of Python, and it applies generally to developing code for interactive use as well. Making
functions and classes as decoupled and modular as possible makes them easier to test
(if you are writing unit tests), debug, and use interactively.

Overcome a fear of longer files

If you come from a Java (or another such language) background, you may have been
told to keep files short. In many languages, this is sound advice; long length is usually
a bad “code smell”, indicating refactoring or reorganization may be necessary. How-
ever, while developing code using IPython, working with 10 small, but interconnected
files (under, say, 100 lines each) is likely to cause you more headache in general than a
single large file or two or three longer files. Fewer files means fewer modules to reload
and less jumping between files while editing, too. I have found maintaining larger
modules, each with high internal cohesion, to be much more useful and pythonic. After
iterating toward a solution, it sometimes will make sense to refactor larger files into
smaller ones.

 

Tips for Productive Code Development Using IPython | 75

Obviously, I don’t support taking this argument to the extreme, which would to be to
put all of your code in a single monstrous file. Finding a sensible and intuitive module
and package structure for a large codebase often takes a bit of work, but it is especially
important to get right in teams. Each module should be internally cohesive, and it
should be as obvious as possible where to find functions and classes responsible for
each area of functionality.

Advanced Python Features

Making Your Own Classes IPython-friendly

IPython makes every effort to display a console-friendly string representation of any
object that you inspect. For many objects, like dicts, lists, and tuples, the built-in
pprint module is used to do the nice formatting. In user-defined classes, however, you
have to generate the desired string output yourself. Suppose we had the following sim-
ple class:
class Message:
def _ init__(self, msg):

self.msg = msg
If you wrote this, you would be disappointed to discover that the default output for
your class isn’t very nice:

In [576]: x = Message('I have a secret’)

In [577]: x
Out[577]: <__main__.Message instance at 0x60ebbd8>

IPython takes the string returned by the _ repr magic method (by doing output =
repr(obj)) and prints that to the console. Thus, we can adda simple __repr__ method
to the above class to get a more helpful output:
class Message:
def init__(self, msg):
self.msg = msg

def _repr_ (self):
return ‘Message: %s' % self.msg
In [579]: x = Message('I have a secret’)

In [580]: x
Out[580]: Message: I have a secret

 

76 | Chapter3: Python: An Interactive Computing and Development Environment

Profiles and Configuration

Most aspects of the appearance (colors, prompt, spacing between lines, etc.) and be-
havior of the [Python shell are configurable through an extensive configuration system.
Here are some of the things you can do via configuration:

¢ Change the color scheme

¢ Change how the input and output prompts look, or remove the blank line after
Out and before the next In prompt

¢ Change how the input and output prompts look

¢ Execute an arbitrary list of Python statements. These could be imports that you
use all the time or anything else you want to happen each time you launch IPython

e Enable [Python extensions, like the %lprun magic in line_profiler

¢ Define your own magics or system aliases

All of these configuration options are specified in a special ipython_config. py file which
will be found in the ~/.config/ipython/ directory on UNIX-like systems and %HOME
%/.ipython/ directory on Windows. Where your home directory is depends on your
system. Configuration is performed based on a particular profile. When you start IPy-
thon normally, you load up, by default, the default profile, stored in the pro
file default directory. Thus, on my Linux OS the full path to my default [Python
configuration file is:

/home/wesm/.config/ipython/profile_default/ipython_config.py

Dll spare you the gory details of what’s in this file. Fortunately it has comments de-
scribing what each configuration option is for, so I will leave it to the reader to tinker
and customize. One additional useful feature is that it’s possible to have multiple pro-
files. Suppose you wanted to have an alternate [Python configuration tailored for a
particular application or project. Creating a new profile is as simple is typing something
like

ipython profile create secret_project

Once you’ve done this, edit the config files in the newly-created pro
file_secret_project directory then launch [Python like so

$ ipython --profile=secret_project
Python 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul 3 2011, :51)
Type "copyright", "credits" or "license" for more information.

IPython 0.13 -- An enhanced Interactive Python.

2 -> Introduction and overview of IPython's features.
#quickref -> Quick reference.

help -> Python's own help system.

object? -> Details about ‘object’, use 'object??' for extra details.

IPython profile: secret_project

 

Advanced IPython Features | 77

In [1]:

As always, the online [Python documentation is an excellent resource for more on
profiles and configuration.

Credits

Parts of this chapter were derived from the wonderful documentation put together by
the [Python Development Team. I can’t thank them enough for all of their work build-
ing this amazing set of tools.

 

78 | Chapter3: Python: An Interactive Computing and Development Environment

CHAPTER 4
NumPy Basics: Arrays and Vectorized
Computation

 

NumPy, short for Numerical Python, is the fundamental package required for high
performance scientific computing and data analysis. It is the foundation on which
nearly all of the higher-level tools in this book are built. Here are some of the things it
provides:

¢ ndarray, a fast and space-efficient multidimensional array providing vectorized
arithmetic operations and sophisticated broadcasting capabilities

¢ Standard mathematical functions for fast operations on entire arrays of data
without having to write loops

* Tools for reading / writing array data to disk and working with memory-mapped
files

¢ Linear algebra, random number generation, and Fourier transform capabilities

* Tools for integrating code written in C, C++, and Fortran

The last bullet point is also one of the most important ones from an ecosystem point
of view. Because NumPy provides an easy-to-use C API, it is very easy to pass data to
external libraries written in a low-level language and also for external libraries to return
data to Python as NumPy arrays. This feature has made Python a language of choice
for wrapping legacy C/C++/Fortran codebases and giving them a dynamic and easy-
to-use interface.

While NumPy by itself does not provide very much high-level data analytical func-
tionality, having an understanding of NumPy arrays and array-oriented computing will
help you use tools like pandas much more effectively. If you’re new to Python and just
looking to get your hands dirty working with data using pandas, feel free to give this
chapter a skim. For more on advanced NumPy features like broadcasting, see Chap-
ter 12.

 

79

For most data analysis applications, the main areas of functionality I'll focus on are:

¢ Fast vectorized array operations for data munging and cleaning, subsetting and
filtering, transformation, and any other kinds of computations

¢ Common array algorithms like sorting, unique, and set operations

¢ Efficient descriptive statistics and aggregating/summarizing data

¢ Data alignment and relational data manipulations for merging and joining together
heterogeneous data sets

e Expressing conditional logic as array expressions instead of loops with if-elif-
else branches

* Group-wise data manipulations (aggregation, transformation, function applica-
tion). Much more on this in Chapter 5

While NumPy provides the computational foundation for these operations, you will
likely want to use pandas as your basis for most kinds of data analysis (especially for
structured or tabular data) as it provides a rich, high-level interface making most com-
mon data tasks very concise and simple. pandas also provides some more domain-
specific functionality like time series manipulation, which is not present in NumPy.

 

Vs

4
eS In this chapter and throughout the book, I use the standard NumPy
<> convention of always using import numpy as np. You are, of course,
“s 4|3* welcome to put from numpy import * in your code to avoid having to

 

write np., but I would caution you against making a habit of this.

The NumPy ndarray: A Multidimensional Array Object

One of the key features of NumPy is its N-dimensional array object, or ndarray, which
is a fast, flexible container for large data sets in Python. Arrays enable you to perform
mathematical operations on whole blocks of data using similar syntax to the equivalent
operations between scalar elements:

In [8]: data

Out [8]:

array([[ 0.9526, -0.246 , -0.8856],

[ 0.5639, 0.2379, 0.9104]])

In [9]: data * 10 In [10]: data + data

Out [9]: Out[10]:

array([[ 9.5256, -2.4601, -8.8565], array([[ 1.9051, -0.492 , -1.7713],
[ 5.6385, 2.3794, 9.104 ]]) [ 1.1277, 0.4759, 1.8208]])

An ndarray is a generic multidimensional container for homogeneous data; that is, all
of the elements must be the same type. Every array has a shape, a tuple indicating the
size of each dimension, and a dtype, an object describing the data type of the array:

In [11]: data.shape
Out[11]: (2, 3)

 

80 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

In [12]: data.dtype

Out[12]: dtype('float64')
This chapter will introduce you to the basics of using NumPy arrays, and should be
sufficient for following along with the rest of the book. While it’s not necessary to have
a deep understanding of NumPy for many data analytical applications, becoming pro-
ficient in array-oriented programming and thinking is a key step along the way to be-
coming a scientific Python guru.

>

Whenever you see “array”, “NumPy array”, or “ndarray” in the text,
with few exceptions they all refer to the same thing: the ndarray object.

 

 

Creating ndarrays

The easiest way to create an array is to use the array function. This accepts any se-
quence-like object (including other arrays) and produces a new NumPy array contain-
ing the passed data. For example, a list is a good candidate for conversion:

In [13]: data1 = [6, 7.5, 8, 0, 1]
In [14]: arr1 = np.array(data1)

In [15]: arr1

Out[15]: array([ 6. , 7.5, 8, 0., 1. J)
Nested sequences, like a list of equal-length lists, will be converted into a multidimen-
sional array:

In [16]: data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]

In [17]: arr2 = np.array(data2)

In [18]: arr2

Out [18]:

array([[1, 2, 3, 4],
[5, 6, 7, 8]])

In [19]: arr2.ndim
Out[19]: 2

In [20]: arr2.shape
Out[20]: (2, 4)

 

Unless explicitly specified (more on this later), np. array tries to infer a good data type
for the array that it creates. The data type is stored in a special dtype object; for example,
in the above two examples we have:

In [21]: arr1.dtype
Out[21]: dtype('float64' )

 

The NumPy ndarray: A Multidimensional Array Object | 81

In [22]: arr2.dtype

Out[22]: dtype(‘int64')
In addition to np. array, there are a number of other functions for creating new arrays.
As examples, zeros and ones create arrays of 0’s or 1’s, respectively, with a given length
or shape. empty creates an array without initializing its values to any particular value.
To create a higher dimensional array with these methods, pass a tuple for the shape:

In [23]: np.zeros(10)
Out[23]: array([ 0., 0., 0., 0., 0., 0., O., O., 0., 0.])

In [24]: np.zeros((3, 6))
Out [24]:
array([

a

0., 0., 0.
0., 0., 0.
0., 0., 0.

ve we
vue

ooo
ooo
noe
noe ©

7,

a
a
a

°o
vee

In [25]: np.empty((2, 3, 2))
Out[25]:

[ 4.94065646e-324,
[ 3.87491056e-297,
[ 4.94065646e-324,

array([[

[ 1.90723115e+083,
[ -2.33568637e+124,
[ 4.42786966e+160,

[

 

4.94065646e-324],
2.46845796e-130],
4.94065646e-324]],

5.73293533e-053],
-6.70608105e-012],
1.27100354e+025]]])

It’s not safe to assume that np.empty will return an array of all zeros. In
ta) many cases, as previously shown, it will return uninitialized garbage

values.

 

 

 

arange is an array-valued version of the built-in Python range function:
In [26]: np.arange(15)
Out[26]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])

See Table 4-1 for a short list of standard array creation functions. Since NumPy is
focused on numerical computing, the data type, if not specified, will in many cases be
float64 (floating point).

Table 4-1. Array creation functions

Function Description

array Convert input data (list, tuple, array, or other sequence type) to an ndarray either by
inferring a dtype or explicitly specifying a dtype. Copies the input data by default.

asarray Convert input to ndarray, but do not copy if the input is already an ndarray

arange Like the built-in range but returns an ndarray instead of a list.

ones, ones_like

zeros, zeros_like

Produce an array of all 1’s with the given shape and dtype. ones_ like takes another
array and produces a ones array of the same shape and dtype.

Like ones and ones_like but producing arrays of 0's instead

 

82 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

Function Description

 

empty, empty like Create new arrays by allocating new memory, but do not populate with any values like
ones and zeros
eye, identity Create a square Nx N identity matrix (1’s on the diagonal and 0's elsewhere)
Data Types for ndarrays

The data type or dtype is a special object containing the information the ndarray needs
to interpret a chunk of memory as a particular type of data:

In [27]: arr1 = np.array([1, 2, 3], dtype=np.float64)
In [28]: arr2 = np.array([1, 2, 3], dtype=np.int32)

In [29]: arr1.dtype In [30]: arr2.dtype

Out[29]: dtype('float64') Out[30]: dtype('int32")
Dtypes are part of what make NumPy so powerful and flexible. In most cases they map
directly onto an underlying machine representation, which makes it easy to read and
write binary streams of data to disk and also to connect to code written in a low-level
language like C or Fortran. The numerical dtypes are named the same way: a type name,
like float or int, followed by a number indicating the number of bits per element. A
standard double-precision floating point value (what’s used under the hood in Python’s
float object) takes up 8 bytes or 64 bits. Thus, this type is known in NumPy as
float64. See Table 4-2 for a full listing of NumPy’s supported data types.

 

Vs

4
eS Don’t worry about memorizing the NumPy dtypes, especially if you're
“> a new user. It’s often only necessary to care about the general kind of
“s' 4|3* data you’re dealing with, whether floating point, complex, integer,

 

boolean, string, or general Python object. When you need more control
over how data are stored in memory and on disk, especially large data
sets, it is good to know that you have control over the storage type.

Table 4-2. NumPy data types

Type Type Code Description

int8, uint8 i1, ul Signed and unsigned 8-bit (1 byte) integer types

int16, uint16 i2, u2 Signed and unsigned 16-bit integer types

int32, uint32 i4, u4 Signed and unsigned 32-bit integer types

int64, uint64 i8, u8 Signed and unsigned 32-bit integer types

float16 #2 Half-precision floating point

float32 4 or f Standard single-precision floating point. Compatible with C float

float64 8 or d Standard double-precision floating point. Compatible with C double
and Python float object

 

The NumPy ndarray: A Multidimensional Array Object | 83

Type Type Code Description

float128 f16 or g Extended-precision floating point

complex64, complex128, c8, c16, Complexnumbersrepresented by two 32, 64, or 128 floats, respectively
complex256 c32

bool ? Boolean type storing True and False values

object 0 Python object type

string_ S Fixed-length string type (1 byte per character). For example, to create

a string dtype with length 10, use 'S10'.

unicode_ U Fixed-length unicode type (number of bytes platform specific). Same
specification semantics as string (e.g. 'U10').

 

You can explicitly convert or cast an array from one dtype to another using ndarray’s
astype method:

In [31]: arr = np.array([1, 2, 3, 4, 5])

In [32]: arr.dtype
Out[32]: dtype(‘int64')

In [33]: float_arr = arr.astype(np.float64)

In [34]: float_arr.dtype
Out[34]: dtype('float64' )

In this example, integers were cast to floating point. If I cast some floating point num-
bers to be of integer dtype, the decimal part will be truncated:
In [35]: arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])

In [36]: arr
Out[36]: array([ 3.7, -1.2, -2.6, 0.5, 12.9, 10.1])

In [37]: arr.astype(np.int32)
Out[37]: array([ 3, -1, -2, 0, 12, 10], dtype=int32)

Should you have an array of strings representing numbers, you can use astype to convert
them to numeric form:
In [38]: numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_)

In [39]: numeric_strings.astype( float)
Out[39]: array([ 1.25, -9.6, 42. ])

If casting were to fail for some reason (like a string that cannot be converted to
float64), a TypeError will be raised. See that I was a bit lazy and wrote float instead of
np.float64; NumPy is smart enough to alias the Python types to the equivalent dtypes.
You can also use another array’s dtype attribute:

In [40]: int_array = np.arange(10)

 

84 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

In [41]: calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)

In [42]: int_array.astype(calibers.dtype)
Out[42]: array([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])

There are shorthand type code strings you can also use to refer to a dtype:

In [43]: empty_uint32 = np.empty(8, dtype='u4')

In [44]: empty_uint32

Out [44]:

array([ 0, 0, 65904672, 0, 64856792, 0,
39438163, 0], dtype=uint32)

Calling astype always creates a new array (a copy of the data), even if
the new dtype is the same as the old dtype.

 

It’s worth keeping in mind that floating point numbers, such as those
in float64 and float32 arrays, are only capable of approximating frac-
tional quantities. In complex computations, you may accrue some
floating point error, making comparisons only valid up to a certain num-
ber of decimal places.

 

Operations between Arrays and Scalars

Arrays are important because they enable you to express batch operations on data
without writing any for loops. This is usually called vectorization. Any arithmetic op-
erations between equal-size arrays applies the operation elementwise:

In [45]: arr = np.array([[1., 2., 3.], [4., 5., 6.]])

In [46]: arr

Out [46]:

array({[ 1., 2.5 3+],
[4., 5.5 6.]])

In [47]: arr * arr In [48]: arr - arr

Out [47]: Out [48]:

array([[ 1., 4., 9.], array([[ 0., 0., 0.],
[ 16., 25., 36.]]) [0., 0., 0.]])

Arithmetic operations with scalars are as you would expect, propagating the value to
each element:

In [49]: 1 / arr In [50]: arr ** 0.5

Out [49]: Out[50]:

array([{[ 1. » 0.5 4, 0.3333], array([{[ 1. » 1.4142, 1.7321],
[0.25 , 0.2 , 0.1667]]) [ 2 » 2.2361, 2.4495]])

 

The NumPy ndarray: A Multidimensional Array Object | 85

Operations between differently sized arrays is called broadcasting and will be discussed
in more detail in Chapter 12. Having a deep understanding of broadcasting is not nec-
essary for most of this book.

Basic Indexing and Slicing

NumbPy array indexing is a rich topic, as there are many ways you may want to select
a subset of your data or individual elements. One-dimensional arrays are simple; on
the surface they act similarly to Python lists:

In [51]: arr = np.arange(10)

52]: arr
52]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

53]: arr[5]
53]: 5

54]: arr[5:8]
54]: array([5, 6, 7])

In [55]: arr[5:8] = 12

56]: arr
56]: array([ 0, 1, 2, 3, 4, 12, 12, 12, 8, 9])

 

As you can see, if you assign a scalar value to a slice, as in arr[5:8] = 12, the value is
propagated (or broadcasted henceforth) to the entire selection. An important first dis-
tinction from lists is that array slices are views on the original array. This means that
the data is not copied, and any modifications to the view will be reflected in the source
array:

In [57]: arr_slice = arr[5:8]
In [58]: arr_slice[1] = 12345

In [59]: arr
Out[59]: array([ 0, 1, 2, 3, 4, 12, 12345, 12, 8, 9])

In [60]: arr_slice[:] = 64

In [61]: arr

Out[61]: array([ 0, 1, 2, 3, 4, 64, 64, 64, 8, 9])
If you are new to NumPy, you might be surprised by this, especially if you have used
other array programming languages which copy data more zealously. As NumPy has
been designed with large data use cases in mind, you could imagine performance and
memory problems if NumPy insisted on copying data left and right.

 

86 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

If you want a copy of a slice of an ndarray instead of a view, you will
~~) need to explicitly copy the array; for example arr[5:8].copy().

 

With higher dimensional arrays, you have many more options. In a two-dimensional
array, the elements at each index are no longer scalars but rather one-dimensional
arrays:

In [62]: arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

In [63]: arr2d[2]

Out[63]: array([7, 8, 9])
Thus, individual elements can be accessed recursively. But that is a bit too much work,
so you can pass a comma-separated list of indices to select individual elements. So these
are equivalent:

In [64]: arr2d[o][2]
Out [64]: 3

In [65]: arr2d[o, 2]
Out [65]: 3

See Figure 4-1 for an illustration of indexing on a 2D array.

 

axis 1
0 1 2

Figure 4-1. Indexing elements in a NumPy array

 

 

 

In multidimensional arrays, if you omit later indices, the returned object will be a lower-
dimensional ndarray consisting of all the data along the higher dimensions. So in the
2x 2x 3 array arr3d

In [66]: arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])

In [67]: arr3d
Out [67]:
array({[[ 1, 2, 3],

 

The NumPy ndarray: A Multidimensional Array Object | 87

[4, 5, 6]],
[ [ 75 8, 9] ’
[10, 11, 12]]])
arr3d[0] is a 2 x 3 array:
In [68]: arr3d[0]
Out [68]:
array([[1, 2, 3],
[4, 5, 6]])
Both scalar values and arrays can be assigned to arr3d[0]:

In [69]: old_values = arr3d[0].copy()

In [70]: arr3d[0] = 42

In [71]: arr3d

Out[71]:

array([[[42, 42, 42],

[42, 42, 42]],

[[ 7, 8 9],
12]]])

[10, 11,
In [72]: arr3d[0] = old_values

In [73]: arr3d

Out [73]:

array([[[ 1, 2, 3],
[4, 5, 6]],
[[ 7, 8, 9],
[10, 11, 12]]])

Similarly, arr3d[1, 0] gives you all of the values whose indices start with (1, 0), form-
ing a 1-dimensional array:

In [74]: arr3d[1, 0]

Out[74]: array([7, 8, 9])
Note that in all of these cases where subsections of the array have been selected, the
returned arrays are views.

Indexing with slices

Like one-dimensional objects such as Python lists, ndarrays can be sliced using the
familiar syntax:

In [75]: arr[1:6]

Out[75]: array([ 1, 2, 3, 4, 64])
Higher dimensional objects give you more options as you can slice one or more axes
and also mix integers. Consider the 2D array above, arr2d. Slicing this array is a bit
different:

In [76]: arr2d In [77]: arr2d[:2]
Out [76]: Out [77]:

 

88 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

array([[1, 2, 3], array([[1, 2, 3],
[4, 5, 6], [4, 5, 6]])
[7, 8, 9]])
As you can see, it has sliced along axis 0, the first axis. A slice, therefore, selects a range
of elements along an axis. You can pass multiple slices just like you can pass multiple
indexes:
In [78]: arr2d[:2, 1:]
Out [78]:
array([[2, 3],
[5, 6]])
When slicing like this, you always obtain array views of the same number of dimensions.
By mixing integer indexes and slices, you get lower dimensional slices:
In [79]: arr2d[1, :2] In [80]: arr2d[2, :4]
Out[79]: array([4, 5]) Out[80]: array([7])
See Figure 4-2 for an illustration. Note that a colon by itself means to take the entire
axis, so you can slice only higher dimensional axes by doing:
In [81]: arr2d[:, :1]
Out [81]:
array([[1],

d

[7]])

Of course, assigning to a slice expression assigns to the whole selection:
In [82]: arr2d[:2, 1:] = 0

Boolean Indexing

Let’s consider an example where we have some data in an array and an array of names
with duplicates. I’m going to use here the randn function in numpy. random to generate
some random normally distributed data:

In [83]: names = np.array(['Bob', ‘Joe’, 'Will', 'Bob', ‘Will’, 'Joe', 'Joe'])
In [84]: data = randn(7, 4)

In [85]: names

Out [85]:
array(['Bob', ‘Joe’, 'Will', 'Bob', ‘Will’, ‘Joe’, 'Joe'],
dtype=" |S4")

In [86]: data

Out [86]:

array([[-0.048 , 0.5433, -0.2349, 1.2792],
[-0.268 , 0.5465, 0.0939, -2.0445],
[-0.047 , -2.026 , 0.7719, 0.3103],
[ 2.1452, 0.8799, -0.0523, 0.0672],
[-1.0023, -0.1698, 1.1503, 1.7289],

 

The NumPy ndarray: A Multidimensional Array Object | 89

[ 0.1913, 0.4544, 0.4519, 0.5535],
[ 0.5994, 0.8174, -0.9297, -1.2564]])

 

Expression Shape

a. arr[:2, 1:] (2, 2)
arr[2] (3,)

arr[2, :] (35)

arr[2:, :] (1, 3)

4 arr[:, :2] (3, 2)
arr[1, :2] (2,)

arr[1:2, :2] (1, 2)

 

 

 

Figure 4-2. Two-dimensional array slicing

Suppose each name corresponds to a row in the data array and we wanted to select all
the rows with corresponding name '‘Bob'. Like arithmetic operations, comparisons
(such as ==) with arrays are also vectorized. Thus, comparing names with the string
‘Bob' yields a boolean array:

In [87]: names == 'Bob'
Out[87]: array([ True, False, False, True, False, False, False], dtype=bool)

This boolean array can be passed when indexing the array:

In [88]: data[names == 'Bob']

Out[88]:

array([[-0.048 , 0.5433, -0.2349, 1.2792],
[ 2.1452, 0.8799, -0.0523, 0.0672]])

The boolean array must be of the same length as the axis it’s indexing. You can even
mix and match boolean arrays with slices or integers (or sequences of integers, more
on this later):

In [89]: data[names == 'Bob', 2:]

Out [89]:

array([[-0.2349, 1.2792],

 

90 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

[-0.0523, 0.0672]])

In [90]: data[names == 'Bob', 3]
Out[90]: array([ 1.2792, 0.0672])

To select everything but 'Bob', you can either use != or negate the condition using -:

In [91]: names != 'Bob'
Out[91]: array([False, True, True, False, True, True, True], dtype=bool)

In [92]: data[-(names == 'Bob')]

Out[92]:

array([[-0.268 , 0.5465, 0.0939, -2.0445],
[-0.047 , -2.026 , 0.7719, 0.3103],
[-1.0023, -0.1698, 1.1503, 1.7289],
[ 0.1913, 0.4544, 0.4519, 0.5535],
[ 0.5994, 0.8174, -0.9297, -1.2564]])

Selecting two of the three names to combine multiple boolean conditions, use boolean
arithmetic operators like & (and) and | (or):

In [93]: mask = (names == 'Bob') | (names == 'Will')

In [94]: mask
Out[94]: array([True, False, True, True, True, False, False], dtype=bool)

In [95]: data[mask]

Out [95]:

array([[-0.048 , 0.5433, -0.2349, 1.2792],
[-0.047 , -2.026 , 0.7719, 0.3103],
[ 2.1452, 0.8799, -0.0523, 0.0672],
[-1.0023, -0.1698, 1.1503, 1.7289]])

Selecting data from an array by boolean indexing always creates a copy of the data,
even if the returned array is unchanged.

The Python keywords and and or do not work with boolean arrays.

 

Setting values with boolean arrays works in a common-sense way. To set all of the
negative values in data to 0 we need only do:

In [96]: data[data < 0] = 0

In [97]: data

Out [97]:

array([[ 0. » 0.5433, 0. » 1.2792],
[ 0. , 0.5465, 0.0939, 0. ],
[ 0. » 0. » 0.7719, 0.3103],
[ 2.1452, 0.8799, 0. » 0.0672],
[ 0. > 0. » 1.1503, 1.7289],
[ 0.1913, 0.4544, 0.4519, 0.5535],
[ 0.5994, 0.8174, 0 » 0 1)

 

The NumPy ndarray: A Multidimensional Array Object | 91

Setting whole rows or columns using a 1D boolean array is also easy:
n [98]: data[names != 'Joe'] = 7

n [99]: data

Out[99]:

array([[ 7 » Te 5 Te 5 % Js
[ 0 » 0.5465, 0.0939, 0. J],
[ 7. » 7 » 7 » 7 ],
[ 7. » 7 » 7 » 7 ],
[ 7. » 7 » 7 » 7 ],
[ 0.1913, 0.4544, 0.4519, 0.5535],
[ 0.5994, 0.8174, 0. » O. }))

Fancy Indexing

Fancy indexing is a term adopted by NumPy to describe indexing using integer arrays.
Suppose we had a 8 x 4 array:

In [100]: arr = np.empty((8, 4))

In [101]: for i in range(8):

sere a arr[i] =i

In [102]:

Out[102]:

array({[ 0., 0., 0., 0.],
[1., 1., 1., 1.],
[ 2., 2., 2., 2.],
[3-5 3-5 3-5 3-];
[4., 4., 4, 4],
[5-5 55 55 5e]5
[ 6., 6., 6., 6.],
[7-5 Tes Ts 7-]])

To select out a subset of the rows in a particular order, you can simply pass a list or
ndarray of integers specifying the desired order:

In [103]: arr[[4, 3, 0, 6]]

Out [103]:

array([[ 4., 4., 4.5 4.],
[3-5 3-5 3-5 3-],
[0., 0., 0., 0.],
[6., 6., 6., 6.]])

Hopefully this code did what you expected! Using negative indices select rows from
the end:

In [104]: arr[[-3, -5, -7]]

Out [104]:

array([[ 5., 5-, 5-, 5-],
[ 3-5 Bey Be, 3+],
[4.5 1, 1., 1.]])

 

92 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

Passing multiple index arrays does something slightly different; it selects a 1D array of
elements corresponding to each tuple of indices:

# more on reshape in Chapter 12
In [105]: arr = np.arange(32).reshape((8, 4))

In [106]: arr

Out [106]:

array([[ 0, 1, 2, 3],
[4, 5, 6, 7],
[ 8, 9, 10, 11],
[12, 13, 14, 15],
[16, 17, 18, 19],
[20, 21, 22, 23],
[24, 25, 26, 27],
[28, 29, 30, 31]])

In [107]: arr[[1, 5, 7, 2], [0, 3, 1, 2]]
Out[107]: array([ 4, 23, 29, 10])

Take a moment to understand what just happened: the elements (1, 0), (5, 3), (75
1),and (2, 2) were selected. The behavior of fancy indexing in this case is a bit different
from what some users might have expected (myself included), which is the rectangular
region formed by selecting a subset of the matrix’s rows and columns. Here is one way
to get that:
In [108]: arr[[1, 5, 7, 2]][:, [0, 3, 4, 2]]
Out [108]:
array([[ 4, 7, 5, 6],
[20, 23, 21, 22],
[28, 31, 29, 30],
[ 8, 11, 9, 10]])
Another way is to use the np.ix_ function, which converts two 1D integer arrays to an
indexer that selects the square region:
In [109]: arr[np.ix_([1, 5, 7, 2], [0, 3, 1, 2])]
Out[109]:
array([[ 4, 7, 5, 6],
[20, 23, 21, 22],
[28, 31, 29, 30],
[ 8, 11, 9, 10]])

Keep in mind that fancy indexing, unlike slicing, always copies the data into a new array.

Transposing Arrays and Swapping Axes

Transposing is a special form of reshaping which similarly returns a view on the un-
derlying data without copying anything. Arrays have the transpose method and also
the special T attribute:

In [110]: arr = np.arange(15).reshape((3, 5))

In [111]: arr In [112]: arr.T

 

The NumPy ndarray: A Multidimensional Array Object | 93

Out[111]: Out[112]:
array([[ 0, 1, 2, 3, 4], array({[ 0, 5, 10],
[5, 6, 7, 8 9], [ 41, 6, 11],
[10, 11, 12, 13, 14]]) [ 2, 7, 12],
[ 3, 8, 13],
[ 4, 9, 14]])

When doing matrix computations, you will do this very often, like for example com-
puting the inner matrix product XX using np. dot:

In [113]: arr = np.random.randn(6, 3)

In [114]: np.dot(arr.T, arr)

Out[114]:

array([[ 2.584 , 1.8753, 0.8888],
[ 1.8753, 6.6636, 0.3884],
[ 0.8888, 0.3884, 3.9781]])

For higher dimensional arrays, transpose will accept a tuple of axis numbers to permute
the axes (for extra mind bending):

In [115]: arr = np.arange(16).reshape((2, 2, 4))

In [116]: arr

Out [116]:

array([[[ 0, 1, 2, 3]
[4, 5, 6 7]
[[ 8, 9, 10, 11]
[12, 13, 14, 15]

In [117]: arr.transpose((1, 0, 2))
Out [117]:
array([[[ 0, 1, 2, 3],
[ 8, 9, 10, 11]],
[[4, 5, 6 7],
1]))

[12, 13, 14, 15
Simple transposing with .T is just a special case of swapping axes. ndarray has the
method swapaxes which takes a pair of axis numbers:

In [118]: arr In [119]: arr.swapaxes(1, 2)
Out [118]: Out[119]:
array([[[ 0, 1, 2, 3], array([[[ 0, 4],
[ 4, 55 6, 71], [ 1, 5],
[ 2, 6],
[[ 8, 9, 10, 12], [3, 7]],
[12, 13, 14, 15]]])
[[ 8, 12],
[ 9, 13],
[10, 14],

[11, 15]]])

swapaxes similarly returns a view on the data without making a copy.

 

94 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

Universal Functions: Fast Element-wise Array Functions

A universal function, or ufunc, is a function that performs elementwise operations on
data in ndarrays. You can think of them as fast vectorized wrappers for simple functions
that take one or more scalar values and produce one or more scalar results.

Many ufuncs are simple elementwise transformations, like sqrt or exp:

In [120]: arr = np.arange(10)

In [121]: np.sqrt(arr)

Out[121]:

array([ 0. 5 te » 1.4142, 1.7321, 2. » 2.2361, 2.4495,
2.6458, 2.8284, 3. ])

In [122]: np.exp(arr)

Out[122]:

array([ 1. ; 2.7183, 7.3891, 20.0855, 54.5982,
148.4132, 403.4288, 1096.6332, 2980.958 , 8103.0839])

These are referred to as unary ufuncs. Others, such as add or maximum, take 2 arrays
(thus, binary ufuncs) and return a single array as the result:

In [123]: x = randn(8)

In [124]: y = randn(8)

In [125]: x

Out[125]:

array([ 0.0749, 0.0974, 0.2002, -0.2551, 0.4655, 0.9222, 0.446 ,
-0.9337])

In [126]: y

Out[126]:

array([ 0.267 , -1.1131, -0.3361, 0.6117, -1.2323, 0.4788, 0.4315,
-0.7147])

In [127]: np.maximum(x, y) # element-wise maximum

Out[127]:

array([ 0.267 , 0.0974, 0.2002, 0.6117, 0.4655, 0.9222, 0.446 ,
-0.7147])

 

While not common, a ufunc can return multiple arrays. modf is one example, a vector-
ized version of the built-in Python divmod: it returns the fractional and integral parts of
a floating point array:

In [128]: arr = randn(7) * 5

In [129]: np.modf(arr)

Out[129]:

(array([-0.6808, 0.0636, -0.386 , 0.1393, -0.8806, 0.9363, -0.883 ]),
array([-2., 4., -3., 5.) -3-, 3., -6.]))

 

Universal Functions: Fast Element-wise Array Functions | 95

See Table 4-3 and Table 4-4 for a listing of available ufuncs.

Table 4-3. Unary ufuncs

Function
abs, fabs

sqrt

square

exp

log, logio, log2, logip
sign

ceil
floor

rint
modf
isnan

isfinite, isinf

cos, cosh, sin, sinh,
tan, tanh

arccos, arccosh, arcsin,
arcsinh, arctan, arctanh

logical_not

Description

Compute the absolute value element-wise for integer, floating point, or complex values.
Use fabs as a faster alternative for non-complex-valued data

Compute the square root of each element. Equivalent to arr ** 0.5
Compute the square of each element. Equivalent to arr ** 2

Compute the exponent e* of each element

Natural logarithm (base e), log base 10, log base 2, and log(1 + x), respectively
Compute the sign of each element: 1 (positive), 0 (zero), or -1 (negative)

Compute the ceiling of each element, i.e. the smallest integer greater than or equal to
each element

Compute the floor of each element, i.e. the largest integer less than or equal to each
element

Round elements to the nearest integer, preserving the dtype
Return fractional and integral parts of array as separate array
Return boolean array indicating whether each value is NaN (Not a Number)

Return boolean array indicating whether each element is finite (non-in*, non-NaN) or
infinite, respectively
Regular and hyperbolic trigonometric functions

Inverse trigonometric functions

Compute truth value of not x element-wise. Equivalent to - arr.

 

Table 4-4. Binary universal functions

Function

add

subtract

multiply

divide, floor_divide
power

maximum, max
minimum, fmin

mod

copysign

Description

Add corresponding elements in arrays

Subtract elements in second array from first array

Multiply array elements

Divide or floor divide (truncating the remainder)

Raise elements in first array to powers indicated in second array
Element-wise maximum. fmax ignores NaN

Element-wise minimum. fmin ignores NaN

Element-wise modulus (remainder of division)

Copy sign of values in second argument to values in first argument

 

96 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

Function Description

greater, greater_equal, Perform element-wise comparison, yielding boolean array. Equivalent to infix operators
ess, less equal, equal, » >=, <, <=, =5, Is
1 less equal, equal, >, >=, <, < |

not_equal
logical_and, Compute element-wise truth value of logical operation. Equivalent to infix operators &
logical _or, logical _xor lx &

 

Data Processing Using Arrays

Using NumPy arrays enables you to express many kinds of data processing tasks as
concise array expressions that might otherwise require writing loops. This practice of
replacing explicit loops with array expressions is commonly referred to as vectoriza-
tion. In general, vectorized array operations will often be one or two (or more) orders
of magnitude faster than their pure Python equivalents, with the biggest impact in any
kind of numerical computations. Later, in Chapter 12, I will explain broadcasting, a
powerful method for vectorizing computations.

As a simple example, suppose we wished to evaluate the function sqrt(x*2 + y*2)
across a regular grid of values. The np.meshgrid function takes two 1D arrays and pro-
duces two 2D matrices corresponding to all pairs of (x, y) in the two arrays:

In [130]: points = np.arange(-5, 5, 0.01) # 1000 equally spaced points
In [131]: xs, ys = np.meshgrid(points, points)

In [132]: ys

Out [132]:

array([[-5« 5 <5s 9 “Se 5 wees “Se yy SSe yy M50 J,
[-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99],
[-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98],

eey

[ 4.97, 4.97, 4.97, ..., 4.97, 4.97, 4.97],
[ 4.98, 4.98, 4.98, ..., 4.98, 4.98, 4.98],
[ 4.99, 4.99, 4.99, ..., 4.99, 4.99, 4.99]])

Now, evaluating the function is a simple matter of writing the same expression you
would write with two points:

In [134]: import matplotlib.pyplot as plt
In [135]: z = np.sqrt(xs ** 2 + ys ** 2)

In [136]: z

Out [136]:

array([[ 7.0711, 7.064 , 7.0569, ..., 7.0499, 7.0569, 7.064 ],
[ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569],
[ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499],
seey
[ 7.0499, 7.0428, 7.0357, ..., 7.0286, 7.0357, 7.0428],
[ 7.0569, 7.0499, 7.0428, ..., 7.0357, 7.0428, 7.0499],
[ 7.064 , 7.0569, 7.0499, ..., 7.0428, 7.0499, 7.0569]])

~s

~s

~s

 

Data Processing Using Arrays | 97

In [137]: plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()
Out [137]: <matplotlib.colorbar.Colorbar instance at 0x4e46d40>

In [138]: plt.title("Image plot of $\sqrt{x*2 + y*2}$ for a grid of values")
Out [138]: <matplotlib.text.Text at 0x4565790>

See Figure 4-3. Here I used the matplotlib function imshow to create an image plot from
a 2D array of function values.

 

Image plot of x? +y’ for a grid of values

 

200

400

600

800

 

0 200 400 600 800

 

 

 

Figure 4-3. Plot of function evaluated on grid

Expressing Conditional Logic as Array Operations

The numpy.where function is a vectorized version of the ternary expression x if condi
tion else y. Suppose we had a boolean array and two arrays of values:

In [140]: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])
In [141]: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])
In [142]: cond = np.array([True, False, True, True, False])

Suppose we wanted to take a value from xarr whenever the corresponding value in
cond is True otherwise take the value from yarr. A list comprehension doing this might

look like:

In [143]: result = [(x if c else y)
eens : for x, y, c in zip(xarr, yarr, cond) ]

In [144]: result
Out[144]: [1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5]

 

98 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

This has multiple problems. First, it will not be very fast for large arrays (because all
the work is being done in pure Python). Secondly, it will not work with multidimen-
sional arrays. With np.where you can write this very concisely:

In [145]: result = np.where(cond, xarr, yarr)

In [146]: result
Out[146]: array([ 1.1, 2.2, 1.3, 1.4, 2.5])

The second and third arguments to np.where don’t need to be arrays; one or both of
them can be scalars. A typical use of where in data analysis is to produce a new array of
values based on another array. Suppose you had a matrix of randomly generated data
and you wanted to replace all positive values with 2 and all negative values with -2.
This is very easy to do with np.where:

In [147]: arr = randn(4, 4)

In [148]: arr

Out [148]:

array([[ 0.6372, 2.2043, 1.7904, 0.0752],
[-1.5926, -1.1536, 0.4413, 0.3483],
[-0.1798, 0.3299, 0.7827, -0.7585],
[ 0.5857, 0.1619, 1.3583, -1.3865]])

In [149]: np.where(arr > 0, 2, -2)

Out[149]:
array({[ 2, 2, 2, 2],
[-2, -2, 2; 2],
[-2, 2, 2; -2];
[ 2, 2, 2, -2]])
In [150]: np.where(arr > 0, 2, arr) # set only positive values to 2
Out[150]:
array([[ 2. 3 de gy 2s 5 2s 1
[-1.5926, -1.1536, 2. 3 de 1
[-0.1798, 2. y 2 » 70.7585],
[ 2. Ds >» 2. » 71.3865]])

The arrays passed to where can be more than just equal sizes array or scalars.

With some cleverness you can use where to express more complicated logic; consider
this example where I have two boolean arrays, cond1 and cond2, and wish to assign a
different value for each of the 4 possible pairs of boolean values:

result = []
for i in range(n):
if condi[i] and cond2[i]:
result .append(0)
elif cond1[i]:
result.append(1)
elif cond2[i]:
result.append(2)
else:
result .append(3)

 

Data Processing Using Arrays | 99

While perhaps not immediately obvious, this for loop can be converted into a nested
where expression:

np.where(cond1 & cond2, 0,
np.where(cond1, 1,
np.where(cond2, 2, 3)))

In this particular example, we can also take advantage of the fact that boolean values
are treated as 0 or 1 in calculations, so this could alternatively be expressed (though a
bit more cryptically) as an arithmetic operation:

result = 1 * (cond1 & -cond2) + 2 * (cond2 & -cond1) + 3 * -(cond1 | cond2)

Mathematical and Statistical Methods

A set of mathematical functions which compute statistics about an entire array or about
the data along an axis are accessible as array methods. Aggregations (often called
reductions) like sum, mean, and standard deviation std can either be used by calling the
array instance method or using the top level NumPy function:

In [151]: arr = np.random.randn(5, 4) # normally-distributed data

In [152]: arr.mean()
Out[152]: 0.062814911084854597

In [153]: np.mean(arr)
Out[153]: 0.062814911084854597

In [154]: arr.sum()
Out[ 154]: 1.2562982216970919

Functions like mean and sum take an optional axis argument which computes the statistic
over the given axis, resulting in an array with one fewer dimension:
In [155]: arry.mean(axis=1)

Out[155]: array([-1.2833, 0.2844, 0.6574, 0.6743, -0.0187])

In [156]: arr.sum(0)
Out [156]: array([-3.1003, -1.6189, 1.4044, 4.5712])

Other methods like cumsum and cumprod do not aggregate, instead producing an array
of the intermediate results:

In [157]: arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])

In [158]: arr.cumsum(0) In [159]: arr.cumprod(1)

Out[158]: Out[159]:

array([[ 0, 1, 2], array([[ 0, 0, ol],
[3, 5, 7], [ 3, 12, 60],
[ 9, 12, 15]]) [ 6, 42, 336]])

See Table 4-5 for a full listing. We'll see many examples of these methods in action in
later chapters.

 

100 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

Table 4-5. Basic array statistical methods

Method Description

sum Sum of all the elements in the array or along an axis. Zero-length arrays have sum 0.

mean Arithmetic mean. Zero-length arrays have NaN mean.

std, var Standard deviation and variance, respectively, with optional degrees of freedom adjust-
ment (default denominator n).

min, max Minimum and maximum.

argmin, argmax Indices of minimum and maximum elements, respectively.

cumsum Cumulative sum of elements starting from 0

cumprod Cumulative product of elements starting from 1

 

Methods for Boolean Arrays
Boolean values are coerced to 1 (True) and 0 (False) in the above methods. Thus, sum
is often used as a means of counting True values in a boolean array:

In [160]: arr = randn(100)

In [161]: (arr > 0).sum() # Number of positive values
Out[161]: 44

There are two additional methods, any and al11, useful especially for boolean arrays.
any tests whether one or more values in an array is True, while all checks if every value
is True:

In [162]: bools = np.array([False, False, True, False])

In [163]: bools.any()
Out[163]: True

In [164]: bools.all1()
Out[164]: False

These methods also work with non-boolean arrays, where non-zero elements evaluate
to True.

Sorting
Like Python’s built-in list type, NumPy arrays can be sorted in-place using the sort
method:

In [165]: arr = randn(8)

In [166]: arr

Out [166]:

array([ 0.6903, 0.4678, 0.0968, -0.1349, 0.9879, 0.0185, -1.3147,
-0.5425])

In [167]: arr.sort()

 

Data Processing Using Arrays | 101

In [168]: arr

Out [168]:

array([-1.3147, -0.5425, -0.1349, 0.0185, 0.0968, 0.4678, 0.6903,
0.9879])

Multidimensional arrays can have each 1D section of values sorted in-place along an
axis by passing the axis number to sort:

In [169]: arr = randn(5, 3)

In [170]: arr

Out [170]:

array([[-0.7139, -1.6331, -0.4959],
[ 0.8236, -1.3132, -0.1935],
[-1.6748, 3.0336, -0.863 ],
[-0.3161, 0.5362, -2.468 ],
[ 0.9058, 1.1184, -1.0516]])

In [171]: arr.sort(1)
In [172]: arr

Out [172]:
array([[-1.6331, -0.7139, -0.4959],

[-1.3132, -0.1935, 0.8236],
[-1.6748, -0.863 , 3.0336],
[-2.468 , -0.3161, 0.5362],
[-1.0516, 0.9058, 1.1184]])

The top level method np. sort returns a sorted copy of an array instead of modifying
the array in place. A quick-and-dirty way to compute the quantiles of an array is to sort
it and select the value at a particular rank:

In [173]: large_arr = randn(1000)
In [174]: large_arr.sort()

In [175]: large arr[int(0.05 * len(large arr))] # 5% quantile

Out[175]: -1.5791023260896004
For more details on using NumPy’s sorting methods, and more advanced techniques
like indirect sorts, see Chapter 12. Several other kinds of data manipulations related to
sorting (for example, sorting a table of data by one or more columns) are also to be
found in pandas.

Unique and Other Set Logic

NumPy has some basic set operations for one-dimensional ndarrays. Probably the most
commonly used one is np.unique, which returns the sorted unique values in an array:
In [176]: names = np.array(['Bob', ‘Joe’, 'Will', ‘Bob’, ‘Will’, ‘Joe’, ‘Joe'])

In [177]: np.unique(names)
Out[177]:

 

102 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

array(['Bob', ‘Joe’, ‘Will'],
dtype=" |S4")

In [178]: ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])

In [179]: np.unique(ints)
Out[179]: array([1, 2, 3, 4])

Contrast np.unique with the pure Python alternative:

In [180]: sorted(set(names) )
Out[180]: ['Bob', ‘Joe’, 'Will']

Another function, np.inid, tests membership of the values in one array in another,
returning a boolean array:

In [181]: values = np.array([6, 0, 0, 3, 2, 5, 6])

In [182]: np.inid(values, [2, 3, 6])
Out[182]: array([ True, False, False, True, True, False, True], dtype=bool)

See Table 4-6 for a listing of set functions in NumPy.

Table 4-6. Array set operations

Method Description

unique(x) Compute the sorted, unique elements in x

intersectid(x, y) Compute the sorted, common elements in x and y

unionid(x, y) Compute the sorted union of elements

inid(x, y) Compute a boolean array indicating whether each element of x is contained in y
setdiffid(x, y) Set difference, elements in x that are not in y

setxorid(x, y) Set symmetric differences; elements that are in either of the arrays, but not both

File Input and Output with Arrays

NumPy is able to save and load data to and from disk either in text or binary format.
In later chapters you will learn about tools in pandas for reading tabular data into
memory.

Storing Arrays on Disk in Binary Format

np. save and np. load are the two workhorse functions for efficiently saving and loading
array data on disk. Arrays are saved by default in an uncompressed raw binary format
with file extension .npy.

In [183]: arr = np.arange(10)

In [184]: np.save('some_array', arr)

 

File Input and Output with Arrays | 103

If the file path does not already end in .npy, the extension will be appended. The array
on disk can then be loaded using np. load:

In [185]: np.load('some_array.npy' )
Out[185]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

You save multiple arrays in a zip archive using np.savez and passing the arrays as key-
word arguments:

In [186]: np.savez('array_archive.npz', a=arr, b=arr)

When loading an .npz file, you get back a dict-like object which loads the individual
arrays lazily:

In [187]: arch = np.load('array_archive.npz' )

In [188]: arch['b']
Out[188]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

Saving and Loading Text Files

Loading text from files is a fairly standard task. The landscape of file reading and writing
functions in Python can be a bit confusing for a newcomer, so I will focus mainly on
the read_csv and read_table functions in pandas. It will at times be useful to load data
into vanilla NumPy arrays using np.loadtxt or the more specialized np.genfromtxt.

These functions have many options allowing you to specify different delimiters, con-
verter functions for certain columns, skipping rows, and other things. Take a simple
case of a comma-separated file (CSV) like this:

In [191]: !cat array _ex.txt

0.580052, 0.186730,1.040717,1.134411
0.194163, -0.636917, -0.938659,0.124094
-0.126410, 0.268607, -0.695724, 0.047428
-1.484413,0.004176, -0.744203, 0.005487
2.302869, 0.200131, 1.670238, -1.881090
-0.193230,1.047233, 0.482803 ,0.960334

This can be loaded into a 2D array like so:

In [192]: arr = np.loadtxt('array_ex.txt', delimiter=',')

In [193]: arr

Out[193]:

array([[ 0.5801, 0.1867, 1.0407, 1.1344],
[ 0.1942, -0.6369, -0.9387, 0.1241],
[-0.1264, 0.2686, -0.6957, 0.0474],
[-1.4844, 0.0042, -0.7442, 0.0055],
[ 2.3029, 0.2001, 1.6702, -1.8811],
[-0.1932, 1.0472, 0.4828, 0.9603]])

np.savetxt performs the inverse operation: writing an array to a delimited text file.
genfromtxt is similar to loadtxt but is geared for structured arrays and missing data
handling; see Chapter 12 for more on structured arrays.

 

104 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

For more on file reading and writing, especially tabular or spreadsheet-
like data, see the later chapters involving pandas and DataFrame objects.

 

 

Linear Algebra

Linear algebra, like matrix multiplication, decompositions, determinants, and other
square matrix math, is an important part of any array library. Unlike some languages
like MATLAB, multiplying two two-dimensional arrays with * is an element-wise
product instead of a matrix dot product. As such, there is a function dot, both an array
method, and a function in the numpy namespace, for matrix multiplication:

In [194]: x = np.array([[1., 2., 3.], [4., 5.5 6.]])

In [195]: y = np.array([[6., 23-], [-1, 7], [8 9]])
In [196]: x In [197]: y
Out[196]: Out[197]:
array([[ 1., 2., 3.], array({[ 6., 23.],

[ 4., 5.; 6.]]) [ -1., Tals

[ 8., 9.]])

In [198]: x.dot(y) # equivalently np.dot(x, y)
Out [198]:

array([[ 28., 64.],
[ 67., 181.]])

A matrix product between a 2D array and a suitably sized 1D array results ina 1D array:

In [199]: np.dot(x, np.ones(3))

Out[199]: array([ 6., 15.])
numpy.linalg has a standard set of matrix decompositions and things like inverse and
determinant. These are implemented under the hood using the same industry-standard
Fortran libraries used in other languages like MATLAB and R, such as like BLAS, LA-
PACK, or possibly (depending on your NumPy build) the Intel MKL:

In [201]: from numpy.linalg import inv, qr
In [202]: X = randn(5, 5)
In [203]: mat = X.T.dot(X)

In [204]: inv(mat)

Out[ 204]:

array([[ 3.0361, -0.1808, -0.6878, -2.8285, -1.1911]
[-0.1808, 0.5035, 0.1215, 0.6702, 0.0956]
[-0.6878, 0.1215, 0.2904, 0.8081, 0.3049]
[-2.8285, 0.6702, 0.8081, 3.4152, 1.1557]
[-1.1911, 0.0956, 0.3049, 1.1557, 0.6051]

In [205]: mat.dot(inv(mat) )

 

Linear Algebra | 105

Out [205]:

array([

'
°o

[1., 0. » 0., -0.],
[ 0., 1., -0., 0., 0.],
[ 0., -0., 1., 0., 0.],
[ 0., -0., -0., 1., -0.],
[ 0., 0., 0., 0., 1.]])

In [206]: q, r = qr(mat)

In [207]: r

Out [207]:

array([[ -6.9271, 7.389 , 6.1227, -7.1163, -4.9215],

[ 0. » 73-9735, -0.8671, 2.9747, -5.7402],
[ 0. » 0. » 10.2681, 1.8909, 1.6079],
[ 0. » 0. » 0. » 71.2996, 3.3577],
[ 0. » oO. » 0. » oO. »  0.5571]])

See Table 4-7 for a list of some of the most commonly-used linear algebra functions.

 

 

The scientific Python community is hopeful that there may be a matrix
multiplication infix operator implemented someday, providing syntac-

¢12° tically nicer alternative to using np.dot. But for now this is the way.

Table 4-7. Commonly-used numpy.linalg functions

Function

diag

dot
trace
det
eig
inv
pinv
qr
svd
solve

Istsq

Description

Return the diagonal (or off-diagonal) elements of a square matrix as a 1D array, or convert a 1D array into a square
matrix with zeros on the off-diagonal

Matrix multiplication

Compute the sum of the diagonal elements

Compute the matrix determinant

Compute the eigenvalues and eigenvectors of a square matrix
Compute the inverse of a square matrix

Compute the Moore-Penrose pseudo-inverse inverse of a matrix
Compute the QR decomposition

Compute the singular value decomposition (SVD)

Solve the linear system Ax = b for x, where A is a square matrix

Compute the least-squares solution to Ax =b

Random Number Generation

The numpy.random module supplements the built-in Python random with functions for
efficiently generating whole arrays of sample values from many kinds of probability

 

106 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

distributions. For example, you can get a 4 by 4 array of samples from the standard
normal distribution using normal:
In [208]: samples = np.random.normal(size=(4, 4))
In [209]: samples
Out [209]:
array([[ 0.1241, 0.3026, 0.5238, 0.0009],
[ 1.3438, -0.7135, -0.8312, -2.3702],

[-1.8608, -0.8608, 0.5601, -1.2659],
[ 0.1198, -1.0635, 0.3329, -2.3594]])

Python’s built-in random module, by contrast, only samples one value at a time. As you
can see from this benchmark, numpy.random is well over an order of magnitude faster
for generating very large samples:

In [210]: from random import normalvariate
In [211]: N = 1000000

In [212]: %timeit samples = [normalvariate(0, 1) for _ in xrange(N)]
1 loops, best of 3: 1.33 s per loop

In [213]: %timeit np.random.normal(size=N)
10 loops, best of 3: 57.7 ms per loop

See Table 4-8 for a partial list of functions available in numpy.random. I’ll give some
examples of leveraging these functions’ ability to generate large arrays of samples all at
once in the next section.

Table 4-8. Partial list of numpy.random functions
Function Description
seed Seed the random number generator

permutation — Return a random permutation of a sequence, or return a permuted range

shuffle Randomly permute a sequence in place

rand Draw samples from a uniform distribution

randint Draw random integers from a given low-to-high range

randn Draw samples from a normal distribution with mean 0 and standard deviation 1 (MATLAB-like interface)
binomial Draw samples from a binomial distribution

normal Draw samples from a normal (Gaussian) distribution

beta Draw samples from a beta distribution

chisquare Draw samples from a chi-square distribution

gamma Draw samples from a gamma distribution

uniform Draw samples from a uniform [0, 1) distribution

 

Random Number Generation | 107

Example: Random Walks

An illustrative application of utilizing array operations is in the simulation of random

walks.

Let’s first consider a simple random walk starting at 0 with steps of 1 and -1

occurring with equal probability. A pure Python way to implement a single random
walk with 1,000 steps using the built-in random module:

import random

position = 0

walk = [position]
steps = 1000

for i in xrange(steps):

step = 1 if random.randint(0, 1) else -1
position += step
walk.append(position)

See Figure 4-4 for an example plot of the first 100 values on one of these random walks.

 

10

10

-15

 

 

Random walk with +1/-1 steps

 

 

0 20 40 60 80 100

 

 

Figure 4-4. A simple random walk

You might make the observation that walk is simply the cumulative sum of the random
steps and could be evaluated as an array expression. Thus, I use the np. random module
to draw 1,000 coin flips at once, set these to 1 and -1, and compute the cumulative sum:

In

[215]: nsteps = 1000
[216]: draws = np.random.randint(0, 2, size=nsteps)
[217]: steps = np.where(draws > 0, 1, -1)

[218]: walk = steps.cumsum()

 

108 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

From this we can begin to extract statistics like the minimum and maximum value along
the walk’s trajectory:

In [219]: walk.min() In [220]: walk.max()
Out[219]: -3 Out[220]: 31

A more complicated statistic is the first crossing time, the step at which the random
walk reaches a particular value. Here we might want to know how long it took the
random walk to get at least 10 steps away from the origin O in either direction.
np.abs(walk) >= 10 gives us a boolean array indicating where the walk has reached or
exceeded 10, but we want the index of the first 10 or -10. Turns out this can be com-
puted using argmax, which returns the first index of the maximum value in the boolean
array (True is the maximum value):

In [221]: (np.abs(walk) >= 10).argmax()

Out[221]: 37
Note that using argmax here is not always efficient because it always makes a full scan
of the array. In this special case once a True is observed we know it to be the maximum
value.

Simulating Many Random Walks at Once

If your goal was to simulate many random walks, say 5,000 of them, you can generate
all of the random walks with minor modifications to the above code. The numpy.ran
dom functions if passed a 2-tuple will generate a 2D array of draws, and we can compute
the cumulative sum across the rows to compute all 5,000 random walks in one shot:

In [222]: nwalks = 5000

In [223]: nsteps = 1000

In [224]: draws = np.random.randint(0, 2, size=(nwalks, nsteps)) #0 or 1
In [225]: steps = np.where(draws > 0, 1, -1)

In [226]: walks = steps.cumsum(1)

In [227]: walks

Out [227]:

array([[ 1, 0, 1, ..., 8, 7, 8],
[ 1, 0, -1, ..., 34, 33, 32],
[ 1, 0, “1, we, 4; 5» 4],

[ 1, 2, 1, ..., 24, 25, 26],

[ 1, 2, 3, ..-, 14, 13, 14],

[ -1, -2, “3, «+, -24, -23, -22]])
Now, we can compute the maximum and minimum values obtained over all of the
walks:

In [228]: walks.max() In [229]: walks.min()
Out [228]: 138 Out[229]: -133

 

Example: Random Walks | 109

Out of these walks, let’s compute the minimum crossing time to 30 or -30. This is
slightly tricky because not all 5,000 of them reach 30. We can check this using the
any method:

In [230]: hits30 = (np.abs(walks) >= 30).any(1)

In [231]: hits30
Out[231]: array([False, True, False, ..., False, True, False], dtype=bool)

In [232]: hits30.sum() # Number that hit 30 or -30

Out[232]: 3410
We can use this boolean array to select out the rows of walks that actually cross the
absolute 30 level and call argmax across axis 1 to get the crossing times:

In [233]: crossing times = (np.abs(walks[hits30]) >= 30).argmax(1)

In [234]: crossing times.mean()
Out[234]: 498.88973607038122

Feel free to experiment with other distributions for the steps other than equal sized
coin flips. You need only use a different random number generation function, like
normal to generate normally distributed steps with some mean and standard deviation:

In [235]: steps = np.random.normal(loc=0, scale=0.25,
sees : size=(nwalks, nsteps))

 

110 | Chapter 4: NumPy Basics: Arrays and Vectorized Computation

CHAPTER 5
Getting Started with pandas

 

pandas will be the primary library of interest throughout much of the rest of the book.
It contains high-level data structures and manipulation tools designed to make data
analysis fast and easy in Python. pandas is built on top of NumPy and makes it easy to
use in NumPy-centric applications.

As a bit of background, I started building pandas in early 2008 during my tenure at
AQR, a quantitative investment management firm. At the time, I had a distinct set of
requirements that were not well-addressed by any single tool at my disposal:

e Data structures with labeled axes supporting automatic or explicit data alignment.
This prevents common errors resulting from misaligned data and working with
ditterently-indexed data coming from different sources.

¢ Integrated time series functionality.

¢ The same data structures handle both time series data and non-time series data.

¢ Arithmetic operations and reductions (like summing across an axis) would pass
on the metadata (axis labels).

¢ Flexible handling of missing data.

¢ Merge and other relational operations found in popular database databases (SQL-
based, for example).

I wanted to be able to do all of these things in one place, preferably in a language well-
suited to general purpose software development. Python was a good candidate lan-
guage for this, but at that time there was not an integrated set of data structures and
tools providing this functionality.

Over the last four years, pandas has matured into a quite large library capable of solving
a much broader set of data handling problems than I ever anticipated, but it has ex-
panded in its scope without compromising the simplicity and ease-of-use that I desired
from the very beginning. I hope that after reading this book, you will find it to be just
as much of an indispensable tool as I do.

Throughout the rest of the book, I use the following import conventions for pandas:

 

111

In [1]: from pandas import Series, DataFrame
In [2]: import pandas as pd

Thus, whenever you see pd. in code, it’s referring to pandas. Series and DataFrame are
used so much that I find it easier to import them into the local namespace.

Introduction to pandas Data Structures

To get started with pandas, you will need to get comfortable with its two workhorse
data structures: Series and DataFrame. While they are not a universal solution for every
problem, they provide a solid, easy-to-use basis for most applications.

Series

A Series is a one-dimensional array-like object containing an array of data (of any
NumPy data type) and an associated array of data labels, called its index. The simplest
Series is formed from only an array of data:

In [4]: obj = Series([4, 7, -5, 3])

n[
ut [

5]: obj
5

WNROOH
wun Bos

The string representation of a Series displayed interactively shows the index on the left
and the values on the right. Since we did not specify an index for the data, a default
one consisting of the integers 0 through N - 1 (where N is the length of the data) is
created. You can get the array representation and index object of the Series via its values
and index attributes, respectively:

In [6]: obj.values

Out[6]: array([ 4, 7, -5, 3])

In [7]: obj.index
Out[7]: Int64Index([0, 1, 2, 3])

Often it will be desirable to create a Series with an index identifying each data point:
In [8]: obj2 = Series([4, 7, -5, 3], index=['d', 'b', ‘a’, 'c'])

a
wun Bos

nv
'

 

112 | Chapter 5: Getting Started with pandas

In [10]: obj2.index
Out[10]: Index([d, b, a, c], dtype=object)

Compared with a regular NumPy array, you can use values in the index when selecting
single values or a set of values:

In [11]: obj2['a']
Out[11]: -5

In [12]: obj2['d'] =

In [13]: obj2[['c', ‘a’, ‘d']]
Out [13]:

c 3

a -5

d 6

NumbPy array operations, such as filtering with a boolean array, scalar multiplication,
or applying math functions, will preserve the index-value link:

In [14]: obj2

Out[14]:

d 6

b 7

a -5

c 3

n [15]: obj2[obj2 > 0] In [16]: obj2 * 2 In [17]: np.exp(obj2)

outa 5]: Out[16]: Out[17]:

d 6 d 12 d 403 .428793

b 7 b 14 b 1096. 633158

c 3 a -10 a 0.006738
C 6 Cc 20.085537

Another way to think about a Series is as a fixed-length, ordered dict, as it is a mapping
of index values to data values. It can be substituted into many functions that expect a
dict:

In [18]: 'b' in obj2
Out[18]: True

In [19]: ‘e' in obj2
Out[19]: False

Should you have data contained in a Python dict, you can create a Series from it by
passing the dict:

In [20]: sdata = {'Ohio': 35000, 'Texas': 71000, ‘Oregon’: 16000, ‘Utah': 5000}
In [21]: obj3 = Series(sdata)

In [22]: obj3
Out [22]:

Ohio 35000
Oregon 16000

 

Introduction to pandas Data Structures | 113

Texas 71000
Utah 5000

When only passing a dict, the index in the resulting Series will have the dict’s keys in
sorted order.

In [23]: states = ['California', 'Ohio', 'Oregon', 'Texas']
In [24]: obj4 = Series(sdata, index=states)

In [25]: obj4

Out [25]:

California NaN
Ohio 35000
Oregon 16000
Texas 71000

In this case, 3 values found in sdata were placed in the appropriate locations, but since
no value for 'California' was found, it appears as NaN (not a number) which is con-
sidered in pandas to mark missing or NA values. I will use the terms “missing” or “NA”
to refer to missing data. The isnul1 and notnul1 functions in pandas should be used to
detect missing data:

In [26]: pd.isnull(obj4) In [27]: pd.notnull(obj4)
Out [26]: Out [27]:

California True California False

Ohio False Ohio True
Oregon False Oregon True
Texas False Texas True

Series also has these as instance methods:
In [28]: obj4.isnull()

Out [28]:

California True
Ohio False
Oregon False
Texas False

I discuss working with missing data in more detail later in this chapter.

A critical Series feature for many applications is that it automatically aligns differently-
indexed data in arithmetic operations:

In [29]: obj3 In [30]: obj4
Out[29]: Out [30]:

Ohio 35000 California NaN
Oregon 16000 Ohio 35000
Texas 71000 Oregon 16000
Utah 5000 Texas 71000

In [31]: obj3 + obj4

Out [31]:

California NaN
Ohio 70000
Oregon 32000

 

114 | Chapter 5: Getting Started with pandas

Texas 142000
Utah NaN

Data alignment features are addressed as a separate topic.

Both the Series object itself and its index have a name attribute, which integrates with
other key areas of pandas functionality:

In [32]: obj4.name = 'population'
In [33]: obj4.index.name = 'state'

In [34]: obj4

Out [34]:

state

California NaN
Ohio 35000
Oregon 16000
Texas 71000

Name: population

A Series’s index can be altered in place by assignment:
In [35]: obj.index = ['Bob', ‘Steve’, 'Jeff', ‘Ryan']

In [36]: obj

Out [36]:

Bob 4

Steve 7

Jeff “5

Ryan 3
DataFrame

A DataFrame represents a tabular, spreadsheet-like data structure containing an or-
dered collection of columns, each of which can be a different value type (numeric,
string, boolean, etc.). The DataFrame has both a row and column index; it can be
thought of as a dict of Series (one for all sharing the same index). Compared with other
such DataFrame-like structures you may have used before (like R’s data. frame), row-
oriented and column-oriented operations in DataFrame are treated roughly symmet-
rically. Under the hood, the data is stored as one or more two-dimensional blocks rather
than a list, dict, or some other collection of one-dimensional arrays. The exact details
of DataFrame’s internals are far outside the scope of this book.

While DataFrame stores the data internally in a two-dimensional for-
mat, you can easily represent much higher-dimensional data ina tabular
“8° format using hierarchical indexing, a subject of a later section and a key
* ingredient in many of the more advanced data-handling features in pan-

das.

 
 

 

 

Introduction to pandas Data Structures | 115

There are numerous ways to construct a DataFrame, though one of the most common
is from a dict of equal-length lists or NumPy arrays
data = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
"year': [2000, 2001, 2002, 2001, 2002],

"pop': [1.5, 1.7, 3.6, 2.4, 2.9]}
frame = DataFrame(data)

The resulting DataFrame will have its index assigned automatically as with Series, and
the columns are placed in sorted order:

In [38]: frame

Out [38]:

pop state year
1.5 Ohio 2000
Td Ohio 2001
3.6 Ohio 2002
2.4 Nevada 2001
2.9 Nevada 2002

BWNRO

If you specify a sequence of columns, the DataFrame’s columns will be exactly what
you pass:

In [39]: DataFrame(data, columns=['year', ‘state’, ‘pop'])

Out [39]:

year state pop
0 2000 Ohio 1.5
1 2001 Ohio 1.7
2 2002 Ohio 3.6
3 2001 Nevada 2.4
4 2002 Nevada 2.9

As with Series, if you pass a column that isn’t contained in data, it will appear with NA
values in the result:

In [40]: frame2 = DataFrame(data, columns=['year', ‘state’, 'pop', ‘debt'],
ataa i index=['one', 'two', 'three', 'four', 'five'])

In [41]: frame2
Out [41]:

year state pop debt
one 2000 Ohio 1.5 NaN

two 2001 Ohio 1.7. NaN
three 2002 Ohio 3.6 NaN
four 2001 Nevada 2.4 NaN
five 2002 Nevada 2.9 NaN

In [42]: frame2.columns
Out[42]: Index([year, state, pop, debt], dtype=object)

Acolumn in a DataFrame can be retrieved as a Series either by dict-like notation or by
attribute:

In [43]: frame2['state'] In [44]: frame2.year
Out [43]: Out[44]:
one Ohio one 2000

 

116 | Chapter 5: Getting Started with pandas

two Ohio two 2001

three Ohio three 2002
four Nevada four 2001
five Nevada five 2002
Name: state Name: year

Note that the returned Series have the same index as the DataFrame, and their name
attribute has been appropriately set.

Rows can also be retrieved by position or name by a couple of methods, such as the
ix indexing field (much more on this later):

In [45]: frame2.ix['three' ]

Out [45]:

year 2002
state Ohio
pop 3.6
debt NaN

Name: three

Columns can be modified by assignment. For example, the empty ‘debt ' column could
be assigned a scalar value or an array of values:
In [46]: frame2['debt'] = 16.5

In [47]: frame2
Out [47]:

year state pop debt
one 2000 Ohio 1.5 16.5
two 2001 Ohio 1.7 16.5
three 2002 Ohio 3.6 16.5
four 2001 Nevada 2.4 16.5
five 2002 Nevada 2.9 16.5

In [48]: frame2['debt'] = np.arange(5.)

In [49]: frame2
Out [49]:

year state pop debt
one 2000 Ohio 1.5 0
two 2001 Ohio 1.7 4.
three 2002 Ohio 3.6 2
four 2001 Nevada 2.4 3
five 2002 Nevada 2.9 4

When assigning lists or arrays to a column, the value’s length must match the length
of the DataFrame. If you assign a Series, it will be instead conformed exactly to the
DataFrame’s index, inserting missing values in any holes:

In [50]: val = Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])
In [51]: frame2['debt'] = val
In [52]: frame2

Out [52]:
year state pop debt

 

Introduction to pandas Data Structures | 117

one 2000 Ohio 1.5 NaN
two 2001 Ohio 1.7 -1.2
three 2002 Ohio 3.6 NaN
four 2001 Nevada 2.4 -1.5
five 2002 Nevada 2.9 -1.7

Assigning a column that doesn’t exist will create a new column. The del keyword will
delete columns as with a dict:

In [53]: frame2['eastern'] = frame2.state == 'Ohio'
In [54]: frame2
Out[54]:
year state pop debt eastern
one 2000 Ohio 1.5 NaN True
two 2001 Ohio 1.7 -1.2 True
three 2002 Ohio 3.6 NaN True
four 2001 Nevada 2.4 -1.5 False
five 2002 Nevada 2.9 -1.7_ False
In [55]: del frame2['eastern' ]
In [56]: frame2.columns
Out[56]: Index([year, state, pop, debt], dtype=object)

 

 

 

 

The column returned when indexing a DataFrame is a view on the un-
derlying data, not a copy. Thus, any in-place modifications to the Series
will be reflected in the DataFrame. The column can be explicitly copied
using the Series’s copy method.

Another common form of data is a nested dict of dicts format:

In [57]:

pop = {'Nevada': {2001: 2.4, 2002: 2.9},
‘Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}

If passed to DataFrame, it will interpret the outer dict keys as the columns and the inner
keys as the row indices:

In [58]:

In [59]:
Out [59]:

Nevada

2000
2001
2002

frame3 = DataFrame(pop)
frame3

Ohio
1.5
1.7
3.6

NaN
2.4
2.9

Of course you can always transpose the result:

In [60]: frame3.T

Out [60]:

Nevada
Ohio

2000 2001 2002
NaN

2.4
1.7

2.9

ded 3.6

 

118 | Chapter 5: Getting Started with pandas

The keys in the inner dicts are unioned and sorted to form the index in the result. This
isn’t true if an explicit index is specified:
In [61]: DataFrame(pop, index=[2001, 2002, 2003])

Out [61]:

Nevada Ohio
2001 2.4 1.7
2002 2.9 3.6

2003 NaN NaN

Dicts of Series are treated much in the same way:

In [62]: pdata = {'Ohio': frame3['Ohio'][:-1],
were t "Nevada': frame3[ 'Nevada' ][:2]}

In [63]: DataFrame(pdata)

Out [63]:

Nevada Ohio
2000 NaN 1.5
2001 2.4 1s7

For a complete list of things you can pass the DataFrame constructor, see Table 5-1.

If a DataFrame’s index and columns have their name attributes set, these will also be
displayed:

In [64]: frame3.index.name = 'year'; frame3.columns.name = ‘state’

In [65]: frame3

Out [65]:

state Nevada Ohio
year

2000 NaN 1.5
2001 2.4 1.7
2002 2.9 3.6

Like Series, the values attribute returns the data contained in the DataFrame as a 2D
ndarray:

In [66]: frame3.values

Out [66]:

array([[ nan,
[ 2.4,
[ 2.9,

1.5],
1.7],
3.6]])

If the DataFrame’s columns are different dtypes, the dtype of the values array will be
chosen to accomodate all of the columns:

In [67]: frame2.values

Out [67]:

array([[2000, Ohio, 1.5, nan]
[2001, Ohio, 1.7, -1.2
[2002, Ohio, 3.6, nan]
[2001, Nevada, 2.4,
[2002, Nevada, 2.9,

I
1,
2

1.5],
-1.7]], dtype=object)

 

Introduction to pandas Data Structures | 119

Table 5-1. Possible data inputs to DataFrame constructor

Type Notes
2D ndarray A matrix of data, passing optional row and column labels
dict of arrays, lists, or tuples Each sequence becomes a column in the DataFrame. All sequences must be the same length.

NumPy structured/record array Treated as the “dict of arrays” case

dict of Series Each value becomes a column. Indexes from each Series are unioned together to form the
result’s row index if no explicit index is passed.

dict of dicts Each inner dict becomes a column. Keys are unioned to form the row index as in the “dict of
Series” case.

list of dicts or Series Each item becomes a row in the DataFrame. Union of dict keys or Series indexes become the
DataFrame’s column labels

List of lists or tuples Treated as the “2D ndarray” case

Another DataFrame The DataFrame’s indexes are used unless different ones are passed

NumPy MaskedArray Like the “2D ndarray” case except masked values become NA/missing in the DataFrame result

Index Objects

pandas’s Index objects are responsible for holding the axis labels and other metadata
(like the axis name or names). Any array or other sequence of labels used when con-
structing a Series or DataFrame is internally converted to an Index:

In [68]: obj = Series(range(3), index=['a', 'b', 'c'])
In [69]: index = obj.index

In [70]: index
Out[70]: Index([a, b, c], dtype=object)

In [71]: index[1:]
Out[71]: Index([b, c], dtype=object)

Index objects are immutable and thus can’t be modified by the user:
In [72]: index[1] = 'd'

Exception Traceback (most recent call last)

<ipython-input-72-676fdeb26a68> in <module>()

----> 1 index[1] = 'd'

/Users/wesm/code/pandas/pandas/core/index.pyc in _setitem_(self, key, value)
302 def _setitem_(self, key, value):

303 """Disable the setting of values."""
--> 304 raise Exception(str(self._class__) + ' object is immutable’)
305

306 def _ getitem_(self, key):
Exception: <class 'pandas.core.index.Index'> object is immutable

 

120 | Chapter 5: Getting Started with pandas

Immutability is important so that Index objects can be safely shared among data
structures:

In [73]: index = pd.Index(np.arange(3) )
In [74]: obj2 = Series([1.5, -2.5, 0], index=index)

In [75]: obj2.index is index
Out[75]: True

Table 5-2 has a list of built-in Index classes in the library. With some development
effort, Index can even be subclassed to implement specialized axis indexing function-
ality.

a a,

   

Many users will not need to know much about Index objects, but they’ re
nonetheless an important part of pandas’s data model.

 

Table 5-2. Main Index objects in pandas

Class Description

Index The most general Index object, representing axis labels in a NumPy array of Python objects.
Int64Index Specialized Index for integer values.

MultiIndex “Hierarchical” index object representing multiple levels of indexing on a single axis. Can be thought of

as similar to an array of tuples.
DatetimeIndex — Stores nanosecond timestamps (represented using NumPy’s datetime64 dtype).

PeriodIndex Specialized Index for Period data (timespans).

In addition to being array-like, an Index also functions as a fixed-size set:
In [76]: frame3

Out [76]:

state Nevada Ohio
year

2000 NaN 1.5
2001 2.4 1.7
2002 2.9 3.6

In [77]: 'Ohio' in frame3.columns
Out[77]: True

In [78]: 2003 in frame3.index
Out[78]: False

Each Index has a number of methods and properties for set logic and answering other
common questions about the data it contains. These are summarized in Table 5-3.

 

Introduction to pandas Data Structures | 121

Table 5-3. Index methods and properties

Method
append

diff
intersection
union

isin

delete

drop

insert
is_monotonic
is_unique

unique

Description

Concatenate with additional Index objects, producing a new Index

Compute set difference as an Index

Compute set intersection

Compute set union

Compute boolean array indicating whether each value is contained in the passed collection
Compute new Index with element at index i deleted

Compute new index by deleting passed values

Compute new Index by inserting element at index i

Returns True if each element is greater than or equal to the previous element
Returns True if the Index has no duplicate values

Compute the array of unique values in the Index

Essential Functionality

In this section, I’ll walk you through the fundamental mechanics of interacting with
the data contained in a Series or DataFrame. Upcoming chapters will delve more deeply
into data analysis and manipulation topics using pandas. This book is not intended to
serve as exhaustive documentation for the pandas library; I instead focus on the most
important features, leaving the less common (that is, more esoteric) things for you to
explore on your own.

Reindexing

A critical method on pandas objects is reindex, which means to create a new object
with the data conformed to a new index. Consider a simple example from above:

In [79]:

obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', ‘a’, 'c'])

Calling reindex on this Series rearranges the data according to the new index, intro-
ducing missing values if any index values were not already present:

n [81]:

n [82]:
Out [82]:
a -5.3

obj2 = obj.reindex(['a', 'b', 'c', ‘d', ‘e'])

obj2

 

122 | Chapter 5: Getting Started with pandas

For ordered data like time series, it may be desirable to do some interpolation or filling
of values when reindexing. The method option allows us to do this, using a method such
as ffi11 which forward fills the values:

In [84]: obj3 = Series(['blue', ‘purple’, 'yellow'], index=[0, 2, 4])

In [85]: obj3.reindex(range(6), method='ffil1')
Out [85]:

0 blue

1 blue

2 purple

3 purple

4 yellow

5 yellow

Table 5-4 lists available method options. At this time, interpolation more sophisticated
than forward- and backfilling would need to be applied after the fact.
Table 5-4. reindex method (interpolation) options

Argument Description
fil] or pad Fill (or carry) values forward
bfillorbackfill _ Fill (or carry) values backward

 

With DataFrame, reindex can alter either the (row) index, columns, or both. When
passed just a sequence, the rows are reindexed in the result:

In [86]: frame = DataFrame(np.arange(9).reshape((3, 3)), index=['a', ‘c', ‘d'],
wwe columns=['Ohio', 'Texas', ‘California’ ])

In [87]: frame

Out [87]:

Ohio Texas California
a 0 dl 2
Cc 5 4 5
d 6 7 8

In [88]: frame2 = frame.reindex(['a', 'b', ‘c', 'd'])

In [89]: frame2
Out [89]:

 

Essential Functionality | 123

Ohio Texas California

a 0 1 2
b NaN NaN NaN
c 3 4 5
d 6 7 8

The columns can be reindexed using the columns keyword:
In [90]: states = ['Texas', 'Utah', 'California']

In [91]: frame.reindex(columns=states)

Out[91]:

Texas Utah California
a 1 NaN 2
c 4 NaN 5
d 7 NaN 8

Both can be reindexed in one shot, though interpolation will only apply row-wise (axis
0):

In [92]: frame.reindex(index=['a', 'b', 'c', 'd'], method='ffill',
wwe columns=states)

Out[92]:

Texas Utah California
a 1 NaN 2
b 1 NaN 2
c 4 NaN 5
d 7 NaN 8

As you'll see soon, reindexing can be done more succinctly by label-indexing with ix:
In [93]: frame.ix[['a', 'b', 'c', 'd'], states]

Out [93]:

Texas Utah California
a 1 NaN 2
b NaN NaN NaN
Cc 4 NaN 5
d 7 ~~ ~NaN 8

Table 5-5. reindex function arguments

Argument Description

index New sequence to use as index. Can be Index instance or any other sequence-like Python data structure. An
Index will be used exactly as is without any copying

method Interpolation (fill) method, see Table 5-4 for options.

fill_value — Substitute value to use when introducing missing data by reindexing

limit When forward- or backfilling, maximum size gap to fill
level Match simple Index on level of Multilndex, otherwise select subset of
copy If True, always copy underlying data even if new index is equivalent to old index. Otherwise, do not copy the

data when the indexes are equivalent.

 

124 | Chapter 5: Getting Started with pandas

Dropping entries from an axis

Dropping one or more entries from an axis is easy if you have an index array or list
without those entries. As that can require a bit of munging and set logic, the drop
method will return a new object with the indicated value or values deleted from an axis:

In [94]: obj = Series(np.arange(5.), index=['a', 'b', 'c', ‘d', ‘e'])

In [95]: new_obj = obj.drop('c')

In [96]: new_obj
Out [96]

a 0

b 1

d 3

e 4

In [97]: obj.drop(['d', 'c'])
Out [97]:

a 0

b 1

e 4

With DataFrame, index values can be deleted from either axis:

In [98]: data = DataFrame(np.arange(16).reshape((4, 4)),
ataa i index=['Ohio', 'Colorado', ‘Utah', ‘New York'],
asa? columns=['one', 'two', ‘three’, 'four'])

In [99]: data.drop(['Colorado', 'Ohio'])

Out [99]:

one two three four
Utah 8 9 10 11
New York 12 13 14 15
In [100]: data.drop('two', axis=1) In [101]: data.drop(['two', 'four'], axis=1)
Out[100]: Out[101]:

one three four one three
Ohio 0 2 3 Ohio 0 2
Colorado 4 6 7 Colorado 4 6
Utah 8 10 11 Utah 8 10
New York 12 14 15 New York 12 14

Indexing, selection, and filtering

Series indexing (obj[...]) works analogously to NumPy array indexing, except you can
use the Series’s index values instead of only integers. Here are some examples of this:

In [102]: obj = Series(np.arange(4.), index=['a', 'b', 'c', ‘d'])

In [103]: obj['b'] In [104]: obj[1]

Out[103]: 1.0 Out[104]: 1.0

In [105]: obj[2:4] In [106]: obj[['b', ‘a’, ‘d']]
Out [105]: Out [106]:

 

Essential Functionality | 125

c 2 b 1

d 3 a
d 3
In [107]: obj[[1, 3]] In [108]: obj[obj < 2]
Out [107]: Out [108]:
b 1 a 0
d 3 b 1

Slicing with labels behaves differently than normal Python slicing in that the endpoint
is inclusive:

In [109]: obj['b':'c']

Out[109]:
b 1
Cc 2

Setting using these methods works just as you would expect:
In [110]: obj['b':'c'] = 5

As you’ve seen above, indexing into a DataFrame is for retrieving one or more columns
either with a single value or sequence:
In [112]: data = DataFrame(np.arange(16).reshape((4, 4)),

aerosol index=['Ohio', 'Colorado', ‘Utah', ‘New York'],
sere a columns=['one', 'two', ‘three’, 'four'])

Out [113]

one two three four
Ohio 0 1 2 3
Colorado 4 5 6 7
Utah 8 9 10 11
New York 12 13 14 15
In [114]: data['two' ] In [115]: data[['three', ‘one']]
Out [114]: Out [115]:
Ohio 1 three one
Colorado 5 Ohio 2 0
Utah 9 Colorado 6 4
New York 13 Utah 10 8
Name: two New York 14 12

Indexing like this has a few special cases. First selecting rows by slicing or a boolean
array:
In [116]: data[:2] In [117]: data[data['three'] > 5]

Out [116]: Out[117]:
one two three four one two three four

 

126 | Chapter5: Getting Started with pandas

Ohio 0 1 2 3 Colorado 4 5 6 7
Colorado 4 5 6 7 Utah 8 9 10 11
New York 12 13 14 15

This might seem inconsistent to some readers, but this syntax arose out of practicality
and nothing more. Another use case is in indexing with a boolean DataFrame, such as
one produced by a scalar comparison:

In [118]: data < 5

Out [118]:

one two three four
Ohio True True True True
Colorado True False False False
Utah False False False False

New York False False False False
In [119]: data[data < 5] = 0

In [120]: data

Out[120]:

one two three four
Ohio 0 0 0 0
Colorado 0 5 6 7
Utah 8 9 10 11
New York 12 13 14 15

This is intended to make DataFrame syntactically more like an ndarray in this case.

For DataFrame label-indexing on the rows, I introduce the special indexing field ix. It
enables you to select a subset of the rows and columns from a DataFrame with NumPy-
like notation plus axis labels. As I mentioned earlier, this is also a less verbose way to
do reindexing:

In [121]: data.ix['Colorado', ['two', 'three']]

Out[121]:
two 5
three 6

Name: Colorado

In [122]: data.ix[['Colorado', 'Utah'], [3, 0, 1]]
Out[122]:

four one two
Colorado 7 ) 5

Utah 11 8 9

In [123]: data.ix[2] In [124]: data.ix[:'Utah', 'two']
Out [123]: Out [124]:

one 8 Ohio 0

two 9 Colorado 5

three 10 Utah 9

four 11 Name: two

Name: Utah

In [125]: data.ix[data.three > 5, :3]
Out [125]:

 

Essential Functionality | 127

one two three

Colorado 0 5 6
Utah 8 9 10
New York 12 13 14

So there are many ways to select and rearrange the data contained in a pandas object.
For DataFrame, there is a short summary of many of them in Table 5-6. You have a
number of additional options when working with hierarchical indexes as you’ll later
see.

When designing pandas, I felt that having to type frame[:, col] to select
a column was too verbose (and error-prone), since column selection is
18° one of the most common operations. Thus I made the design trade-off
to push all of the rich label-indexing into ix.

  

 

Table 5-6. Indexing options with DataFrame

Type Notes

obj[val] Select single column or sequence of columns from the DataFrame. Special case con-
veniences: boolean array (filter rows), slice (slice rows), or boolean DataFrame (set
values based on some criterion).

obj.ix[val] Selects single row or subset of rows from the DataFrame.

obj.ix[:, val] Selects single column of subset of columns.

obj.ix[val1, val2] Select both rows and columns.

reindex method Conform one or more axes to new indexes.

xs method Select single row or column as a Series by label.

icol, irowmethods Select single column or row, respectively, as a Series by integer location.

get_value, set_valuemethods — Select single value by row and column label.

Arithmetic and data alignment

One of the most important pandas features is the behavior of arithmetic between ob-
jects with different indexes. When adding together objects, if any index pairs are not
the same, the respective index in the result will be the union of the index pairs. Let’s
look at a simple example:

In [126]: s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', ‘e'])

In [127]: s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', ‘e', 'f', 'g'])

In [128]: sa In [129]: s2
Out [128]: Out[129]:
a 7.3 a -2.1
cc -2.5 c 3.6
d 3.4 e -1.5

 

128 | Chapter5: Getting Started with pandas

1.5

Adding these together yields:

In [130]: s1 + s2

Out [130]:

a

AoaAan

8g

502
1.
NaN
0.0
NaN
NaN

The internal data alignment introduces NA values in the indices that don’t overlap.
Missing values propagate in arithmetic computations.

In the case of DataFrame, alignment is performed on both the rows and the columns:

In [131]: df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'),

In [133]:
Out [133]:

Ohio

Texas
Colorado

index=['Ohio', 'Texas', 'Colorado'])

: df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'),

index=['Utah', ‘Ohio’, 'Texas', ‘Oregon'])

df4 In [134]: df2
Out [134]:
bcd b die
Oo 1 2 Utah 0 1 2
34°55 Ohio 3 4 °5
6 7 8 Texas 6 7 8
Oregon 9 10 11

Adding these together returns a DataFrame whose index and columns are the unions
of the ones in each DataFrame:

In [135]: df1 + df2

Out [135]:

bc dee

Colorado NaN NaN NaN NaN

Ohio

Oregon
Texas
Utah

3 NaN 6 NaN

NaN NaN NaN NaN

9 NaN 12 NaN

NaN NaN NaN NaN

Arithmetic methods with fill values

In arithmetic operations between differently-indexed objects, you might want to fill
with a special value, like 0, when an axis label is found in one object but not the other:

In [136]: df1 = DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))

In [137]: df2 = DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde' ))

In [138]: df1 In [139]: df2
Out [138]: Out [139]:
ab ocd a b cde

 

Essential Functionality | 129

001 2 3 0 oO 1 2 3 4
145 6 7 1 5 6 7 8 9
2 8 9 10 411 2 10 11 12 #13 14

3 15 16 17 18 19

Adding these together results in NA values in the locations that don’t overlap:

In [140]: df1 + df2
Out [140]:

a b oc dee
0 0 2 4. 6 NaN
1 9 11 13 15 NaN
2 18 20 22 24 NaN
3 NaN NaN NaN NaN NaN

Using the add method on df1, I pass df2 and an argument to fill_value:

In [141]: df1.add(df2, fill_value=o)
Out [141]:

a bc dee

0 2 4 6 4

9 11 13 #15 #9

18 20 22 24 14

15 16 17 #18 19

WNrR OO

Relatedly, when reindexing a Series or DataFrame, you can also specify a different fill

value:
In [142]: df1.reindex(columns=df2.columns, fill_value=o)
Out [142]:
a bc de
001 2 3 0
145 6 7 0
2 8 9 10 11 0

Table 5-7. Flexible arithmetic methods

Method _ Description
add Method for addition (+)

sub Method for subtraction (-)
div Method for division (/)
mul Method for multiplication (*)

 

Operations between DataFrame and Series

As with NumPy arrays, arithmetic between DataFrame and Series is well-defined. First,
as a motivating example, consider the difference between a 2D array and one of its rows:

In [143]: arr = np.arange(12.).reshape((3, 4))

In [144]: arr
Out [144]:
array([[ 0., 1., 2., 3.],

[ 4.5 5-5 6.5 7-];

 

130 | Chapter5: Getting Started with pandas

[ 8., 9., 10., 11.]])

In [145]: arr[o]
Out[145]: array([ 0., 1., 2., 3.])

In [146]: arr - arr[o]
Out [146]:

array([[ 0. >
[ 4. ,
[ 8. ])
This is referred to as broadcasting and is explained in more detail in Chapter 12. Op-
erations between a DataFrame and a Series are similar:

of Oo
vw ewe
of Oo
ay

of oO

vee

*)
*)
*)

In [147]: frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'),
ugiea § : index=['Utah', 'Ohio', 'Texas', '‘Oregon'])

In [148]: series = frame.ix[0]

In [149]: frame In [150]: series
Out[149]: Out[150]:

b die b 0)
Utah 0 1 2 d 1
Ohio 3 4 5 e 2
Texas 6 7 8 Name: Utah
Oregon 9 10 11

By default, arithmetic between DataFrame and Series matches the index of the Series
on the DataFrame's columns, broadcasting down the rows:

In [151]: frame - series

Out[151]:
bde
Utah 00 0
Ohio 3 3 3
Texas 6 6 6
Oregon 9 9 9

If an index value is not found in either the DataFrame’s columns or the Series’s index,
the objects will be reindexed to form the union:

In [152]: series2 = Series(range(3), index=['b', 'e', 'f'])

In [153]: frame + series2
Out[153]:

b de f
Utah O NaN 3 NaN
Ohio 3 NaN 6 NaN
Texas 6 NaN 9 NaN
Oregon 9 NaN 12 NaN

If you want to instead broadcast over the columns, matching on the rows, you have to
use one of the arithmetic methods. For example:

In [154]: series3 = frame['d']

In [155]: frame In [156]: series3

 

Essential Functionality | 131

Out[155]: Out[156]:

b die Utah 1
Utah o 1 2 Ohio 4
Ohio 3 4 #5 __ Texas 7
Texas 6 7 8 Oregon 10
Oregon 9 10 11 Name: d

In [157]: frame.sub(series3, axis=0)

Out[157]:
bde
Utah -1 0 1
Ohio -1 0 1
Texas -1 0 1
Oregon -1 0 1

The axis number that you pass is the axis to match on. In this case we mean to match
on the DataFrame’s row index and broadcast across.

Function application and mapping

NumPy ufuncs (element-wise array methods) work fine with pandas objects:

In [158]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'),
ore : index=['Utah', 'Ohio', 'Texas', ‘Oregon'])

In [159]: frame In [160]: np.abs(frame)
Out[ 159]: Out [160]:

b d e b d e
Utah  -0.204708 0.478943 -0.519439 Utah 0.204708 0.478943 0.519439
Ohio -0.555730 1.965781 1.393406 Ohio 0.555730 1.965781 1.393406
Texas 0.092908 0.281746 0.769023 Texas 0.092908 0.281746 0.769023
Oregon 1.246435 1.007189 -1.296221 Oregon 1.246435 1.007189 1.296221

Another frequent operation is applying a function on 1D arrays to each column or row.
DataFrame’s apply method does exactly this:

In [161]: f = lambda x: x.max() - x.min()

In [162]: frame.apply(f) In [163]: frame.apply(f, axis=1)
Out [162]: Out [163]:

b 1.802165 Utah 0.998382

d 1.684034 Ohio 24521517

e 2.689627 Texas 0.676115

Oregon 2.542656

Many of the most common array statistics (like sum and mean) are DataFrame methods,
so using apply is not necessary.

The function passed to apply need not return a scalar value, it can also return a Series
with multiple values:
In [164]: def f(x):
soneed return Series([x.min(), x.max()], index=['min', 'max'])

In [165]: frame.apply(f)

 

132 | Chapter5: Getting Started with pandas

Out [165]:
b

d e

min -0.555730 0.281746 -1.296221
max 1.246435 1.965781 1.393406

Element-wise Python functions can be used, too. Suppose you wanted to compute a
formatted string from each floating point value in frame. You can do this with applymap:

In [166]: format = lambda x: '%.2f' % x

In [167]: frame.applymap(format)

Out [167]:

b
Utah -0.20
Ohio -0.56

Texas 0.09
Oregon 1.25

d
0.48
1.97
0.28
1.01

e
-0.52
1.39
0.77
-1.30

The reason for the name applymap is that Series has a map method for applying an ele-
ment-wise function:

In [168]: frame['e'].map(format)

Out [168]:
Utah -0.52
Ohio 1.39
Texas 0.77
Oregon -1.30
Name: e

Sorting and ranking

Sorting a data set by some criterion is another important built-in operation. To sort
lexicographically by row or column index, use the sort_index method, which returns
a new, sorted object:

In [169]: obj = Series(range(4), index=['d', 'a', 'b', 'c'])

In [170]: obj.sort_index()

Out[170]:
a 1
b 2
c 3
d 0

With a DataFrame, you can sort by index on either axis:

In [171]: frame

= DataFrame(np.arange(8).reshape((2, 4)), index=['three', ‘one'],

columns=['d', 'a', 'b', 'c'])

In [172]: frame.sort_index() In [173]: frame.sort_index(axis=1)
Out[172]: Out[173]:
dabe a bcd
one 45 67 three 1 2 3 0
three 0 1 2 3 one 5 6 7 4

 

Essential Functionality | 133

The data is sorted in ascending order by default, but can be sorted in descending order,
too:

In [174]: frame.sort_index(axis=1, ascending=False)
Out[174]:

dc b
three 0 3 2
one 4 7 6

mR w&

To sort a Series by its values, use its order method:
In [175]: obj = Series([4, 7, -3, 2])

In [176]: obj.order()

Out [176]:
2 -3
3 2
0 4
1 7

Any missing values are sorted to the end of the Series by default:

In [177]: obj = Series([4, np.nan, 7, np.nan, -3, 2])

In [178]: obj.order()
Out [178]:

4 “3
5 2
0 4
2 7
1 NaN
3 NaN

On DataFrame, you may want to sort by the values in one or more columns. To do so,
pass one or more column names to the by option:

In [179]: frame = DataFrame({'b': [4, 7, -3, 2], ‘a’: [0, 1, 0, 1]})

In [180]: frame In [181]: frame.sort_index(by='b')
Out [180]: Out [181]:
a b a b
00 4 2 0-3
117 3 1 2
2 0 -3 00 4
3 1 2 117

To sort by multiple columns, pass a list of names:

In [182]: frame.sort_index(by=['a', 'b'])
Out[182]:
a b
2 0-3
0 0 4
3 12
117

 

134 | Chapter5: Getting Started with pandas

Ranking is closely related to sorting, assigning ranks from one through the number of
valid data points in an array. It is similar to the indirect sort indices produced by
numpy.argsort, except that ties are broken according to a rule. The rank methods for
Series and DataFrame are the place to look; by default rank breaks ties by assigning
each group the mean rank:

In [183]: obj = Series([7, -5, 7, 4, 2, 0, 4])
In [184]: obj.rank()

Out [184]:
6.5

aAuFPWNPR OO
PNWAB WE
Wmuoouwunne

Ranks can also be assigned according to the order they’re observed in the data:
In [185]: obj.rank(method=' first’)

Out [185]:
0 6
1 1
2 7
3 4
4 3
5 2
6 5

Naturally, you can rank in descending order, too:

In [186]: obj.rank(ascending=False, method='max' )

Out [186]:
0 2
1 7
2 2
3 4
4 5
5 6
6 4

See Table 5-8 fora list of tie-breaking methods available. DataFrame can compute ranks
over the rows or the columns:

In [187]: frame = DataFrame({'b': [4.3, 7, -3, 2], ‘a': [0, 1, 0, 1],
sees : "c': [-2, 5, 8, -2.5]})

In [188]: frame In [189]: frame.rank(axis=1)
Out [188]: Out[189]:
a b c a be
0 0 4.3 -2.0 0 3 4
1 1 7.0 5.0 113 2
2 0-3.0 8.0 22 1 3
3 1 2.0 -2.5 3 2 3 1

 

Essential Functionality | 135

Table 5-8. Tie-breaking methods with rank

Method Description

‘average’ _ Default: assign the average rank to each entry in the equal group.
"min' Use the minimum rank for the whole group.

"max" Use the maximum rank for the whole group.

"first' Assign ranks in the order the values appear in the data.

 

Axis indexes with duplicate values

Up until now all of the examples I’ve showed you have had unique axis labels (index
values). While many pandas functions (like reindex) require that the labels be unique,
it’s not mandatory. Let’s consider a small Series with duplicate indices:

In [190]: obj = Series(range(5), index=['a', 'a', 'b', ‘'b’, ‘c'])

]
]

191]: obj
191

PWNPRP OW WO

The index’s is_unique property can tell you whether its values are unique or not:
In [192]: obj.index.is unique
Out[192]: False

Data selection is one of the main things that behaves differently with duplicates. In-
dexing a value with multiple entries returns a Series while single entries return a scalar
value:

In [193]: obj['a'] In [194]: obj['c']

Out [193]: Out[194]: 4
a 0
a 1

The same logic extends to indexing rows in a DataFrame:
In [195]: df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])

In [196]: df
Out [196]:

0 4, 2
0.274992 0.228913 1.352917
0.886429 -2.001637 -0.371843
1.669025 -0.438570 -0.539741
0.476985 3.248944 -1.021228

coon wy

In [197]: df.ix['b']
Out[197]:
0 1 2

 

136 | Chapter5: Getting Started with pandas

b 1.669025 -0.438570 -0.539741
b 0.476985 3.248944 -1.021228

Summarizing and Computing Descriptive Statistics

pandas objects are equipped with a set of common mathematical and statistical meth-
ods. Most of these fall into the category of reductions or summary statistics, methods
that extract a single value (like the sum or mean) froma Series or a Series of values from
the rows or columns of a DataFrame. Compared with the equivalent methods of vanilla
NumbPy arrays, they are all built from the ground up to exclude missing data. Consider
a small DataFrame:
In [198]: df = DataFrame([[1.4, np.nan], [7.1, -4.5],

wnarere & : [np.nan, np.nan], [0.75, -1.3]],

newest index=['a', 'b', 'c', ‘d'],

marry 2 columns=['one', '‘two'])

In [199]: df
Out[199]:
one two
a 1.40 NaN
b 7.10 -4.5
c NaN NaN
d 0.75 -1.3

Calling DataFrame’s sum method returns a Series containing column sums:
In [200]: df.sum()

Out[200]:
one 9.25
two -5.80

Passing axis=1 sums over the rows instead:
In [201]: df.sum(axis=1)

Out[201]:
a 1.40
b 2.60
c NaN
d  -0.55

NA values are excluded unless the entire slice (row or column in this case) is NA. This
can be disabled using the skipna option:

In [202]: df.mean(axis=1, skipna=False)

Out [202]:

a NaN
b 1.300
Cc NaN
d -0.275

See Table 5-9 for a list of common options for each reduction method options.

 

Summarizing and Computing Descriptive Statistics | 137

Table 5-9. Options for reduction methods

Method Description
axis Axis to reduce over. 0 for DataFrame’s rows and 1 for columns.
skipna Exclude missing values, True by default.

level Reduce grouped by level if the axis is hierarchically-indexed (Multilndex).

Some methods, like idxmin and idxmax, return indirect statistics like the index value
where the minimum or maximum values are attained:

In [203]: df.idxmax()

Out [203]:
one b
two d

Other methods are accumulations:

In [204]: df.cumsum()
Out[204]:

one two

1.40 NaN

8.50 -4.5

NaN NaN

9.25 -5.8

aq ow

Another type of method is neither a reduction nor an accumulation. describe is one
such example, producing multiple summary statistics in one shot:

In [205]: df.describe()

Out [205]:
one two
count 3.000000 2.000000
mean 3.083333 -2.900000
std 3.493685 2.262742
min 0.750000 -4.500000
25% 1.075000 -3.700000
50% 1.400000 -2.900000
75% 4.250000 -2.100000

7

max -100000 -1.300000

On non-numeric data, describe produces alternate summary statistics:
In [206]: obj = Series(['a', 'a', 'b', 'c'] * 4)

In [207]: obj.describe()

Out[207]:

count 16
unique 3
top a
freq 8

See Table 5-10 for a full list of summary statistics and related methods.

 

138 | Chapter5: Getting Started with pandas

Table 5-10. Descriptive and summary statistics

Method

count

describe

min, max
argmin, argmax
idxmin, idxmax
quantile

sum

mean

median

mad

var

std

skew

kurt

cumsum

cummin, cummax
cumprod

diff
pct_change

Description

Number of non-NA values

Compute set of summary statistics for Series or each DataFrame column

Compute minimum and maximum values

Compute index locations (integers) at which minimum or maximum value obtained, respectively
Compute index values at which minimum or maximum value obtained, respectively
Compute sample quantile ranging from 0 to 1

Sum of values

Mean of values

Arithmetic median (50% quantile) of values

Mean absolute deviation from mean value

Sample variance of values

Sample standard deviation of values

Sample skewness (3rd moment) of values

Sample kurtosis (4th moment) of values

Cumulative sum of values

Cumulative minimum or maximum of values, respectively

Cumulative product of values

Compute 1st arithmetic difference (useful for time series)

Compute percent changes

Correlation and Covariance

Some summary statistics, like correlation and covariance, are computed from pairs of
arguments. Let’s consider some DataFrames of stock prices and volumes obtained from

Yahoo! Finance:

import pandas.io.data as web

all data =

{}

for ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']:
all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2000', '1/1/2010')

price = DataFrame({tic: data['Adj Close']

for tic, data in all_data.iteritems()})

volume = DataFrame({tic: data['Volume' ]

for tic, data in all_data.iteritems()})

I now compute percent changes of the prices:

In [209]: returns = price.pct_change()

In [210]: returns.tail()

 

Summarizing and Computing Descriptive Statistics | 139

Out[210]:

AAPL GOOG IBM MSFT
Date
2009-12-24 0.034339 0.011117 0.004420 0.002747
2009-12-28 0.012294 0.007098 0.013282 0.005479
2009-12-29 -0.011861 -0.005571 -0.003474 0.006812
2009-12-30 0.012147 0.005376 0.005468 -0.013532
2009-12-31 -0.004300 -0.004416 -0.012609 -0.015432

The corr method of Series computes the correlation of the overlapping, non-NA,
aligned-by-index values in two Series. Relatedly, cov computes the covariance:

In [211]: returns.MSFT.corr(returns. IBM)

Out[211]: 0.49609291822168838

In [212]: returns.MSFT.cov(returns. IBM)
Out[212]: 0.00021600332437329015

DataFrame’s corr and cov methods, on the other hand, return a full correlation or
covariance matrix as a DataFrame, respectively:

In [213]: returns.corr()

Out [213]:

AAPL GOOG IBM MSFT
AAPL 1.000000 0.470660 0.410648 0.424550
GOOG 0.470660 1.000000 0.390692 0.443334
IBM 0.410648 0.390692 1.000000 0.496093
MSFT 0.424550 0.443334 0.496093 1.000000
In [214]: returns.cov()
Out[214]:

AAPL GOOG IBM MSFT
AAPL 0.001028 0.000303 0.000252 0.000309
GOOG 0.000303 0.000580 0.000142 0.000205
IBM 0.000252 0.000142 0.000367 0.000216
MSFT 0.000309 0.000205 0.000216 0.000516

Using DataFrame’s corrwith method, you can compute pairwise correlations between
a DataFrame’s columns or rows with another Series or DataFrame. Passing a Series
returns a Series with the correlation value computed for each column:

In [215]: returns.corrwith(returns. IBM)
Out[215]:

AAPL 0.410648

GOOG 0.390692

IBM 1.000000

MSFT 0.496093

Passing a DataFrame computes the correlations of matching column names. Here I
compute correlations of percent changes with volume:

In [216]: returns.corrwith(volume)
Out[216]:

AAPL = -0.057461

GOOG 0.062644

 

140 | Chapter5: Getting Started with pandas

IBM -0.007900
MSFT -0.014175

Passing axis=1 does things row-wise instead. In all cases, the data points are aligned by
label before computing the correlation.

Unique Values, Value Counts, and Membership
Another class of related methods extracts information about the values contained in a
one-dimensional Series. To illustrate these, consider this example:
In [217]: obj = Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', ‘c'])
The first function is unique, which gives you an array of the unique values in a Series:
In [218]: uniques = obj.unique()
In [219]: uniques

Out[219]: array([c, a, d, b], dtype=object)

The unique values are not necessarily returned in sorted order, but could be sorted after
the fact if needed (uniques.sort()). Relatedly, value_counts computes a Series con-
taining value frequencies:

In [220]: obj.value_counts()

Out[220]:
c 3
a 3
b 2
d 1

The Series is sorted by value in descending order as a convenience. value_counts is also
available as a top-level pandas method that can be used with any array or sequence:

In [221]: pd.value_counts(obj.values, sort=False)

Out[221]:
a 3
b 2
c 3
d 1

Lastly, isin is responsible for vectorized set membership and can be very useful in
filtering a data set down to a subset of values in a Series or column in a DataFrame:

In [222]: mask = obj.isin(['b', 'c'])

In [223]: mask In [224]: obj[mask]
Out [223]: Out [224]:

True 0 (a

False
False
False
False
True
True

Aun PWNR OO
CoN OW
nnowe

 

Summarizing and Computing Descriptive Statistics | 141

7 True
8 True

See Table 5-11 for a reference on these methods.

Table 5-11. Unique, value counts, and binning methods

Method
isin

unique

value_counts

Description

Compute boolean array indicating whether each Series value is contained in the passed sequence of values.

Compute array of unique values in a Series, returned in the order observed.

Return a Series containing unique values as its index and frequencies as its values, ordered count in

descending order.

 

In some cases, you may want to compute a histogram on multiple related columns in
a DataFrame. Here’s an example:

In [225]: data

In [226]: data
Out [226]:

Qu1
1

BWNRO
Pw pw

Qu2
2

WN RW

Qu3

1
5
2
4
4

= DataFrame({'Qui': [1, 3, 4, 3, 4],

‘Qu2': [2, 3, 1, 2, 3],
"Qu3': [1, 5, 2, 4, 4]})

Passing pandas.value_counts to this DataFrame’s apply function gives:

In [227]: result = data.apply(pd.value_counts).fillna(o)

In [228]: result
Out [228]:

Qu1
1

WPWN PR
ONNO

Qu2

CONNER

Qu3
1

RPNOR

Handling Missing Data

Missing data is common in most data analysis applications. One of the goals in de-
signing pandas was to make working with missing data as painless as possible. For
example, all of the descriptive statistics on pandas objects exclude missing data as
you ve seen earlier in the chapter.

 

142 | Chapter 5: Getting Started with pandas

pandas uses the floating point value NaN (Not a Number) to represent missing data in
both floating as well as in non-floating point arrays. It is just used as a sentinel that can
be easily detected:

In [229]: string data = Series(['aardvark', ‘artichoke’, np.nan, ‘avocado'])

In [230]: string data In [231]: string data.isnull()
Out [230]: Out [231]:

0 aardvark 0 False

1 artichoke 1 False

2 NaN 2 True

3 avocado 3 False

The built-in Python None value is also treated as NA in object arrays:
In [232]: string data[o] = None
In [233]: string data.isnull()
Out [233]:
0 True
1 False
2
3

True
False

I do not claim that pandas’s NA representation is optimal, but it is simple and reason-
ably consistent. It’s the best solution, with good all-around performance characteristics
and a simple API, that I could concoct in the absence of a true NA data type or bit
pattern in NumPy’s data types. Ongoing development work in NumPy may change this
in the future.

Table 5-12. NA handling methods

Argument Description

dropna Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much
missing data to tolerate.

fillna Fill in missing data with some value or using an interpolation method such as 'ffil1' or 'bfill'.
isnull Return like-type object containing boolean values indicating which values are missing / NA.

notnull Negation of isnu11.

Filtering Out Missing Data

You have a number of options for filtering out missing data. While doing it by hand is
always an option, dropna can be very helpful. On a Series, it returns the Series with only
the non-null data and index values:

In [234]: from numpy import nan as NA
In [235]: data = Series([1, NA, 3.5, NA, 7])

In [236]: data.dropna()
Out [236]:

 

Handling Missing Data | 143

Naturally, you could have computed this yourself by boolean indexing:
In [237]: data[data.notnull()]

Out [237]:
0 1.0
2 3.5
4 7.0

With DataFrame objects, these are a bit more complex. You may want to drop rows
or columns which are all NA or just those containing any NAs. dropna by default drops
any row containing a missing value:

In [238]: data = DataFrame([[1., 6.5, 3.], [1., NA, NA],
veeee : [NA, NA, NA], [NA, 6.5, 3.]])

In [239]: cleaned = data.dropna()

In [240]: data In [241]: cleaned
Out [240]: Out[241]:
0 1. 2 0 12
0 1 6.5 3 0 1 6.5 3
1 1 NaN NaN
2 NaN NaN NaN
3 NaN 6.5 3

Passing how='al1' will only drop rows that are all NA:
In [242]: data.dropna(how='all1')

Out [242]:

0 1 2
0 1 6.5 3
1 1 NaN NaN
3 NaN 6.5 3

Dropping columns in the same way is only a matter of passing axis=1:
In [243]: data[4] = NA

In [244]: data In [245]: data.dropna(axis=1, how='all')
Out [244]: Out[245]:
0 d. 2 4 0 1 2
0 1 6.5 3 NaN 0 1 6.5 3
1 1 NaN NaN NaN 1 1 NaN NaN
2 NaN NaN NaN NaN 2 NaN NaN NaN
3 NaN 6.5 3 NaN 3 NaN 6.5 3

A related way to filter out DataFrame rows tends to concern time series data. Suppose
you want to keep only rows containing a certain number of observations. You can
indicate this with the thresh argument:

In [246]: df = DataFrame(np.random.randn(7, 3))

In [247]: df.ix[:4, 1] = NA; df.ix[:2, 2] = NA

 

144 | Chapter 5: Getting Started with pandas

In [248]: df
Out [248]:

0 1. 2
0 -0.577087 NaN NaN
1 0.523772 NaN NaN
2 -0.713544 NaN NaN
3 -1.860761 NaN 0.560145
4 -1.265934 NaN -1.063512
5 0.332883 -2.359419 -0.199543
6 -1.541996 -0.970736 -1.307030

Filling in Missing Data

In [249]: df.dropna(thresh=3)
Out[249]:

0 1 2
5 0.332883 -2.359419 -0.199543
6 -1.541996 -0.970736 -1.307030

Rather than filtering out missing data (and potentially discarding other data along with
it), you may want to fill in the “holes” in any number of ways. For most purposes, the
fillna method is the workhorse function to use. Calling fillna with a constant replaces

missing values with that value:
In [250]: df.fillna(o)

Out [250]:

0 1 2
0 -0.577087 0.000000 0.000000
1 0.523772 0.000000 0.000000
2 -0.713544 0.000000 0.000000
3 -1.860761 0.000000 0.560145
4 -1.265934 0.000000 -1.063512
5 0.332883 -2.359419 -0.199543
6 -1.541996 -0.970736 -1.307030

Calling fillna with a dict you can use a different fill value for each column:

In [251]: df.fillna({1: 0.5, 3:

Out [251]:

0 1. 2
0 -0.577087 0.500000 NaN
1 0.523772 0.500000 NaN
2 -0.713544 0.500000 NaN
3 -1.860761 0.500000 0.560145
4 -1.265934 0.500000 -1.063512
5 0.332883 -2.359419 -0.199543
6 -1.541996 -0.970736 -1.307030

-1})

fillna returns a new object, but you can modify the existing object in place:

# always returns a reference to
In [252]: _
In [253]: df
Out [253]:

0 1 2
0 -0.577087 0.000000 0.000000
1 0.523772 0.000000 0.000000
2 -0.713544 0.000000 0.000000
3 -1.860761 0.000000 0.560145

the filled object

= df.fillna(0, inplace=True)

 

Handling Missing Data | 145

4 -1.265934 0.000000 -1.063512
5 0.332883 -2.359419 -0.199543
6 -1.541996 -0.970736 -1.307030

The same interpolation methods available for reindexing can be used with fillna:

In [254]:
In [255]: df.
In [256]: df
Out [256]:

0
0 0.286350
1 0.331286
2 0.246674
3 1.327195
4 0.022185
5 0.862580
In [257]: df.
Out[257]:

0
0 0.286350
1 0.331286
2 0.246674
3 1.327195
4 0.022185
5 0.862580

With fillna you can do lots of other things with a little creativity. For example, you

df = DataFrame(np.random.randn(6, 3))

ix[2:, 1] = NA; df.ix[4:, 2] = NA

1 2
0.377984 -0.753887
1.349742 0.069877

NaN 1.004812
NaN -1.549106
NaN NaN
NaN NaN

fillna(method=' fill" )

1 2
0.377984 -0.753887
-349742 0.069877
+349742 1.004812
-349742 -1.549106
-349742 -1.549106
-349742 -1.549106

PRPPRPRB

In [258]: df.fillna(method='ffill', limit=2)

Out [258]:

UBPWNPR OO

0
0.286350
0.331286
0.246674
1.327195
0.022185
0.862580

might pass the mean or median value of a Series:

In [259]: data = Series([1., NA, 3.5, NA, 7])

In [260]: data. fillna(data.mean())

Out[260]:

0 1.000000
1 3.833333
2 3.500000
3 3.833333
4 7.000000

See Table 5-13 for a reference on fillna.

Table 5-13. fillna function arguments

Argument Description

value
method

axis

inplace

limit

Scalar value or dict-like object to use to fill missing values

1 2
0.377984 -0.753887
1.349742 0.069877
1.349742 1.004812
1.349742 -1.549106

NaN -1.549106
NaN -1.549106

Interpolation, by default '££i11' iffunction called with no other arguments

Axis to fill on, default axis=0

Modify the calling object without producing a copy

For forward and backward filling, maximum number of consecutive periods to fill

 

146 | Chapter5: Getting Started with pandas

Hierarchical Indexing

Hierarchical indexing is an important feature of pandas enabling you to have multiple
(two or more) index levels on an axis. Somewhat abstractly, it provides a way for you
to work with higher dimensional data in a lower dimensional form. Let’s start with a
simple example; create a Series with a list of lists or arrays as the index:
In [261]: data = Series(np.random.randn(10),
eee : index=[['a', 'a', ‘a’, 'b', 'b', 'b', 'c', 'c', ‘d', ‘d'],
wore! [4, 2, 3, 4, 2, 3, 2, 2, 2, 3]))

In [262]: data

Out [262]:

ail 0.670216
2 0.852965
3 =-0.955869

b 1 -0.023493
2  -2.304234
3 --0.652469

c 1 -1.218302
2 -1.332610

d 2 1.074623
3 0.723642

What you're seeing is a prettified view of a Series with a MultiIndex as its index. The
“gaps” in the index display mean “use the label directly above”:

In [263]: data.index

Out [263]:

MultiIndex

[('a', 1) ('a', 2) (‘a', 3) (‘b', 1) ('b’, 2) (b', 3) (c', 1)

(‘c', 2) (‘d', 2) (‘d', 3)]

With a hierarchically-indexed object, so-called partial indexing is possible, enabling
you to concisely select subsets of the data:

In [264]: data['b']

Out[264]:

1 -0.023493

2  -2.304234

3 --0.652469

In [265]: data['b':'c'] In [266]: data.ix[['b', 'd']]

Out [265]: Out [266]:

b 1 -0.023493 b 1 -0.023493
2  -2.304234 2  -2.304234
3 -0.652469 3 -0.652469

c 1 -1.218302 d 2 1.074623
2 -1.332610 3 0.723642

Selection is even possible in some cases from an “inner” level:

In [267]: data[:, 2]
Out [267]:
a 0.852965

 

Hierarchical Indexing | 147

b -2.304234
c -1.332610
d 1.074623

Hierarchical indexing plays a critical role in reshaping data and group-based operations
like forming a pivot table. For example, this data could be rearranged into a DataFrame
using its unstack method:

In [268]: data.unstack()
Out [268]:

1 2 3
a 0.670216 0.852965 -0.955869
b -0.023493 -2.304234 -0.652469
Cc -1.218302 -1.332610 NaN
d NaN 1.074623 0.723642

The inverse operation of unstack is stack:

In [269]: data.unstack().stack()
Out[269]:

ai 0.670216
0.852965
-0.955869
-0.023493
-2.304234
-0.652469
-1.218302
-1.332610
1.074623
0.723642

WNN PWN PW DY

stack and unstack will be explored in more detail in Chapter 7.

With a DataFrame, either axis can have a hierarchical index:

In [270]: frame = DataFrame(np.arange(12).reshape((4, 3)),
sees i index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],
scone ad columns=[['Ohio', 'Ohio', 'Colorado'],
scone ad ['Green', 'Red', 'Green']])

In [271]: frame

Out [271]:
Ohio Colorado
Green Red Green
ai 0 1 2
2 3 4 5
bi 6 7 8
2 9 10 11

The hierarchical levels can have names (as strings or any Python objects). If so, these
will show up in the console output (don’t confuse the index names with the axis labels!):

In [272]: frame.index.names = ['key1', 'key2']
In [273]: frame.columns.names = ['state', ‘color']

In [274]: frame

 

148 | Chapter 5: Getting Started with pandas

Out [274]:

state Ohio Colorado

color Green Red Green

key1 key2

a 1 0 1 2
2 3 4 5

b 1 6 7 8
2 9 10 11

With partial column indexing you can similarly select groups of columns:
In [275]: frame['Ohio' ]

Out[275]:

color Green Red

key1 key2

a 1 0 1
2 3 4

b 1 6 7
2 9 10

A MultilIndex can be created by itself and then reused; the columns in the above Data-
Frame with level names could be created like this:

MultilIndex.from_arrays([['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']],
names=['state', 'color'])

Reordering and Sorting Levels

At times you will need to rearrange the order of the levels on an axis or sort the data
by the values in one specific level. The swaplevel takes two level numbers or names and
returns a new object with the levels interchanged (but the data is otherwise unaltered):

In [276]: frame.swaplevel('key1', '‘key2')

Out [276]:

state Ohio Colorado
color Green Red Green
key2 key1

1 a 0 1 2
2 a 3 4 5
1 b 6 7 8
2 b 9 10 11

sortlevel, on the other hand, sorts the data (stably) using only the values in a single
level. When swapping levels, it’s not uncommon to also use sortlevel so that the result
is lexicographically sorted:

In [277]: frame.sortlevel(1) In [278]: frame.swaplevel(0, 1).sortlevel(0o)
Out [277]: Out[278]:

state Ohio Colorado state Ohio Colorado

color Green Red Green color Green Red Green

key1 key2 key2 key1

a 1 0 1 2 1 a 0 1 2

b 1 6 7 8 b 6 7 8

a 2 3 4 5 2 a 3 4 5

b 2 9 10 11 b 9 10 11

 

Hierarchical Indexing | 149

Data selection performance is much better on hierarchically indexed
objects if the index is lexicographically sorted starting with the outer-
12° most level, that is, the result of calling sortlevel(0) or sort_index().

 
 

 

Summary Statistics by Level

Many descriptive and summary statistics on DataFrame and Series have a level option
in which you can specify the level you want to sum by on a particular axis. Consider
the above DataFrame; we can sum by level on either the rows or columns like so:

In [279]: frame.sum(level='key2')

Out [279]:

state Ohio Colorado
color Green Red Green
key2

1 6 8 10
2 12 14 16

In [280]: frame.sum(level='color', axis=1)

Out [280]:

color Green Red

key1 key2

a 1 2 1
2 8 4

b 1 14 7
2 20 10

Under the hood, this utilizes pandas’s groupby machinery which will be discussed in
more detail later in the book.

Using a DataFrame’s Columns

It’s not unusual to want to use one or more columns from a DataFrame as the row
index; alternatively, you may wish to move the row index into the DataFrame’s col-
umns. Here’s an example DataFrame:
acmnned ‘c': ['one', 'one', 'one', 'two', ‘two', ‘two', ‘two'],
acmnned "d': [0, 1, 2, 0, 1, 2, 3]})

In [281]: frame = DataFrame({'a': range(7), 'b': range(7, 0, -1),

In

7 one
6 one
5 one
4 two
3 two
2 two
1 two

Aun PWNR OO
WNRONR OQ

 

150 | Chapter5: Getting Started with pandas

DataFrame’s set_index function will create a new DataFrame using one or more of its
columns as the index:

In [283]: frame2 = frame.set_index(['c', 'd'])
In [284]: frame2

Out [284]:
a b

ot

=

°

o
anu PWNPR O
RPNWHU ON

By default the columns are removed from the DataFrame, though you can leave them in:
In [285]: frame.set_index(['c', 'd'], drop=False)

Out [285]:
a b c d
c d
one 0 O 7 one O
1 1 6 one 1
2 2 5 one 2
twoO0 3 4 two O
1 4 3 two 1
2 5 2 two 2
3 6 1 two 3

reset_index, on the other hand, does the opposite of set_index; the hierarchical index
levels are are moved into the columns:

In [286]: frame2.reset_index()
Out [286]:

c
one
one
one
two
two
two
two

Au RPWNPR OC

WNHRONrFR OQ
DAuRPWNPR OO W
PNWHRUDN OT

Other pandas Topics

Here are some additional topics that may be of use to you in your data travels.

Integer Indexing

Working with pandas objects indexed by integers is something that often trips up new
users due to some differences with indexing semantics on built-in Python data

 

Other pandas Topics | 151

structures like lists and tuples. For example, you would not expect the following code
to generate an error:

ser = Series(np.arange(3.))

ser[-1]
In this case, pandas could “fall back” on integer indexing, but there’s not a safe and
general way (that I know of) to do this without introducing subtle bugs. Here we have
an index containing 0, 1, 2, but inferring what the user wants (label-based indexing or
position-based) is difficult::

In [288]: ser

Out [288]:
0 0
1 1
2 2

On the other hand, with a non-integer index, there is no potential for ambiguity:

In [289]: ser2 = Series(np.arange(3.), index=['a', 'b', ‘c'])

In [290]: ser2[-1]

Out[290]: 2.0
To keep things consistent, if you have an axis index containing indexers, data selection
with integers will always be label-oriented. This includes slicing with ix, too:

In [291]: ser.ix[:1]

Out[291]:
0 0
1 1

In cases where you need reliable position-based indexing regardless of the index type,
you can use the iget_value method from Series and irow and icol methods from Da-
taFrame:

In [292]: ser3 = Series(range(3), index=[-5, 1, 3])

In [293]: ser3.iget_value(2)
Out[293]: 2

In [294]: frame = DataFrame(np.arange(6).reshape(3, 2), index=[2, 0, 1])

In [295]: frame.irow(0)
Out[295]:

0 0

1 1.

Name: 2

Panel Data

While not a major topic of this book, pandas has a Panel data structure, which you can
think of as a three-dimensional analogue of DataFrame. Much of the development focus
of pandas has been in tabular data manipulations as these are easier to reason about,

 

152 | Chapter5: Getting Started with pandas

and hierarchical indexing makes using truly N-dimensional arrays unnecessary in a lot
of cases.

To create a Panel, you can use a dict of DataFrame objects or a three-dimensional
ndarray:

import pandas.io.data as web

pdata = pd.Panel(dict((stk, web.get_data_yahoo(stk, '1/1/2009', '6/1/2012'))
for stk in ["AAPL’, 'GOOG', 'MSFT', 'DELL']))

Each item (the analogue of columns in a DataFrame) in the Panel is a DataFrame:

In [297]: pdata

Out [297]:

<class 'pandas.core.panel.Panel'>

Dimensions: 4 (items) x 861 (major) x 6 (minor)

Items: AAPL to MSFT

Major axis: 2009-01-02 :00 to 2012-06-01 :00
Minor axis: Open to Adj Close

In [298]: pdata = pdata.swapaxes('items', ‘minor’ )

In [299]: pdata['Adj Close']

Out [299]:

<class 'pandas.core. frame.DataFrame' >

DatetimeIndex: 861 entries, 2009-01-02 :00 to 2012-06-01 :00
Data columns:

AAPL 861 non-null values

DELL 861 non-null values

GOOG 861 non-null values

MSFT 861 non-null values

dtypes: float64(4)

ix-based label indexing generalizes to three dimensions, so we can select all data at a
particular date or a range of dates like so:
In [300]: pdata.ix[:, '6/1/2012', :]
Out [300]:
Open High Low Close Volume Adj Close
AAPL 569.16 572.65 560.52 560.99 18606700 560.99

DELL 12.15 12.30 12.05 12.07 19396700 12.07
GOOG 571.79 572.65 568.35 570.98 3057900 570.98
MSFT 28.76 28.96 28.44 28.45 56634300 28.45
In [301]: pdata.ix['Adj Close’, '5/22/2012':, :]
Out [301]:

AAPL DELL GOOG = MSFT
Date

2012-05-22 556.97 15.08 600.80 29.76
2012-05-23 570.56 12.49 609.46 29.11
2012-05-24 565.32 12.45 603.66 29.07
2012-05-25 562.29 12.46 591.53 29.06
2012-05-29 572.27 12.66 594.34 29.56
2012-05-30 579.17 12.56 588.23 29.34

 

Other pandas Topics | 153

2012-05-31 577.73 12.33 580.86 29.19
2012-06-01 560.99 12.07 570.98 28.45

An alternate way to represent panel data, especially for fitting statistical models, is in
“stacked” DataFrame form:

In [302]: stacked = pdata.ix[:, '5/30/2012':, :].to_frame()

In [303]: stacked

Out [303]:
Open High Low Close Volume Adj Close
major minor
2012-05-30 AAPL 569.20 579.99 566.56 579.17 18908200 579.17
DELL 12.59 12.70 12.46 12.56 19787800 12.56
GOOG 588.16 591.90 583.53 588.23 1906700 588.23
MSFT 29.35 29.48 29.12 29.34 41585500 29.34
2012-05-31 AAPL 580.74 581.50 571.46 577.73 17559800 577.73
DELL 12.53 12.54 12.33 12.33 19955500 12.33
GOOG 588.72 590.00 579.00 580.86 2968300 580.86
MSFT 29.30 29.42 28.94 29.19 39134000 29.19
2012-06-01 AAPL 569.16 572.65 560.52 560.99 18606700 560.99
DELL 12.15 12.30 12.05 12.07 19396700 12.07
GOOG §= 571.79 572.65 568.35 570.98 3057900 570.98
MSFT 28.76 28.96 28.44 28.45 56634300 28.45

DataFrame has a related to_panel method, the inverse of to_frame:

In [304]: stacked.to_panel()

Out [304]:

<class 'pandas.core.panel.Panel'>

Dimensions: 6 (items) x 3 (major) x 4 (minor)

Items: Open to Adj Close

Major axis: 2012-05-30 :00 to 2012-06-01 :00
Minor axis: AAPL to MSFT

 

154 | Chapter5: Getting Started with pandas

CHAPTER 6
Data Loading, Storage, and File
Formats

 

The tools in this book are of little use if you can’t easily import and export data in
Python. I’m going to be focused on input and output with pandas objects, though there
are of course numerous tools in other libraries to aid in this process. NumPy, for ex-
ample, features low-level but extremely fast binary data loading and storage, including
support for memory-mapped array. See Chapter 12 for more on those.

Input and output typically falls into a few main categories: reading text files and other
more efficient on-disk formats, loading data from databases, and interacting with net-
work sources like web APIs.

Reading and Writing Data in Text Format

Python has become a beloved language for text and file munging due to its simple syntax
for interacting with files, intuitive data structures, and convenient features like tuple
packing and unpacking.

pandas features a number of functions for reading tabular data as a DataFrame object.
Table 6-1 has a summary of all of them, though read_csv and read_table are likely the
ones you'll use the most.

Table 6-1. Parsing functions in pandas

Function Description

read_csv Load delimited data from a file, URL, or file-like object. Use comma as default delimiter
read_table Load delimited data from a file, URL, or file-like object. Use tab (‘\t') as default delimiter
read_fwf Read data in fixed-width column format (that is, no delimiters)

read_clipboard _ Versionofread_tablethatreads data fromthe clipboard. Useful for converting tables from web pages

 

155

Pll give an overview of the mechanics of these functions, which are meant to convert
text data into a DataFrame. The options for these functions fall into a few categories:

¢ Indexing: can treat one or more columns as the returned DataFrame, and whether
to get column names from the file, the user, or not at all.

¢ Type inference and data conversion: this includes the user-defined value conver-
sions and custom list of missing value markers.

e Datetime parsing: includes combining capability, including combining date and
time information spread over multiple columns into a single column in the result.

¢ Iterating: support for iterating over chunks of very large files.

* Unclean data issues: skipping rows or a footer, comments, or other minor things
like numeric data with thousands separated by commas.

Type inference is one of the more important features of these functions; that means you
don’t have to specify which columns are numeric, integer, boolean, or string. Handling
dates and other custom types requires a bit more effort, though. Let’s start with a small
comma-separated (CSV) text file:

In [846]: !cat ch06/ex1.csv

a,b,c,d,message

1,2,3,4,hello

5,6,7,8,world

9,10,11,12, foo

Since this is comma-delimited, we can use read_csv to read it into a DataFrame:
In [847]: df = pd.read_csv('ch06/ex1.csv')

[
[
a b c_ d message
1 2 3 4 _ hello
5 6 7 8 world
9 10 11 12 foo

We could also have used read_table and specifying the delimiter:

In [849]: pd.read_table('cho6/ex1.csv', sep=',')
Out [849]:

a b c_ d message

1 2 3 4 hello
5
9

 

0

1 6 7 8 world

2 10 11 12 foo

a,

sO Here I used the Unix cat shell command to print the raw contents of
“3 the file to the screen. If you’re on Windows, you can use type instead
“S48 of cat to achieve the same effect.

 

 

156 | Chapter6: Data Loading, Storage, and File Formats

A file will not always have a header row. Consider this file:

In [850]: !cat ch06/ex2.csv
1,2,3,4,hello
5,6,7,8,world
9,10,11,12,f00

To read this in, you have a couple of options. You can allow pandas to assign default
column names, or you can specify names yourself:

In [851]: pd.read_csv('cho6/ex2.csv', header=None)
Out [851]:
X.1 X.2 X.3 X.4 X.5
0 1 2 3 4 hello
1 5 6 7 8 world
2 9 10 11 12 foo

[
[
a bc. d message
1 2 3 4 _ hello
5 6 7 8 world
9 10 11 12 foo

Suppose you wanted the message column to be the index of the returned DataFrame.
You can either indicate you want the column at index 4 or named 'message' using the
index_col argument:

In [853]: names = ['a', 'b', 'c', ‘d', ‘message’ ]

In [854]: pd.read_csv('cho6/ex2.csv', names=names, index_col='message' )
Out [854]:
a boc d
message
hello 1 2 3 4
world 5 6 7 8
foo 9 10 11 12

In the event that you want to form a hierarchical index from multiple columns, just
pass a list of column numbers or names:

In [855]: !cat cho6/csv_mindex.csv
key1, key2, value1, value2

one,a,1,2

one,b,3,4

one,c,5,6

one,d,7,8

two,a,9,10

two,b,11,12

two,c,13,14

two,d,15,16

In [856]: parsed = pd.read_csv('ch06/csv_mindex.csv', index_col=['key1', 'key2'])

In [857]: parsed
Out [857]:

 

Reading and Writing Data in Text Format | 157

value1 value2

key1 key2

one a 1 2
b 3 4
c 5 6
d 7 8

two a 9 10
b 11 12
c 13 14
d 15 16

In some cases, a table might not have a fixed delimiter, using whitespace or some other
pattern to separate fields. In these cases, you can pass a regular expression as a delimiter
for read_table. Consider a text file that looks like this:

In [858]: list(open('cho6/ex3.txt'))

Out [858]:

[' A B C\n',
‘aaa -0.264438 -1.026059 -0.619500\n',
‘bbb 0.927272 0.302904 -0.032399\n',
"ccc -0.264273 -0.386314 -0.217601\n',
‘ddd -0.871858 -0.348382 1.100491\n']

While you could do some munging by hand, in this case fields are separated by a variable
amount of whitespace. This can be expressed by the regular expression \s+, so we have
then:

In [859]: result = pd.read_table('cho6/ex3.txt', sep='\s+')

In [860]: result
Out [860]:

A B C
aaa -0.264438 -1.026059 -0.619500
bbb 0.927272 0.302904 -0.032399
ccc -0.264273 -0.386314 -0.217601
ddd -0.871858 -0.348382 1.100491

Because there was one fewer column name than the number of data rows, read_table
infers that the first column should be the DataFrame’s index in this special case.

The parser functions have many additional arguments to help you handle the wide
variety of exception file formats that occur (see Table 6-2). For example, you can skip
the first, third, and fourth rows of a file with skiprows:

In [861]: !cat cho6/ex4.csv

# hey!

a,b,c,d,message

# just wanted to make things more difficult for you

# who reads CSV files with computers, anyway?

1,2,3,4,hello

5,6,7,8,world

9,10,11,12, foo

In [862]: pd.read_csv('ch06/ex4.csv', skiprows=[0, 2, 3])

Out [862]:
a b c_ d message

 

158 | Chapter6: Data Loading, Storage, and File Formats

01 2 3 4 hello
1 5 6 7 8 world
2 9 10 11 12 foo

Handling missing values is an important and frequently nuanced part of the file parsing
process. Missing data is usually either not present (empty string) or marked by some
sentinel value. By default, pandas uses a set of commonly occurring sentinels, such as
NA, -1.#IND, and NULL:

In [863]: !cat ch06/ex5.csv

something ,a,b,c,d,message

one,1,2,3,4,NA

two,5,6,,8,world

three,9,10,11,12, foo

In [864]: result = pd.read_csv('ch06/ex5.csv')

In [865]: result

Out [865]:

something a b c_ d message
0 one 1 2 3 4 NaN
a two 5 6NaN 8 ~ world
2 three 9 10 11 12 foo
In [866]: pd.isnull(result)
Out [866]:

something a b c d message
0 False False False False False True
1 False False False True False’ False
2 False False False False False’ False

The na_values option can take either a list or set of strings to consider missing values:
In [867]: result = pd.read_csv('ch06/ex5.csv', na_values=['NULL'])

In [868]: result

Out [868]:

something a b c_ d message
0 one 1 2 3 4 NaN
1 two 5 6NaN 8 world
2 three 9 10 11 12 foo

Different NA sentinels can be specified for each column in a dict:

In [869]: sentinels = {'message': ['foo', 'NA'], ‘something’: ['two']}

In [870]: pd.read_csv('cho6/ex5.csv', na_values=sentinels)

Out [870]:

something a b c_ d message
0 one 1 2 3 4 NaN
1 NaN 5 6NaN 8 world
2 three 9 10 11 12 NaN

 

Reading and Writing Data in Text Format | 159

Table 6-2. read_csv /read_table function arguments

Argument Description

path String indicating filesystem location, URL, or file-like object

sepordelimiter Character sequence or regular expression to use to split fields in each row

header Row number to use as column names. Defaults to 0 (first row), but should be None if there is no header
row

index_col Column numbers or names to use as the row index in the result. Can be a single name/number or a list
of them for a hierarchical index

names List of column names for result, combine with header=None

skiprows Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip

na_values Sequence of values to replace with NA

comment Character or characters to split comments off the end of lines

parse_dates

keep_date_col

Attemptto parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise
can specify a list of column numbers or name to parse. If element of list is tuple or list, will combine
multiple columns together and parse to date (for example if date/time split across two columns)

If joining columns to parse date, keep the joined columns. Default False

converters Dict containing column number of name mapping to functions. Forexample {'foo': £}would apply
the function f to all values in the 'foo' column
dayfirst When parsing potentially ambiguous dates, treat as international format (e.g. 7/6/2012 -> June 7,

date_parser

2012). Default False

Function to use to parse dates

nrows Number of rows to read from beginning of file
iterator Return a TextParser object for reading file piecemeal
chunksize For iteration, size of file chunks

skip_footer

Number of lines to ignore at end of file

verbose Print various parser output information, like the number of missing values placed in non-numeric
columns

encoding Text encoding for unicode. For example 'utf-8" for UTF-8 encoded text

squeeze If the parsed data only contains one column return a Series

thousands Separator for thousands, e.g.',' or'.'

Reading Text Files in Pieces

When processing very large files or figuring out the right set of arguments to correctly
process a large file, you may only want to read ina small piece of a file or iterate through
smaller chunks of the file.

In [871]: result = pd.read_csv('ch06/ex6.csv')

In [872]: result
Out [872]:

 

160 | Chapter6: Data Loading, Storage, and File Formats

<class 'pandas.core. frame.DataFrame' >
Int64Index: 10000 entries, 0 to 9999
Data columns:

one 10000 non-null values
two 10000 non-null values
three 10000 non-null values
four 10000 non-null values
key 10000 non-null values

dtypes: float64(4), object(1)

If you want to only read out a small number of rows (avoiding reading the entire file),
specify that with nrows:

In [873]: pd.read_csv('ch06/ex6.csv', nrows=5)
Out [873]:

one two three four key
0 0.467976 -0.038649 -0.295344 -1.824726
1 -0.358893 1.404453 0.704965 -0.200638
2 -0.501840 0.659254 -0.421691 -0.057688
3 0.204886 1.074134 1.388361 -0.982404
4 0.354628 -0.133116 0.283763 -0.837063

RBDAAaAWr

To read out a file in pieces, specify a chunksize as a number of rows:

In [874]: chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)

In [875]: chunker
Out[875]: <pandas.io.parsers.TextParser at 0x8398150>

The TextParser object returned by read_csv allows you to iterate over the parts of the
file according to the chunksize. For example, we can iterate over ex6.csv, aggregating
the value counts in the ‘key’ column like so:

chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)

tot = Series([])
for piece in chunker:
tot = tot.add(piece['key'].value_counts(), fill_value=0)

tot = tot.order(ascending=False)

We have then:

In [877]: tot[:10]
Out [877]:
368
364
346
343
340
338
337
335
334
330

maAmUYzoorx<nm

 

Reading and Writing Data in Text Format | 161

TextParser is also equipped with a get_chunk method which enables you to read pieces
of an arbitrary size.

Writing Data Out to Text Format

Data can also be exported to delimited format. Let’s consider one of the CSV files read
above:

In [878]: data = pd.read_csv('ch06/ex5.csv')

In [879]: data

Out [879]:

something a b c_ d message
0 one 1 2 3 4 NaN
1 two 5 6NaN 8 world
2 three 9 10 11 12 foo

Using DataFrame’s to_csv method, we can write the data out toa comma-separated file:
In [880]: data.to_csv('cho6/out.csv')

In [881]: !cat cho6/out.csv
,something,a,b,c,d,message
0,one,1,2,3.0,4,

1, two,5,6,,8,world
2,three,9,10,11.0,12, foo

Other delimiters can be used, of course (writing to sys.stdout so it just prints the text
result):

In [882]: data.to_csv(sys.stdout, sep='"|')
| something |a|b|c|d|message
O|one|1|2|3.0|4|

1|two|5|6| |8|world
2|three|9|10|11.0|12| foo

Missing values appear as empty strings in the output. You might want to denote them
by some other sentinel value:

In [883]: data.to_csv(sys.stdout, na_rep='NULL')

,something,a,b,c,d,message

0,one,1,2,3.0,4,NULL

1,two,5,6,NULL,8,world

2,three,9,10,11.0,12, foo

With no other options specified, both the row and column labels are written. Both of
these can be disabled:

In [884]: data.to_csv(sys.stdout, index=False, header=False)

one,1,2,3.0,4,

two,5,6,,8,world

three,9,10,11.0,12, foo

You can also write only a subset of the columns, and in an order of your choosing:

 

162 | Chapter6: Data Loading, Storage, and File Formats

n [885]: data.to_csv(sys.stdout, index=False, cols=['a', 'b', 'c'])

a,b,c
1,2,3.0
5,6,
9,10,11.0

Series also has a to_csv method:
In [886]: dates = pd.date_range('1/1/2000', periods=7)

In [887]: ts = Series(np.arange(7), index=dates)
In [888]: ts.to_csv('ch06/tseries.csv')

In [889]: !cat cho6/tseries.csv
2000-01-01 :00,0
2000-01-02 :00,1
2000-01-03 :00,2
2000-01-04 :00, 3
2000-01-05 :00,4
2000-01-06 :00,5
2000-01-07 :00,6

With a bit of wrangling (no header, first column as index), you can read a CSV version
of a Series with read_csv, but there is also a from_csv convenience method that makes
it a bit simpler:

n [890]: Series.from_csv('ch06/tseries.csv', parse_dates=True)
Out [890]:
2000-01-01
2000-01-02
2000-01-03
2000-01-04
2000-01-05
2000-01-06
2000-01-07

aun PWN R O

See the docstrings for to_csv and from_csv in [Python for more information.

Manually Working with Delimited Formats

Most forms of tabular data can be loaded from disk using functions like pan
das.read_ table. In some cases, however, some manual processing may be necessary.
It’s not uncommon to receive a file with one or more malformed lines that trip up
read Save To illustrate the basic tools, consider a small CSV file:

n [891]: !cat chO06/ex7.csv

se "bm Mc"
myn nye mgm
Te

For any file with a single-character delimiter, you can use Python’s built-in csv module.
To use it, pass any open file or file-like object to csv. reader:

 

Reading and Writing Data in Text Format | 163

import csv
£ = open('ch06/ex7.csv')

reader = csv.reader(f)
Iterating through the reader like a file yields tuples of values in each like with any quote

characters removed:

In [893]: for line in reader:
averse 33 print line
[ a ' P 'b' 3 'c ' ]
[ ' 1 ' F 2 1 3 1 3 ' ]
[ 1 F "" 3 1 3 ' 5 “4 ]
From there, it’s up to you to do the wrangling necessary to put the data in the form
that you need it. For example:

In [894]: lines = list(csv.reader(open('ch06/ex7.csv')))
In [895]: header, values = lines[o0], lines[1:]
In [896]: data_dict = {h: v for h, v in zip(header, zip(*values) )}
In [897]: data_dict
Out[897]: {'a': ('1', '1'), ‘b's ('2', '2'), ‘c's ('3', '3')}
CSV files come in many different flavors. Defining a new format with a different de-

limiter, string quoting convention, or line terminator is done by defining a simple sub-
class of csv.Dialect:

class my_dialect(csv.Dialect):
lineterminator = '\n'
delimiter = ';'
quotechar =

reader = csv.reader(f, dialect=my_dialect)

Individual CSV dialect parameters can also be given as keywords to csv. reader without
having to define a subclass:

reader = csv.reader(f, delimiter='|')

The possible options (attributes of csv.Dialect) and what they do can be found in
Table 6-3.

Table 6-3. CSV dialect options

Argument Description

delimiter One-character string to separate fields. Defaults to’, '.

lineterminator Line terminator for writing, defaults to ' \\r\\n' . Reader ignores this and recognizes
cross-platform line terminators.

quotechar Quote character for fields with special characters (like a delimiter). Default is '"'.

quoting Quoting convention. Options include csv.QUOTE_ALL (quote all fields),

csv.QUOTE_MINIMAL (only fields with special characters like the delimiter),

 

164 | Chapter6: Data Loading, Storage, and File Formats

Argument Description
csv.QUOTE_NONNUMERIC, and csv.QUOTE_NON (no quoting). See Python’s
documentation for full details. Defaults to QUOTE_MINIMAL.

skipinitialspace Ignore whitespace after each delimiter. Default False.

doublequote How to handle quoting character inside a field. If True, itis doubled. See online
documentation for full detail and behavior.

escapechar String to escape the delimiter if quoting is set to csv. QUOTE_NONE. Disabled by
default

 

For files with more complicated or fixed multicharacter delimiters, you
ase will not be able to use the csv module. In those cases, you'll have to do
12° the line splitting and other cleanup using string’s split method or the
* regular expression method re. split.

 

 

To write delimited files manually, you can use csv.writer. It accepts an open, writable
file object and the same dialect and format options as csv. reader:
with open('mydata.csv', ‘w') as f:
writer = csv.writer(f, dialect=my_dialect)
writer.writerow(('one', 'two', 'three'))

writer.writerow(('1', '2', '3'))
writer.writerow(('4', '5', '6'))
writer.writerow(('7', '8', '9'))

JSON Data

JSON (short for JavaScript Object Notation) has become one of the standard formats

for sending data by HTTP request between web browsers and other applications. It is

a much more flexible data format than a tabular text form like CSV. Here is an example:
obj = """

"name": "Wes",

“places lived": ["United States", "Spain", "Germany"],

"pet": null,

"siblings": [{"name": "Scott", "age": 25, "pet": "Zuko"},

"name": "Katie", "age": 33, "pet": "Cisco"}]

JSON is very nearly valid Python code with the exception of its null value null and
some other nuances (such as disallowing trailing commas at the end of lists). The basic
types are objects (dicts), arrays (lists), strings, numbers, booleans, and nulls. All of the
keys in an object must be strings. There are several Python libraries for reading and
writing JSON data. I'll use json here as it is built into the Python standard library. To
convert a JSON string to Python form, use json. loads:

In [899]: import json

 

Reading and Writing Data in Text Format | 165

In [900]: result = json. loads(obj)

In [901]: result
Out [901]:
{u'name': u'Wes',
u'pet': None,
u'places_lived': [u'United States’, u'Spain', u'Germany'],
u'siblings': [{u'age': 25, u'name': u'Scott', u'pet': u'Zuko'},
{u'age': 33, u'name': u'Katie', u'pet': u'Cisco'}]}
json.dumps on the other hand converts a Python object back to JSON:

In [902]: asjson = json.dumps(result)

How you convert a JSON object or list of objects to a DataFrame or some other data
structure for analysis will be up to you. Conveniently, you can passa list of JSON objects
to the DataFrame constructor and select a subset of the data fields:

In [903]: siblings = DataFrame(result['siblings'], columns=['name', ‘age'])

In [904]: siblings

Out [904]:
name age

0 Scott 25

1 Katie 33

For an extended example of reading and manipulating JSON data (including nested
records), see the USDA Food Database example in the next chapter.

 

Va,
sO An effort is underway to add fast native JSON export (to_json) and
“ s . decoding (from_json) to pandas. This was not ready at the time of writ-
‘ek’; ing.

 

XML and HTML: Web Scraping

Python has many libraries for reading and writing data in the ubiquitous HTML and
XML formats. lxml (http://lxml.de) is one that has consistently strong performance in
parsing very large files. Ixml has multiple programmer interfaces; first I'll show using
1xml.html for HTML, then parse some XML using lxml.objectify.

Many websites make data available in HTML tables for viewing in a browser, but not
downloadable as an easily machine-readable format like JSON, HTML, or XML. I no-
ticed that this was the case with Yahoo! Finance’s stock options data. If you aren’t
familiar with this data; options are derivative contracts giving you the right to buy
(call option) or sell (put option) a company’s stock at some particular price (the
strike) between now and some fixed point in the future (the expiry). People trade both
call and put options across many strikes and expiries; this data can all be found together
in tables on Yahoo! Finance.

 

166 | Chapter6: Data Loading, Storage, and File Formats

To get started, find the URL you want to extract data from, open it with urllib2 and
parse the stream with lxml like so:

from lxml.html import parse
from urllib2 import urlopen

parsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options' ))
doc = parsed.getroot()

Using this object, you can extract all HTML tags of a particular type, such as table tags
containing the data of interest. As a simple motivating example, suppose you wanted
to get a list of every URL linked to in the document; links are a tags in HTML. Using
the document root’s findall method along with an XPath (a means of expressing
“queries” on the document):

In [906]: links = doc.findall('.//a')

In [907]: links[]

Out [907]:

[<Element a at 0x6c488fo>,
<Element a at 0x6c48950>,
<Element a at 0x6c489b0>,
<Element a at 0x6c48a10>,
<Element a at 0x6c48a70>]

But these are objects representing HTML elements; to get the URL and link text you
have to use each element’s get method (for the URL) and text_content method (for
the display text):

In [908]: Ink = links[28]

In [909]: Ink
Out[909]: <Element a at Ox6c48ddo>

In [910]: 1nk.get('href')
Out[910]: ‘http://biz.yahoo.com/special.htm1'

In [911]: 1Ink.text_content()
Out[911]: ‘Special Editions’

Thus, getting a list of all URLs in the document is a matter of writing this list compre-
hension:

In [912]: urls = [1nk.get(‘href') for Ink in doc.findall('.//a')]

In [913]: urls[-10:]

Out [913]:

[ http: //info.yahoo.com/privacy/us/yahoo/finance/details.html',
‘http: //info.yahoo.com/relevantads/' ,
"http: //docs.yahoo.com/info/terms/',
‘http://docs.yahoo.com/info/copyright/copyright.html',
‘http://help.yahoo.com/1/us/yahoo/finance/forms_index.html',
"http: //help.yahoo.com/1/us/yahoo/finance/quotes/fitadelay.html',
"http: //help.yahoo.com/1/us/yahoo/finance/quotes/fitadelay.html',

 

Reading and Writing Data in Text Format | 167

"http: //www.capitaliq.com',
"http: //www.csidata.com',
‘http: //www.morningstar.com/' ]

Now, finding the right tables in the document can be a matter of trial and error; some
websites make it easier by giving a table of interest an id attribute. I determined that
these were the two tables containing the call data and put data, respectively:

tables = doc.findall('.//table' )

calls = tables[9]
puts = tables[13]

Each table has a header row followed by each of the data rows:
In [915]: rows = calls.findall('.//tr')

For the header as well as the data rows, we want to extract the text from each cell; in
the case of the header these are th cells and td cells for the data:
def _unpack(row, kind='td'):
elts = row.findall('.//%s' % kind)
return [val.text_content() for val in elts]

Thus, we obtain:

In [917]: _unpack(rows[0], kind='th')
Out[917]: ['Strike', 'Symbol', 'Last', 'Chg', 'Bid', 'Ask', 'Vol', ‘Open Int']

In [918]: _unpack(rows[1], kind='td')
Out [918]:
['295.00',
"AAPL120818C00295000' ,
'310.40',
' 0.00",
"289.80',
'290.80',
a,
1 169 n ]
Now, it’s a matter of combining all of these steps together to convert this data into a
DataFrame. Since the numerical data is still in string format, we want to convert some,
but perhaps not all of the columns to floating point format. You could do this by hand,

but, luckily, pandas has a class TextParser that is used internally in the read_csv and
other parsing functions to do the appropriate automatic type conversion:

from pandas.io.parsers import TextParser

def parse_options data(table):
rows = table.findall('.//tr')
header = _unpack(rows[0], kind='"th')
data = [_unpack(r) for r in rows[1:]]
return TextParser(data, names=header).get_chunk()

Finally, we invoke this parsing function on the lxml table objects and get DataFrame
results:

 

168 | Chapter6: Data Loading, Storage, and File Formats

In [920]: call_data = parse options data(calls)
In [921]: put_data = parse options data(puts)

In [922]: call_data[:10]

Out [922]:

Strike Symbol Last Chg Bid Ask Vol Open Int
0 295 AAPL120818C00295000 310.40 0.0 289.80 290.80 1 169
1 300 AAPL120818C00300000 277.10 1.7 284.80 285.60 2 478
2 305 AAPL120818C00305000 300.97 0.0 279.80 280.80 10 316
3 310 AAPL120818C00310000 267.05 0.0 274.80 275.65 6 239
4 315 AAPL120818C00315000 296.54 0.0 269.80 270.80 22 88
5 320 AAPL120818C00320000 291.63 0.0 264.80 265.80 96 173
6 325 AAPL120818C00325000 261.34 0.0 259.80 260.80 N/A 108
7 330 AAPL120818C00330000 230.25 0.0 254.80 255.80 N/A 21
8 335 AAPL120818C00335000 266.03 0.0 249.80 250.65 4 46
9 340 AAPL120818C00340000 272.58 0.0 244.80 245.80 4 30

Parsing XML with Ixml.objectify

XML (extensible markup language) is another common structured data format sup-
porting hierarchical, nested data with metadata. The files that generate the book you

are reading actually form a series of large XML documents.

Above, I showed the lxml library and its xml. html interface. Here I show an alternate

interface that’s convenient for XML data, 1xml.objectify.

The New York Metropolitan Transportation Authority (MTA) publishes a number of
data series about its bus and train services (http://www.mta.info/developers/download
-html). Here we'll look at the performance data which is contained in a set of XML files.
Each train or bus service has a different file (like Performance MNR.xml for the Metro-
North Railroad) containing monthly data as a series of XML records that look like this:

<INDICATOR>
<INDICATOR_SEQ>373889</INDICATOR_SEQ>
<PARENT_SEQ></PARENT_SEQ>
<AGENCY_NAME>Metro-North Railroad</AGENCY_NAME>
<INDICATOR_NAME>Escalator Availability</INDICATOR_NAME>
<DESCRIPTION>Percent of the time that escalators are operational
systemwide. The availability rate is based on physical observations performed
the morning of regular business days only. This is a new indicator the agency
began reporting in 2009.</DESCRIPTION>
<PERIOD_YEAR>2011</PERIOD_YEAR>
<PERIOD_MONTH>12</PERIOD_MONTH>
<CATEGORY>Service Indicators</CATEGORY>
<FREQUENCY>M</FREQUENCY>
<DESIRED_CHANGE>U</DESIRED_CHANGE>
<INDICATOR_UNIT>%</INDICATOR_UNIT>
<DECIMAL_PLACES>1</DECIMAL_PLACES>
<YTD_TARGET>97.00</YTD_TARGET>
<YTD_ACTUAL></YTD_ACTUAL>
<MONTHLY_TARGET>97.00</MONTHLY_TARGET>
<MONTHLY_ACTUAL></MONTHLY_ACTUAL>

</INDICATOR>

 

Reading and Writing Data in Text Format | 169

Using 1xml.objectify, we parse the file and get a reference to the root node of the XML
file with getroot:

from lxml import objectify

path = 'Performance_MNR.xml"
parsed = objectify.parse(open(path) )
root = parsed.getroot()

root. INDICATOR return a generator yielding each <INDICATOR> XML element. For each
record, we can populate a dict of tag names (like YTD_ACTUAL) to data values (excluding
a few tags):

data = []

skip fields = ['PARENT_SEQ', ‘INDICATOR SEQ",
"DESIRED CHANGE’, ‘DECIMAL PLACES' ]

for elt in root. INDICATOR:
el_data = {}
for child in elt.getchildren():
if child.tag in skip fields:
continue
el_data[child.tag] = child.pyval
data.append(el_data)

Lastly, convert this list of dicts into a DataFrame:
In [927]: perf = DataFrame(data)

In [928]: perf

Out[928]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 648 entries, 0 to 647
Data columns:

AGENCY_NAME 648 non-null values
CATEGORY 648 non-null values
DESCRIPTION 648 non-null values
FREQUENCY 648 non-null values

INDICATOR_NAME 648 non-null values
INDICATOR_UNIT 648 non-null values
MONTHLY_ACTUAL 648 non-null values
MONTHLY_ TARGET 648 non-null values

PERIOD_MONTH 648 non-null values
PERIOD_YEAR 648 non-null values
YTD_ACTUAL 648 non-null values
YTD_TARGET 648 non-null values

dtypes: int64(2), object(10)

XML data can get much more complicated than this example. Each tag can have met-
adata, too. Consider an HTML link tag which is also valid XML:

from StringIO import StringI0
tag = ‘<a href="http://www.google.com">Google</a>'

root = objectify.parse(StringI0(tag) ).getroot()

 

170 | Chapter6: Data Loading, Storage, and File Formats

You can now access any of the fields (like href) in the tag or the link text:

In [930]: root

Out[930]: <Element a at Ox88bd4bo>
In [931]: root.get('href')
Out[931]: ‘http://www.google.com'
In [932]: root.text

Out[932]: ‘Google’

Binary Data Formats

One of the easiest ways to store data efficiently in binary format is using Python’s built-
in pickle serialization. Conveniently, pandas objects all have a save method which
writes the data to disk as a pickle:

In [933]: frame = pd.read_csv('ch06/ex1.csv')
In [934]: frame
Out [934]
a b c_ d message
o 1 2 3 4 _ hello
1 5 6 7 8 world
2 9 10 11 12 foo
In [935]: frame.save('cho6/frame_pickle' )

You read the data back into Python with pandas.load, another pickle convenience
function:

In [936]: pd.load('cho6/frame_pickle' )
Out[936]:
a bc. d message
o 1 2 3 4 _ hello
1 5 6 7 8 world
2 9 10 11 12 foo

pickle is only recommended as a short-term storage format. The prob-
lem is that it is hard to guarantee that the format will be stable over time;
an object pickled today may not unpickle with a later version ofa library.
Ihave made every effort to ensure that this does not occur with pandas,
but at some point in the future it may be necessary to “break” the pickle
format.

 

 

Using HDF5 Format

There are a number of tools that facilitate efficiently reading and writing large amounts
of scientific data in binary format on disk. A popular industry-grade library for this is
HDF5, which is a C library with interfaces in many other languages like Java, Python,
and MATLAB. The “HDF” in HDF5 stands for hierarchical data format. Each HDF5

 

Binary Data Formats | 171

file contains an internal file system-like node structure enabling you to store multiple
datasets and supporting metadata. Compared with simpler formats, HDF5 supports
on-the-fly compression with a variety of compressors, enabling data with repeated pat-
terns to be stored more efficiently. For very large datasets that don’t fit into memory,
HDFS5 is a good choice as you can efficiently read and write small sections of much
larger arrays.

There are not one but two interfaces to the HDF5 library in Python, PyTables and h5py,
each of which takes a different approach to the problem. h5py provides a direct, but
high-level interface to the HDF5 API, while PyTables abstracts many of the details of
HDFS5 to provide multiple flexible data containers, table indexing, querying capability,
and some support for out-of-core computations.

pandas has a minimal dict-like HDFStore class, which uses PyTables to store pandas
objects:

In [937]: store = pd.HDFStore('mydata.hs')
In [938]: store['obj1'] = frame
In [939]: store['obj1_col'] = frame['a']

In [940]: store

Out[940]:

<class 'pandas.io.pytables.HDFStore'>
File path: mydata.h5

obj1 DataFrame

obj1_col Series

Objects contained in the HDF5 file can be retrieved in a dict-like fashion:

In [941]: store['obj1']
Out [941]:

a b c_ d message
o1 2 3 4 _ hello
1 5 6 7 8 world
2 9 10 11 12 foo
If you work with huge quantities of data, I would encourage you to explore PyTables
and h5py to see how they can suit your needs. Since many data analysis problems are
1O-bound (rather than CPU-bound), using a tool like HDF5 can massively accelerate
your applications.

HDF5 is not a database. It is best suited for write-once, read-many da-
tea) tasets. While data can be added to a file at any time, if multiple writers

do so simultaneously, the file can become corrupted.

 

Reading Microsoft Excel Files

pandas also supports reading tabular data stored in Excel 2003 (and higher) files using
the ExcelFile class. Interally ExcelFile uses the xlrd and openpyxl packages, so you

 

172 | Chapter6: Data Loading, Storage, and File Formats

may have to install them first. To use ExcelFile, create an instance by passing a path
to an xls or x1sx file:

xls_file = pd.ExcelFile('data.xls')

Data stored in a sheet can then be read into DataFrame using parse:
table = xls_file.parse('Sheet1')

Interacting with HTML and Web APIs

Many websites have public APIs providing data feeds via JSON or some other format.
There are a number of ways to access these APIs from Python; one easy-to-use method
that I recommend is the requests package (http://docs.python-requests.org). To search
for the words “python pandas” on Twitter, we can make an HTTP GET request like so:

In [944]: import requests
In [945]: url = 'http://search.twitter.com/search.json?q=python%20pandas '
In [946]: resp = requests.get(url)

In [947]: resp
Out[947]: <Response [200]>

The Response object’s text attribute contains the content of the GET query. Many web
APIs will return a JSON string that must be loaded into a Python object:

In [948]: import json
In [949]: data = json.loads(resp.text)

In [950]: data.keys()
Out[950]:
[u'next_page',
u'completed_in',
u'max_id_str',
u'since_id str’,
u'refresh_url',
u'results',
u'since_id',
u'results per page’,
u'query',
u'max_id',
u'page' ]

The results field in the response contains a list of tweets, each of which is represented
as a Python dict that looks like:

{u'created_at': u'Mon, 25 Jun 2012 :33 +0000',
u'from_user': u'wesmckinn',

u'from_user_id': 115494880,

u'from_user_id str': u'115494880',
u'from_user_name': u'Wes McKinney’,

u'geo': None,

 

Interacting with HTML and Web APIs | 173

u'id': 217313849177686018,
u'id_str': u'217313849177686018' ,

u'iso language code’: u'pt',

u'metadata': {u'result_type': u'recent'},

u'source': u'<a href="http://twitter.com/">web</a>',

u'text': u'Lunchtime pandas-fu http://t.co/SI70xZZQ #pydata',
u'to_user': None,

u'to_user_id': 0,

u'to_user_id str': u'O',

u'to_user_name': None}

We can then make a list of the tweet fields of interest then pass the results list to Da-
taFrame:

In [951]: tweet_fields = ['created_at', 'from_user', ‘id', ‘text']
In [952]: tweets = DataFrame(data['results'], columns=tweet_fields)

In [953]: tweets

Out [953]:

<class 'pandas.core. frame.DataFrame' >
Int64Index: 15 entries, 0 to 14

Data columns:

created_at 15 non-null values

from_user 15 non-null values
id 15 non-null values
text 15 non-null values

dtypes: int64(1), object(3)

Each row in the DataFrame now has the extracted data from each tweet:

In [121]: tweets.ix[7]

Out[121]:

created_at Thu, 23 Jul 2012 :00 +0000
from_user deblike
id 227419585803059201
text pandas: powerful Python data analysis toolkit
Name: 7

With a bit of elbow grease, you can create some higher-level interfaces to common web
APIs that return DataFrame objects for easy analysis.

Interacting with Databases

In many applications data rarely comes from text files, that being a fairly inefficient
way to store large amounts of data. SQL-based relational databases (such as SQL Server,
PostgreSQL, and MySQL) are in wide use, and many alternative non-SQL (so-called
NoSQL) databases have become quite popular. The choice of database is usually de-
pendent on the performance, data integrity, and scalability needs of an application.

Loading data from SQL into a DataFrame is fairly straightforward, and pandas has
some functions to simplify the process. As an example, I’ll use an in-memory SQLite
database using Python’s built-in sqlite3 driver:

 

174 | Chapter6: Data Loading, Storage, and File Formats

import sqlite3
query = """
CREATE TABLE test

(a VARCHAR(20), b VARCHAR(20) ,
c REAL, d INTEGER

——"

con = sqlite3.connect(':memory:')
con.execute(query)
con.commit()

Then, insert a few rows of data:

data = [(‘Atlanta', ‘Georgia’, 1.25, 6),
('Tallahassee', ‘Florida’, 2.6, 3),
('Sacramento', ‘California’, 1.7, 5)]

stmt = "INSERT INTO test VALUES(?, ?, ?, ?)"

con.executemany(stmt, data)
con.commit()

Most Python SQL drivers (PyODBC, psycopg2, MySQLdb, pymssal, etc.) return a list
of tuples when selecting data from a table:

In [956]: cursor = con.execute('select * from test')
In [957]: rows = cursor.fetchall()

In [958]: rows

Out [958]:

[(u'Atlanta', u'Georgia', 1.25, 6),
(u'Tallahassee', u'Florida', 2.6, 3),
(u'Sacramento', u'California', 1.7, 5)]

You can pass the list of tuples to the DataFrame constructor, but you also need the
column names, contained in the cursor’s description attribute:

In [959]: cursor.description

Out[959]:

(('a', None, None, None, None, None, None),
('b', None, None, None, None, None, None),
('c', None, None, None, None, None, None),
('d', None, None, None, None, None, None))

In [960]: DataFrame(rows, columns=zip(*cursor.description)[0])

Out [960]:

a b c d
) Atlanta Georgia 1.25 6
1 Tallahassee Florida 2.60 3
2 Sacramento California 1.70 5

This is quite a bit of munging that you’d rather not repeat each time you query the
database. pandas has a read_frame function in its pandas.io.sql module that simplifies
the process. Just pass the select statement and the connection object:

 

Interacting with Databases | 175

In [961]: import pandas.io.sql as sql

In [962]: sql.read_frame('select * from test', con)

Out [962]:

a b c d
0 Atlanta Georgia 1.25 6
1 Tallahassee Florida 2.60 3
2 Sacramento California 1.70 5

Storing and Loading Data in MongoDB

NoSQIL databases take many different forms. Some are simple dict-like key-value stores
like BerkeleyDB or Tokyo Cabinet, while others are document-based, with a dict-like
object being the basic unit of storage. I've chosen MongoDB (http://mongodb.org) for
my example. I started a MongoDB instance locally on my machine, and connect to it
on the default port using pymongo, the official driver for MongoDB:

import pymongo

con = pymongo.Connection('localhost', port=27017)
Documents stored in MongoDB are found in collections inside databases. Each running
instance of the MongoDB server can have multiple databases, and each database can
have multiple collections. Suppose I wanted to store the Twitter API data from earlier
in the chapter. First, I can access the (currently empty) tweets collection:

tweets = con.db.tweets
Then, I load the list of tweets and write each of them to the collection using
tweets.save (which writes the Python dict to MongoDB):

import requests, json
url = 'http://search.twitter.com/search. json?q=python%20pandas'
data = json.loads(requests.get(url).text)

for tweet in data['results']:
tweets. save(tweet)

Now, if I wanted to get all of my tweets (if any) from the collection, I can query the
collection with the following syntax:

cursor = tweets.find({'from_user': 'wesmckinn' })
The cursor returned is an iterator that yields each document as a dict. As above I can

convert this into a DataFrame, optionally extracting a subset of the data fields in each
tweet:

tweet_fields = ['created_at', ‘from_user', ‘id', ‘text']
result = DataFrame(list(cursor), columns=tweet_fields)

 

176 | Chapter6: Data Loading, Storage, and File Formats

CHAPTER 7
Data Wrangling: Clean, Transform,
Merge, Reshape

 

Much of the programming work in data analysis and modeling is spent on data prep-
aration: loading, cleaning, transforming, and rearranging. Sometimes the way that data
is stored in files or databases is not the way you need it for a data processing application.
Many people choose to do ad hoc processing of data from one form to another using
a general purpose programming, like Python, Perl, R, or Java, or UNIX text processing
tools like sed or awk. Fortunately, pandas along with the Python standard library pro-
vide you with a high-level, flexible, and high-performance set of core manipulations
and algorithms to enable you to wrangle data into the right form without much trouble.

If you identify a type of data manipulation that isn’t anywhere in this book or elsewhere
in the pandas library, feel free to suggest it on the mailing list or GitHub site. Indeed,
much of the design and implementation of pandas has been driven by the needs of real
world applications.

Combining and Merging Data Sets
Data contained in pandas objects can be combined together in a number of built-in
ways:

* pandas.merge connects rows in DataFrames based on one or more keys. This will
be familiar to users of SQL or other relational databases, as it implements database
join operations.

¢ pandas.concat glues or stacks together objects along an axis.

* combine first instance method enables splicing together overlapping data to fill
in missing values in one object with values from another.

I will address each of these and give a number of examples. They’ll be utilized in ex-
amples throughout the rest of the book.

 

177

Database-style DataFrame Merges

Merge or join operations combine data sets by linking rows using one or more keys.
These operations are central to relational databases. The merge function in pandas is
the main entry point for using these algorithms on your data.

Let’s start with a simple example:
In [15]: dfa = DataFrame({'key': ['b', 'b', ‘a’, 'c', ‘a’, ‘a’, ‘b'],

In [16]: df2

In [17]: df1

Out[17]:

data1 key

0

aAuPWNPR O
Du PWN PR

orTovuonve Ss

‘data1': range(7)})

= DataFrame({'key': ['a', 'b', ‘d'],
"data2': range(3)})

In [18]: df2
Out [18]:
data2 key
0 0 a
1 1 b
2 2 d

This is an example of a many-to-one merge situation; the data in df1 has multiple rows
labeled a and b, whereas df2 has only one row for each value in the key column. Calling
merge with these objects we obtain:

In [19]: pd.merge(df1, df2)

data1 key data2

Out[19]:
0 2
1 4
2 5
3 0
4 1
5 6

a
a
a
b
b

b

PRROO;O

Note that I didn’t specify which column to join on. If not specified, merge uses the
overlapping column names as the keys. It’s a good practice to specify explicitly, though:

In [20]: pd.merge(df1, df2, on='key')

data1 key data2

Out[20]:
0 2
1 4
2 >
3 0
4 1
5 6

a

cTrooo wy

PRROO;O

If the column names are different in each object, you can specify them separately:
In [21]: df3 = DataFrame({'lkey': ['b', 'b', ‘a’, 'c', ‘a’, ‘a’, ‘b'],

‘data1': range(7)})

 

178 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

In [22]: df4 = DataFrame({'rkey': ['a', 'b', ‘d'],
scone t "data2': range(3)})

In [23]: pd.merge(df3, df4, left_on="lkey', right_on='rkey')

Out [23]:

data1 lkey data2 rkey
0 2 a 0 a
1 4 a 0 a
2 5 a 0 a
3 ) b 1 b
4 1 b 1 b
5 6 b 1 b

You probably noticed that the 'c' and 'd' values and associated data are missing from
the result. By default merge does an ‘inner' join; the keys in the result are the intersec-
tion. Other possible options are 'left', 'right', and ‘outer’. The outer join takes the
union of the keys, combining the effect of applying both left and right joins:

In [24]: pd.merge(df1, df2, how='outer')

Out[24]:

datai key data2
0 2 a 0
1 4 a 0
2 5 a 0
3 0 »b 1
4 an) 1
5 6 b 1
6 3 ¢ NaN
7 NaN d 2

Many-to-many merges have well-defined though not necessarily intuitive behavior.
Here’s an example:
In [25]: dfa = DataFrame({'key': ['b', 'b', ‘a’, 'c', ‘a’, ‘b'],
asiae? "data1': range(6)})
In [26]: df2 = DataFrame({'key': ['a', ‘b', ‘a’, 'b', ‘d'],
asiae? "data2': range(5)})

In [27]: df1 In [28]: df2
Out [27]: Out [28]:
data1 key data2 key
0 0 b 0 0 a
1 1 b 1 1 b
2 2 a 2 2 a
3 3. 2C¢ 3 3 »b
4 4 a 4 4 d
5 5 b
In [29]: pd.merge(df1, df2, on='key', how='left')
Out[29]:
data1 key data2
0 2 a 0
1 2 a 2

 

Combining and Merging DataSets | 179

FPwoO ON OU BWN

0

WuUuMrPRPOOH SL

qoonooTo csc 8 ®
Swrwrwrn o

=
mw

Many-to-many joins form the Cartesian product of the rows. Since there were 3 'b'
rows in the left DataFrame and 2 in the right one, there are 6 'b' rows in the result.
The join method only affects the distinct key values appearing in the result:

In [30]: pd.merge(df1, df2, how='inner')

Out [30]:

data1 key data2

2

WO CONAUBWNF OO
UUrPRPOOL. HN

a

ooo oof w WY
WRPRWRPWRPN ON O

To merge with multiple keys, pass a list of column names:
In [31]: left = DataFrame({'key1': ['foo', 'foo', ‘bar'],

"key2': ['one', ‘two', ‘one'],
"lval': [1, 2, 3]})

In [32]: right = DataFrame({'key1': ['foo', 'foo', ‘bar’, ‘bar'],

‘key2': ['one', ‘one’, ‘one’, 'two'],
‘rval': [4, 5, 6, 7]})

In [33]: pd.merge(left, right, on=['key1', ‘key2'], how='outer')

Out [33]:

key1 key2 lval rval
0 bar one 3 6
1 bar two NaN 7
2 foo one 1 4
3 foo one 1 5
4 foo two 2 NaN

To determine which key combinations will appear in the result depending on the choice
of merge method, think of the multiple keys as forming an array of tuples to be used
as a single join key (even though it’s not actually implemented that way).

 

When joining columns-on-columns, the indexes on the passed Data-

ta) Frame objects are discarded.

 

180 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

A last issue to consider in merge operations is the treatment of overlapping column
names. While you can address the overlap manually (see the later section on renaming
axis labels), merge has a suffixes option for specifying strings to append to overlapping
names in the left and right DataFrame objects:

In [34]: pd.merge(left, right, on='key1')

Out [34]:

key1 key2_x lval key2_y rval

bar
bar
foo
foo
foo
foo

MW BPWNP OO

one 3 one 6
one 3 two 7
one d one 4
one d one 5
two 2 one 4
two 2 one 5

In [35]: pd.merge(left, right, on='key1', suffixes=('_left', ' right'))

Out [35]:

key1 key2_left lval key2_right rval

bar
bar
foo
foo
foo
foo

MW BPWNP OO

one 3 one 6
one 3 two 7
one 1 one 4
one 1 one 5
two 2 one 4
two 2 one 5

See Table 7-1 for an argument reference on merge. Joining on index is the subject of the

next section.

Table 7-1. merge function arguments

Argument
left
right
how

on

left_on
right_on
left_index
right_index

sort

suffixes

copy

Description

DataFrame to be merged on the left side

DataFrame to be merged on the right side

Oneof'inner', ‘outer’, ‘left’ or'right'.'inner' by default

Column names to join on. Must be found in both DataFrame objects. If not specified and no other join keys
given, will use the intersection of the column names in left and right as the join keys

Columns in left DataFrame to use as join keys

Analogous to Left_on for left DataFrame

Use row index in Left as its join key (or keys, if a Multilndex)
Analogous to left_index

Sort merged data lexicographically by join keys; True by default. Disable to get better performance in some
cases on large datasets

Tuple of string values to append to column names in case of overlap; defaults to ('_x', '_y').For
example, if ‘data' in both DataFrame objects, would appear as 'data_x' and 'data_y' inresult

__IfFalse, avoid copying data into resulting data structure in some exceptional cases. By default always copies

 

Combining and Merging DataSets | 181

Merging on Index

In some cases, the merge key or keys in a DataFrame will be found in its index. In this
case, you can pass left_index=True or right_index=True (or both) to indicate that the
index should be used as the merge key:
In [36]: left1 = DataFrame({'key': ['a', 'b', ‘a’, ‘a', 'b', ‘c'],
wwe ‘value’: range(6)})

In [37]: right1 = DataFrame({'group val': [3.5, 7]}, index=['a', 'b'])

In [38]: left In [39]: right1
Out [38]: Out [39]:
key value group val
a a 3.5
b 7.0

UWPWNP O
nrwvo es
MWPWN PO

In [40]: pd.merge(left1, right1, left_on='key', right_index=True)

Out [40]:

key value group val
O a 0 3.5
2 a 2 3.5
3 a 3 3.5
1 »b 1 7.0
4 b 4 7.0

Since the default merge method is to intersect the join keys, you can instead form the
union of them with an outer join:

In [41]: pd.merge(left1, right1, left_on='key', right_index=True, how='outer')

Out [41]:

key value group val
O a 0 3.5
2 a 2 3.5
30a 3 3.5
1 »b 1 7.0
4 b 4 7.0
5 ¢ 5 NaN

With hierarchically-indexed data, things are a bit more complicated:

In [42]: lefth = DataFrame({'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],
aera t ‘key2': [2000, 2001, 2002, 2001, 2002],
mare 2 ‘data': np.arange(5.)})

In [43]: righth = DataFrame(np.arange(12).reshape((6, 2)),
aera t index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', ‘Ohio'],
aera t [2001, 2000, 2000, 2000, 2001, 2002]],
mare 2 columns=['event1', '‘event2'])

In [44]: lefth In [45]: righth
Out [44]: Out [45]:

 

182 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

BWNRO

key1 key2 event1 event2
Ohio 2000 Nevada 2001 0 1
Ohio 2001 2000 2 3
Ohio 2002 Ohio 2000 4 5
Nevada 2001 2000 6 7
Nevada 2002 2001 8 9
2002 10 11

In this case, you have to indicate multiple columns to merge on as a list (pay attention
to the handling of duplicate index values):

In [46]

: pd.mer

Out [46]:

data
3

0

NROOW
b

N

key1
Nevada
Ohio
Ohio
Ohio
Ohio

: pd.mer

Out [47]:

data
4 NaN
3 3
4 4
0 0
0 0
1 1
2 2

key1
Nevada
Nevada
Nevada
Ohio
Ohio
Ohio
Ohio

ge(lefth, righth, left_on=['key1', ‘key2'], right_index=True)
key2 event1 event2
2001 0 1
2000 4 5
2000 6 7
2001 8 9
2002 10 11
ge(lefth, righth, left_on=['key1', ‘key2'],

right_index=True, how='outer' )

key2 event1 event2
2000 2 3
2001 0 1
2002 NaN NaN
2000 4 5
2000 6 7
2001 8 9
2002 10 11

Using the indexes of both sides of the merge is also not an issue:

In [48]

In [50]:

: left2

: right2

left2

Out[50]:

Ohio

In [52]:

Nevada
2

4
6

Out[52]:

Ohio
1
NaN
3
NaN
5

onaAnwd ®

Nevada
2

NaN

4

NaN

6

DataFrame([[1., 2.], [3., 4.], [5., 6-]], index=['a', 'c', ‘e'],
columns=['Ohio', 'Nevada'])

DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]],
index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])

In [51]: right2
Out [51]:

Missouri Alabama
b 7 8
c 9 10
d 11 12
e 13 14

pd.merge(left2, right2, how='outer', left_index=True, right_index=True)

Missouri Alabama
NaN NaN

7 8

9 10

11 12

13 14

 

Combining and Merging DataSets | 183

DataFrame has a more convenient join instance for merging by index. It can also be
used to combine together many DataFrame objects having the same or similar indexes
but non-overlapping columns. In the prior example, we could have written:

In [53]: left2.join(right2, how='outer')

Out [53]:

Ohio Nevada Missouri Alabama
a 1 2 NaN NaN
b NaN NaN 7 8
Cc 3 4 9 10
d NaN NaN 11 12
e 5 6 13 14

In part for legacy reasons (much earlier versions of pandas), DataFrame’s join method
performs a left join on the join keys. It also supports joining the index of the passed
DataFrame on one of the columns of the calling DataFrame:

In [54]: lefta.join(right1, on='key')

Out [54]:

key value group val
oO a 0 3.5
1 »b 1 7.0
2 a 2 3.5
3 a 3 3.5
4 b 4 7.0
5 oC 5 NaN

Lastly, for simple index-on-index merges, you can pass a list of DataFrames to join as
an alternative to using the more general concat function described below:

In [55]: another = DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]],

sores index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])

In [56]: left2.join([right2, another ])

Out [56]:

Ohio Nevada Missouri Alabama New York Oregon
a 1 2 NaN NaN 7 8
c 3 4 9 10 9 10
e 5 6 13 14 11 12

In [57]: left2.join([right2, another], how='outer')

Out [57]:

Ohio Nevada Missouri Alabama New York Oregon
a 1 2 NaN NaN 7 8
b NaN NaN i 8 NaN NaN
c 3 4 9 10 9 10
d NaN NaN 11 12 NaN NaN
e 5 6 13 14 11 12
f NaN NaN NaN NaN 16 17

 

184 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

Concatenating Along an Axis

Another kind of data combination operation is alternatively referred to as concatena-
tion, binding, or stacking. NumPy has a concatenate function for doing this with raw
NumPy arrays:

In [58]: arr = np.arange(12).reshape((3, 4))

In [59]: arr
Out [59]:
array([[ 0, 1, 2, 3],
[ 5, 6, 7],
[ 8, 9, 10, 11]])

of Oo
.

In [60]: np.concatenate([arr, arr], axis=1)
Out [60]:

array([[ 0, 1, 2, 3, 0, 1, 2, 3],

[ o
[ 4, 5» 6, Ts 4, 5» 6, 7];
[ 8, 9, 10, 11, 8, 9, 10, 11]])

In the context of pandas objects such as Series and DataFrame, having labeled axes
enable you to further generalize array concatenation. In particular, you have a number
of additional things to think about:

¢ If the objects are indexed differently on the other axes, should the collection of
axes be unioned or intersected?
* Do the groups need to be identifiable in the resulting object?
* Does the concatenation axis matter at all?
The concat function in pandas provides a consistent way to address each of these con-

cerns. Ill give a number of examples to illustrate how it works. Suppose we have three
Series with no index overlap:

In [61]: s1 = Series([0, 1], index=['a', 'b'])
In [62]: s2 = Series([2, 3, 4], index=['c', 'd', ‘'e'])
In [63]: s3 = Series([5, 6], index=['f', 'g'])

Calling concat with these object in a list glues together the values and indexes:

In [64]: pd.concat([s1, s2, s3])
Out [64]:
a 0

manoanse
Du PWN PR

 

Combining and Merging DataSets | 185

By default concat works along axis=0, producing another Series. If you pass axis=1, the
result will instead be a DataFrame (axis=1 is the columns):

In [65]: pd.concat([s1, s2, s3], axis=1)
Out [65]:
0 1 2
0 NaN NaN
1 NaN NaN
NaN 2 NaN
NaN 3 NaN
NaN 4 NaN
NaN NaN 5
NaN NaN 6

manApmnDanaey

In this case there is no overlap on the other axis, which as you can see is the sorted
union (the 'outer' join) of the indexes. You can instead intersect them by passing
join='inner':

In [66]: s4 = pd.concat([s1 * 5, s3])

In [67]: pd.concat([s1, s4], axis=1) In [68]: pd.concat([s1, s4], axis=1, join='inner')
Out [67]: Out [68]:
0 01
a 0 0 0
a5

1
0
bo4 5
5
6

ow

f NaN
g NaN

You can even specify the axes to be used on the other axes with join_axes:

In [69]: pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])
Out [69]:
0 1
a 0 0
c NaN NaN
bo1 65
e NaN NaN

One issue is that the concatenated pieces are not identifiable in the result. Suppose
instead you wanted to create a hierarchical index on the concatenation axis. To do this,
use the keys argument:

In [70]: result = pd.concat([s1, s1, s3], keys=['one', 'two', ‘three'])

In [71]: result

Out[71]:

one a 0
b 1

two a 0
b 1

three f 5
g 6

# Much more on the unstack function later
In [72]: result.unstack()
Out[72]:

 

186 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

a b f g
one O 1 NaN NaN
two O 1 NaN NaN
three NaN NaN 5 6

In the case of combining Series along axis=1, the keys become the DataFrame column
headers:

In [73]: pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', ‘three'])

Out[73]:

one two three
a O NaN NaN
b 1 NaN NaN
c NaN 2 NaN
d NaN 3 NaN
e NaN 4 NaN
f NaN NaN 5
g NaN NaN 6

The same logic extends to DataFrame objects:

In [74]: df1 = DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', ‘c'],
wwe columns=['one', '‘two'])

In [75]: df2 = DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', ‘c'],
wwe columns=['three', 'four'])

In [76]: pd.concat([df1, df2], axis=1, keys=['level1', ‘level2'])

Out [76]:
level1 level2
one two three four
a 0 1 5 6
b 2 3 NaN NaN
c 4 5 7 8

If you pass a dict of objects instead of a list, the dict’s keys will be used for the keys
option:

In [77]: pd.concat({'level1': df1, ‘level2': df2}, axis=1)

Out[77]:
level1 level2
one two three four
a 0 a 5 6
b 2 3 NaN NaN
c 4 5 7 8

There are a couple of additional arguments governing how the hierarchical index is
created (see Table 7-2):

In [78]: pd.concat([df1, df2], axis=1, keys=['level1', ‘level2'],
were t names=['upper', 'lower'])

Out [78]:

upper level1 level2

lower one two three four
a 0 1 5 6
b 2 3 NaN NaN
Cc 4 5 7 8

 

Combining and Merging DataSets | 187

A last consideration concerns DataFrames in which the row index is not meaningful in
the context of the analysis:

In [79]: dfa1 = DataFrame(np.random.randn(3, 4), columns=['a', 'b', ‘c', 'd'])
In [80]: df2 = DataFrame(np.random.randn(2, 3), columns=['b', 'd', ‘a'])
In [81]: dfa In [82]: df2
Out [81]: Out [82]:

a b c d b d a
0 -0.204708 0.478943 -0.519439 -0.555730 O 0.274992 0.228913 1.352917
1 1.965781 1.393406 0.092908 0.281746 1 0.886429 -2.001637 -0.371843

2 0.769023 1

-246435 1.007189 -1.296221

In this case, you can pass ignore_index=True:

n [83]: pd.concat([df1, df2], ignore _index=True)

Out [83]:
a

BWNRO

b c d

-0.204708 0.478943 -0.519439 -0.555730
1.965781 1.393406 0.092908 0.281746
0.769023 1.246435 1.007189 -1.296221
1.352917 0.274992 NaN 0.228913

-0.371843 0.886429 NaN -2.001637

Table 7-2. concat function arguments

Argument
objs

axis

join
join_axes

keys

levels
names

verify_integrity

ignore index

Description
List or dict of pandas objects to be concatenated. The only required argument
Axis to concatenate along; defaults to 0

Oneof'inner', 'outer', defaultingto ‘outer ';whetherto intersection (inner) orunion
(outer) together indexes along the other axes

Specific indexes to use for the other n-1 axes instead of performing union/intersection logic

Values to associate with objects being concatenated, forming a hierarchical index along the
concatenation axis. Can either be a list or array of arbitrary values, an array of tuples, ora list of
arrays (if multiple level arrays passed in levels)

Specific indexes to use as hierarchical index level or levels if keys passed
Names for created hierarchical levels if keys and / or levels passed

Check new axis in concatenated object for duplicates and raise exception if so. By default
(False) allows duplicates

Do not preserve indexes along concatenation axis, instead producing a new
range(total_length) index

Combining Data with Overlap

Another data combination situation can’t be expressed as either a merge or concate-
nation operation. You may have two datasets whose indexes overlap in full or part. As
a motivating example, consider NumPy’s where function, which expressed a vectorized

if-else:

 

188 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

Hi
s
=
eo
aS
a
©
"

Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan],
ccoseve S index=['f', 'e', 'd', 'c', 'b', ‘a'])

H
=
a
co
uw
a
oa
i]

Series(np.arange(len(a), dtype=np.float64),
saree index=['f', 'e', 'd', 'c', 'b', ‘a'])

In [86]: b[-1] = np.nan

In [87]: a In [88]: b In [89]: np.where(pd.isnull(a), b, a)
Out [87]: Out [88]: Out [89]:
f NaN f 0 f 0.0
e 205 e 1 e 205
d NaN d 2 d 2.0
Cc 3.5 Cc 3 Cc 3.5
b 4.5 b 4 b 4.5
a NaN a NaN a NaN

Series has a combine first method, which performs the equivalent of this operation
plus data alignment:

In [90]: b[:-2].combine_first(a[2:])

Out[90]:
a NaN
b 4.5
c 3.0
d 2.0
e 1.0
f 0.0

With DataFrames, combine first naturally does the same thing column by column, so
you can think of it as “patching” missing data in the calling object with data from the
object you pass:

In [91]: df1 = DataFrame({'a': [1., np.nan, 5., np.nan],
weeat "b': [np.nan, 2., np.nan, 6.],
sates § "c': range(2, 18, 4)})

H
J
—
\o
N
=
a
+
N
I

DataFrame({'a': [5., 4., np.nan, 3., 7.],
sates § "b': [np.nan, 3., 4., 6., 8.]})

: df1.combine_first(df2)

b

N 2
2 6
4 10
6 14
8 NaN

BWNPRO

Reshaping and Pivoting

There are a number of fundamental operations for rearranging tabular data. These are
alternatingly referred to as reshape or pivot operations.

 

Reshaping and Pivoting | 189

Reshaping with Hierarchical Indexing

Hierarchical indexing provides a consistent way to rearrange data in a DataFrame.
There are two primary actions:

¢ stack: this “rotates” or pivots from the columns in the data to the rows

* unstack: this pivots from the rows into the columns
[ll illustrate these operations through a series of examples. Consider a small DataFrame
with string arrays as row and column indexes:

In [94]: data = DataFrame(np.arange(6).reshape((2, 3)),
wwe index=pd.Index(['Ohio', 'Colorado'], name='state'),
wwe? columns=pd.Index(['one', 'two', ‘'three'], name='number' ))

In [95]: data

Out [95]:

number one two three
state

Ohio 0 1 2
Colorado 3 4 5

Using the stack method on this data pivots the columns into the rows, producing a
Series:

In [96]: result = data.stack()

In [97]: result

Out [97]:

state number

Ohio one 0
two 1.
three 2

Colorado one 3
two 4
three 5

Froma hierarchically-indexed Series, you can rearrange the data back into a DataFrame
with unstack:

In [98]: result.unstack()

Out [98]:

number one two three
state

Ohio 0 1 2
Colorado 3 4 5

By default the innermost level is unstacked (same with stack). You can unstack a dif-
ferent level by passing a level number or name:

In [99]: result.unstack(0)

In [100]: result.unstack('state')

Out [99]: Out[100]:

state Ohio Colorado state Ohio Colorado
number number

one 0 3 one 0 3

 

190 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

two
three

1 4 two 1 4
2 5 three 2 5

Unstacking might introduce missing data if all of the values in the level aren’t found in
each of the subgroups:

In [101]:
In [102]:
In [103]:

In [104]:
Out[104]:

a
one 0

two NaN Na

s1 = Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])

s2 = Series([4, 5, 6], index=['c', 'd', ‘e'])

data2 = pd.concat([s1, s2], keys=['one', ‘two'])
data2.unstack()

bcdeoe
1 2 3 NaN
N45 6

Stacking filters out missing data by default, so the operation is easily invertible:

In [105]: data2.unstack().stack() In [106]: data2.unstack().stack(dropna=False)
Out [105]: Out[106]:
one a 0 one a 0
b 1 b 1
c 2 Cc 2
d 3 d 3
two c 4 e NaN
d 5 two a_ NaN
e 6 b NaN
c 4
d 5
e 6
When unstacking in a DataFrame, the level unstacked becomes the lowest level in the
result:
In [107]: df = DataFrame({'left': result, 'right': result + 5},
sere a columns=pd.Index(['left', 'right'], name='side'))
In [108]: df
Out [108]:
side left right
state number
Ohio one 0 5
two 1 6
three 2 7
Colorado one 3 8
two 4 9
three 5 10
In [109]: df.unstack('state' ) In [110]: df.unstack('state').stack('side' )
Out[109]: Out[110]:
side left right state Ohio Colorado
state Ohio Colorado Ohio Colorado number side
number one left 0 3
one 0 3 5 8 right 5 8
two 1 4 6 9 two left 1 4

 

Reshaping and Pivoting | 191

three 2 5 7 10 right 6 9
three left 2 5
right 7 10

Pivoting “long” to “wide” Format

A common way to store multiple time series in databases and CSV is in so-called long
or stacked format (code to create this DataFrame omitted for brevity):

In [116]: ldata[:10]

Out [116]:

date item value
0 1959-03-31 :00 realgdp 2710.349
1 1959-03-31 :00 infl 0.000
2 1959-03-31 :00 unemp 5.800
3 1959-06-30 :00 realgdp 2778.801
4 1959-06-30 :00 infl 2.340
5 1959-06-30 :00 unemp 5.100
6 1959-09-30 :00 realgdp 2775-488
7 1959-09-30 :00 infl 2.740
8 1959-09-30 :00 unemp 5.300
9 1959-12-31 :00 realgdp 2785.204

Data is frequently stored this way in relational databases like MySQL as a fixed schema
(column names and data types) allows the number of distinct values in the item column
to increase or decrease as data is added or deleted in the table. In the above example
date and item would usually be the primary keys (in relational database parlance),
offering both relational integrity and easier joins and programmatic queries in many
cases. The downside, of course, is that the data may not be easy to work with in long
format; you might prefer to have a DataFrame containing one column per distinct
item value indexed by timestamps in the date column. DataFrame’s pivot method per-
forms exactly this transformation:

In [117]: pivoted = ldata.pivot('date', 'item', 'value')

In [118]: pivoted.head()
Out [118]:

item infl realgdp unemp
date

1959-03-31 0.00 2710.349
1959-06-30 2.34 2778.801
1959-09-30 2.74 2775.488
1959-12-31 0.27 2785.204
1960-03-31 2.31 2847.699

uuwnuwuwnw
N DWP OC

The first two values passed are the columns to be used as the row and column index,
and finally an optional value column to fill the DataFrame. Suppose you had two value
columns that you wanted to reshape simultaneously:

In [119]: ldata['value2'] = np.random.randn(len(ldata))

In [120]: ldata[:10]
Out[120]:

 

192 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

1959-03-31
1959-03-31
1959-03-31
1959-06-30
1959-06-30
1959-06-30
1959-09-30
1959-09-30
1959-09-30
1959-12-31

WO ONADAUBPWNPF OO

00:
00:
00:
00:
00:
00:
00:
00:
00:
00:

d

00

00:
00:
00:
00:
00:
00:
00:

00:
00:

ate item
00 realgdp
00 infl
00 unemp
00 realgdp
00 infl
00 unemp
00 realgdp
200 infl
00 unemp

va

2710.

0.
5.

2778.

00 realgdp 2785.

lue

349 1.
000 -0O.
800 -O
801 0.

-340 3.
-100 -1.

488 -0.

-740 0.
-300 0.

204 0.

value2
669025
438570

-539741

476985
248944
021228
577087
124121
302614
523772

By omitting the last argument, you obtain a DataFrame with hierarchical columns:

In [121]: pivoted = ldata.pivot('date', 'item')

In [122]: pivoted[:5]

Out[122]:

item

date

1959-03-31
1959-06-30
1959-09-30
1959-12-31
1960-03-31

val
in

0.
2.
2.
0.
2.

ue
fl

00
34
74
27
31

realgdp unemp

2710.349
2778.801
2775.488
2785 .204
2847 .699

In [123]: pivoted['value'][:5]

Out [123]:
item

date
1959-03-31
1959-06-30
1959-09-30
1959-12-31
1960-03-31

inf

0.0

1

0

2.34
2.74

0.2
2.3

7
al

wuuwww uw

realgdp unemp

2710.349
2778.801
2775-488
2785.204
2847.699

uuMnuoow

NOW PR OC

NOW PR OC

ooOowo

value2
infl

-438570
- 248944
- 124121
-000940
- 831154

realgdp

1.669025
0.476985
-0.577087
0.523772
-0.713544

-0.
“1.
0.
1.
=2',

unemp

539741
021228
302614
343810
370232

Note that pivot is just a shortcut for creating a hierarchical index using set_index and
reshaping with unstack:

In [124]: unstacked = ldata.set_index(['date', 'item']).unstack('item')

In [125]: unstacked[:7]

Out[125]:

item

date

1959-03-31
1959-06-30
1959-09-30
1959-12-31
1960-03-31
1960-06-30
1960-09-30

val
in

NONONNO

ue
fl

00
34
.74
£27
31
14
-70

realgdp unemp

2710.349
2778.801
2775.488
2785 .204
2847 .699
2834.390
2839.022

Vuunuwuuwnwoww

DNN AW FE CO

value2
infl

-438570
- 248944
- 124121
-000940
831154
-860757
-119827

realgdp

1.669025
0.476985
-0.577087
0.523772
-0.713544
-1.860761
-1.265934

unemp

-539741
-021228
- 302614
- 343810
- 370232
- 560145
- 063512

 

Reshaping and Pivoting | 193

Data Transformation

So far in this chapter we’ve been concerned with rearranging data. Filtering, cleaning,
and other tranformations are another class of important operations.

Removing Duplicates

Duplicate rows may be found in a DataFrame for any number of reasons. Here is an
example:

In [126]: data = DataFrame({'ki': ['one'] * 3 + ['two'] * 4,
eee : "k2': [1, 1, 2, 3, 3, 4, 4]})

In [127]: data
Out [127]:
k1 k2

one
one
one
two
two
two
two

Aun PWNR OO
PPWWNP PRB

The DataFrame method duplicated returns a boolean Series indicating whether each
row is a duplicate or not:

In [128]: data.duplicated()

Out [128]:

0 False
a True
2 False
3 False
4 True
5 False
6 True

Relatedly, drop duplicates returns a DataFrame where the duplicated array is True:

In [129]: data.drop_duplicates()
Out [129]:

k1 k2
one
one
two
two

mwn o
PWN PR

Both of these methods by default consider all of the columns; alternatively you can
specify any subset of them to detect duplicates. Suppose we had an additional column
of values and wanted to filter duplicates only based on the 'k1' column:

In [130]: data['v1'] = range(7)

In [131]: data.drop_duplicates(['k1'])

 

194 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

Out [131]:

kit k2 vi
0 one 1 O
3 two 3 3

duplicated and drop_duplicates by default keep the first observed value combination.
Passing take_last=True will return the last one:

In [132]: data.drop_duplicates(['k1', 'k2'], take last=True)

Out [13

kt
one
one
two
two

DPN EF

2]:
k2 v1
1 ll
2 2
3 4
4 6

Transforming Data Using a Function or Mapping

For many data sets, you may wish to perform some transformation based on the values
in an array, Series, or column ina DataFrame. Consider the following hypothetical data
collected about some kinds of meat:

In [133]: data

In [134]: data

Out [13

CON DU BPWNPRP O

4]:

food

bacon

pulled pork

bacon
Pastrami

corned beef

Bacon
pastrami

honey ham

nova lox

DataFrame({'food': ['bacon', ‘pulled pork', ‘bacon’, ‘Pastrami’,

ounces

Aww on an

oooouwunoo°cjoe

"corned beef', ‘Bacon’, ‘pastrami’, ‘honey ham',
‘nova lox'],
‘ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})

Suppose you wanted to add a column indicating the type of animal that each food came
from. Let’s write down a mapping of each distinct meat type to the kind of animal:

meat_to_animal = {
"bacon': 'pig',

"pulled pork': 'pig',

' eh te
pastrami’: ‘cow’,

"corned beef’: ‘cow’,
"honey ham': 'pig',
"nova lox':

"salmon'

 

Data Transformation | 195

The map method on a Series accepts a function or dict-like object containing a mapping,
but here we have a small problem in that some of the meats above are capitalized and
others are not. Thus, we also need to convert each value to lower case:

In [136]: data['animal'] = data['food'].map(str.lower).map(meat_to_animal)

In [137]: data

Out [137]:

food ounces animal
0 bacon 4.0 pig
1 pulled pork 3.0 pig
2 bacon 12.0 pig
3 Pastrami 6.0 cow
4 corned beef 7.5 cow
5 Bacon 8.0 pig
6 pastrami 3.0 cow
7 honey ham 5.0 pig
8 nova lox 6.0 salmon

We could also have passed a function that does all the work:

In [138]: data['food'].map(lambda x: meat_to_animal[x.lower()])

Out [138]:
0 pig
1 pig
2 pig
3 cow
4 cow
5 pig
6 cow
7 pig
8 salmon
Name: food

Using map is a convenient way to perform element-wise transformations and other data
cleaning-related operations.

Replacing Values

Filling in missing data with the fillna method can be thought of as a special case of
more general value replacement. While map, as you’ve seen above, can be used to modify
a subset of values in an object, replace provides a simpler and more flexible way to do
so. Let’s consider this Series:

In [139]: data = Series([1., -999., 2., -999., -1000., 3.])

In [140]: data

Out [140]:
0 1
1 -999
2 2
3 -999
4 -1000
5 3

 

196 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

The -999 values might be sentinel values for missing data. To replace these with NA
values that pandas understands, we can use replace, producing a new Series:

In [141]: data.replace(-999, np.nan)

Out [141]:
0 1
a NaN
2 2
3 NaN
4 -1000
5 3

If you want to replace multiple values at once, you instead pass a list then the substitute
value:

In [142]: data.replace([-999, -1000], np.nan)
Out [142]:
1
NaN
2
NaN
NaN
3

UWPWNP OO

To use a different replacement for each value, pass a list of substitutes:

In [143]: data.replace([-999, -1000], [np.nan, 0])
Out [143]:
1
NaN
2
NaN
0
3

UWPWNPRP OO

The argument passed can also be a dict:

In [144]: data.replace({-999: np.nan, -1000: 0})
Out[144]:
1
NaN
2
NaN
0
3

UWPWNPRP OO

Renaming Axis Indexes

Like values in a Series, axis labels can be similarly transformed by a function or mapping
of some form to produce new, differently labeled objects. The axes can also be modified
in place without creating a new data structure. Here’s a simple example:

In [145]: data = DataFrame(np.arange(12).reshape((3, 4)),

awrmeet index=['Ohio', 'Colorado', ‘New York'],
wea et columns=['one', 'two', ‘three’, 'four'])

 

Data Transformation | 197

Like a Series, the axis indexes have a map method:

In [146]: data.index.map(str.upper)
Out[146]: array([OHIO, COLORADO, NEW YORK], dtype=object)

You can assign to index, modifying the DataFrame in place:

In [147]: data.index = data.index.map(str.upper)

In [148]: data

Out [148]:

one two three four
OHIO 0 1 2 3
COLORADO 4 5 6 7

NEW YORK 8 9 10 11

If you want to create a transformed version of a data set without modifying the original,
a useful method is rename:

In [149]: data.rename(index=str.title, columns=str.upper)

Out[149]:

ONE TWO THREE FOUR
Ohio 0 1 2 3
Colorado 4 5 6 7
New York 8 9 10 11

Notably, rename can be used in conjunction with a dict-like object providing new values
for a subset of the axis labels:

In [150]: data.rename(index={'OHIO': 'INDIANA'},
sees : columns={'three': 'peekaboo'})

Out [150]:

one two peekaboo four
INDIANA 0 1 2 3
COLORADO 4 5 6 7
NEW YORK 8 9 10 11

rename saves having to copy the DataFrame manually and assign to its index and col
umns attributes. Should you wish to modify a data set in place, pass inplace=True:

# Always returns a reference to a DataFrame
In [151]: _ = data.rename(index={'OHIO': 'INDIANA'}, inplace=True)

In [152]: data
Out [152]:

one two three four
INDIANA 0 1 2 3
COLORADO 4 5 6 7
NEW YORK 8 9 10 11

 

198 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

Discretization and Binning

Continuous data is often discretized or otherwised separated into “bins” for analysis.
Suppose you have data about a group of people in a study, and you want to group them
into discrete age buckets:

In [153]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]
Let’s divide these into bins of 18 to 25, 26 to 35, 35 to 60, and finally 60 and older. To
do so, you have to use cut, a function in pandas:

In [154]: bins = [18, 25, 35, 60, 100]

In [155]: cats = pd.cut(ages, bins)
In [156]: cats
Out [156]:
Categorical:
array([(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], (18, 25],
(35, 60], (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]], dtype=object)
Levels (4): Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)

The object pandas returns is a special Categorical object. You can treat it like an array
of strings indicating the bin name; internally it contains a levels array indicating the
distinct category names along with a labeling for the ages data in the labels attribute:

In [157]: cats.labels
Out[157]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1])

In [158]: cats.levels
Out[158]: Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)

In [159]: pd.value_counts(cats)
Out[159]:

(18, 25] 5

(35, 60] 3

(25, 35] 3

(60, 100] 1

Consistent with mathematical notation for intervals, a parenthesis means that the side
is open while the square bracket means it is closed (inclusive). Which side is closed can
be changed by passing right=False:

In [160]: pd.cut(ages, [18, 26, 36, 61, 100], right=False)

Out[160]:

Categorical:

array([[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), [18, 26),

[36, 61), [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)], dtype-object)
Levels (4): Index([[18, 26), [26, 36), [36, 61), [61, 100)], dtype=object)

You can also pass your own bin names by passing a list or array to the labels option:

In [161]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', ‘Senior’ ]

In [162]: pd.cut(ages, bins, labels=group_names)
Out[162]:

 

Data Transformation | 199

Categorical:
array([Youth, Youth, Youth, YoungAdult, Youth, Youth, MiddleAged,

YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult], dtype=object)
Levels (4): Index([Youth, YoungAdult, MiddleAged, Senior], dtype=object)

If you pass cut a integer number of bins instead of explicit bin edges, it will compute
equal-length bins based on the minimum and maximum values in the data. Consider
the case of some uniformly distributed data chopped into fourths:

In [163]: data = np.random.rand(20)

In [164]: pd.cut(data, 4, precision=2)
Out [164]:
Categorical:
array([(0.45, 0.67], (0.23, 0.45], (0.0037, 0.23], (0.45, 0.67],
(0.67, 0.9], (0.45, 0.67], (0.67, 0.9], (0.23, 0.45], (0.23, 0.45],
(0.67, 0.9], (0.67, 0.9], (0.67, 0.9], (0.23, 0.45], (0.23, 0.45],
(0.23, 0.45], (0.67, 0.9], (0.0037, 0.23], (0.0037, 0.23],
(0.23, 0.45], (0.23, 0.45]], dtype=object)
Levels (4): Index([(0.0037, 0.23], (0.23, 0.45], (0.45, 0.67],
(0.67, 0.9]], dtype=object)

A closely related function, qcut, bins the data based on sample quantiles. Depending
on the distribution of the data, using cut will not usually result in each bin having the
same number of data points. Since qcut uses sample quantiles instead, by definition
you will obtain roughly equal-size bins:

In [165]: data = np.random.randn(1000) # Normally distributed
In [166]: cats = pd.qcut(data, 4) # Cut into quartiles

In [167]: cats
Out [167]:
Categorical:
array([(-0.022, 0.641], [-3.745, -0.635], (0.641, 3.26], ...,
(-0.635, -0.022], (0.641, 3.26], (-0.635, -0.022]], dtype=object)
Levels (4): Index([[-3.745, -0.635], (-0.635, -0.022], (-0.022, 0.641],
(0.641, 3.26]], dtype=object)

In [168]: pd.value_counts(cats)

Out[168]:

[-3.745, -0.635] 250
(0.641, 3.26] 250
(-0.635, -0.022] 250
(-0.022, 0.641] 250

Similar to cut you can pass your own quantiles (numbers between 0 and 1, inclusive):

In [169]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])
Out[169]:
Categorical:
array([(-0.022, 1.302], (-1.266, -0.022], (-0.022, 1.302], ...,
(-1.266, -0.022], (-0.022, 1.302], (-1.266, -0.022]], dtype=object)
Levels (4): Index([[-3.745, -1.266], (-1.266, -0.022], (-0.022, 1.302],
(1.302, 3.26]], dtype=object)

 

200 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

We'll return to cut and qcut later in the chapter on aggregation and group operations,
as these discretization functions are especially useful for quantile and group analysis.

Detecting and Filtering Outliers

Filtering or transforming outliers is largely a matter of applying array operations. Con-
sider a DataFrame with some normally distributed data:

In [170]: np.random.seed(12345)
In [171]: data = DataFrame(np.random.randn(1000, 4))

In [172]: data.describe()

Out [172]:

0 1 2 3
count 1000.000000 1000.000000 1000.000000 1000.000000
mean -0.067684 0.067924 0.025598 -0.002298
std 0.998035 0.992106 1.006835 0.996794
min -3.428254 -3.548824 -3.184377 -3.745356
25% -0.774890 -0.591841 -0.641675 -0.644144
50% -0.116401 0.101143 0.002073 -0.013611
75% 0.616366 0.780282 0.680391 0.654328
max 3.366626 2.653656 3.260383 3.927528

Suppose you wanted to find values in one of the columns exceeding three in magnitude:
In [173]: col = data[3]

In [174]: col[np.abs(col) > 3]
Out[174]:

97 3.927528

305 = - 3.399312

400 -3.745356

Name: 3

To select all rows having a value exceeding 3 or -3, you can use the any method on a
boolean DataFrame:

In [175]: data[(np.abs(data) > 3).any(1)]
Out[175]:

0 1 2 3
5 -0.539741 0.476985 3.248944 -1.021228
97 -0.774363 0.552936 0.106061 3.927528
102 -0.655054 -0.565230 -176873 0.959533
305 -2.315555 0.457246 -0.025907 -3.399312
324 0.050188 1.951312 3.260383 0.963301
400 0.146326 0.508391 -0.196713 -3.745356
499 -0.293333 -0.242459 -3.056990 1.918403
523 -3.428254 -0.296336 -0.439938 -0.867165
586 0.275144 1.179227 -3.184377 1.369891
808 -0.362528 -3.548824 1.553205 -2.186301
900 3.366626 -2.372214 0.851010 1.332846

OoOWwWow

Values can just as easily be set based on these criteria. Here is code to cap values outside
the interval -3 to 3:

 

DataTransformation | 201

In [176]: data[np.abs(data) > 3] = np.sign(data) * 3

In [177]: data.describe()

Out[177]:

0 1 2 3
count 1000.000000 1000.000000 1000.000000 1000.000000
mean -0.067623 0.068473 0.025153 -0.002081
std 0.995485 0.990253 1.003977 0.989736
min -3.000000 -3.000000 -3.000000 -3.000000
25% -0.774890 -0.591841 -0.641675 -0.644144
50% -0.116401 0.101143 0.002073 -0.013611
75% 0.616366 0.780282 0.680391 0.654328
max 3.000000 2.653656 3.000000 3.000000

The ufunc np.sign returns an array of 1 and -1 depending on the sign of the values.

Permutation and Random Sampling

Permuting (randomly reordering) a Series or the rows ina DataFrame is easy to do using
the numpy.random. permutation function. Calling permutation with the length of the axis
you want to permute produces an array of integers indicating the new ordering:

In [178]: df = DataFrame(np.arange(5 * 4).reshape(5, 4))
In [179]: sampler = np.random.permutation(5)

In [180]: sampler
Out[180]: array([1, 0, 2, 3, 4])

That array can then be used in ix-based indexing or the take function:

In [181]: df In [182]: df.take(sampler)
Out [181]: Out [182]:
Oo 4 2 3 Oo 1 2 3
Oo 8600 1 2 3 1 4 5 6 7
1 4 5 6 7 0 0 d. 2 3
2 8 9 10 11 2 8 9 10 11
3 12 13 14 #15 3 12 13 #14 #15
4 16 17 18 19 4 16 17 18 19

To select a random subset without replacement, one way is to slice off the first k ele-
ments of the array returned by permutation, where k is the desired subset size. There
are much more efficient sampling-without-replacement algorithms, but this is an easy
strategy that uses readily available tools:

In [183]: df.take(np.random.permutation(len(df))[:3])

Out [183]:

0 1 2 3

1 4 5 6 7

3 12 13 14 #15

4 16 17 18 19

To generate a sample with replacement, the fastest way is to use np. random. randint to
draw random integers:

 

202 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

In [184]:
In [185]:

In [186]:
Out [186]:

In [187]:

In [188]:
Out [188]:

bag = np.array([5, 7, -1, 6, 4])
sampler = np.random.randint(0, len(bag), size=10)

sampler
array([4, 4, 2, 2, 2, 0, 3, 0, 4, 1])

draws = bag.take(sampler)

draws
array([ 4, 4, -1, -1, -1, 5, 6, 5, 4, 7])

Computing Indicator/Dummy Variables

Another type of transformation for statistical modeling or machine learning applica-
tions is converting a categorical variable into a “dummy” or “indicator” matrix. If a
column in a DataFrame has k distinct values, you would derive a matrix or DataFrame
containing k columns containing all 1’s and 0’s. pandas has a get_dummies function for
doing this, though devising one yourself is not difficult. Let’s return to an earlier ex-
ample DataFrame:

In [189]: df = DataFrame({'key': ['b', 'b', ‘a’, 'c', ‘a’, ‘b'],

In [
Out[190]:
a b
001
1041
2 1 0
3 0 0
4 1 0
5 01

oorooon

‘data1': range(6)})

190]: pd.get_dummies(d#f[ 'key'])

In some cases, you may want to add a prefix to the columns in the indicator DataFrame,
which can then be merged with the other data. get_dummies has a prefix argument for
doing just this:

In [191]
In [192]
In [193]

data1
0

UWPPWN PO
MW PWN PR

: dummies = pd.get_dummies(df['key'], prefix='key')
: df_with_dummy = df[['data1']].join(dummies)

: df _with_dummy
Out[ 193]:

key_a key_b key_

1

a
0
0
1
0
1
0

RPOOOR

 

Data Transformation | 203

If a row in a DataFrame belongs to multiple categories, things are a bit more compli-
cated. Let’s return to the MovieLens 1M dataset from earlier in the book:

In [194]: mnames = ['movie_id', ‘title’, 'genres']

In [195]: movies = pd.read_table('ch02/movielens/movies.dat', sep='::',
sranee @ 3 header=None, names=mnames)

In [196]: movies[ :10]

Out [196]:

movie_id title genres
0 1 Toy Story (1995) Animation|Children's|Comedy
1 2 Jumanji (1995) Adventure|Children's|Fantasy
2 3 Grumpier Old Men (1995) Comedy | Romance
3 4 Waiting to Exhale (1995) Comedy | Drama
4 5 Father of the Bride Part II (1995) Comedy
5 6 Heat (1995) Action|Crime|Thriller
6 7 Sabrina (1995) Comedy | Romance
7 8 Tom and Huck (1995) Adventure |Children's
8 9 Sudden Death (1995) Action
9 10 GoldenEye (1995) Action |Adventure | Thriller

Adding indicator variables for each genre requires a little bit of wrangling. First, we
extract the list of unique genres in the dataset (using a nice set.union trick):

In [197]: genre iter = (set(x.split('|')) for x in movies.genres)
In [198]: genres = sorted(set.union(*genre_iter))

Now, one way to construct the indicator DataFrame is to start with a DataFrame of all
Zeros:

In [199]: dummies = DataFrame(np.zeros((len(movies), len(genres))), columns=genres)

Now, iterate through each movie and set entries in each row of dummies to 1:

In [200]: for i, gen in enumerate(movies.genres):
satay 62 dummies.ix[i, gen.split('|')] = 1

Then, as above, you can combine this with movies:

In [201]: movies _windic = movies. join(dummies.add_prefix('Genre_'))

In [202]: movies_windic.ix[0]

Out[202]:

movie_id 1
title Toy Story (1995)
genres Animation|Children's | Comedy
Genre_Action 0

Genre_Adventure
Genre_Animation
Genre_Children's
Genre_Comedy
Genre_Crime
Genre_Documentary
Genre_Drama
Genre_Fantasy

oooorRrRRO

 

204 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

Genre_Film-Noir
Genre_Horror
Genre_Musical
Genre_Mystery
Genre_Romance
Genre_Sci-Fi
Genre_Thriller
Genre_War
Genre_Western
Name: 0

oooooo 0o00

For much larger data, this method of constructing indicator variables
with multiple membership is not especially speedy. A lower-level func-
#ia° tion leveraging the internals of the DataFrame could certainly be writ-
* ten.

 

 

A useful recipe for statistical applications is to combine get_dummies with a discretiza-
tion function like cut:

In [204]: values = np.random.rand(10)

In [205]: values

Out [205]:

array([ 0.9296, 0.3164, 0.1839, 0.2046, 0.5677, 0.5955, 0.9645,
0.6532, 0.7489, 0.6536])

In [206]: bins = [0, 0.2, 0.4, 0.6, 0.8, 1]
In [207]: pd.get_dummies(pd.cut(values, bins))

Out [207]:
(0, 0.2] (0.2, 0.4] (0.4, 0.6] (0.6, 0.8] (0.8, 1]
0

]
0
0
1
0
0
0
0
0
0
0

WON ADU BWNF OO
oooooorRoR

oooorRRrRO OOO
PRPRPOOGOOOCOCOOCOO
ooorROOCOCOCOOR

String Manipulation

Python has long been a popular data munging language in part due to its ease-of-use
for string and text processing. Most text operations are made simple with the string
object’s built-in methods. For more complex pattern matching and text manipulations,
regular expressions may be needed. pandas adds to the mix by enabling you to apply
string and regular expressions concisely on whole arrays of data, additionally handling
the annoyance of missing data.

 

String Manipulation | 205

String Object Methods

In many string munging and scripting applications, built-in string methods are suffi-
cient. As an example, a comma-separated string can be broken into pieces with split:

In [208]: val = 'a,b, guido’

In [209]: val.split(','

Out[209]: ['a', 'b', ' guido']
split is often combined with strip to trim whitespace (including newlines):

In [210]: pieces = [x.strip() for x in val.split(',')]

In [211]: pieces

Out[211]: ['a', 'b', ‘guido']
These substrings could be concatenated together with a two-colon delimiter using ad-
dition:

In [212]: first, second, third = pieces

In [213]: first + '::' + second + '::' + third

Out[213]: ‘a::b::guido'
But, this isn’t a practical generic method. A faster and more Pythonic way is to pass a
list or tuple to the join method on the string '::':

In [214]: '::'.join(pieces)

Out[214]: 'a::b::guido'
Other methods are concerned with locating substrings. Using Python’s in keyword is
the best way to detect a substring, though index and find can also be used:

In [215]: 'guido' in val

Out[215]: True

In [216]: val.index(',' In [217]: val.find(':')

Out[216]: 1 Out[217]: -1
Note the difference between find and index is that index raises an exception if the string
isn’t found (versus returning -1):

In [218]: val.index(':')

ValueError Traceback (most recent call last)
<ipython-input-218-280f8b2856ce> in <module>()

----> 1 val.index(':')

ValueError: substring not found

Relatedly, count returns the number of occurrences of a particular substring:

In [219]: val.count(','
Out[219]: 2

replace will substitute occurrences of one pattern for another. This is commonly used
to delete patterns, too, by passing an empty string:

 

206 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

In [220]: val.replace(',', '::') In [221]: val.replace(',', '')
Out[220]: 'a::b:: guido’ Out[221]: ‘ab guido'

Regular expressions can also be used with many of these operations as you'll see below.

Table 7-3. Python built-in string methods

Argument

count

endswith, startswith
join

index

find

rfind

replace

strip, rstrip, lstrip

split
lower, upper

ljust, rjust

Description

Return the number of non-overlapping occurrences of substring in the string.
Returns True if string ends with suffix (starts with prefix).

Use string as delimiter for concatenating a sequence of other strings.

Return position of first character in substring if found in the string. Raises ValueEr
ror if not found.

Return position of first character of first occurrence of substring in the string. Like
index, but returns -1 if not found.

Return position of first character of ast occurrence of substring in the string. Returns -1
if not found.

Replace occurrences of string with another string.

Trim whitespace, including newlines; equivalent to x. strip() (andrstrip,
1strip, respectively) for each element.

Break string into list of substrings using passed delimiter.
Convert alphabet characters to lowercase or uppercase, respectively.

Left justify or right justify, respectively. Pad opposite side of string with spaces (or some
other fill character) to return a string with a minimum width.

Regular expressions

Regular expressions provide a flexible way to search or match string patterns in text. A
single expression, commonly called a regex, is a string formed according to the regular
expression language. Python’s built-in re module is responsible for applying regular
expressions to strings; I’ll give a number of examples of its use here.

 
 

Vs

4
a The art of writing regular expressions could be a chapter of its own and
43 thus is outside the book’s scope. There are many excellent tutorials and
“s' 4]3° references on the internet, such as Zed Shaw’s Learn Regex The Hard

 

Way (http://regex.learncodethehardway.org/book/).

The re module functions fall into three categories: pattern matching, substitution, and
splitting. Naturally these are all related; a regex describes a pattern to locate in the text,
which can then be used for many purposes. Let’s look at a simple example: suppose I
wanted to split a string with a variable number of whitespace characters (tabs, spaces,
and newlines). The regex describing one or more whitespace characters is \s+:

 

String Manipulation | 207

In [222]: import re
In [223]: text = "foo bar\t baz \tqux"

In [224]: re.split('\s+', text)
Out[224]: ['foo', ‘bar’, ‘baz’, ‘qux']

When you call re.split('\s+', text), the regular expression is first compiled, then its
split method is called on the passed text. You can compile the regex yourself with
re.compile, forming a reusable regex object:

In [225]: regex = re.compile('\s+')

In [226]: regex.split(text)
Out[226]: ['foo', ‘bar’, 'baz', ‘qux']

If, instead, you wanted to get a list of all patterns matching the regex, you can use the
findall method:

In [227]: regex. findall(text)

Out[227]: [' nM At

a,

SO To avoid unwanted escaping with \ in a regular expression, use raw
“ s string literals like r'C:\x' instead of the equivalent 'C:\\x'.

 

 

Creating a regex object with re.compile is highly recommended if you intend to apply
the same expression to many strings; doing so will save CPU cycles.

match and search are closely related to findall. While findall returns all matches in a
string, search returns only the first match. More rigidly, match only matches at the
beginning of the string. As a less trivial example, let’s consider a block of text and a
regular expression capable of identifying most email addresses:

text = """Dave dave@google.com

Steve steve@gmail.com

Rob rob@gmail.com
Ryan ryan@yahoo.com

pattern = r'[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4}'

# re. IGNORECASE makes the regex case-insensitive
regex = re.compile(pattern, flags=re.IGNORECASE)

Using findal1 on the text produces a list of the e-mail addresses:
In [229]: regex. findall(text)

Out[229]: ['dave@google.com', ‘steve@gmail.com', 'rob@gmail.com', 'ryan@yahoo.com' ]

search returns a special match object for the first email address in the text. For the
above regex, the match object can only tell us the start and end position of the pattern
in the string:

 

208 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

In [230]: m = regex.search(text)

In [231]: m
Out[231]: <_sre.SRE_Match at 0x10a05de00>

In [232]: text[m.start():m.end()]
Out[232]: '‘dave@google.com'

regex.match returns None, as it only will match if the pattern occurs at the start of the
string:

In [233]: print regex.match(text)
None

Relatedly, sub will return a new string with occurrences of the pattern replaced by the
a new string:

In [234]: print regex.sub('REDACTED', text)

Dave REDACTED

Steve REDACTED

Rob REDACTED
Ryan REDACTED

Suppose you wanted to find email addresses and simultaneously segment each address
into its 3 components: username, domain name, and domain suffix. To do this, put
parentheses around the parts of the pattern to segment:

In [235]: pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\.([A-Z]{2,4})'
In [236]: regex = re.compile(pattern, flags=re.IGNORECASE)

A match object produced by this modified regex returns a tuple of the pattern compo-
nents with its groups method:

In [237]: m = regex.match('wesm@bright.net' )

In [238]: m.groups()
Out[238]: (‘'wesm', ‘bright’, 'net')

findall returns a list of tuples when the pattern has groups:

In [239]: regex. findall(text)

Out [239]:

[('dave', ‘google’, ‘com'),
(‘steve', ‘gmail’, ‘com'),
(‘rob', 'gmail', ‘com'),
(‘ryan', 'yahoo', 'com')]

sub also has access to groups in each match using special symbols like \1, \2, etc.:

In [240]: print regex.sub(r'Username: \1, Domain: \2, Suffix: \3', text)
Dave Username: dave, Domain: google, Suffix: com

Steve Username: steve, Domain: gmail, Suffix: com

Rob Username: rob, Domain: gmail, Suffix: com

Ryan Username: ryan, Domain: yahoo, Suffix: com

 

String Manipulation | 209

There is much more to regular expressions in Python, most of which is outside the

book’s scope. To give you a flavor, one variation on the above email regex gives names

to the match groups:
regex = re.compile(r"""

(?P<username>[A-Z0-9._%+- ]+)

@

(?P<domain>[A-Z0-9.-]+)

Na

(?P<suffix>[A-Z]{2,4})""", flags=re. IGNORECASE | re.VERBOSE)

The match object produced by such a regex can produce a handy dict with the specified
group names:

In [242]: m = regex.match('wesm@bright.net' )

In [243]: m.groupdict()
Out[243]: {'domain': 'bright', 'suffix': 'net', ‘username’: 'wesm'}

Table 7-4. Regular expression methods

Argument Description

findall, finditer Return all non-overlapping matching patterns ina string. Findal1 returns a list of all
patterns while finditer returns them one by one from an iterator.

match Match pattern at start of string and optionally segment pattern components into groups.
If the pattern matches, returns a match object, otherwise None.

search Scan string for match to pattern; returning a match object if so. Unlike match, the match
can be anywhere in the string as opposed to only at the beginning.

split Break string into pieces at each occurrence of pattern.

sub, subn Replace all (sub) or first n occurrences (subn) of pattern in string with replacement
expression. Use symbols \1, \2, ... toreferto match group elements in the re-
placement string.

 

Vectorized string functions in pandas

Cleaning up a messy data set for analysis often requires a lot of string munging and
regularization. To complicate matters, a column containing strings will sometimes have
missing data:

In [244]: data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com',
weeee : "Rob': 'rob@gmail.com', 'Wes': np.nan}

In [245]: data = Series(data)

In [246]: data In [247]: data.isnull()
Out [246]: Out[247]:

Dave dave@google.com Dave False

Rob rob@gmail.com Rob False

Steve steve@gmail.com Steve False

Wes NaN Wes True

 

210 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

String and regular expression methods can be applied (passing a lambda or other func-
tion) to each value using data.map, but it will fail on the NA. To cope with this, Series
has concise methods for string operations that skip NA values. These are accessed
through Series’s str attribute; for example, we could check whether each email address
has 'gmail' in it with str.contains:

In [248]: data.str.contains('gmail')

Out [248]:

Dave False
Rob True
Steve True
Wes NaN

Regular expressions can be used, too, along with any re options like IGNORECASE:

In [249]: pattern
Out[249]: '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\.([A-Z]{2,4})'

In [250]: data.str.findall(pattern, flags=re.IGNORECASE)

Out [250]:

Dave [('dave', ‘google’, 'com')]
Rob [('rob', ‘gmail’, ‘com')]
Steve [('steve', 'gmail', 'com')]
Wes NaN

There are a couple of ways to do vectorized element retrieval. Either use str.get or
index into the str attribute:

In [251]: matches = data.str.match(pattern, flags=re.IGNORECASE)

In [252]: matches

Out [252]:

Dave (‘dave', ‘google’, 'com')

Rob (‘rob', ‘gmail’, 'com')

Steve (‘steve', ‘gmail’, 'com')

Wes NaN

In [253]: matches.str.get(1) In [254]: matches.str[0]
Out[253]: Out[254]:

Dave google Dave dave
Rob gmail Rob rob
Steve gmail Steve steve
Wes NaN Wes NaN

You can similarly slice strings using this syntax:
In [255]: data.str[:5]

Out[255]:

Dave dave@
Rob rob@g
Steve steve
Wes NaN

 

String Manipulation | 211

Table 7-5. Vectorized string methods

Method Description

cat Concatenate strings element-wise with optional delimiter

contains Return boolean array if each string contains pattern/regex

count Count occurrences of pattern

endswith, startswith Equivalent to x. endswith(pattern) orx.startswith(pattern) for each el-
ement.

findall Compute list of all occurrences of pattern/regex for each string

get Index into each element (retrieve i-th element)

join Join strings in each element of the Series with passed separator

len Compute length of each string

lower, upper Convert cases; equivalent to x. lower () or x. upper () for each element.

match Use re.match with the passed regular expression on each element, returning matched
groups as list.

pad Add whitespace to left, right, or both sides of strings

center Equivalent to pad(side='both' )

repeat Duplicate values; for examples. str.repeat (3) equivalenttox * 3 foreachstring.

replace Replace occurrences of pattern/regex with some other string

slice Slice each string in the Series.

split Split strings on delimiter or regular expression

strip, rstrip, lstrip Trim whitespace, including newlines; equivalent to x. strip() (andrstrip,

1strip, respectively) for each element.

 

Example: USDA Food Database

The

US Department of Agriculture makes available a database of food nutrient infor-

mation. Ashley Williams, an English hacker, has made available a version of this da-

taba

se in JSON format (http://ashleyw.co.uk/project/food-nutrient-database). The re-

cords look like this:

"id": 21441,

"description": "KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,
Wing, meat and skin with breading",

"tags": ["KFC"],

"manufacturer": "Kentucky Fried Chicken",

"group": “Fast Foods",

"portions": [

"amount": 1,
"unit": "wing, with skin",
"grams": 68.0

},

 

212 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

—

"nutrients": [

{
"value": 20.8,
"units": "g",
"description": "Protein",
"group": "Composition"
},
]

}

Each food has a number of identifying attributes along with two lists of nutrients and
portion sizes. Having the data in this form is not particularly amenable for analysis, so
we need to do some work to wrangle the data into a better form.

After downloading and extracting the data from the link above, you can load it into
Python with any JSON library of your choosing. I’ll use the built-in Python json mod-
ule:

In [256]: import json
In [257]: db = json.load(open('cho7/foods-2011-10-03.json'))

In [258]: len(db)
Out[258]: 6636

Each entry in db is a dict containing all the data for a single food. The 'nutrients' field
is a list of dicts, one for each nutrient:

In [259]: db[0].keys() In [260]: db[o]['nutrients' ][o]
Out [259]: Out[260]:

[u'portions', {u'description': u'Protein',
u'description', u'group': u'Composition',
u'tags', u'units': u'g',

u'nutrients', u'value': 25.18}

u'group',

u'id',

u'manufacturer' ]
In [261]: nutrients = DataFrame(db[o]['nutrients'])

In [262]: nutrients[:7]

Out [262]:

description group units value
) Protein Composition g 25.18
1 Total lipid (fat) Composition g 29.20
2 Carbohydrate, by difference Composition g 3.06
3 Ash Other g 3.28
4 Energy Energy kcal 376.00
5 Water Composition g 39.28
6 Energy Energy kJ 1573.00

 

Example: USDA Food Database | 213

When converting a list of dicts toa DataFrame, we can specify a list of fields to extract.
We'll take the food names, group, id, and manufacturer:

In [263]: info_keys = ['description'’, 'group', 'id', ‘manufacturer’ ]
In [264]: info = DataFrame(db, columns=info_ keys)

In [265]: info[:5]
Out [265]:
description group id manufacturer

0 Cheese, caraway Dairy and Egg Products 1008

1 Cheese, cheddar Dairy and Egg Products 1009

2 Cheese, edam Dairy and Egg Products 1018

3 Cheese, feta Dairy and Egg Products 1019

4 Cheese, mozzarella, part skim milk Dairy and Egg Products 1028

In [266]: info

Out [266]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 6636 entries, 0 to 6635
Data columns:

description 6636 non-null values
group 6636 non-null values
id 6636 non-null values

manufacturer 5195 non-null values
dtypes: int64(1), object(3)

You can see the distribution of food groups with value_counts:

In [267]: pd.value_counts(info.group)[:10]

Out [267]:

Vegetables and Vegetable Products 812
Beef Products 618
Baked Products 496
Breakfast Cereals 403
Legumes and Legume Products 365
Fast Foods 365
Lamb, Veal, and Game Products 345
Sweets 341
Pork Products 328
Fruits and Fruit Juices 328

Now, to do some analysis on all of the nutrient data, it’s easiest to assemble the nutrients
for each food into a single large table. To do so, we need to take several steps. First, I'll
convert each list of food nutrients to a DataFrame, add a column for the food id, and
append the DataFrame to a list. Then, these can be concatenated together with concat:

nutrients = []

for rec in db:
fnuts = DataFrame(rec[ ‘nutrients’ ])
fnuts['id'] = rec['id']
nutrients.append(fnuts)

nutrients = pd.concat(nutrients, ignore_index=True)

 

214 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

If all goes well, nutrients should look like this:

In [269]: nutrients

Out [269]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 389355 entries, 0 to 389354
Data columns:

description 389355 non-null values

group 389355 non-null values
units 389355 non-null values
value 389355 non-null values
id 389355 non-null values

dtypes: float64(1), int64(1), object(3)

I noticed that, for whatever reason, there are duplicates in this DataFrame, so it makes
things easier to drop them:

In [270]: nutrients.duplicated().sum()
Out[270]: 14179

In [271]: nutrients = nutrients.drop_duplicates()

Since 'group' and ‘description’ is in both DataFrame objects, we can rename them to
make it clear what is what:

In [272]: col_mapping = {'description' : 'food',
weeee : ‘group’ : 'fgroup'}

In [273]: info = info.rename(columns=col_mapping, copy=False)

In [274]: info

Out [274]:

<class 'pandas.core. frame.DataFrame' >
Int64Index: 6636 entries, 0 to 6635
Data columns:

food 6636 non-null values
fgroup 6636 non-null values
id 6636 non-null values

manufacturer 5195 non-null values
dtypes: int64(1), object(3)

In [275]: col_mapping = {'description' : ‘nutrient’,
wees ; ‘group’ : ‘nutgroup'}

In [276]: nutrients = nutrients.rename(columns=col_mapping, copy=False)

In [277]: nutrients

Out [277]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 375176 entries, 0 to 389354
Data columns:

nutrient 375176 non-null values
nutgroup 375176 non-null values
units 375176 non-null values
value 375176 non-null values

 

Example: USDA Food Database | 215

id 375176 non-null values
dtypes: float64(1), int64(1), object(3)

With all of this done, we’re ready to merge info with nutrients:

In [278]: ndata = pd.merge(nutrients, info, on='id', how='outer')

In [279]: ndata

Out [279]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 375176 entries, 0 to 375175
Data columns:

nutrient 375176 non-null values
nutgroup 375176 non-null values
units 375176 non-null values
value 375176 non-null values
id 375176 non-null values
food 375176 non-null values
fgroup 375176 non-null values

manufacturer 293054 non-null values
dtypes: float64(1), int64(1), object(6)

In [280]: ndata.ix[30000]

Out [280]:

nutrient Folic acid
nutgroup Vitamins
units mcg
value 0
id 5658
food Ostrich, top loin, cooked
fgroup Poultry Products
manufacturer

Name: 30000

The tools that you need to slice and dice, aggregate, and visualize this dataset will be
explored in detail in the next two chapters, so after you get a handle on those methods
you might return to this dataset. For example, we could a plot of median values by food
group and nutrient type (see Figure 7-1):

In [281]: result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)
In [282]: result['Zinc, Zn'].order().plot(kind='barh')

With a little cleverness, you can find which food is most dense in each nutrient:

by_nutrient = ndata.groupby(['nutgroup', 'nutrient'])

get_maximum = lambda x: x.xs(x.value.idxmax())
get_minimum = lambda x: x.xs(x.value.idxmin())

max_foods = by_nutrient.apply(get_maximum)[['value', 'food']]

# make the food a little smaller
max_foods.food = max_foods.food.str[:50]

 

216 | Chapter7: Data Wrangling: Clean, Transform, Merge, Reshape

 

 

Beef Products

Lamb, Veal, and Game Products|
Nut and Seed Products
Breakfast Cereals

Spices and Herbs

Poultry Products

Pork Products

Sausages and Luncheon Meats
Snacks

Dairy and Egg Products

Fast Foods

Legumes and Legume Products
Cereal Grains and Pasta

fgroup

Ethnic Foods

Restaurant Foods

Finfish and Shellfish Products|
Baked Products

Meals, Entrees, and Sidedishes
Baby Foods

Sweets

Vegetables and Vegetable Products|
Soups, Sauces, and Gravies
Fruits and Fruit Juices
Beverages

Fats and Oils

 

 

 

 

 

 

Figure 7-1. Median Zinc values by nutrient group

The resulting DataFrame is a bit too large to display in the book; here is just the ' Amino
Acids' nutrient group:

In [284]: max_foods.ix['Amino Acids']['food' ]

Out [284]:

nutrient

Alanine Gelatins, dry powder, unsweetened
Arginine Seeds, sesame flour, low-fat
Aspartic acid Soy protein isolate
Cystine Seeds, cottonseed flour, low fat (glandless)
Glutamic acid Soy protein isolate
Glycine Gelatins, dry powder, unsweetened
Histidine Whale, beluga, meat, dried (Alaska Native)
Hydroxyproline KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINAL R
Isoleucine Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA
Leucine Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA
Lysine Seal, bearded (Oogruk), meat, dried (Alaska Nativ
Methionine Fish, cod, Atlantic, dried and salted
Phenylalanine Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA
Proline Gelatins, dry powder, unsweetened
Serine Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA
Threonine Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA
Tryptophan Sea lion, Steller, meat with fat (Alaska Native)
Tyrosine Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA
Valine Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA
Name: food

 

Example: USDA Food Database | 217


CHAPTER 8
Plotting and Visualization

 

Making plots and static or interactive visualizations is one of the most important tasks
in data analysis. It may be a part of the exploratory process; for example, helping iden-
tify outliers, needed data transformations, or coming up with ideas for models. For
others, building an interactive visualization for the web using a toolkit like d3.js (http:
//d3js.org/) may be the end goal. Python has many visualization tools (see the end of
this chapter), but I'll be mainly focused on matplotlib (hitp://matplotlib.sourceforge
net).

matplotlib is a (primarily 2D) desktop plotting package designed for creating publica-
tion-quality plots. The project was started by John Hunter in 2002 to enable a MAT-
LAB-like plotting interface in Python. He, Fernando Pérez (of IPython), and others have
collaborated for many years since then to make [Python combined with matplotlib a
very functional and productive environment for scientific computing. When used in
tandem with a GUI toolkit (for example, within IPython), matplotlib has interactive
features like zooming and panning. It supports many different GUI backends on all
operating systems and additionally can export graphics to all of the common vector
and raster graphics formats: PDF, SVG, JPG, PNG, BMP, GIF, etc. I have used it to
produce almost all of the graphics outside of diagrams in this book.

matplotlib has a number of add-on toolkits, such as mplot3d for 3D plots and basemap
for mapping and projections. I will give an example using basemap to plot data on a map
and to read shapefiles at the end of the chapter.

To follow along with the code examples in the chapter, make sure you have started
IPython in Pylab mode (ipython --pylab) or enabled GUI event loop integration with
the %gui magic.

A Brief matplotlib API Primer

There are several ways to interact with matplotlib. The most common is through pylab
mode in [Python by running ipython --pylab. This launches [Python configured to be
able to support the matplotlib GUI backend of your choice (Tk, wxPython, PyQt, Mac

 

219

 

SPY daily

 

RSI (14) >70 = overbought
70 ot 7 |

 

30+ =

 

<30 = oversold

 

+ + + +
A, 22-May-2012 0:130.16 H:132.02 L:129.95 C:131.97, V:177.8M Chg:+1.81

   

 

 

 

9"

| |
os" a”

ow” “> aad

 

 

 

Figure 8-1. A more complex matplotlib financial plot

OS X native, GTK). For most users, the default backend will be sufficient. Pylab mode
also imports a large set of modules and functions into [Python to provide a more MAT-
LAB-like interface. You can test that everything is working by making a simple plot:

plot (np.arange(10) )
If everything is set up right, a new window should pop up with a line plot. You can
close it by using the mouse or entering close(). Matplotlib API functions like plot and

close are all in the matplotlib.pyplot module, which is typically imported by conven-
tion as:

import matplotlib.pyplot as plt
While the pandas plotting functions described later deal with many of the mundane

details of making plots, should you wish to customize them beyond the function op-
tions provided you will need to learn a bit about the matplotlib API.

 

 

a
ee There is not enough room in the book to give a comprehensive treatment
4s to the breadth and depth of functionality in matplotlib. It should be
“s' 4|3* enough to teach you the ropes to get up and running. The matplotlib
* gallery and documentation are the best resource for becoming a plotting
guru and using advanced features.
Figures and Subplots

Plots in matplotlib reside within a Figure object. You can create a new figure with
plt. figure:

In [13]: fig = plt.figure()

 

220 | Chapter8: Plotting and Visualization

If you are in pylab mode in IPython, a new empty window should pop up. plt.fig
ure has a number of options, notably figsize will guarantee the figure has a certain size
and aspect ratio if saved to disk. Figures in matplotlib also support a numbering scheme
(for example, plt.figure(2)) that mimics MATLAB. You can get a reference to the
active figure using plt.gcf().

You can’t make a plot with a blank figure. You have to create one or more subplots
using add_subplot:

In [14]: ax1 = fig.add_subplot(2, 2, 1)
This means that the figure should be 2 x 2, and we’re selecting the first of 4 subplots

(numbered from 1). If you create the next two subplots, you'll end up with a figure that
looks like Figure 8-2.

In [15]: ax2 = fig.add_subplot(2, 2, 2)

In [16]: ax3 = fig.add_subplot(2, 2, 3)

 

1.0 -——_____—_——_

0.87 1

0.6F 4

0.4+ 4

0.2} 4

 

 

 

0.85 0.2 0.4 0.6 0.8 1.0
1.0 y ‘ ‘ ‘

 

0.8F 1

0.6F 1

0.44 4

0.2} 4

 

 

 

0-85 0.2 0.4 0.6 0.8 1.0

 

 

 

Figure 8-2. An empty matplotlib Figure with 3 subplots

When you issue a plotting command like plt.plot([1.5, 3.5, -2, 1.6]), matplotlib
draws on the last figure and subplot used (creating one if necessary), thus hiding the
figure and subplot creation. Thus, if we run the following command, you'll get some-
thing like Figure 8-3:

In [17]: from numpy.random import randn
In [18]: plt.plot(randn(50).cumsum(), 'k--')

The 'k--' is astyle option instructing matplotlib to plot a black dashed line. The objects
returned by fig.add_subplot above are AxesSubplot objects, on which you can directly
plot on the other empty subplots by calling each one’s instance methods, see Figure 8-4:

 

A Brief matplotlib API Primer | 221

 

 

 

 

 

 

 

 

mn v7 N\
a 4K ow J . \ |
6F nll SO! \ |
4+ / Na 4
b r? “y 4
2 7 \
Okv \. |
9h \ 4
\ /
—4t v7, 4
~% 10 20 30 40 50

 

 

 

 

 

e
r 1 1 1 1 1
-5 0 5 10 15 20 25 30 35

 

 

 

 

 

 

 

Figure 8-4. Figure after additional plots
In [19]: _ = axi1.hist(randn(100), bins=20, color='k', alpha=0.3)
In [20]: ax2.scatter(np.arange(30), np.arange(30) + 3 * randn(30))
You can find a comprehensive catalogue of plot types in the matplotlib documentation.

Since creating a figure with multiple subplots according to a particular layout is such
a common task, there is a convenience method, plt.subplots, that creates a new figure
and returns a NumPy array containing the created subplot objects:

 

222 | Chapter8: Plotting and Visualization

In [22]: fig, axes = plt.subplots(2, 3)

In [23]: axes
Out [23]:
array([[Axes(0.125,0.536364;0.227941x0. 363636),
Axes (0.398529, 0.536364; 0.227941Xx0. 363636),
Axes (0.672059, 0.536364; 0.227941x0. 363636) |,
[Axes(0.125,0.1;0.227941x0. 363636),
Axes (0.398529, 0.130.227941x0. 363636),
Axes (0.672059,0.1;0.227941x0.363636)]], dtype=object)

This is very useful as the axes array can be easily indexed like a two-dimensional array;
for example, axes[0, 1]. You can also indicate that subplots should have the same X
or Y axis using sharex and sharey, respectively. This is especially useful when comparing
data on the same scale; otherwise, matplotlib auto-scales plot limits independently. See
Table 8-1 for more on this method.

Table 8-1. pyplot.subplots options

Argument Description

nrows Number of rows of subplots

ncols Number of columns of subplots

sharex All subplots should use the same X-axis ticks (adjusting the x1 im will affect all subplots)
sharey All subplots should use the same Y-axis ticks (adjusting the y1im will affect all subplots)

subplot_kw _ Dict of keywords passed to add_subp1ot call used to create each subplot.

**fig kw Additional keywords to subplots are used when creating the figure, such as plt.subplots(2, 2,
figsize=(8, 6))

 

Adjusting the spacing around subplots

By default matplotlib leaves a certain amount of padding around the outside of the
subplots and spacing between subplots. This spacing is all specified relative to the
height and width of the plot, so that if you resize the plot either programmatically or
manually using the GUI window, the plot will dynamically adjust itself. The spacing
can be most easily changed using the subplots adjust Figure method, also available as
a top-level function:

subplots adjust(left=None, bottom=None, right=None, top=None,
wspace=None, hspace=None)

wspace and hspace controls the percent of the figure width and figure height, respec-
tively, to use as spacing between subplots. Here is a small example where I shrink the
spacing all the way to zero (see Figure 8-5):

fig, axes = plt.subplots(2, 2, sharex=True, sharey=True)
for i in range(2):
for j in range(2):
axes[i, j].hist(randn(500), bins=50, color='k', alpha=0.5)
plt.subplots_adjust(wspace=0, hspace=0)

 

ABrief matplotlib API Primer | 223

 

40
35F
30;
25r
20
15
10}

SF
40
35-
30;
25+
20;
15+
10;

 

  

 

 

 

o

 

 

 

Figure 8-5. Figure with no inter-subplot spacing

You may notice that the axis labels overlap. matplotlib doesn’t check whether the labels
overlap, so in a case like this you would need to fix the labels yourself by specifying
explicit tick locations and tick labels. More on this in the coming sections.

Colors, Markers, and Line Styles

Matplotlib’s main plot function accepts arrays of X and Y coordinates and optionally
a string abbreviation indicating color and line style. For example, to plot x versus y with
green dashes, you would execute:

ax.plot(x, y, 'g--')

This way of specifying both color and linestyle in a string is provided as a convenience;
in practice if you were creating plots programmatically you might prefer not to have to
munge strings together to create plots with the desired style. The same plot could also
have been expressed more explicitly as:

ax.plot(x, y, linestyle='--', color='g')

There are a number of color abbreviations provided for commonly-used colors, but any
color on the spectrum can be used by specifying its RGB value (for example, '#CECE
CE'). You can see the full set of linestyles by looking at the docstring for plot.

Line plots can additionally have markers to highlight the actual data points. Since mat-
plotlib creates a continuous line plot, interpolating between points, it can occasionally
be unclear where the points lie. The marker can be part of the style string, which must
have color followed by marker type and line style (see Figure 8-6):

In [28]: plt.plot(randn(30).cumsum(), ‘ko--')

 

224 | Chapter8: Plotting and Visualization

 

1.0

0.5}

0.0;

—0.57

—1.0}

—1.5}
ry

—2.0F

—2.5-

—3.0}F

 

 

 

 

—3.5p

 

10 15 30

 

 

Figure 8-6. Line plot with markers example

This could also have been written more explicitly as:

plot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')

For line plots, you will notice that subsequent points are linearly interpolated by de-

fault. This can be altered with the drawstyle option:

In [30]: data = randn(30).cumsum()

In [31]: plt.plot(data, 'k--', label='Default')

Out [31]:

In [32]:
Out [32]:

In [33]:

[<matplotlib.lines.Line2D at 0x461cddo>]

plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')
[<matplotlib.lines.Line2D at 0x461f350>]

plt.legend(loc='best')

Ticks, Labels, and Legends

For most kinds of plot decorations, there are two main ways to do things: using the
procedural pyplot interface (which will be very familiar to MATLAB users) and the
more object-oriented native matplotlib API.

The pyplot

interface, designed for interactive use, consists of methods like xlim,

xticks, and xticklabels. These control the plot range, tick locations, and tick labels,
respectively. They can be used in two ways:

¢ Called with no arguments returns the current parameter value. For example
plt.xlim() returns the current X axis plotting range

 

ABrief matplotlib API Primer | 225

 

10

 

-- Default
— __ steps-post

8+

 

 

 

 

0 5 10 a5 20 25 30

 

 

 

Figure 8-7. Line plot with different drawstyle options

¢ Called with parameters sets the parameter value. So plt.xlim([0, 10]), sets the X
axis range to 0 to 10

All such methods act on the active or most recently-created AxesSubplot. Each of them
corresponds to two methods on the subplot object itself; in the case of xlim these are
ax.get_xlim and ax.set_xlim. I prefer to use the subplot instance methods myself in
the interest of being explicit (and especially when working with multiple subplots), but
you can certainly use whichever you find more convenient.

Setting the title, axis labels, ticks, and ticklabels

To illustrate customizing the axes, I’ll create a simple figure and plot of a random walk
(see Figure 8-8):

In [34]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)
In [35]: ax.plot(randn(1000) .cumsum())

To change the X axis ticks, it’s easiest to use set_xticks and set_xticklabels. The
former instructs matplotlib where to place the ticks along the data range; by default
these locations will also be the labels. But we can set any other values as the labels using
set_xticklabels:

In [36]: ticks = ax.set_xticks([0, 250, 500, 750, 1000])

In [37]: labels = ax.set_xticklabels(['one', 'two', ‘three’, 'four', 'five'],
aweal rotation=30, fontsize='small')

Lastly, set_xlabel gives a name to the X axis and set_title the subplot title:

 

226 | Chapter 8: Plotting and Visualization

 

40

30};

20+

10}

 

—205

 

1 1 1 1
200 400 600 800 1000

 

 

Figure 8-8. Simple plot for illustrating xticks

In [38]: ax.set_title('My first matplotlib plot’)
Out[38]: <matplotlib.text.Text at 0x79190912850>

In [39]: ax.set_xlabel('Stages')

See Figure 8-9 for the resulting figure. Modifying the Y axis consists of the same process,
substituting y for x in the above.

 

My first matplotlib plot

40

 

 

 

 

 

e
OS x? ente® or ae

Stages

 

 

 

Figure 8-9. Simple plot for illustrating xticks

 

ABrief matplotlib API Primer | 227

 

 

30

20+

10;

    
 

hw
yl ‘it hy
Vy ey

   

—10}
—20+
—30}+

—40+

 

505 200 400 600 800 1000

 

 

 

 

Figure 8-10. Simple plot with 3 lines and legend
Adding legends

Legends are another critical element for identifying plot elements. There are a couple
of ways to add one. The easiest is to pass the label argument when adding each piece
of the plot:

In [40]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)

In [41]: ax.plot(randn(1000).cumsum(), 'k', label='one')
Out[41]: [<matplotlib.lines.Line2D at 0x4720a90>]

In [42]: ax.plot(randn(1000).cumsum(), ‘k--', label='two')
Out[42]: [<matplotlib.lines.Line2D at 0x4720f90>]

In [43]: ax.plot(randn(1000).cumsum(), 'k.', label='three')
Out[43]: [<matplotlib.lines.Line2D at 0x4723550>]

Once you’ve done this, you can either call ax. legend() or plt. legend() to automatically
create a legend:

In [44]: ax.legend(loc='best')
See Figure 8-10. The loc tells matplotlib where to place the plot. If you aren’t picky

‘best’ is a good option, as it will choose a location that is most out of the way. To
exclude one or more elements from the legend, pass no label or label='_nolegend_'.

Annotations and Drawing on a Subplot

In addition to the standard plot types, you may wish to draw your own plot annotations,
which could consist of text, arrows, or other shapes.

 

228 | Chapter 8: Plotting and Visualization

Annotations and text can be added using the text, arrow, and annotate functions.
text draws text at given coordinates (x, y) on the plot with optional custom styling:

ax.text(x, y, ‘Hello world!',
family='monospace', fontsize=10)

Annotations can draw both text and arrows arranged appropriately. As an example,
let’s plot the closing S&P 500 index price since 2007 (obtained from Yahoo! Finance)
and annotate it with some of the important dates from the 2008-2009 financial crisis.
See Figure 8-11 for the result:

from datetime import datetime

fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

data = pd.read_csv('ch08/spx.csv', index_col=0, parse_dates=True)
spx = data['SPX']

spx.plot(ax=ax, style='k-')

crisis data = [
(datetime(2007, 10, 11), ‘Peak of bull market'),
(datetime(2008, 3, 12), ‘Bear Stearns Fails'),
(datetime(2008, 9, 15), ‘Lehman Bankruptcy')

]

for date, label in crisis data:
ax.annotate(label, xy=(date, spx.asof(date) + 50),
xytext=(date, spx.asof(date) + 200),
arrowprops=dict(facecolor='black'),
horizontalalignment='left', verticalalignment='top' )

# Zoom in on 2007-2010
ax.set_xlim(['1/1/2007', '1/1/2011'])
ax.set_ylim([600, 1800])

ax.set_title('Important dates in 2008-2009 financial crisis')
See the online matplotlib gallery for many more annotation examples to learn from.

Drawing shapes requires some more care. matplotlib has objects that represent many
common shapes, referred to as patches. Some of these, like Rectangle and Circle are
found in matplotlib.pyplot, but the full set is located in matplotlib.patches.

To add a shape to a plot, you create the patch object shp and add it to a subplot by
calling ax.add_patch(shp) (see Figure 8-12):

fig = plt.figure()

ax = fig.add_subplot(1, 1, 1)
rect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)
circ = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)

pgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],
color='g', alpha=0.5)

 

ABrief matplotlib API Primer | 229

 

ax.add_patch(rect)
ax.add_patch(circ)
ax.add_patch(pgon)

 

1800

1600

1400F

1200

 

 

 

1000

Bit once mnmnemdnomcnmad emma flim rise a fascresee

a 7 3 3S 3 5 9 9 \

08 8 iS) oo oe oe oY eo oy
ge . yt yo? y? ee y? a . RW ge

 

 

Figure 8-11. Important dates in 2008-2009 financial crisis

 

 

0.67

0.4+

0.2-

  

 

 

1 1 1 1
085 0.2 0.4 0.6 0.8 1.0

 

 

 

Figure 8-12. Figure composed from 3 different patches

If you look at the implementation of many familiar plot types, you will see that they
are assembled from patches.

 

230 | Chapter8: Plotting and Visualization

Saving Plots to File

The active figure can be saved to file using plt.savefig. This method is equivalent to
the figure object’s savefig instance method. For example, to save an SVG version of a
figure, you need only type:

plt.savefig('figpath.svg')
The file type is inferred from the file extension. So if you used .pdf instead you would
get a PDF. There are a couple of important options that I use frequently for publishing
graphics: dpi, which controls the dots-per-inch resolution, and bbox_inches, which can
trim the whitespace around the actual figure. To get the same plot as a PNG above with
minimal whitespace around the plot and at 400 DPI, you would do:

plt.savefig('figpath.png', dpi=400, bbox_inches='tight' )
savefig doesn’t have to write to disk; it can also write to any file-like object, such as a
StringI0:

from io import StringIO
buffer = StringI0()
plt.savefig (buffer)

plot_data = buffer.getvalue()

For example, this is useful for serving dynamically-generated images over the web.

Table 8-2. Figure.savefig options

Argument Description

fname String containing a filepath or a Python file-like object. The figure format is inferred from the file
extension, e.g. . pdf for PDF or . png for PNG.

dpi The figure resolution in dots per inch; defaults to 100 out of the box but can be configured

facecolor, edge The color of the figure background outside of the subplots. 'w' (white), by default
color

format The explicit file format to use (‘png', ‘pdf', ‘svg', 'ps', ‘eps’, ...)
bbox_inches The portion of the figure to save. If ‘tight’ is passed, will attempt to trim the empty space around
the figure

matplotlib Configuration

matplotlib comes configured with color schemes and defaults that are geared primarily
toward preparing figures for publication. Fortunately, nearly all of the default behavior
can be customized via an extensive set of global parameters governing figure size, sub-
plot spacing, colors, font sizes, grid styles, and so on. There are two main ways to
interact with the matplotlib configuration system. The first is programmatically from
Python using the rc method. For example, to set the global default figure size to be 10
x 10, you could enter:

plt.rc('figure', figsize=(10, 10))

 

A Brief matplotlib API Primer | 231

The first argument to rc is the component you wish to customize, such as 'figure',
‘axes’, 'xtick', ‘ytick', ‘'grid', ‘legend’ or many others. After that can follow a
sequence of keyword arguments indicating the new parameters. An easy way to write
down the options in your program is as a dict:

font_options = {'family' : ‘monospace’,
‘weight’ : ‘bold’,
‘size' =: 'small'}

plt.rc('font', **font_options)

For more extensive customization and to see a list of all the options, matplotlib comes
with a configuration file matplotlibrc in the matplotlib/mp1-data directory. If you cus-
tomize this file and place it in your home directory titled .matplotlibrc, it will be loaded
each time you use matplotlib.

Plotting Functions in pandas

As you’ve seen, matplotlib is actually a fairly low-level tool. You assemble a plot from
its base components: the data display (the type of plot: line, bar, box, scatter, contour,
etc.), legend, title, tick labels, and other annotations. Part of the reason for this is that
in many cases the data needed to make a complete plot is spread across many objects.
In pandas we have row labels, column labels, and possibly grouping information. This
means that many kinds of fully-formed plots that would ordinarily require a lot of
matplotlib code can be expressed in one or two concise statements. Therefore, pandas
has an increasing number of high-level plotting methods for creating standard visual-
izations that take advantage of how data is organized in DataFrame objects.

 

As of this writing, the plotting functionality in pandas is undergoing
tea) quite a bit of work. As part of the 2012 Google Summer of Code pro-

gram, a student is working full time to add features and to make the
interface more consistent and usable. Thus, it’s possible that this code
may fall out-of-date faster than the other things in this book. The online
pandas documentation will be the best resource in that event.

 

 

 

Line Plots

Series and DataFrame each have a plot method for making many different plot types.
By default, they make line plots (see Figure 8-13):

In [55]: s = Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))
In [56]: s.plot()

The Series object’s index is passed to matplotlib for plotting on the X axis, though this
can be disabled by passing use_index=False. The X axis ticks and limits can be adjusted
using the xticks and xlim options, and Y axis respectively using yticks and ylim. See

 

232 | Chapter 8: Plotting and Visualization

 

 

 

1 L L L i L L L

10 20 30 40 50 60 70 80 90

 

 

 

 

Figure 8-13. Simple Series plot example

Table 8-3 fora full listing of plot options. I’ll comment ona few more of them through-
out this section and leave the rest to you to explore.

Most of pandas’s plotting methods accept an optional ax parameter, which can be a
matplotlib subplot object. This gives you more flexible placement of subplots in a grid
layout. There will be more on this in the later section on the matplotlib API.

DataFrame’s plot method plots each of its columns as a different line on the same
subplot, creating a legend automatically (see Figure 8-14):

In [57]: df = DataFrame(np.random.randn(10, 4).cumsum(0),
aiaa t columns=['A', 'B', 'C', 'D'],
asa? index=np.arange(0, 100, 10))

 

a,

sO Additional keyword arguments to plot are passed through to the re-
“3 spective matplotlib plotting function, so you can further customize
“s' 4|3° these plots by learning more about the matplotlib API.

 

Table 8-3. Series.plot method arguments

Argument Description

label Label for plot legend

ax matplotlib subplot object to plot on. If nothing passed, uses active matplotlib subplot
style Style string, like 'ko--', to be passed to matplotlib.

alpha The plot fill opacity (from 0 to 1)

 

Plotting Functions in pandas | 233

 

 

 

 

0 10 20 30 40 50 60 70 80 90

 

 

 

Figure 8-14. Simple DataFrame plot example

Argument Description
kind Canbe'line', 'bar', 'barh', 'kde'
logy Use logarithmic scaling on the Y axis

use_index — Use the object index for tick labels

rot Rotation of tick labels (0 through 360)
xticks Values to use for X axis ticks

yticks Values to use for Y axis ticks

xlim X axis limits (e.g. [0, 10])

ylim Y axis limits

grid Display axis grid (on by default)

 

DataFrame has a number of options allowing some flexibility with how the columns
are handled; for example, whether to plot them all on the same subplot or to create
separate subplots. See Table 8-4 for more on these.

Table 8-4. DataFrame-specific plot arguments

Argument Description

subplots Plot each DataFrame column in a separate subplot

sharex If subplots=True, share the same X axis, linking ticks and limits
sharey If subplots=True, share the same Y axis

figsize Size of figure to create as tuple

 

234 | Chapter 8: Plotting and Visualization

Argument Description
title Plot title as string
legend Add a subplot legend (True by default)

sort_columns Plot columns in alphabetical order; by default uses existing column order

 

Vs
4
For time series plotting, see Chapter 10.

 

 

Bar Plots

Making bar plots instead of line plots is as simple as passing kind='bar' (for vertical
bars) or kind='barh' (for horizontal bars). In this case, the Series or DataFrame index
will be used as the X (bar) or Y (barh) ticks (see Figure 8-15):

In [59]: fig, axes = plt.subplots(2, 1)
In [60]: data = Series(np.random.rand(16), index=list('abcdefghijklmnop' ))

In [61]: data.plot(kind="bar', ax=axes[0], color='k', alpha=0.7)
Out[61]: <matplotlib.axes.AxesSubplot at 0x4ee7750>

In [62]: data.plot(kind="barh', ax=axes[1], color='k', alpha=0.7)

For more on the plt.subplots function and matplotlib axes and figures,
see the later section in this chapter.

 

 

With a DataFrame, bar plots group the values in each row together in a group in bars,
side by side, for each value. See Figure 8-16:
In [63]: df = DataFrame(np.random.rand(6, 4),
aera t index=['one', 'two', 'three', 'four', 'five', 'six'],
mare 2 columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))

In [64]: df

Out [64]:

Genus A B C D
one 0.301686 0.156333 0.371943 0.270731
two 0.750589 0.525587 0.689429 0.358974
three 0.381504 0.667707 0.473772 0.632528
four 0.942408 0.180186 0.708284 0.641783
five 0.840278 0.909589 0.010041 0.653207
six 0.062854 0.589813 0.811318 0.060217

In [65]: df.plot(kind=' bar’)

 

Plotting Functions in pandas | 235

 

 

 

 

 

oO 9oNA0-ET_- X_33500

oO

0.2 0.4 0.6 0.8 1.0

 

 

Figure 8-15. Horizonal and vertical bar plot example

Note that the name “Genus” on the DataFrame’s columns is used to title the legend.

Stacked bar plots are created from a DataFrame by passing stacked=True, resulting in
the value in each row being stacked together (see Figure 8-17):

In [67]: df.plot(kind="barh', stacked=True, alpha=0.5)

Vs,

    
 

A useful recipe for bar plots (as seen in an earlier chapter) is to visualize
a Series’s value frequency using value_counts: s.value_counts
ne! () «plot (kind='bar' )

 

Returning to the tipping data set used earlier in the book, suppose we wanted to make
a stacked bar plot showing the percentage of data points for each party size on each
day. I load the data using read_csv and make a cross-tabulation by day and party size:

In [68]: tips = pd.read_csv('cho8/tips.csv')
In [69]: party_counts = pd.crosstab(tips.day, tips.size)

In [70]: party_counts

Out[70]:

size 1 2 3 45 6
day

Fri 1 16 1 1 00
Sat 2 53 18 13 1 0
Sun oO 39 15 18 3 1
Thur 1 48 4 5 1 3

 

236 | Chapter8: Plotting and Visualization

 

# Not many 1- and 6-person parties
In [71]: party_counts = party_counts.ix[:, 2:5]

 

 

 

 

 

 

 

 

 

0.0

one
two
three
four
five
SIX

 

 

 

Figure 8-16. DataFrame bar plot example

 

six

five

 

four |

three |

two 1

 

0.0 0.5 1.0 1.5 2.0 2.5

 

 

 

Figure 8-17. DataFrame stacked bar plot example

Then, normalize so that each row sums to 1 (I have to cast to float to avoid integer
division issues on Python 2.7) and make the plot (see Figure 8-18):

# Normalize to sum to 1
In [72]: party_pcts = party_counts.div(party_counts.sum(1).astype(float), axis=0)

 

Plotting Functions in pandas | 237

In [73]: party_pcts

Out [73]:

size 2 3 4 5
day

Fri 0.888889 0.055556 0.055556 0.000000
Sat 0.623529 0.211765 0.152941 0.011765
Sun 0.520000 0.200000 0.240000 0.040000
Thur 0.827586 0.068966 0.086207 0.017241

In [74]: party_pcts.plot(kind="bar', stacked=True)

 

1.0

0.8

0.6

0.4

0.2

 

 

0.0

Fri
Sat
Sun
Thur

day

 

 

 

Figure 8-18. Fraction of parties by size on each day

So you can see that party sizes appear to increase on the weekend in this data set.

Histograms and Density Plots

A histogram, with which you may be well-acquainted, is a kind of bar plot that gives a
discretized display of value frequency. The data points are split into discrete, evenly
spaced bins, and the number of data points in each bin is plotted. Using the tipping
data from before, we can make a histogram of tip percentages of the total bill using the
hist method on the Series (see Figure 8-19):

In [76]: tips['tip_pct'] = tips['tip'] / tips['total_bill']

In [77]: tips['tip_pct'].hist(bins=50)

 

238 | Chapter 8: Plotting and Visualization

 

 

 

 

 

i) 0.1 0.2 0.3

 

 

 

Figure 8-19. Histogram of tip percentages

A related plot type is a density plot, which is formed by computing an estimate of a
continuous probability distribution that might have generated the observed data. A
usual procedure is to approximate this distribution as a mixture of kernels, that is,
simpler distributions like the normal (Gaussian) distribution. Thus, density plots are
also known as KDE (kernel density estimate) plots. Using plot with kind='kde' makes
a density plot using the standard mixture-of-normals KDE (see Figure 8-20):

In [79]: tips['tip_pct'].plot(kind='kde' )

These two plot types are often plotted together; the histogram in normalized form (to
give a binned density) with a kernel density estimate plotted on top. As an example,
consider a bimodal distribution consisting of draws from two different standard normal
distributions (see Figure 8-21):

In [81]: comp1 = np.random.normal(0, 1, size=200) # N(0, 1)
In [82]: comp2 = np.random.normal(10, 2, size=200) #N(10, 4)
In [83]: values = Series(np.concatenate([comp1, comp2]))

In [84]: values.hist(bins=100, alpha=0.3, color='k', normed=True)
Out[84]: <matplotlib.axes.AxesSubplot at 0x5cd2350>

In [85]: values.plot(kind='kde', style='k--')

Scatter Plots

Scatter plots are a useful way of examining the relationship between two one-dimen-
sional data series. matplotlib has a scatter plotting method that is the workhorse of

 

Plotting Functions in pandas | 239

 

 

 

 

1 i 1 i 1 i 1
{oa -0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2

 

 

 

Figure 8-20. Density plot of tip percentages

 

0.30

 

 

Density
oO
BR
ur
T

0.10

0.05

 

 

 

08070 -5 0 5 10 15 20 25

 

 

 

Figure 8-21. Normalized histogram of normal mixture with density estimate

making these kinds of plots. To give an example, I load the macrodata dataset from the
statsmodels project, select a few variables, then compute log differences:

In [86]: macro = pd.read_csv('ch08/macrodata.csv' )
In [87]: data = macro[['cpi', 'mi', 'tbilrate', 'unemp']]

In [88]: trans data = np.log(data).diff().dropna()

 

240 | Chapter8: Plotting and Visualization

In [89]: trans data[-5:]

Out [89]:

cpi
198 -0.007904
199 -0.021979
200 0.002340
201 0.008419
202 0.008894

It’s easy to plot a simple scatter plot using plt.scatter (see Figure 8-22):

mi tbilrate unemp
0.045361 -0.396881 0.105361
0.066753 -2.277267 0.139762
0.010286 0.606136 0.160343
0.037461 -0.200671 0.127339
0.012202 -0.405465 0.042560

In [91]: plt.scatter(trans data['m1'], trans_data['unemp'])
Out[91]: <matplotlib.collections.PathCollection at 0x43c31d0>

In [92]: plt.title('Changes in log %s vs. log %s' % (‘m1', ‘unemp'))

 

Changes in log m1 vs. log unemp

 

 

 

 

0.25 : .
e
0.20;
e
0.15} ar * é .
e ° ‘
e . .
0.10} ‘7 e
e
oo ym eo ° 8
e o. .
0.05+ * wow e e
* 1s o ~ oe of ‘ e
Pee Pom 0°88
0.00} © 00 come cam 0 © C00 000 08 0 Oo
©, e ee
e °° oe? aA ed age ee e
-0.05} & om fo gee
e
@ 2 %.
—0.10} 0
e
- 1 1 L L .
0-19 04 —0.02 0.00 0.02 0.04 0.06

 

0.08

 

 

Figure 8-22. A simple scatter plot

In exploratory data analysis it’s helpful to be able to look at all the scatter plots among
a group of variables; this is known as a pairs plot or scatter plot matrix. Making such a
plot from scratch is a bit of work, so pandas has a scatter_matrix function for creating
one from a DataFrame. It also supports placing histograms or density plots of each
variable along the diagonal. See Figure 8-23 for the resulting plot:

In [93]: pd.scatter_matrix(trans data, diagonal='kde', color='k', alpha=0.3)

Plotting Maps: Visualizing Haiti Earthquake Crisis Data

Ushahidi is a non-profit software company that enables crowdsourcing of information
related to natural disasters and geopolitical events via text message. Many of these data
sets are then published on their website for analysis and visualization. I downloaded

 

Plotting Maps: Visualizing Haiti Earthquake Crisis Data | 241

 

Scatter plot matrix of statsmodels macro data
ml

    

-1.0

tbilrate

tbilrate

 

 

 

Figure 8-23. Scatter plot matrix of statsmodels macro data

the data collected during the 2010 Haiti earthquake crisis and aftermath, and I’ll show
you how I prepared the data for analysis and visualization using pandas and other tools
we have looked at thus far. After downloading the CSV file from the above link, we can
load it into a DataFrame using read _csv:

In [94]: data = pd.read_csv('cho8/Haiti.csv')

In [95]: data

Out[95]:

<class 'pandas.core.frame.DataFrame'>
Int64Index: 3593 entries, 0 to 3592
Data columns:

Serial 3593 non-null values
INCIDENT TITLE 3593 non-null values
INCIDENT DATE 3593 non-null values

LOCATION 3593 non-null values
DESCRIPTION 3593 non-null values
CATEGORY 3587 non-null values
LATITUDE 3593 non-null values
LONGITUDE 3593 non-null values
APPROVED 3593 non-null values
VERIFIED 3593 non-null values

dtypes: float64(2), int64(1), object(7)

 

242 | Chapter8: Plotting and Visualization

It’s easy now to tinker with this data set to see what kinds of things we might want to
do with it. Each row represents a report sent from someone’s mobile phone indicating
an emergency or some other problem. Each has an associated timestamp and a location
as latitude and longitude:

In [96]: data[['INCIDENT DATE', 'LATITUDE', ‘LONGITUDE']][:10]
Out [96]:

INCIDENT DATE LATITUDE LONGITUDE
05/07/2010  18.233333 -72.533333
28/06/2010  50.226029 5.729886
24/06/2010  22.278381 114.174287
20/06/2010  44.407062 8.933989
18/05/2010  18.571084 -72.334671
26/04/2010  18.593707 -72.310079
26/04/2010  18.482800 -73.638800
26/04/2010  18.415000 -73.195000
15/03/2010  18.517443 -72.236841
15/03/2010  18.547790 -72.410010

Ww ONAUBWNF OO

The CATEGORY field contains a comma-separated list of codes indicating the type of
message:

In [97]: data['CATEGORY' ][:6]

Out [97]:

0 1. Urgences | Emergency, 3. Public Health,
1 1. Urgences | Emergency, 2. Urgences logistiques
2 2. Urgences logistiques | Vital Lines, 8. Autre |
3 1. Urgences | Emergency,
4 1. Urgences | Emergency,
5 5e. Communication lines down,
Name: CATEGORY

If you notice above in the data summary, some of the categories are missing, so we
might want to drop these data points. Additionally, calling describe shows that there
are some aberrant locations:

In [98]: data.describe()
Out [98]:

Serial LATITUDE LONGITUDE
count 3593.000000 3593.000000 3593.000000
mean 2080.277484 18.611495 -72.322680
std 1171.100360 0.738572 3.650776
min 4.000000 18.041313 -74.452757
25% 1074.000000 18.524070 -72.417500
50% 2163 .000000 18.539269 -72.335000
75% 3088 .000000 18.561820 -72.293570
max 4052.000000 50.226029 114.174287

Cleaning the bad locations and removing the missing categories is now fairly simple:

In [99]: data = data[(data.LATITUDE > 18) & (data.LATITUDE < 20) &
eormeat (data.LONGITUDE > -75) & (data.LONGITUDE < -70)
were t & data.CATEGORY.notnull() ]

 

Plotting Maps: Visualizing Haiti Earthquake Crisis Data | 243

Now we might want to do some analysis or visualization of this data by category, but
each category field may have multiple categories. Additionally, each category is given
as a code plus an English and possibly also a French code name. Thus, a little bit of
wrangling is required to get the data into a more agreeable form. First, I wrote these
two functions to get a list of all the categories and to split each category into a code and
an English name:

def to_cat_list(catstr):

stripped = (x.strip() for x in catstr.split(','))
return [x for x in stripped if x]

def get_all_categories(cat_series):
cat_sets = (set(to_cat_list(x)) for x in cat_series)
return sorted(set.union(*cat_sets))

def get_english(cat):
code, names = cat.split('.')
if '|' in names:
names = names.split(' | ')[1]
return code, names.strip()

You can test out that the get_english function does what you expect:

In [101]: get_english('2. Urgences logistiques | Vital Lines')

Out[101]: ('2', 'Vital Lines')
Now, I make a dict mapping code to name because we’ll use the codes for analysis.
We'll use this later when adorning plots (note the use of a generator expression in lieu
of a list comprehension):

In [102]: all_cats = get_all_categories(data.CATEGORY)

# Generator expression
In [103]: english_mapping = dict(get_english(x) for x in all cats)

In [104]: english _mapping['2a']
Out[104]: ‘Food Shortage’

In [105]: english _mapping[ '6c']

Out[105]: ‘Earthquake and aftershocks’
There are many ways to go about augmenting the data set to be able to easily select
records by category. One way is to add indicator (or dummy) columns, one for each
category. To do that, first extract the unique category codes and construct a DataFrame
of zeros having those as its columns and the same index as data:

def get_code(seq):
return [x.split('.')[0] for x in seq if x]

all_codes = get_code(all_cats)

code_index = pd.Index(np.unique(all_codes))

dummy frame = DataFrame(np.zeros((len(data), len(code_index))),
index=data.index, columns=code_index)

If all goes well, dummy frame should look something like this:

 

244 | Chapter 8: Plotting and Visualization

In [107]: dummy frame.ix[:, :6]

Out [107]:

<class 'pandas.core. frame.DataFrame' >
Int64Index: 3569 entries, 0 to 3592
Data columns:

i 3569 non-null values

ta 3569 non-null values

1b 3569 non-null values

1c 3569 non-null values

1d 3569 non-null values

2 3569 non-null values

dtypes: float64(6)

As you recall, the trick is then to set the appropriate entries of each row to 1, lastly
joining this with data:

for row, cat in zip(data.index, data.CATEGORY):
codes = get_code(to_cat_list(cat))
dummy _frame.ix[row, codes] = 1

data = data. join(dummy_frame.add_prefix('category_'))

data finally now has new columns like:

In [109]: data.ix[:, ]
Out[109]:

<class 'pandas.core. frame.DataFrame' >
Int64Index: 3569 entries, 0 to 3592
Data columns:

category 1 3569 non-null values
category 1a 3569 non-null values
category 1b 3569 non-null values
category 1c 3569 non-null values
category 1d 3569 non-null values
dtypes: float64(5)

Let’s make some plots! As this is spatial data, we’d like to plot the data by category on
a map of Haiti. The basemap toolkit (http://matplotlib.github.com/basemap), an add-on
to matplotlib, enables plotting 2D data on maps in Python. basemap provides many
different globe projections and a means for transforming projecting latitude and lon-
gitude coordinates on the globe onto a two-dimensional matplotlib plot. After some
trial and error and using the above data as a guideline, I wrote this function which draws
a simple black and white map of Haiti:

from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt

def basic_haiti_map(ax=None, lllat=17.25, urlat=20.25,
lllon=-75, urlon=-71):
# create polar stereographic Basemap instance.
m = Basemap(ax=ax, projection='stere',
lon_o=(urlon + lllon) / 2,
lat_o=(urlat + lllat) / 2,
llcrnrlat=lllat, urcrnrlat=urlat,
llcrnrlon=lllon, urcrnrlon=urlon,

 

Plotting Maps: Visualizing Haiti Earthquake Crisis Data | 245

resolution='f')
# draw coastlines, state and country boundaries, edge of map.
m.drawcoastlines()
m.drawstates()
m.drawcountries()
return m

The idea, now, is that the returned Basemap object, knows how to transform coordinates
onto the canvas. I wrote the following code to plot the data observations for a number
of report categories. For each category, I filter down the data set to the coordinates
labeled by that category, plot a Basemap on the appropriate subplot, transform the co-
ordinates, then plot the points using the Basemap’s plot method:

fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))
fig.subplots_adjust(hspace=0.05, wspace=0.05)

to_plot = ['2a', '1', '3c', '7a']
lllat=17.25; urlat=20.25; lllon=-75; urlon=-71

for code, ax in zip(to_plot, axes.flat):
m = basic_haiti_map(ax, lllat=lllat, urlat=urlat,
lllon=1llon, urlon=urlon)

cat_data = data[data['category %s' % code] == 1]

# compute map proj coordinates.
x, y = m(cat_data.LONGITUDE, cat_data.LATITUDE)

m.plot(x, y, 'k.', alpha=0.5)
ax.set_title('%s: %s' % (code, english _mapping[code]))

The resulting figure can be seen in Figure 8-24.

It seems from the plot that most of the data is concentrated around the most populous
city, Port-au-Prince. basemap allows you to overlap additional map data which comes
from what are called shapefiles. I first downloaded a shapefile with roads in Port-au-
Prince (see http://cegrp.cga.harvard.edu/shaiti/?q=resources_data). The Basemap object
conveniently has a readshapefile method so that, after extracting the road data archive,
1 added just the following lines to my code:

shapefile path = 'cho8/PortAuPrince Roads/PortAuPrince_Roads'
m.readshapefile(shapefile_path, 'roads')

After a little more trial and error with the latitude and longitude boundaries, I was able
to make Figure 8-25 for the “Food shortage” category.

 

246 | Chapter 8: Plotting and Visualization

 

2a: Food Shortage 1: Emergency

 

 

 

 

 

 

 

 

 

 

 

 

Figure 8-24. Haiti crisis data for 4 categories

 

Food shortages reported in Port-au-Prince

 

 

 

 

 

 

 

 

Figure 8-25. Food shortage reports in Port-au-Prince during the Haiti earthquake crisis

Python Visualization Tool Ecosystem

As is common with open source, there are a plethora of options for creating graphics
in Python (too many to list). In addition to open source, there are numerous commercial
libraries with Python bindings.

 

Python Visualization Tool Ecosystem | 247

In this chapter and throughout the book, I have been primarily concerned with mat-
plotlib as it is the most widely used plotting tool in Python. While it’s an important
part of the scientific Python ecosystem, matplotlib has plenty of shortcomings when it
comes to the creation and display of statistical graphics. MATLAB users will likely find
matplotlib familiar, while R users (especially users of the excellent ggplot2 and trel
lis packages) may be somewhat disappointed (at least as of this writing). It is possible
to make beautiful plots for display on the web in matplotlib, but doing so often requires
significant effort as the library is designed for the printed page. Aesthetics aside, it is
sufficient for most needs. In pandas, I, along with the other developers, have sought to
build a convenient user interface that makes it easier to make most kinds of plots com-
monplace in data analysis.

There are a number of other visualization tools in wide use. I list a few of them here
and encourage you to explore the ecosystem.

Chaco

Chaco (http://code.enthought.com/chaco/), developed by Enthought, is a plotting tool-
kit suitable both for static plotting and interactive visualizations. It is especially well-
suited for expressing complex visualizations with data interrelationships. Compared
with matplotlib, Chaco has much better support for interacting with plot elements and
rendering is very fast, making it a good choice for building interactive GUI applications.

 

 

 

 

 

Figure 8-26. A Chaco example plot

 

248 | Chapter 8: Plotting and Visualization

mayavi

The mayavi project, developed by Prabhu Ramachandran, Gaél Varoquaux, and others,
is a 3D graphics toolkit built on the open source C++ graphics library VTK. mayavi,
like matplotlib, integrates with [Python so that it is easy to use interactively. The plots
can be panned, rotated, and zoomed using the mouse and keyboard. I used mayavi to
make one of the illustrations of broadcasting in Chapter 12. While I don’t show any
mayavi-using code here, there is plenty of documentation and examples available on-
line. In many cases, I believe it is a good alternative to a technology like WebGL, though
the graphics are harder to share in interactive form.

Other Packages

Of course, there are numerous other visualization libraries and applications available
in Python: PyQwt, Veusz, gnuplot-py, biggles, and others. I have seen PyQwt put to
good use in GUI applications built using the Qt application framework using PyQt.
While many of these libraries continue to be under active development (some of them
are part of much larger applications), I have noted in the last few years a general trend
toward web-based technologies and away from desktop graphics. I’ll say a few more
words about this in the next section.

The Future of Visualization Tools?

Visualizations built on web technologies (that is, JavaScript-based) appear to be the
inevitable future. Doubtlessly you have used many different kinds of static or interactive
visualizations built in Flash or JavaScript over the years. New toolkits (such as d3.js
and its numerous off-shoot projects) for building such displays are appearing all the
time. In contrast, development in non web-based visualization has slowed significantly
in recent years. This holds true of Python as well as other data analysis and statistical
computing environments like R.

The development challenge, then, will be in building tighter integration between data
analysis and preparation tools, such as pandas, and the web browser. I am hopeful that
this will become a fruitful point of collaboration between Python and non-Python users
as well.

 

Python Visualization Tool Ecosystem | 249


CHAPTER 9
Data Aggregation and Group
Operations

 

Categorizing a data set and applying a function to each group, whether an aggregation
or transformation, is often a critical component of a data analysis workflow. After
loading, merging, and preparing a data set, a familiar task is to compute group statistics
or possibly pivot tables for reporting or visualization purposes. pandas provides a flex-
ible and high-performance groupby facility, enabling you to slice and dice, and sum-
marize data sets in a natural way.

One reason for the popularity of relational databases and SQL (which stands for
“structured query language”) is the ease with which data can be joined, filtered, trans-
formed, and aggregated. However, query languages like SQL are rather limited in the
kinds of group operations that can be performed. As you will see, with the expressive-
ness and power of Python and pandas, we can perform much more complex grouped
operations by utilizing any function that accepts a pandas object or NumPy array. In
this chapter, you will learn how to:

¢ Split a pandas object into pieces using one or more keys (in the form of functions,
arrays, or DataFrame column names)

* Computing group summary statistics, like count, mean, or standard deviation, or
a user-defined function

¢ Apply a varying set of functions to each column of a DataFrame

¢ Apply within-group transformations or other manipulations, like normalization,
linear regression, rank, or subset selection

* Compute pivot tables and cross-tabulations

¢ Perform quantile analysis and other data-derived group analyses

 

251

Aggregation of time series data, a special use case of groupby, is referred
, to as resampling in this book and will receive separate treatment in
~ 4l8* Chapter 10.

 
 

 

GroupBy Mechanics

Hadley Wickham, an author of many popular packages for the R programming lan-
guage, coined the term split-apply-combine for talking about group operations, and I
think that’s a good description of the process. In the first stage of the process, data
contained in a pandas object, whether a Series, DataFrame, or otherwise, is split into
groups based on one or more keys that you provide. The splitting is performed on a
particular axis of an object. For example, a DataFrame can be grouped on its rows
(axis=0) or its columns (axis=1). Once this is done, a function is applied to each group,
producing a new value. Finally, the results of all those function applications are com-
bined into a result object. The form of the resulting object will usually depend on what’s
being done to the data. See Figure 9-1 for a mockup of a simple group aggregation.

 

Split Apply Combine

Qa.
g
Taal
g

 

 

 

 

Figure 9-1. Illustration of a group aggregation

Each grouping key can take many forms, and the keys do not have to be all of the same
type:
¢ A list or array of values that is the same length as the axis being grouped

e A value indicating a column name in a DataFrame

 

252 | Chapter9: Data Aggregation and Group Operations

e A dict or Series giving a correspondence between the values on the axis being
grouped and the group names

¢ A function to be invoked on the axis index or the individual labels in the index

Note that the latter three methods are all just shortcuts for producing an array of values
to be used to split up the object. Don’t worry if this all seems very abstract. Throughout
this chapter, I will give many examples of all of these methods. To get started, here is
a very simple small tabular dataset as a DataFrame:
In [13]: df = DataFrame({'key1' : ['a', 'a', 'b', ‘'b', ‘a'],

scorned ‘key2' : ['one', ‘two', 'one', ‘two’, 'one'],

semen § ‘data1' : np.random.randn(5),

semen § ‘data2' : np.random.randn(5)})

In [14]: df
Out [14]:

data1 data2 key1 key2
0 -0.204708 1.393406 a one

1 0.478943 0.092908 a two
2 -0.519439 0.281746 b one
3 -0.555730 0.769023 b two
4 1.965781 1.246435 a one

Suppose you wanted to compute the mean of the data1 column using the groups labels
from key1. There are a number of ways to do this. One is to access data1 and call
groupby with the column (a Series) at key1:

In [15]: grouped = df['data1'].groupby(df['key1'])

In [16]: grouped
Out[16]: <pandas.core.groupby.SeriesGroupBy at 0x2d78b10>

This grouped variable is now a GroupBy object. It has not actually computed anything
yet except for some intermediate data about the group key df['key1' ]. The idea is that
this object has all of the information needed to then apply some operation to each of

the groups. For example, to compute group means we can call the GroupBy’s mean
method:

In [17]: grouped.mean()

Out [17]:

key1

a 0.746672
b -0.537585

Later, I'll explain more about what’s going on when you call .mean(). The important
thing here is that the data (a Series) has been aggregated according to the group key,
producing a new Series that is now indexed by the unique values in the key1 column.
The result index has the name 'key1' because the DataFrame column df['key1'] did.

If instead we had passed multiple arrays as a list, we get something different:
In [18]: means = df['data1'].groupby([df['key1'], df['key2']]).mean()

 

GroupBy Mechanics | 253

In [19]: means

Out[19]:

key1 key2

a one 0.880536
two 0.478943

b one -0.519439
two -0.555730

In this case, we grouped the data using two keys, and the resulting Series now has a
hierarchical index consisting of the unique pairs of keys observed:

In [20]: means.unstack()
Out[20]:

key2 one two
key1

a 0.880536 0.478943
b -0.519439 -0.555730

In these examples, the group keys are all Series, though they could be any arrays of the
right length:

In [21]: states = np.array(['Ohio', 'California', ‘California’, 'Ohio', ‘Ohio'])
In [22]: years = np.array([2005, 2005, 2006, 2005, 2006])

In [23]: df['data1'].groupby([states, years]).mean()
Out [23]:
California 2005 0.478943
2006 §=-0.519439
Ohio 2005 -0.380219
2006 1.965781

Frequently the grouping information to be found in the same DataFrame as the data
you want to work on. In that case, you can pass column names (whether those are
strings, numbers, or other Python objects) as the group keys:

In [24]: df.groupby('key1').mean()
Out [24]:
data1 data2
key1
a 0.746672 0.910916
b -0.537585 0.525384

In [25]: df.groupby(['key1', 'key2']).mean()
Out [25]:
data1 data2

key1 key2
a one 0.880536 1.319920

two 0.478943 0.092908
b one -0.519439 0.281746

two -0.555730 0.769023

You may have noticed in the first case df.groupby('key1').mean() that there is no
key2 column in the result. Because df['key2'] is not numeric data, it is said to be a
nuisance column, which is therefore excluded from the result. By default, all of the

 

254 | Chapter9: Data Aggregation and Group Operations

numeric columns are aggregated, though it is possible to filter down to a subset as you'll
see soon.

Regardless of the objective in using groupby, a generally useful GroupBy method is
size which return a Series containing group sizes:

In [26]: df.groupby(['key1', 'key2']).size()

Out [26]:

key1 key2

a one 2
two 1

b one 1
two 1

 

As of this writing, any missing values in a group key will be excluded
— ta) from the result. It’s possible (and, in fact, quite likely), that by the time
you are reading this there will be an option to include the NA group in
the result.

 

 

 

Iterating Over Groups

The GroupBy object supports iteration, generating a sequence of 2-tuples containing
the group name along with the chunk of data. Consider the following small example
data set:
In [27]: for name, group in df.groupby('key1'):
wera t print name
were print group

data1 data2 key1 key2

0 -0.204708 1.393406 a one
1 0.478943 0.092908 a two
4 1.965781 1.246435 a one
b

data1 data2 key1 key2
2 -0.519439 0.281746 b one
3 -0.555730 0.769023 b two

In the case of multiple keys, the first element in the tuple will be a tuple of key values:

In [28]: for (k1, k2), group in df.groupby(['key1', ‘key2']):
awaat print ki, k2
awaat print group

data1 data2 key1 key2
0 -0.204708 1.393406 a one
4 1.965781 1.246435 a one
a two

data1 data2 key1 key2
1 0.478943 0.092908 a two
b one

data1 data2 key1 key2

 

GroupBy Mechanics | 255

2 -0.519439 0.281746 b one
b two

data1 data2 key1 key2
3 -0.55573 0.769023 b two

Of course, you can choose to do whatever you want with the pieces of data. A recipe
you may find useful is computing a dict of the data pieces as a one-liner:

In [29]: pieces = dict(list(df.groupby('key1')))

In [30]: pieces['b']
Out [30]:

data1 data2 key1 key2
2 -0.519439 0.281746 b one
3 -0.555730 0.769023 b two

By default groupby groups on axis=0, but you can group on any of the other axes. For
example, we could group the columns of our example df here by dtype like so:

In [31]: df.dtypes
Out [31]:

data1 float64
data2 float64
key1 object
key2 object

In [32]: grouped = df.groupby(df.dtypes, axis=1)

In [33]: dict(list (grouped) )

Out [33]:

{dtype('float64'): data1 data2
0 -0.204708 1.393406

1 0.478943 0.092908

2 -0.519439 0.281746

3 -0.555730 0.769023

4 1.965781 1.246435,

dtype('object'): key1 key2

0 a one
1 a two
2 b one
3 b two
4 a one}

Selecting a Column or Subset of Columns
Indexing a GroupBy object created from a DataFrame with a column name or array of
column names has the effect of selecting those columns for aggregation. This means that:
df.groupby('key1')['data1']
df.groupby('key1')[['data2']]
are syntactic sugar for:

df['data1'].groupby(df[ 'key1'])
df[['data2']].groupby(df['key1'])

 

256 | Chapter9: Data Aggregation and Group Operations

Especially for large data sets, it may be desirable to aggregate only a few columns. For
example, in the above data set, to compute means for just the data2 column and get
the result as a DataFrame, we could write:
In [34]: df.groupby(['key1', 'key2'])[['data2']].mean()
Out [34]:
data2
key1 key2
a one: 1.319920
two 0.092908
b one 0.281746
two 0.769023

The object returned by this indexing operation is a grouped DataFrame if a list or array
is passed and a grouped Series is just a single column name that is passed as a scalar:

In [35]: s_grouped = df.groupby(['key1', 'key2'])['data2']

In [36]: s_grouped
Out[36]: <pandas.core.groupby.SeriesGroupBy at 0x2e215d0>

In [37]: s_grouped.mean()

Out [37]:

key1 key2

a one 1.319920
two 0.092908

b one 0.281746
two 0.769023
Name: data2

Grouping with Dicts and Series
Grouping information may exist in a form other than an array. Let’s consider another
example DataFrame:
In [38]: people = DataFrame(np.random.randn(5, 5),
weeat columns=['a', 'b', 'c', ‘d', ‘e'],
wewat index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])

In [39]: people.ix[2:3, ['b', ‘c']] = np.nan # Add a few NA values

In [40]: people

Out [40]:

a b c d e
Joe 1.007189 -1.296221 0.274992 0.228913 1.352917
Steve 0.886429 -2.001637 -0.371843 1.669025 -0.438570
Wes -0.539741 NaN NaN -1.021228 -0.577087
Jim 0.124121 0.302614 0.523772 0.000940 1.343810

Travis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757
Now, suppose I havea group correspondence for the columns and want to sum together
the columns by group:

In [41]: mapping = {'a': 'red', 'b': 'red', 'c': ‘blue’,
ates? "d': 'blue', 'e': 'red', 'f' : ‘orange'}

 

GroupBy Mechanics | 257

Now, you could easily construct an array from this dict to pass to groupby, but instead
we can just pass the dict:

In [42]: by_column = people.groupby(mapping, axis=1)

In [43]: by_column.sum()
Out [43]:

blue red
Joe 0.503905 1.063885
Steve 1.297183 -1.553778
Wes -1.021228 -1.116829
Jim 0.524712 1.770545
Travis -4.230992 -2.405455

The same functionality holds for Series, which can be viewed as a fixed size mapping.
When I used Series as group keys in the above examples, pandas does, in fact, inspect
each Series to ensure that its index is aligned with the axis it’s grouping:

In [44]: map_series = Series(mapping)

In [45]: map_series

Out [45]:

a red
b red
c blue
d blue
e red
f orange

In [46]: people.groupby(map series, axis=1).count()

Out [46]:

blue red
Joe 2 3
Steve 2 3
Wes i 2
Jim 2 3
Travis 2 3

Grouping with Functions

Using Python functions in what can be fairly creative ways is a more abstract way of
defining a group mapping compared with a dict or Series. Any function passed as a
group key will be called once per index value, with the return values being used as the
group names. More concretely, consider the example DataFrame from the previous
section, which has people’s first names as index values. Suppose you wanted to group
by the length of the names; you could compute an array of string lengths, but instead
you can just pass the len function:

In [47]: people.groupby(len) .sum()

Out [47]:

a b (a d e
3 0.591569 -0.993608 0.798764 -0.791374 2.119639

 

258 | Chapter9: Data Aggregation and Group Operations

5 0.886429 -2.001637 -0.371843 1.669025 -0.438570
6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757

Mixing functions with arrays, dicts, or Series is not a problem as everything gets con-
verted to arrays internally:

In [48]: key_list = ['one', 'one', ‘one’, 'two', 'two']

In [49]: people.groupby([len, key_list]).min()
Out [49]:
a b c d e
3 one -0.539741 -1.296221 0.274992 -1.021228 -0.577087
two 0.124121 0.302614 0.523772 0.000940 1.343810
5 one 0.886429 -2.001637 -0.371843 1.669025 -0.438570
6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757

Grouping by Index Levels

A final convenience for hierarchically-indexed data sets is the ability to aggregate using
one of the levels of an axis index. To do this, pass the level number or name using the
level keyword:

In [50]: columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],
scone t [1, 3, 5, 1, 3]], names=['cty', 'tenor'])

In [51]: hier_df = DataFrame(np.random.randn(4, 5), columns=columns)

In [52]: hier_df

Out [52]:

cty US JP

tenor 1 3 5 1 3
0 0.560145 -1.265934 0.119827 -1.063512 0.332883
1 -2.359419 -0.199543 -1.541996 -0.970736 -1.307030
2 0.286350 0.377984 -0.753887 0.331286 1.349742
3 0.069877 0.246674 -0.011862 1.004812 1.327195
In [53]: hier_df.groupby(level='cty', axis=1).count()
Out [53]

cty JP US

0 2 3

1 2 3

2 2 3

3 2 3

Data Aggregation

By aggregation, I am generally referring to any data transformation that produces scalar
values from arrays. In the examples above I have used several of them, such as mean,
count, min and sum. You may wonder what is going on when you invoke mean() on a
GroupBy object. Many common aggregations, such as those found in Table 9-1, have
optimized implementations that compute the statistics on the dataset in place. How-
ever, you are not limited to only this set of methods. You can use aggregations of your

 

Data Aggregation | 259

own devising and additionally call any method that is also defined on the grouped
object. For example, as you recall quantile computes sample quantiles of a Series or a
DataFrame’s columns !:

In [54]: df
Out [54]:

data1 data2 key1 key2
0 -0.204708 1.393406
1 0.478943 0.092908
2 -0.519439 0.281746
3 -0.555730 0.769023
4 1.965781 1.246435

a one
two
one
two
one

°r,oT ow

In [55]: grouped = df.groupby('key1')

In [56]: grouped[ 'data1'].quantile(0o.9)

Out [56]:

key1

a 1.668413
b -0.523068

While quantile is not explicitly implemented for GroupBy, it is a Series method and
thus available for use. Internally, GroupBy efficiently slices up the Series, calls
piece. quantile(0.9) foreach piece, then assembles those results together into the result

object.

To use your own aggregation functions, pass any function that aggregates an array to
the aggregate or agg method:

In [57]: def peak_to_peak(arr):
ees return arr.max() - arr.min()

In [58]: grouped.agg(peak_to_peak)
Out [58]:

key1
a
b

data1 data2

2.170488 1.300498
0.036292 0.487276

You'll notice that some methods like describe also work, even though they are not
aggregations, strictly speaking:

In [59]: grouped.describe()
Out [59]:

key1
a

data1

count 3.000000
mean 0.746672
std 1.109736
min -0.204708
25% 0.137118
50% 0.478943

RPOOOOW

data2

- 000000
-910916
-712217
-092908
-669671
«246435

1. Note that quantile performs linear interpolation if there is no value at exactly the passed percentile.

 

260 | Chapter9: Data Aggregation and Group Operations

75% 1.222362 1.319920
max 1.965781 1.393406
b count 2.000000 2.000000
mean -0.537585 0.525384
std 0.025662 0.344556
min -0.555730 0.281746
25% -0.546657 0.403565
50%  -0.537585 0.525384
75% -0.528512 0.647203
max -0.519439 0.769023

I will explain in more detail what has happened here in the next major section on group-
wise operations and transformations.

 

Vs

4
sO You may notice that custom aggregation functions are much slower than
43 the optimized functions found in Table 9-1. This is because there is
Vs 48° significant overhead (function calls, data rearrangement) in construct-

 

ing the intermediate group data chunks.

Table 9-1. Optimized groupby methods

Functionname _ Description

count Number of non-NA values in the group

sum Sum of non-NA values

mean Mean of non-NA values

median Arithmetic median of non-NA values

std, var Unbiased (n - 1 denominator) standard deviation and variance
min, max Minimum and maximum of non-NA values

prod Product of non-NA values

first, last — First and last non-NA values

To illustrate some more advanced aggregation features, I’ll use a less trivial dataset, a
dataset on restaurant tipping. I obtained it from the R reshape2 package; it was origi-
nally found in Bryant & Smith’s 1995 text on business statistics (and found in the book’s
GitHub repository). After loading it with read_csv, I add a tipping percentage column
tip_pct.

In [60]: tips = pd.read_csv('cho8/tips.csv')

# Add tip percentage of total bill
In [61]: tips['tip_pct'] = tips['tip'] / tips['total_bill']

In [62]: tips[:6]

Out [62]:

total bill tip sex smoker day time size tip_pct
0) 16.99 1.01 Female No Sun Dinner 2 0.059447
1 10.34 1.66 Male No Sun Dinner 3 0.160542

 

Data Aggregation | 261

21.01
23.68
24.59
25.29

Mm PWN

Column-wise and Multiple Function Application

As you’ve seen above, aggregating a Series or all of the columns of a DataFrame is a
matter of using aggregate with the desired function or calling a method like mean or
std. However, you may want to aggregate using a different function depending on the
column or multiple functions at once. Fortunately, this is straightforward to do, which
Pl illustrate through a number of examples. First, I’ll group the tips by sex and smoker:

In [63]: grouped = tips.groupby(['sex', 'smoker'])

Note that for descriptive statistics like those in Table 9-1, you can pass the name of the

3.61 Fe

function as a string:

Male No
Male No
male No
Male No

Sun
Sun
Sun
Sun

Dinner
Dinner
Dinner
Dinner

In [64]: grouped pct = grouped['tip_pct']

In [65]: grouped_pct.agg('mean' )

Out [65]:
sex smoker
Female No

Yes
Male No

Yes
Name: tip_pct

If you pass a list of functions or function names instead, you get back a DataFrame with

0.156
0.182
0.160
0.152

921
150
669
771

column names taken from the functions:

In [66]: grouped_pct.agg(['mean',

Out [66]:

sex smoker
Female No

Yes
Male No

Yes

You don’t need to accept the names that GroupBy gives to the columns; notably
lambda functions have the name '<lambda>' which make them hard to identify (you can
see for yourself by looking at a function’s _name__ attribute). As such, if you pass a list
of (name, function) tuples, the first element of each tuple will be used as the DataFrame
column names (you can think of a list of 2-tuples as an ordered mapping):

mean

0.156921
0.182150
0.160669
0.152771

std peak_to_peak

0.036421
0.071595
0.041849
0.090588

0.195876
0.360233
0.220186
0.674707

FRNW

‘std', peak_to_peak])

0.166587
0.139780
0.146808
0.186240

In [67]: grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])

Out [67]:

sex smoker
Female No
Yes

foo

0.156921
0.182150

bar

0.036421
0.071595

 

262 | Chapter9: Data Aggregation and Group Operations

Male No 0.160669 0.041849
Yes 0.152771 0.090588

With a DataFrame, you have more options as you can specify a list of functions to apply
to all of the columns or different functions per column. To start, suppose we wanted
to compute the same three statistics for the tip_pct and total_bill columns:

In [68]: functions = ['count', 'mean', 'max']
In [69]: result = grouped['tip_pct', 'total_bill'].agg(functions)

In [70]: result

Out[70]:
tip_pct total_bill
count mean max count mean max
sex smoker
Female No 54 0.156921 0.252672 54 18.105185 35.83
Yes 33 0.182150 0.416667 33 17.977879 44.30
Male No 97 0.160669 0.291990 97 19.791237 48.33
Yes 60 0.152771 0.710345 60 22.284500 50.81

As you can see, the resulting DataFrame has hierarchical columns, the same as you
would get aggregating each column separately and using concat to glue the results
together using the column names as the keys argument:

In [71]: result['tip_pct']

Out[71]:
count mean max
sex smoker
Female No 54 0.156921 0.252672
Yes 33 0.182150 0.416667
Male No 97 0.160669 0.291990
Yes 60 0.152771 0.710345

As above, a list of tuples with custom names can be passed:

In [72]: ftuples = [('Durchschnitt', 'mean'), (‘'Abweichung', np.var) ]

In [73]: grouped['tip_pct', 'total_bill'].agg(ftuples)
Out [73]:
tip_pct total_bill
Durchschnitt Abweichung Durchschnitt Abweichung
sex smoker

Female No 0.156921 0.001327 18.105185  53.092422
Yes 0.182150 0.005126 17.977879  84.451517
Male No 0.160669 0.001751 19.791237 76.152961
Yes 0.152771 0.008206 22.284500 98 .244673

Now, suppose you wanted to apply potentially different functions to one or more of
the columns. The trick is to pass a dict to agg that contains a mapping of column names
to any of the function specifications listed so far:
In [74]: grouped.agg({'tip' : np.max, 'size' : 'sum'})
Out [74]:
size tip
sex smoker

 

Data Aggregation | 263

Female No 140 5.2
Yes 74 6.5
Male No 263 9.0
Yes 150 10.0

In [75]: grouped.agg({'tip_pct' : ['min', 'max', ‘mean', ‘std'],

cconee § ‘size' : 'sum'})
Out [75]

tip_pct size
min max mean std sum

sex smoker
Female No 0.056797 0.252672 0.156921 0.036421 140
Yes 0.056433 0.416667 0.182150 0.071595 74
Male No 0.071804 0.291990 0.160669 0.041849 263

Yes 0.035638 0.710345 0.152771 0.090588 150

A DataFrame will have hierarchical columns only if multiple functions are applied to
at least one column.

Returning Aggregated Data in “unindexed” Form

In all of the examples up until now, the aggregated data comes back with an index,
potentially hierarchical, composed from the unique group key combinations observed.
Since this isn’t always desirable, you can disable this behavior in most cases by passing
as_index=False to groupby:

In [76]: tips.groupby(['sex', 'smoker'], as_index=False) .mean()

Out[76]:

sex smoker total bill tip size tip_pct
O Female No 18.105185 2.773519 2.592593 0.156921
1 Female Yes 17.977879 2.931515 2.242424 0.182150

2 Male No 19.791237 3.113402 2.711340 0.160669
3 Male Yes 22.284500 3.051167 2.500000 0.152771

Of course, it’s always possible to obtain the result in this format by calling
reset_index on the result.

 

Using groupby in this way is generally less flexible; results with hier-
tS) archical columns, for example, are not currently implemented as the
form of the result would have to be somewhat arbitrary.

 

 

 

Group-wise Operations and Transformations

Aggregation is only one kind of group operation. It is a special case in the more general
class of data transformations; that is, it accepts functions that reduce a one-dimensional
array to a scalar value. In this section, I will introduce you to the transform and apply
methods, which will enable you to do many other kinds of group operations.

Suppose, instead, we wanted to add a column to a DataFrame containing group means
for each index. One way to do this is to aggregate, then merge:

 

264 | Chapter9: Data Aggregation and Group Operations

In [77]: df
Out[77]:

data1 data2 key1 key2
0 -0.204708 1.393406 a one
1 0.478943 0.092908 a two

2 -0.519439 0.281746 b one
3 -0.555730 0.769023 b two
4 1.965781 1.246435 a one

In [78]: ki_means = df.groupby('key1').mean().add_prefix('mean_')

In [79]: k1_means
Out [79]:

mean_data1 mean_data2
key1
a 0.746672 0.910916
b -0.537585 0.525384

In [80]: pd.merge(df, ki_means, left_on='key1', right_index=True)
Out [80]:
data1 data2 key1 key2 mean_data1 mean_data2
0 -0.204708 1.393406 one 0.746672 0.910916
1 0.478943 0.092908 two 0.746672 0.910916
4 1.965781 1.246435 one 0.746672 0.910916
2 -0.519439 0.281746 one = -0.537585 0.525384
3 -0.555730 0.769023 two -0.537585 = 0.525384

cow Y Ww

This works, but is somewhat inflexible. You can think of the operation as transforming
the two data columns using the np.mean function. Let’s look back at the people Data-
Frame from earlier in the chapter and use the transform method on GroupBy:

In [81]: key = ['one', 'two', ‘one’, ‘two’, ‘one']

In [82]: people.groupby(key) .mean()
Out [82]:

a b c d e
one -0.082032 -1.063687 -1.047620 -0.884358 -0.028309
two 0.505275 -0.849512 0.075965 0.834983 0.452620

In [83]: people.groupby(key).transform(np.mean)
Out [83]:

a b c d e
Joe -0.082032 -1.063687 -1.047620 -0.884358 -0.028309
Steve 0.505275 -0.849512 0.075965 0.834983 0.452620
Wes -0.082032 -1.063687 -1.047620 -0.884358 -0.028309
Jim 0.505275 -0.849512 0.075965 0.834983 0.452620
Travis -0.082032 -1.063687 -1.047620 -0.884358 -0.028309

As you may guess, transform applies a function to each group, then places the results
in the appropriate locations. If each group produces a scalar value, it will be propagated
(broadcasted). Suppose instead you wanted to subtract the mean value from each
group. To do this, create a demeaning function and pass it to transform:

In [84]: def demean(arr):
sores return arr - arr.mean()

 

Group-wise Operations and Transformations | 265

In [85]: demeaned = people.groupby(key) .transform(demean)

In [86]: demeaned

Out [86]:

a b Cc d e
Joe 1.089221 -0.232534 1.322612 1.113271 1.381226
Steve 0.381154 -1.152125 -0.447807 0.834043 -0.891190
Wes -0.457709 NaN NaN -0.136869 -0.548778

Jim -0.381154 1.152125 0.447807 -0.834043 0.891190
Travis -0.631512 0.232534 -1.322612 -0.976402 -0.832448

You can check that demeaned now has zero group means:

In [87]: demeaned.groupby(key) .mean()
Out [87]:

a be
one 0-0 O
two -0 0 0

ooa
oon

As you'll see in the next section, group demeaning can be achieved using apply also.

Apply: General split-apply-combine

Like aggregate, transform is a more specialized function having rigid requirements: the
passed function must either produce a scalar value to be broadcasted (like np.mean) or
a transformed array of the same size. The most general purpose GroupBy method is
apply, which is the subject of the rest of this section. As in Figure 9-1, apply splits the
object being manipulated into pieces, invokes the passed function on each piece, then
attempts to concatenate the pieces together.

Returning to the tipping data set above, suppose you wanted to select the top five
tip_pct values by group. First, it’s straightforward to write a function that selects the
rows with the largest values in a particular column:

In [88]: def top(df, n=5, column='tip pct’):
scone t return df.sort_index(by=column)[-n: ]

In [89]: top(tips, n=6)

Out [89]:

total bill tip sex smoker day time size tip_pct
109 14.31 4.00 Female Yes Sat Dinner 2 0.279525
183 23.17 6.50 Male Yes Sun Dinner 4 0.280535
232 11.61 3.39 Male No Sat Dinner 2 0.291990
67 3.07 1.00 Female Yes Sat Dinner 1 0.325733
178 9.60 4.00 Female Yes Sun Dinner 2 0.416667
172 7.25 5.15 Male Yes Sun Dinner 2 0.710345

Now, if we group by smoker, say, and call apply with this function, we get the following:

In [90]: tips.groupby('smoker').apply(top)
Out [90]:

total bill tip sex smoker day time size tip_pct
smoker

 

266 | Chapter9: Data Aggregation and Group Operations

No 88 24.71 5.85 Male No Thur — Lunch 2 0.236746
185 20.69 5.00 Male No Sun Dinner 5 0.241663
51 10.29 2.60 Female No Sun Dinner 2 0.252672
149 7.51 2.00 Male No Thur — Lunch 2 0.266312
232 11.61 3.39 Male No Sat Dinner 2 0.291990
Yes 109 14.31 4.00 Female Yes Sat Dinner 2 0.279525
183 23.17 6.50 Male Yes Sun Dinner 4 0.280535
67 3.07 1.00 Female Yes Sat Dinner 1 0.325733
178 9.60 4.00 Female Yes Sun Dinner 2 0.416667
172 7.25 5.15 Male Yes Sun Dinner 2 0.710345

What has happened here? The top function is called on each piece of the DataFrame,
then the results are glued together using pandas.concat, labeling the pieces with the
group names. The result therefore has a hierarchical index whose inner level contains
index values from the original DataFrame.

If you pass a function to apply that takes other arguments or keywords, you can pass
these after the function:

In [91]: tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')

 

Out[91]:
total _bill tip sex smoker day time size tip_pct
smoker day
No Fri 94 22.75 3.25 Female No Fri Dinner 2 0.142857
Sat 212 48.33 9.00 Male No Sat Dinner 4 0.186220
Sun 156 48.17 5.00 Male No Sun Dinner 6 0.103799
Thur 142 41.19 5.00 Male No Thur Lunch 5 0.121389
Yes Fri 95 40.17. 4.73 Male Yes Fri Dinner 4 0.117750
Sat 170 50.81 10.00 Male Yes Sat Dinner 3 0.196812
Sun 182 45.35 3.50 Male Yes Sun Dinner 3 0.077178
Thur 197 43.11 5.00 Female Yes Thur — Lunch 4 0.115982
a a,
sO Beyond these basic usage mechanics, getting the most out of apply is
“ s largely a matter of creativity. What occurs inside the function passed is

 

& : "
#8" up to you; it only needs to return a pandas object or a scalar value. The
rest of this chapter will mainly consist of examples showing you how to
solve various problems using groupby.

You may recall above I called describe on a GroupBy object:

In [92]: result = tips.groupby('smoker')['tip_pct'].describe()

In [93]: result

Out [93]:

smoker

No count 151.000000
mean 0.159328
std 0.039910
min 0.056797
25% 0.136906
50% 0.155625
75% 0.185014
max 0.291990

 

Group-wise Operations and Transformations | 267

Yes count 93 .000000
mean 0.163196
std 0.085119
min 0.035638
25% 0.106771
50% 0.153846
75% 0.195059
max 0.710345

In [94]: result.unstack('smoker' )

Out[94]:

smoker No Yes
count 151.000000 93.000000
mean 0.159328 0.163196
std 0.039910 0.085119
min 0.056797 0.035638
25% 0.136906 0.106771
50% 0.155625 0.153846
75% 0.185014 0.195059
max 0.291990 0.710345

Inside GroupBy, when you invoke a method like describe, it is actually just a shortcut
for:

f = lambda x: x.describe()

grouped. apply(f)

Suppressing the group keys

In the examples above, you see that the resulting object has a hierarchical index formed
from the group keys along with the indexes of each piece of the original object. This
can be disabled by passing group_keys=False to groupby:

In [95]: tips.groupby('smoker', group_keys=False).apply(top)

Out [95]:
total bill tip sex smoker day time size tip_pct

88 24.71 5.85 Male No Thur — Lunch 2 0.236746
185 20.69 5.00 Male No Sun Dinner 5 0.241663
51 10.29 2.60 Female No Sun Dinner 2 0.252672
149 7.51 2.00 Male No Thur — Lunch 2 0.266312
232 11.61 3.39 Male No Sat Dinner 2 0.291990
109 14.31 4.00 Female Yes Sat Dinner 2 0.279525
183 23.17 6.50 Male Yes Sun Dinner 4 0.280535
67 3.07 1.00 Female Yes Sat Dinner 1 0.325733
178 9.60 4.00 Female Yes Sun Dinner 2 0.416667
172 7.25 5.15 Male Yes Sun Dinner 2 0.710345

Quantile and Bucket Analysis

As you may recall from Chapter 7, pandas has some tools, in particular cut and qcut,
for slicing data up into buckets with bins of your choosing or by sample quantiles.
Combining these functions with groupby, it becomes very simple to perform bucket or

 

268 | Chapter 9: Data Aggregation and Group Operations

quantile analysis on a data set. Consider a simple random data set and an equal-length
bucket categorization using cut:

In [96]: frame = DataFrame({'data1': np.random.randn(1000) ,
: ‘data2': np.random.randn(1000) })

In [97]: factor =

In [98]: factor[:10]

Out [98]:

Categorical:

pd.cut(frame.data1, 4)

array([(-1.23, 0.489], (-2.956, -1.23], (-1.23, 0.489], (0.489, 2.208],
(-1.23, 0.489], (0.489, 2.208], (-1.23, 0.489], (-1.23, 0.489],
(0.489, 2.208], (0.489, 2.208]], dtype=object)

Levels (4): Index([(-2.956, -1.23], (-1.23, 0.489], (0.489, 2.208],

(2.208, 3.928]], dtype=object)

The Factor object returned by cut can be passed directly to groupby. So we could com-
pute a set of statistics for the data2 column like so:

In [99]:

def get_stats(group):
return {'min': group.min(), 'max': group.max(),

‘count’: group.count(), 'mean': group.mean()}

In [100]: grouped = frame.data2.groupby(factor)

In [101]: grouped.apply(get_stats) .unstack()

Out[101]

data1

count

(-1.23, 0.489]
(-2.956, -1.23]
(0.489, 2.208]
(2.208, 3.928]

598
95
297
10

max

min

3.260383 -0.002051 -2.989741
1.670835 -0.039521 -3.399312
2.954439 0.081822 -3.745356
1.765640 0.024750 -1.929776

These were equal-length buckets; to compute equal-size buckets based on sample
quantiles, use qcut. I’ll pass labels=False to just get quantile numbers.

# Return quantile numbers
: grouping = pd.qcut(frame.data1, 10, labels=False)

In [102]
In [103]

In [104]

: grouped = frame.data2.groupby(grouping)

: grouped.apply(get_stats).unstack()

Out[104]:

count
100
100
100
100
100
100
100
100
100
100

WO ONDUNBRWNPRO

NNNNNNWNN PB

max

- 670835
-628441
-527939
- 260383
-074345
- 184810
-458842
+954439
+ 735527
-377020

mean

-049902
- 030989
-067179
-065713
-111653
-052130
-021489
-026459
- 103406
- 220122

-3.
- 950098
ei
-315555
2)
ei
- 223506
-3.
-3.
-064111

=.

=)

=)

=)

min
399312

925113

047939
989741

056990
745356

 

Group-wise Operations and Transformations | 269

Example: Filling Missing Values with Group-specific Values

When cleaning up missing data, in some cases you will filter out data observations
using dropna, but in others you may want to impute (fill in) the NA values using a fixed
value or some value derived from the data. fillna is the right tool to use; for example
here I fill in NA values with the mean:

In [105]:

In [106]:

s = Series(np.random.randn(6) )

s[::2] = np.nan

In [107]: s
Out[107]:

0 NaN
1 -0.125921
2 NaN
3. -0.884475
4 NaN
5 0.227290

ut[ 108]:
-0.261035
-0.125921
-0.261035
-0.884475
-0.261035
0.227290

MWUPPWNP OOH

n [108]: s.fillna(s.mean())

Suppose you need the fill value to vary by group. As you may guess, you need only
group the data and use app1y with a function that calls fillna on each data chunk. Here
is some sample data on some US states divided into eastern and western states:

In [109]: states = ['Ohio', 'New York', 'Vermont', ‘Florida’,
wcwmned ‘Oregon’, 'Nevada', ‘California’, ‘Idaho']

In [110]: group_key = ['East'] * 4 + ['West'] * 4

In [111]: data = Series(np.random.randn(8), index=states)

In [112]: data[['Vermont', 'Nevada', 'Idaho']] = np.nan

In [113]: data

Out [113]:

Ohio 0.922264

New York -2.153545

Vermont NaN

Florida -0.375842

Oregon 0.329939

Nevada NaN

California 1.105913

Idaho NaN

In [114]: data.groupby(group_key).mean()

Out[114]:

 

270 | Chapter 9: Data Aggregation and Group Operations

East -0.535707
West 0.717926

We can fill the NA values using the group means like so:

In [115]: fill_mean = lambda g: g.fillna(g.mean())

In [116]: data.groupby(group_key).apply(fill_mean)

Out [116]:

Ohio 0.922264
New York -2.153545
Vermont -0.535707
Florida -0.375842
Oregon 0.329939
Nevada 0.717926
California 1.105913
Idaho 0.717926

In another case, you might have pre-defined fill values in your code that vary by group.
Since the groups have a name attribute set internally, we can use that:

In [117]: fill_values = {'East': 0.5, ‘West’: -1}
In [118]: fill_func = lambda g: g.fillna(fill_values[g.name])

In [119]: data.groupby(group_key).apply(fill_func)

Out[119]:

Ohio 0.922264
New York -2.153545
Vermont 0.500000
Florida -0.375842
Oregon 0.329939
Nevada -1.000000
California 1.105913
Idaho -1.000000

Example: Random Sampling and Permutation

Suppose you wanted to draw a random sample (with or without replacement) from a
large dataset for Monte Carlo simulation purposes or some other application. There
are anumber of ways to perform the “draws”; some are much more efficient than others.
One way is to select the first K elements of np.random.permutation(N), where N is the
size of your complete dataset and K the desired sample size. As a more fun example,
here’s a way to construct a deck of English-style playing cards:

# Hearts, Spades, Clubs, Diamonds

suits = ['H', 'S', 'C', 'D']

card val = (range(1, 11) + [10] * 3) * 4

base_names = ['A'] + range(2, 11) + ['J', 'K', 'Q']

cards = []

for suit in ['H', 'S', 'C', 'D']:

cards.extend(str(num) + suit for num in base_names)

deck = Series(card_val, index=cards)

 

Group-wise Operations and Transformations | 271

So now we have a Series of length 52 whose index contains card names and values are
the ones used in blackjack and other games (to keep things simple, I just let the ace be
1):

In [121]: deck[:13]

Out[121]:
AH 1
2H 2
3H 3
4H 4
5H 5
6H 6
7H 7
8H 8
9H 9
10H 10
JH 10
KH 10
QH 10

Now, based on what I said above, drawing a hand of 5 cards from the desk could be
written as:

In [122]: def draw(deck, n=5):
marry 2 return deck.take(np.random.permutation(len(deck))[:n])

: draw(deck)

Suppose you wanted two random cards from each suit. Because the suit is the last
character of each card name, we can group based on this and use apply:

In [124]: get_suit = lambda card: card[-1] # last letter is suit

In [125]: deck.groupby(get_suit).apply(draw, n=2)

Out [125]:

Cc 2c 2
3C 3

D KD 10
8D 8

H KH 10
3H 3

S 28 2
45 4

# alternatively
In [126]: deck.groupby(get_suit, group _keys=False).apply(draw, n=2)

Out [126]:
KC 10
JC 10
AD 1

 

272 | Chapter9: Data Aggregation and Group Operations

5D
5H
6H
iS
KS

ON OUMN

BR

Example: Group Weighted Average and Correlation

Under the split-apply-combine paradigm of groupby, operations between columns in a
DataFrame or two Series, such a group weighted average, become a routine affair. As
an example, take this dataset containing group keys, values, and some weights:
In [127]: df = DataFrame({'category': ['a', 'a', ‘a’, 'a', 'b', 'b', 'b', ‘b'],
ugiea § : ‘data’: np.random.randn(8),
were st ‘weights’: np.random.rand(8)})

In [128]: df
Out [128]:

category data weights
0 a 1.561587 0.957515
1 a 1.219984 0.347267
2 a -0.482239 0.581362
3 a 0.315667 0.217091
4 b -0.047852 0.894406
5 b -0.454145 0.918564
6 b -0.556774 0.277825
7 b 0.253321 0.955905

The group weighted average by category would then be:
In [129]: grouped = df.groupby('category' )

In [130]: get_wavg = lambda g: np.average(g['data'], weights=g[ 'weights'])

In [131]: grouped.apply(get_wavg)

Out [131]:
category
a 0.811643
b -0.122262

As a less trivial example, consider a data set from Yahoo! Finance containing end of
day prices for a few stocks and the S&P 500 index (the SPX ticker):

In [132]: close_px = pd.read_csv('cho9/stock_px.csv', parse_dates=True, index_col=0)

In [133]: close_px

Out [133]:

<class 'pandas.core.frame.DataFrame' >

DatetimeIndex: 2214 entries, 2003-01-02 :00 to 2011-10-14 :00
Data columns:

AAPL 2214 non-null values

MSFT 2214 non-null values

XOM 2214 non-null values

SPX 2214 non-null values

dtypes: float64(4)

 

Group-wise Operations and Transformations | 273

In [134]: close _px[-4:]
Out [134]:

AAPL = MSFT XOM SPX
2011-10-11 400.29 27.00 76.27 1195.54
2011-10-12 402.19 26.96 77.16 1207.25
2011-10-13 408.43 27.18 76.37 1203.66
2011-10-14 422.00 27.27 78.11 1224.58

One task of interest might be to compute a DataFrame consisting of the yearly corre-
lations of daily returns (computed from percent changes) with SPX. Here is one way to
do it:

In [135]: rets = close_px.pct_change() .dropna()

In [136]: spx_corr = lambda x: x.corrwith(x['SPX'])
In [137]: by_year = rets.groupby(lambda x: x.year)

In [138]: by_year.apply(spx_corr)

Out [138]:

AAPL MSFT XOM SPX
2003 0.541124 0.745174 0.661265 al
2004 0.374283 0.588531 0.557742 al
2005 0.467540 0.562374 0.631010 al
2006 0.428267 0.406126 0.518514 al
2007 0.508118 0.658770 0.786264 al
2008 0.681434 0.804626 0.828303 1
2009 0.707103 0.654902 0.797921 1
2010 0.710105 0.730118 0.839057 al
2011 0.691931 0.800996 0.859975 1

There is, of course, nothing to stop you from computing inter-column correlations:

# Annual correlation of Apple with Microsoft
In [139]: by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))

Out [139]:

2003 0.480868
2004 0.259024
2005 0.300093
2006 0.161735
2007 0.417738
2008 0.611901
2009 0.432738
2010 0.571946
2011 0.581987

Example: Group-wise Linear Regression

In the same vein as the previous example, you can use groupby to perform more complex
group-wise statistical analysis, as long as the function returns a pandas object or scalar
value. For example, I can define the following regress function (using the statsmo
dels econometrics library) which executes an ordinary least squares (OLS) regression
on each chunk of data:

 

274 | Chapter 9: Data Aggregation and Group Operations

import statsmodels.api as sm
def regress(data, yvar, xvars):
data[yvar]
data[xvars ]
X['intercept'] = 1.
result = sm.OLS(Y, X).fit()
return result.params

Y=
X=

Now, to run a yearly linear regression of AAPL on SPX returns, I execute:

In [141]: by_year.apply(regress, 'AAPL', ['SPX'])
Out [141]:

2003
2004
2005
2006
2007
2008
2009
2010
2011

Pivot Tables and Cross-Tabulation

CORCORRBRRBRER

SPX

- 195406
- 363463
- 766415
-645496
-198761
- 968016
-879103
-052608
- 806605

intercept
0.000710
004201
003246
000080
003438
-0.001110
0.002954
0.001261
0.001514

0.
0.
0.
0.

A pivot table is a data summarization tool frequently found in spreadsheet programs
and other data analysis software. It aggregates a table of data by one or more keys,
arranging the data in a rectangle with some of the group keys along the rows and some
along the columns. Pivot tables in Python with pandas are made possible using the
groupby facility described in this chapter combined with reshape operations utilizing
hierarchical indexing. DataFrame has a pivot_table method, and additionally there is
a top-level pandas.pivot_table function. In addition to providing a convenience inter-
face to groupby, pivot_table also can add partial totals, also known as margins.

Returning to the tipping data set, suppose I wanted to compute a table of group means
(the default pivot_table aggregation type) arranged by sex and smoker on the rows:

In [142]: tips.pivot_table(rows=['sex', 'smoker'])

Out [142]:
sex smoker
Female No

Yes
Male No

Yes

size

2.592593
2.242424
2.711340
2.500000

tip

2.773519
2.931515
3.113402
3.051167

tip_pct total bill

0.156921
0.182150
0.160669
0.152771

18.105185
17.977879
19.791237
22.284500

This could have been easily produced using groupby. Now, suppose we want to aggre-
gate only tip_pct and size, and additionally group by day. I'll put smoker in the table
columns and day in the rows:

In [143]: tips.pivot_table(

Out [143]:

['tip_pct', 'size'], rows=['sex', ‘day'],
cols='smoker' )

 

Pivot Tables and Cross-Tabulation | 275

tip_pct size

smoker No Yes No Yes

sex day

Female Fri 0.165296 0.209129 2.500000 2.000000
Sat 0.147993 0.163817 2.307692 2.200000
Sun 0.165710 0.237075 3.071429 2.500000
Thur 0.155971 0.163073 2.480000 2.428571

Male Fri 0.138005 0.144730 2.000000 2.125000
Sat 0.162132 0.139067 2.656250 2.629630
Sun 0.158291 0.173964 2.883721 2.600000
Thur 0.165706 0.164417 2.500000 2.300000

This table could be augmented to include partial totals by passing margins=True. This
has the effect of adding A11 row and column labels, with corresponding values being
the group statistics for all the data within a single tier. In this below example, the All
values are means without taking into account smoker vs. non-smoker (the All columns)
or any of the two levels of grouping on the rows (the All row):

In [144]: tips.pivot_table(['tip_pct', 'size'], rows=['sex', ‘day'],
awe é : cols='smoker', margins=True)

Out[144]:
size tip_pct

smoker No Yes All No Yes All

sex day

Female Fri 2.500000 2.000000 2.111111 0.165296 0.209129 0.199388
Sat 2.307692 2.200000 2.250000 0.147993 0.163817 0.156470
Sun 3.071429 2.500000 2.944444 0.165710 0.237075 0.181569
Thur 2.480000 2.428571 2.468750 0.155971 0.163073 0.157525

Male Fri 2.000000 2.125000 2.100000 0.138005 0.144730 0.143385
Sat 2.656250 2.629630 2.644068 0.162132 0.139067 0.151577
Sun 2.883721 2.600000 2.810345 0.158291 0.173964 0.162344
Thur 2.500000 2.300000 2.433333 0.165706 0.164417 0.165276

All 2.668874 2.408602 2.569672 0.159328 0.163196 0.160803

To use a different aggregation function, pass it to aggfunc. For example, ‘count’ or
len will give you a cross-tabulation (count or frequency) of group sizes:

In [145]: tips.pivot_table('tip pct', rows=['sex', 'smoker'], cols='day',
eveteie @ ; aggfunc=len, margins=True)

Out[145]:

day Fri Sat Sun Thur All

sex smoker

Female No 2 13 «14 25 54
Yes 7 15 4 7 33

Male No 2 32 43 20 97
Yes 8 27 15 10 60

All 19 87 76 62 244

If some combinations are empty (or otherwise NA), you may wish to pass a fill_value:

In [146]: tips.pivot_table('size', rows=['time', 'sex', ‘smoker'],
mere ye ‘ cols='day', aggfunc='sum', fill_value=0)

Out [146]:

day Fri Sat Sun Thur
time sex smoker

Dinner Female No 2 30 43 2

 

276 | Chapter9: Data Aggregation and Group Operations

Yes 8 33 10 0

Male No 4 85 124 0

Yes 12 ji. 39 0

Lunch Female No 3 0 0 60
Yes 6 0 0 17

Male No 0 0 0 50

Yes 5 0 0 23

See Table 9-2 for a summary of pivot_table methods.

Table 9-2. pivot_table options

Functionname _ Description

values Column name or names to aggregate. By default aggregates all numeric columns

rows Column names or other group keys to group on the rows of the resulting pivot table

cols Column names or other group keys to group on the columns of the resulting pivot table

aggfunc Aggregation function or list of functions; ‘mean’ by default. Can be any function valid in a groupby context

fill_value Replace missing values in result table

margins Add row/column subtotals and grand total, False by default

 

Cross-Tabulations: Crosstab

A cross-tabulation (or crosstab for short) is a special case of a pivot table that computes
group frequencies. Here is a canonical example taken from the Wikipedia page on cross-
tabulation:

In [150]: data
Out [150]:
Sample Gender Handedness
Female Right-handed
Male Left-handed
Female Right-handed
Male Right-handed
Male Left-handed
Male Right-handed
Female Right-handed
Female Left-handed
Male Right-handed
Female Right-handed

WO CONAUBWNF OO
CWO ON DU BWNPR

BR

As part of some survey analysis, we might want to summarize this data by gender and
handedness. You could use pivot_table to do this, but the pandas.crosstab function
is very convenient:

In [151]: pd.crosstab(data.Gender, data.Handedness, margins=True)

Out[151]:

Handedness Left-handed Right-handed All
Gender

Female 1 4 5
Male 2 3 5
All 3 i)

 

Pivot Tables and Cross-Tabulation | 277

The first two arguments to crosstab can each either be an array or Series or a list of
arrays. As in the tips data:

In [152]: pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)

Out [152]:

smoker No Yes All
time day

Dinner Fri 3 9 12

Sat 45 42 87
Sun 57 19 76
Thur 1 0 1
Lunch Fri 1 6 7
Thur 44 17° 61
All 151 93 244

Example: 2012 Federal Election Commission Database

The US Federal Election Commission publishes data on contributions to political cam-
paigns. This includes contributor names, occupation and employer, address, and con-
tribution amount. An interesting dataset is from the 2012 US presidential election
(http://www.fec.gov/disclosurep/PDownload.do). As of this writing (June 2012), the full
dataset for all states is a 150 megabyte CSV file P00000001-ALL.csv, which can be loaded
with pandas.read_csv:

In [13]: fec = pd.read_csv('cho9/P00000001-ALL.csv')

In [14]: fec

Out[14]:

<class 'pandas.core.frame.DataFrame' >
Int64Index: 1001731 entries, 0 to 1001730
Data columns:

cmte_id 1001731 non-null values
cand_id 1001731 non-null values
cand_nm 1001731 non-null values
contbr_nm 1001731 non-null values
contbr_city 1001716 non-null values
contbr_st 1001727 non-null values
contbr_zip 1001620 non-null values
contbr_employer 994314 non-null values

contbr_occupation 994433 non-null values
contb_receipt_amt 1001731 non-null values

contb_receipt_dt 1001731 non-null values
receipt_desc 14166 non-null values
memo_cd 92482 non-null values
memo_text 97770 non-null values
form_tp 1001731 non-null values
file_num 1001731 non-null values

dtypes: float64(1), int64(1), object(14)

A sample record in the DataFrame looks like this:

In [15]: fec.ix[123456]
Out[15]:
cmte_id €00431445

 

278 | Chapter9: Data Aggregation and Group Operations

cand_id P80003338
cand_nm Obama, Barack
contbr_nm ELLMAN, IRA
contbr_city TEMPE
contbr_st AZ
contbr_zip 852816719
contbr_employer ARIZONA STATE UNIVERSITY
contbr_occupation PROFESSOR
contb_receipt_amt 50
contb_receipt_dt 01-DEC-11
receipt_desc NaN
memo_cd NaN
memo_text NaN
form_tp SA17A
file_num 772372

Name: 123456

You can probably think of many ways to start slicing and dicing this data to extract
informative statistics about donors and patterns in the campaign contributions. I’ll
spend the next several pages showing you a number of different analyses that apply
techniques you have learned about so far.

You can see that there are no political party affiliations in the data, so this would be
useful to add. You can get a list of all the unique political candidates using unique (note
that NumPy suppresses the quotes around the strings in the output):

In [16]: unique_cands = fec.cand_nm.unique()

In [17]: unique_cands
Out[17]:
array([Bachmann, Michelle, Romney, Mitt, Obama, Barack,
Roemer, Charles E. ‘Buddy’ III, Pawlenty, Timothy,
Johnson, Gary Earl, Paul, Ron, Santorum, Rick, Cain, Herman,
Gingrich, Newt, McCotter, Thaddeus G, Huntsman, Jon, Perry, Rick], dtype=object)

In [18]: unique_cands[2]
Out[18]: ‘Obama, Barack’

An easy way to indicate party affiliation is using a dict:2

parties = {'Bachmann, Michelle’: ‘Republican’,
"Cain, Herman': ‘Republican’,
"Gingrich, Newt': ‘Republican’,
"Huntsman, Jon': ‘Republican’,
"Johnson, Gary Earl': ‘Republican’,
"McCotter, Thaddeus G': ‘Republican’,
‘Obama, Barack’: ‘Democrat’,
"Paul, Ron': ‘Republican’,
"Pawlenty, Timothy’: ‘Republican’,
"Perry, Rick': ‘Republican’,
"Roemer, Charles E. ‘Buddy’ III": ‘Republican’,

2. This makes the simplifying assumption that Gary Johnson is a Republican even though he later became
the Libertarian party candidate.

 

Example: 2012 Federal Election Commission Database | 279

"Romney, Mitt': ‘Republican’,
"Santorum, Rick': 'Republican'}

Now, using this mapping and the map method on Series objects, you can compute an
array of political parties from the candidate names:

In [20]: fec.cand_nm[12343461]
Out[20]:

123456 Obama, Barack

123457 Obama, Barack

123458 Obama, Barack

123459 Obama, Barack

123460 Obama, Barack

Name: cand_nm

In [21]: fec.cand_nm[12343461].map(parties)
Out[21]:

123456 Democrat

123457 Democrat

123458 Democrat

123459 Democrat

123460 Democrat

Name: cand_nm

# Add it as a column
In [22]: fec['party'] = fec.cand_nm.map(parties)

In [23]: fec['party'].value_counts()
Out [23]:

Democrat 593746

Republican 407985

A couple of data preparation points. First, this data includes both contributions and
refunds (negative contribution amount):

In [24]: (fec.contb_receipt_amt > 0).value_counts()

Out[24]:
True 991475
False 10256

To simplify the analysis, I’ll restrict the data set to positive contributions:

In [25]: fec = fec[fec.contb_receipt_amt > 0]
Since Barack Obama and Mitt Romney are the main two candidates, I'll also prepare
a subset that just has contributions to their campaigns:

In [26]: fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack', ‘Romney, Mitt'])]

Donation Statistics by Occupation and Employer

Donations by occupation is another oft-studied statistic. For example, lawyers (attor-
neys) tend to donate more money to Democrats, while business executives tend to
donate more to Republicans. You have no reason to believe me; you can see for yourself
in the data. First, the total number of donations by occupation is easy:

 

280 | Chapter 9: Data Aggregation and Group Operations

In [27]: fec.contbr_occupation.value_counts()[:10]

Out [27]:

RETIRED 233990
INFORMATION REQUESTED 35107
ATTORNEY 34286
HOMEMAKER 29931
PHYSICIAN 23432
INFORMATION REQUESTED PER BEST EFFORTS 21138
ENGINEER 14334
TEACHER 13990
CONSULTANT 13273
PROFESSOR 12555

You will notice by looking at the occupations that many refer to the same basic job
type, or there are several variants of the same thing. Here is a code snippet illustrates a
technique for cleaning up a few of them by mapping from one occupation to another;
note the “trick” of using dict.get to allow occupations with no mapping to “pass
through”:

occ_mapping = {
“INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',
“INFORMATION REQUESTED’ : ‘NOT PROVIDED’,
“INFORMATION REQUESTED (BEST EFFORTS)’ : ‘NOT PROVIDED’,
"C.E.0.': 'CEO'

}

# If no mapping provided, return x
= lambda x: occ_mapping.get(x, x)
fec.contbr_occupation = fec.contbr_occupation.map(f)

Pll also do the same thing for employers:

emp_mapping = {
“INFORMATION REQUESTED PER BEST EFFORTS' : ‘NOT PROVIDED’,
"INFORMATION REQUESTED' : ‘NOT PROVIDED’,
"SELF' : 'SELF-EMPLOYED',
"SELF EMPLOYED' : ‘SELF-EMPLOYED’,

}

# If no mapping provided, return x
f = lambda x: emp_mapping.get(x, x)
fec.contbr_employer = fec.contbr_employer.map(f)

Now, you can use pivot_table to aggregate the data by party and occupation, then
filter down to the subset that donated at least $2 million overall:
In [34]: by_occupation = fec.pivot_table('contb_receipt_amt',

meee? rows='contbr_occupation',
meee? cols='party', aggfunc="sum' )

In [35]: over_2mm = by occupation[by_occupation.sum(1) > 2000000]

In [36]: over_2mm

Out [36]:

party Democrat Republican
contbr_occupation

 

Example: 2012 Federal Election Commission Database | 281

ATTORNEY 11141982.97 7477194.430000

CEO 2074974.79 4211040.520000
CONSULTANT 2459912.71  2544725.450000
ENGINEER 951525.55  1818373.700000
EXECUTIVE 1355161.05 4138850.090000
HOMEMAKER 4248875.80 13634275.780000
INVESTOR 884133.00 2431768.920000
LAWYER 3160478 .87 391224.320000
MANAGER 762883.22 1444532.370000
NOT PROVIDED 4866973.96 20565473.010000
OWNER 1001567.36 2408286.920000
PHYSICIAN 3735124.94 3594320.240000
PRESIDENT 1878509.95  4720923.760000
PROFESSOR 2165071.08 296702 .730000
REAL ESTATE 528902.09 1625902.250000
RETIRED 25305116.38 23561244.489999
SELF-EMPLOYED 672393.40 1640252.540000

It can be easier to look at this data graphically as a bar plot (‘barh' means horizontal
bar plot, see Figure 9-2):

In [38]: over_2mm.plot(kind='barh' )

 

   

SELF-EMPLOYED

z PHYSICIAN

°

s OWNER

2 NOT PROVIDED

U

g MANAGER

5 LAWYER [bgp 25200 ch nena c nena ne cee ne neat snc ne cece nese secncnesbesenesesesesnesssecees
5 INVESTOR

Vv

party
Mg Democrat
HB Republican

 

0.0 0.5 1.0 15 2.0 2:5 3.0
le7

 

 

 

Figure 9-2. Total donations by party for top occupations

You might be interested in the top donor occupations or top companies donating to
Obama and Romney. To do this, you can group by candidate name and use a variant
of the top method from earlier in the chapter:

def get_top_amounts(group, key, n=5):
totals = group.groupby(key)['contb_receipt_amt'].sum()

# Order totals by key in descending order
return totals.order(ascending=False)[:n]

 

282 | Chapter 9: Data Aggregation and Group Operations

Then aggregated by occupation and employer:
In [40]: grouped = fec_mrbo.groupby('cand_nm')

In [41]: grouped.apply(get_top_amounts, ‘contbr_occupation', n=7)

Out [41]:

cand_nm contbr_occupation

Obama, Barack RETIRED 25305116.38
ATTORNEY 11141982.97
NOT PROVIDED 4866973 .96
HOMEMAKER 4248875 .80
PHYSICIAN 3735124.94
LAWYER 3160478 .87
CONSULTANT 2459912.71

Romney, Mitt RETIRED 11508473.59
NOT PROVIDED 11396894.84
HOMEMAKER 8147446.22
ATTORNEY 5364718 .82
PRESIDENT 2491244.89
EXECUTIVE 2300947 .03
C.E.0. 1968386.11

Name: contb_receipt_amt

In [42]: grouped.apply(get_top_amounts, 'contbr_employer', n=10)

Out [42]:

cand_nm contbr_employer

Obama, Barack RETIRED 22694358.85
SELF-EMPLOYED 18626807 .16
NOT EMPLOYED 8586308. 70
NOT PROVIDED 5053480.37
HOMEMAKER 2605408 .54
STUDENT 318831.45
VOLUNTEER 257104.00
MICROSOFT 215585.36
SIDLEY AUSTIN LLP 168254.00
REFUSED 149516.07

Romney, Mitt NOT PROVIDED 12059527.24
RETIRED 11506225.71
HOMEMAKER 8147196.22
SELF-EMPLOYED 7414115 .22
STUDENT 496490.94
CREDIT SUISSE 281150.00
MORGAN STANLEY 267266.00
GOLDMAN SACH & CO. 238250.00
BARCLAYS CAPITAL 162750.00
H.1.G. CAPITAL 139500.00

Name: contb_receipt_amt

Bucketing Donation Amounts

A useful way to analyze this data is to use the cut function to discretize the contributor
amounts into buckets by contribution size:

In [43]: bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000] )

 

Example: 2012 Federal Election Commission Database | 283

In [44]: labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)

In [45]: labels

Out [45]:

Categorical:contb_receipt_amt

array([(10, 100], (100, 1000], (100, 1000], ..., (1, 10], (10, 100],
(100, 1000]], dtype=object)

Levels (8): array([(0, 1], (1, 10], (10, 100], (100, 1000], (1000, 10000],
(10000, 100000], (100000, 1000000], (1000000, 10000000]], dtype=object)

We can then group the data for Obama and Romney by name and bin label to get a
histogram by donation size:

In [46]: grouped = fec_mrbo.groupby(['cand_nm', labels])

In [47]: grouped.size().unstack(0)

Out [47]:

cand_nm Obama, Barack Romney, Mitt
contb_receipt_amt

(0, 1] 493 77
(1, 10] 40070 3681
(10, 100] 372280 31853
(100, 1000] 153991 43357
(1000, 10000] 22284 26186
(10000, 100000] 2 1
(100000, 1000000] 3 NaN
(1000000, 10000000] 4 NaN

This data shows that Obama has received a significantly larger number of small don-
ations than Romney. You can also sum the contribution amounts and normalize within
buckets to visualize percentage of total donations of each size by candidate:

In [48]: bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)

In [49]: bucket_sums

Out [49]:

cand_nm Obama, Barack Romney, Mitt
contb_receipt_amt

(0, 1] 318.24 77.00
(1, 10] 337267.62 29819.66
(10, 100] 20288981.41 1987783 .76
(100, 1000] 54798531.46  22363381.69
(1000, 10000] 51753705.67  63942145.42
(10000, 100000] 59100.00 12700.00
(100000, 1000000] 1490683 .08 NaN
(1000000, 10000000] 7148839.76 NaN

In [50]: normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)

In [51]: normed_sums

Out[51]:

cand_nm Obama, Barack Romney, Mitt
contb_receipt_amt

(0, 1] 0.805182 0.194818
(1, 10] 0.918767 0.081233
(10, 100] 0.910769 0.089231

 

284 | Chapter9: Data Aggregation and Group Operations

(100, 1000] 0.710176 0.289824

(1000, 10000] 0.447326 0.552674
(10000, 100000] 0.823120 0.176880
(100000, 1000000] 1.000000 NaN
(1000000, 10000000] 1.000000 NaN

In [52]: normed_sums[:-2].plot(kind="barh', stacked=True)

I excluded the two largest bins as these are not donations by individuals. See Fig-

ure 9-3 for the resulting figure.

 

cand_nm
(10000, 100000] gag ~Obama, Barack
HH Romney, Mitt

(1000, 10000]

=
= (100, 1000]
:
3
UO
o
2 (10, 100]
[4
°
U

(1, 10]

 

 
 
 
 
 
 
 
  

 

 

 

 

 

Figure 9-3. Percentage of total donations received by candidates for each donation size

There are of course many refinements and improvements of this analysis. For example,
you could aggregate donations by donor name and zip code to adjust for donors who
gave many small amounts versus one or more large donations. I encourage you to

download it and explore it yourself.

Donation Statistics by State

Aggregating the data by candidate and state is a routine affair:
In [53]: grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])

In [54]: totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(o)

In [55]: totals = totals[totals.sum(1) > 100000]

In [56]: totals[:10]

Out [56]:

cand_nm Obama, Barack Romney, Mitt
contbr_st

 

Example: 2012 Federal Election Commission Database | 285

AK 281840.15 86204.24

AL 543123.48 527303.51
AR 359247.28 105556.00
AZ 1506476.98 1888436.23
CA 23824984.24 11237636.60
co 2132429.49 1506714.12
cT 2068291.26 3499475.45
DC 4373538.80 1025137.50
DE 336669.14 82712.00
FL 7318178 .58 8338458.81

If you divide each row by the total contribution amount, you get the relative percentage
of total donations by state for each candidate:

In [57]: percent = totals.div(totals.sum(1), axis=0)

In [58]: percent[:10]

Out [58]:

cand_nm Obama, Barack Romney, Mitt
contbr_st

AK 0.765778 0.234222
AL 0.507390 0.492610
AR 0.772902 0.227098
AZ 0.443745 0.556255
CA 0.679498 0.320502
co 0.585970 0.414030
CT 0.371476 0.628524
DC 0.810113 0.189887
DE 0.802776 0.197224
FL 0.467417 0.532583

I thought it would be interesting to look at this data plotted on a map, using ideas from
Chapter 8. After locating a shape file for the state boundaries (http://nationalatlas.gov/
atlasftp.html?openChapters=chpbound) and learning a bit more about matplotlib and
its basemap toolkit (I was aided by a blog posting from Thomas Lecocq)3, I ended up
with the following code for plotting these relative percentages:

from mpl_toolkits.basemap import Basemap, cm

import hnumpy as np

from matplotlib import rcParams

from matplotlib.collections import LineCollection
import matplotlib.pyplot as plt

from shapelib import ShapeFile
import dbflib

obama = percent['Obama, Barack" ]

fig = plt.figure(figsize=(12, 12))
ax = fig.add_axes([0.1,0.1,0.8,0.8])

lllat = 21; urlat = 53; lllon = -118; urlon = -62

3. http:/;www.geophysique.be/2011/01/27/matplotlib-basemap-tutorial-07-shapefiles-unleached/

 

286 | Chapter9: Data Aggregation and Group Operations

m = Basemap(ax=ax, projection='stere’,
lon_o=(urlon + lllon) / 2, lat_o=(urlat + lllat) / 2,
licrnrlat=lllat, urcrnrlat=urlat, llcrnrlon=lllon,
urcrnrlon=urlon, resolution='1')

m.drawcoastlines()

m. drawcountries()

shp = ShapeFile('../states/statesp020' )
dbf = dbflib.open('../states/statesp020' )

for npoly in range(shp.info()[0]):

# Draw colored polygons on the map

shpsegs = []

shp_object = shp.read_object(npoly)

verts = shp_object.vertices()

rings = len(verts)

for ring in range(rings):
lons, lats = zip(*verts[ring])
x, y = m(lons, lats)
shpsegs.append(zip(x,y))
if ring == 0:

shapedict = dbf.read_record(npoly)

name = shapedict['STATE']

lines = LineCollection(shpsegs,antialiaseds=(1, ))

# state_to_code dict, e.g. ‘ALASKA’ -> 'AK', omitted
try:

per = obama[state_to_code[name.upper()]]
except KeyError:

continue

lines.set_facecolors('k')
lines.set_alpha(0.75 * per) # Shrink the percentage a bit
lines.set_edgecolors('k')
lines.set_linewidth(0.3)
ax.add_collection(lines)

plt.show()

See Figure 9-4 for the result.

 

Example: 2012 Federal Election Commission Database | 287

 

 

 

 

 

Figure 9-4. US map aggregated donation statistics overlay (darker means more Democratic)

 

288 | Chapter9: Data Aggregation and Group Operations

CHAPTER 10
Time Series

 

Time series data is an important form of structured data in many different fields, such
as finance, economics, ecology, neuroscience, or physics. Anything that is observed or
measured at many points in time forms a time series. Many time series are fixed fre-
quency, which is to say that data points occur at regular intervals according to some
rule, such as every 15 seconds, every 5 minutes, or once per month. Time series can
also be irregular without a fixed unit or time or offset between units. How you mark
and refer to time series data depends on the application and you may have one of the
following:

* Timestamps, specific instants in time
¢ Fixed periods, such as the month January 2007 or the full year 2010

¢ Intervals of time, indicated by a start and end timestamp. Periods can be thought
of as special cases of intervals

¢ Experiment or elapsed time; each timestamp is a measure of time relative to a
particular start time. For example, the diameter of a cookie baking each second
since being placed in the oven

In this chapter, I am mainly concerned with time series in the first 3 categories, though
many of the techniques can be applied to experimental time series where the index may
be an integer or floating point number indicating elapsed time from the start of the
experiment. The simplest and most widely used kind of time series are those indexed
by timestamp.

pandas provides a standard set of time series tools and data algorithms. With this, you
can efficiently work with very large time series and easily slice and dice, aggregate, and
resample irregular and fixed frequency time series. As you might guess, many of these
tools are especially useful for financial and economics applications, but you could cer-
tainly use them to analyze server log data, too.

 

289

Some of the features and code, in particular period logic, presented in
this chapter were derived from the now defunct scikits.timeseries li-

 

 

Date and Time Data Types and Tools

The Python standard library includes data types for date and time data, as well as
calendar-related functionality. The datetime, time, and calendar modules are the main
places to start. The datetime.datetime type, or simply datetime, is widely used:

In [317]: from datetime import datetime

In [318]: now = datetime.now()

In [319]: now

Out[319]: datetime.datetime(2012, 8, 4, 17, 9, 21, 832092)
In [320]: now.year, now.month, now.day

Out[320]: (2012, 8, 4)

datetime stores both the date and time down to the microsecond. datetime.time
delta represents the temporal difference between two datetime objects:

In [321]

In [322]:
Out [322]:

In [323]:
Out [323]:

delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)

delta
datetime.timedelta(926, 56700)

In [324]: delta.seconds
Out[324]: 56700

delta.days
926

You can add (or subtract) a timedelta or multiple thereof to a datetime object to yield
a new shifted object:

In [325]:
In [326]:

In [327]:
Out [327]:

In [328]:
Out [328]:

from datetime import timedelta
start = datetime(2011, 1, 7)

start + timedelta(12)
datetime.datetime(2011, 1, 19, 0, 0)

start - 2 * timedelta(12)
datetime.datetime(2010, 12, 14, 0, 0)

The data types in the datetime module are summarized in Table 10-1. While this chap-
ter is mainly concerned with the data types in pandas and higher level time series ma-
nipulation, you will undoubtedly encounter the datetime-based types in many other
places in Python the wild.

 

290 | Chapter 10: Time Series

Table 10-1. Types in datetime module

Type Description

date Store calendar date (year, month, day) using the Gregorian calendar.

time Store time of day as hours, minutes, seconds, and microseconds

datetime Stores both date and time

timedelta Represents the difference between two datetime values (as days, seconds, and micro-
seconds)

 

Converting between string and datetime
datetime objects and pandas Timestamp objects, which I’ll introduce later, can be for-
matted as strings using str or the strftime method, passing a format specification:

In [329]: stamp = datetime(2011, 1, 3)

In [330]: str(stamp) In [331]: stamp.strftime('%Y-%m-%d' )
Out[330]: ‘2011-01-03 :00' Out [331]: '2011-01-03'

See Table 10-2 for a complete list of the format codes. These same format codes can be
used to convert strings to dates using datetime. strptime:
In [332]: value = '2011-01-03'

In [333]: datetime.strptime(value, '%Y-%m-%d' )
Out[333]: datetime.datetime(2011, 1, 3, 0, 0)

In [334]: datestrs = ['7/6/2011', '8/6/2011']

In [335]: [datetime.strptime(x, '%m/%d/%Y') for x in datestrs]

Out[335]: [datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)]
datetime.strptime is the best way to parse a date with a known format. However, it
can be a bit annoying to have to write a format spec each time, especially for common
date formats. In this case, you can use the parser.parse method in the third party
dateutil package:

In [336]: from dateutil.parser import parse

In [337]: parse('2011-01-03')

Out[337]: datetime.datetime(2011, 1, 3, 0, 0)
dateutil is capable of parsing almost any human-intelligible date representation:

In [338]: parse('Jan 31, 1997  PM')

Out [338]: datetime.datetime(1997, 1, 31, 22, 45)

In international locales, day appearing before month is very common, so you can pass
dayfirst=True to indicate this:

In [339]: parse('6/12/2011', dayfirst=True)
Out[339]: datetime.datetime(2011, 12, 6, 0, 0)

 

Date and Time Data Types and Tools | 291

pandas is generally oriented toward working with arrays of dates, whether used as an
axis index or a column in a DataFrame. The to_datetime method parses many different
kinds of date representations. Standard date formats like ISO8601 can be parsed very

qui

Ita

NaT

ckly.

In [340]: datestrs
Out[340]: ['7/6/2011', '8/6/2011']

In [341]: pd.to_datetime(datestrs)

Out [341]:

<class 'pandas.tseries.index.DatetimeIndex'>
[2011-07-06 :00, 2011-08-06 :00]
Length: 2, Freq: None, Timezone: None

lso handles values that should be considered missing (None, empty string, etc.):
In [342]: idx = pd.to_datetime(datestrs + [None])

In [343]: idx

Out [343]:

<class 'pandas.tseries.index.DatetimeIndex'>
[2011-07-06 :00, ..., NaT]

Length: 3, Freq: None, Timezone: None

In [344]: idx[2]
Out[344]: NaT

In [345]: pd.isnull(idx)
Out[345]: array([False, False, True], dtype=bool)

(Not a Time) is pandas’s NA value for timestamp data.

nize some strings as dates that you might prefer that it didn’t, like
'42' will be parsed as the year 2042 with today’s calendar date.

 

dateutil.parser is a useful, but not perfect tool. Notably, it will recog-

Table 10-2. Datetime format specification (ISO C89 compatible)
Type Description
*Y 4-digit year
ay 2-digit year

2-digit month [01, 12]
2-digit day (01, 31]
Hour (24-hour clock) [00, 23]

 

%1 Hour (12-hour clock) [01, 12]
MI 2-digit minute [00, 59]
%S Second [00, 61] (seconds 60, 61 account for leap seconds)
%w Weekday as integer [0 (Sunday), 6]
292 | Chapter 10: Time Series

Type Description

aU Week number of the year [00, 53]. Sunday is considered the first day of the week, and days before the first
Sunday of the year are “week 0”.

ZW Week number of the year [00, 53]. Monday is considered the first day of the week, and days before the first
Monday of the year are “week 0”.

“Z UTC time zone offset as +HHMM or -HHMM, empty if time zone naive

*F Shortcut for %Y -%m-%d, for example 2012-4-18

%D Shortcut for 4m/%d/%y, for example 04/18/12

datetime objects also have a number of locale-specific formatting options for systems
in other countries or languages. For example, the abbreviated month names will be
different on German or French systems compared with English systems.

Table 10-3. Locale-specific date formatting

 

Type Description
wa Abbreviated weekday name
aN Full weekday name
*D Abbreviated month name
4B Full month name
%C Full date and time, for example ‘Tue 01 May 2012 :57 PM’
zp Locale equivalent of AM or PM
mx Locale-appropriate formatted date; e.g. in US May 1, 2012 yields 05/01/2012’
%X Locale-appropriate time, e.g. ‘:12 PM’
Time Series Basics

The most basic kind of time series object in pandas is a Series indexed by timestamps,
which is often represented external to pandas as Python strings or datetime objects:

In [346]: from datetime import datetime

In [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),
ore : datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]

In [348]: ts = Series(np.random.randn(6), index=dates)

In [349]: ts

Out [349]:

2011-01-02 0.690002
2011-01-05 1.001543
2011-01-07 -0.503087
2011-01-08 -0.622274

 

Time Series Basics | 293

2011-01-10 -0.921169
2011-01-12 -0.726213

Under the hood, these datetime objects have been put in a DatetimeIndex, and the
variable ts is now of type TimeSeries:

In [350]: type(ts)
Out[350]: pandas.core.series.TimeSeries

In [351]: ts.index

Out [351]:

<class 'pandas.tseries.index.DatetimeIndex'>
[2011-01-02 :00, ..., 2011-01-12 :00]
Length: 6, Freq: None, Timezone: None

 

 

Va
,
a It’s not necessary to use the TimeSeries constructor explicitly; when
43 creating a Series with a DatetimeIndex, pandas knows that the object is
“s §ja* a time series.

Like other Series, arithmetic operations between differently-indexed time series auto-
matically align on the dates:

In [352]: ts + ts[::2]

Out [352]:

2011-01-02 1.380004
2011-01-05 NaN
2011-01-07 -1.006175
2011-01-08 NaN
2011-01-10 -1.842337
2011-01-12 NaN

pandas stores timestamps using NumPy’s datetime64 data type at the nanosecond res-
olution:

In [353]: ts.index.dtype
Out[353]: dtype('datetime64[ns]')

Scalar values from a DatetimeIndex are pandas Timestamp objects

In [354]: stamp = ts.index[o]

In [355]: stamp
Out[355]: <Timestamp: 2011-01-02 :00>

A Timestamp can be substituted anywhere you would use a datetime object. Addition-
ally, it can store frequency information (if any) and understands how to do time zone
conversions and other kinds of manipulations. More on both of these things later.

Indexing, Selection, Subsetting

TimeSeries is a subclass of Series and thus behaves in the same way with regard to
indexing and selecting data based on label:

 

294 | Chapter 10: Time Series

In [356]: stamp = ts.index[2]

In [357]: ts[stamp]
Out[357]: -0.50308739136034464

As a convenience, you can also pass a string that is interpretable as a date:

In [358]: ts['1/10/2011' ]
Out [358]: -0.92116860801301081

In [359]: ts['20110110' ]
Out[ 359]: -0.92116860801301081

For longer time series, a year or only a year and month can be passed to easily select

slices of data:

In [360]: longer ts =

In [361]: longer _ts

Out [361]:

2000-01-01
2000-01-02
2000-01-03
2000-01-04

2002-09-23
2002-09-24
2002-09-25
2002-09-26

0.222896
-051316
-1.157719

0.816707

°o

-0.395813
-0.180737

1.337508
-0.416584

Freq: D, Length: 1000

In [362]: longer_ts['2001']

Out [362]:

2001-01-01
2001-01-02
2001-01-03
2001-01-04

2001-12-28
2001-12-29
2001-12-30
2001-12-31

-1.499503
0.545154
0.400823

-1.946230

-1.568139
-0.900887
0.652346
0.871600

Freq: D, Length: 365

Series (np.random.randn(1000),
index=pd.date_range('1/1/2000', periods=1000) )

Out [363]:

2001-05-01
2001-05-02
2001-05-03
2001-05-04

2001-05-28
2001-05-29
2001-05-30
2001-05-31

1.
=

-0.

In [363]: longer_ts['2001-05']

662014

- 189203
-093597
- 539164

- 683066
«950313
-400710

126072

Freq: D, Length: 31

Slicing with dates works just like with a regular Series:

In [364]: ts[datetime(2011, 1, 7):]

Out [364]:

2011-01-07
2011-01-08
2011-01-10
2011-01-12

-0.503087
-0.622274
-0.921169
-0.726213

Because most time series data is ordered chronologically, you can slice with timestamps
not contained in a time series to perform a range query:

In [365]: ts

Out [365]:
2011-01-02

0.690002

In [366]: ts['1/6/2011':'1/11/2011' ]

Out [366]:

2011-01-07 =-0.503087

 

Time Series Basics | 295

2011-01-05 1.001543 2011-01-08  -0.622274
2011-01-07 -0.503087 2011-01-10 -0.921169
2011-01-08  -0.622274
2011-01-10 -0.921169
2011-01-12 -0.726213

As before you can pass either a string date, datetime, or Timestamp. Remember that
slicing in this manner produces views on the source time series just like slicing NumPy
arrays. There is an equivalent instance method truncate which slices a TimeSeries be-
tween two dates:

In [367]: ts.truncate(after='1/9/2011')
Out [367]:

2011-01-02 0.690002

2011-01-05 1.001543

2011-01-07 -0.503087

2011-01-08 -0.622274

All of the above holds true for DataFrame as well, indexing on its rows:

In [368]: dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')

In [369]: long df = DataFrame(np.random.randn(100, 4),
mae : index=dates,
satay 22 columns=['Colorado', 'Texas', 'New York', 'Ohio'])

In [370]: long _df.ix['5-2001']
Out [370]:

Colorado Texas New York Ohio
2001-05-02 0.943479 -0.349366 0.530412 -0.508724
2001-05-09 0.230643 -0.065569 -0.248717 -0.587136
2001-05-16 -1.022324 1.060661 0.954768 -0.511824
2001-05-23 -1.387680 0.767902 -1.164490 1.527070
2001-05-30 0.287542 0.715359 -0.345805 0.470886

Time Series with Duplicate Indices

In some applications, there may be multiple data observations falling on a particular
timestamp. Here is an example:

In [371]: dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000', '1/2/2000',
wee es : '1/3/2000' ])

In [372]: dup_ts = Series(np.arange(5), index=dates)

In [373]: dup_ts
Out [373]:

2000-01-01
2000-01-02
2000-01-02
2000-01-02
2000-01-03 4

WNrR OO

We can tell that the index is not unique by checking its is_unique property:

 

296 | Chapter 10: Time Series

In [374]: dup_ts.index.is unique
Out[374]: False

Indexing into this time series will now either produce scalar values or slices depending
on whether a timestamp is duplicated:

In [375]: dup_ts['1/3/2000'] # not duplicated
Out[375]: 4

In [376]: dup _ts['1/2/2000'] # duplicated
Out [376]:

2000-01-02 1

2000-01-02 2

2000-01-02 3

Suppose you wanted to aggregate the data having non-unique timestamps. One way
to do this is to use groupby and pass level=o (the only level of indexing!):

In [377]: grouped = dup _ts.groupby(level=0)

In [378]: grouped.mean() In [379]: grouped.count()
Out [378]: Out [379]:

2000-01-01 0 2000-01-01 1.
2000-01-02 2 2000-01-02 3
2000-01-03 4 2000-01-03 1.

Date Ranges, Frequencies, and Shifting

Generic time series in pandas are assumed to be irregular; that is, they have no fixed
frequency. For many applications this is sufficient. However, it’s often desirable to work
relative to a fixed frequency, such as daily, monthly, or every 15 minutes, even if that
means introducing missing values into a time series. Fortunately pandas has a full suite
of standard time series frequencies and tools for resampling, inferring frequencies, and
generating fixed frequency date ranges. For example, in the example time series, con-
verting it to be fixed daily frequency can be accomplished by calling resample:

In [380]: ts In [381]: ts.resample('D')
Out [380]: Out [381]:
2011-01-02 0.690002 2011-01-02 0.690002
2011-01-05 1.001543 2011-01-03 NaN
2011-01-07 -0.503087 2011-01-04 NaN
2011-01-08 -0.622274 2011-01-05 1.001543
2011-01-10 -0.921169 2011-01-06 NaN
2011-01-12 -0.726213 2011-01-07 -0.503087
2011-01-08 -0.622274
2011-01-09 NaN
2011-01-10 -0.921169
2011-01-11 NaN
2011-01-12 -0.726213
Freq: D

Conversion between frequencies or resampling is a big enough topic to have its own
section later. Here I’ll show you how to use the base frequencies and multiples thereof.

 

Date Ranges, Frequencies, and Shifting | 297

Generating Date Ranges

While I used it previously without explanation, you may have guessed that pan
das.date_range is responsible for generating a DatetimeIndex with an indicated length
according to a particular frequency:

In [382]: index = pd.date_range('4/1/2012', '6/1/2012')

In [383]: index

Out [383]:

<class 'pandas.tseries.index.DatetimeIndex'>
[2012-04-01 :00, ..., 2012-06-01 :00]
Length: 62, Freq: D, Timezone: None

By default, date_range generates daily timestamps. If you pass only a start or end date,
you must pass a number of periods to generate:

In [384]: pd.date_range(start='4/1/2012', periods=20)

Out [384]:

<class 'pandas.tseries.index.DatetimeIndex'>

[2012-04-01 :00, ..., 2012-04-20 :00]

Length: 20, Freq: D, Timezone: None

In [385]: pd.date_range(end='6/1/2012', periods=20)
Out [385]:

<class 'pandas.tseries.index.DatetimeIndex'>
[2012-05-13 :00, ..., 2012-06-01 :00]
Length: 20, Freq: D, Timezone: None

The start and end dates define strict boundaries for the generated date index. For ex-
ample, if you wanted a date index containing the last business day of each month, you
would pass the 'BM' frequency (business end of month) and only dates falling on or
inside the date interval will be included:

In [386]: pd.date_range('1/1/2000', '12/1/2000', freq='BM')

Out [386]:

<class 'pandas.tseries.index.DatetimeIndex'>

[2000-01-31 :00, ..., 2000-11-30 :00]
Length: 11, Freq: BM, Timezone: None

date_range by default preserves the time (if any) of the start or end timestamp:

In [387]: pd.date_range('5/2/2012 :31', periods=5)
Out [387]:

<class 'pandas.tseries.index.DatetimeIndex'>
[2012-05-02 :31, ..., 2012-05-06 :31]

Length: 5, Freq: D, Timezone: None

Sometimes you will have start or end dates with time information but want to generate
a set of timestamps normalized to midnight as a convention. To do this, there is a
normalize option:

In [388]: pd.date_range('5/2/2012 :31', periods=5, normalize=True)

Out [388]:
<class 'pandas.tseries.index.DatetimeIndex'>

 

298 | Chapter 10: Time Series

[2012-05-02 :00, ..., 2012-05-06 :00]
Length: 5, Freq: D, Timezone: None

Frequencies and Date Offsets

Frequencies in pandas are composed of a base frequency and a multiplier. Base fre-
quencies are typically referred to by a string alias, like 'M' for monthly or 'H' for hourly.
For each base frequency, there is an object defined generally referred to as a date off-
set. For example, hourly frequency can be represented with the Hour class:

In [389]: from pandas.tseries.offsets import Hour, Minute
In [390]: hour = Hour()

In [391]: hour
Out[391]: <1 Hour>

You can define a multiple of an offset by passing an integer:
In [392]: four_hours = Hour(4)

In [393]: four_hours
Out[ 393]: <4 Hours>

In most applications, you would never need to explicitly create one of these objects,
instead using a string alias like 'H' or '4H'. Putting an integer before the base frequency
creates a multiple:

In [394]: pd.date_range('1/1/2000', '1/3/2000 ', freq='4h')

Out [394]:

<class 'pandas.tseries.index.DatetimeIndex'>

[2000-01-01 :00, ..., 2000-01-03 :00]

Length: 18, Freq: 4H, Timezone: None

Many offsets can be combined together by addition:

In [395]: Hour(2) + Minute(30)
Out[395]: <150 Minutes>

Similarly, you can pass frequency strings like '2h30min' which will effectively be parsed
to the same expression:

In [396]: pd.date_range('1/1/2000', periods=10, freq='1h30min' )

Out [396]:

<class 'pandas.tseries.index.DatetimeIndex'>

[2000-01-01 :00, ..., 2000-01-01 :00]

Length: 10, Freq: 90T, Timezone: None

Some frequencies describe points in time that are not evenly spaced. For example,
'M' (calendar month end) and 'BM' (last business/weekday of month) depend on the
number of days in a month and, in the latter case, whether the month ends ona weekend
or not. For lack of a better term, I call these anchored offsets.

See Table 10-4 fora listing of frequency codes and date offset classes available in pandas.

 

Date Ranges, Frequencies, and Shifting | 299

 

 

Table 10-4. Base Time Series Frequencies

Alias
D
B
H

Tormin

Lorms
U

M

BM

MS
BMS

W-MON, W-TUE, ...

WOM-1MON, WOM-2MON, ...

Q-JAN, Q-FEB, ...

BO-JAN, BO-FEB, ...

QS-JAN, QS-FEB, ...

BOS-JAN, BOS-FEB, ...

A-JAN, A-FEB, ...

BA-JAN, BA-FEB, ...
AS-JAN, AS-FEB, ...
BAS-JAN, BAS-FEB, ...

Offset Type

Day

BusinessDay

Hour

Minute

Second

Milli

Micro

MonthEnd
BusinessMonthEnd
MonthBegin
BusinessMonthBegin
Week

WeekOfMonth

QuarterEnd

BusinessQuarterEnd

QuarterBegin

BusinessQuarterBegin

YearEnd

BusinessYearEnd
YearBegin

BusinessYearBegin

Users can define their own custom frequency classes to provide date
logic not available in pandas, though the full details of that are outside
< 4|S° the scope of this book.

Description

Calendar daily

Business daily

Hourly

Minutely

Secondly

Millisecond (1/1000th of 1 second)
Microsecond (1/1000000th of 1 second)
Last calendar day of month

Last business day (weekday) of month
First calendar day of month

First weekday of month

Weekly on given day of week: MON, TUE, WED, THU, FRI, SAT,
or SUN.

Generate weekly dates in the first, second, third, or fourth week
of the month. For example, WOM- 3FRI for the 3rd Friday of
each month.

Quarterly dates anchored on last calendar day of each month,
for year ending in indicated month: JAN, FEB, MAR, APR, MAY,
JUN, JUL, AUG, SEP, OCT, NOV, or DEC.

Quarterly dates anchored on last weekday day of each month,
for year ending in indicated month

Quarterly dates anchored on first calendar day of each month,
for year ending in indicated month

Quarterly dates anchored on first weekday day of each month,
for year ending in indicated month

Annual dates anchored on last calendar day of given month:
JAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, or DEC.

Annual dates anchored on last weekday of given month
Annual dates anchored on first day of given month

Annual dates anchored on first weekday of given month

 

300 | Chapter 10: Time Series

Week of month dates

One useful frequency class is “week of month”, starting with WOM. This enables you to
get dates like the third Friday of each month:

In [397]: rng = pd.date_range('1/1/2012', '9/1/2012', freq='WOM-3FRI')

In [398]: list(xrng)

Out [398]:

[<Timestamp: 2012-01-20 :00>,
<Timestamp: 2012-02-17 :00>,
<Timestamp: 2012-03-16 :00>,
<Timestamp: 2012-04-20 :00>,
<Timestamp: 2012-05-18 :00>,
<Timestamp: 2012-06-15 :00>,
<Timestamp: 2012-07-20 :00>,
<Timestamp: 2012-08-17 :00>]

Traders of US equity options will recognize these dates as the standard dates of monthly
expiry.

Shifting (Leading and Lagging) Data

“Shifting” refers to moving data backward and forward through time. Both Series and
DataFrame have a shift method for doing naive shifts forward or backward, leaving
the index unmodified:

In [399]: ts = Series(np.random.randn(4),
aaa 8 : index=pd.date_range('1/1/2000', periods=4, freq='M'))

In [400]: ts In [401]: ts.shift(2) In [402]: ts.shift(-2)
Out [400]: Out[401]: Out [402]:

2000-01-31 0.575283 2000-01-31 NaN 2000-01-31 1.814582
2000-02-29 0.304205 2000-02-29 NaN 2000-02-29 1.634858
2000-03-31 1.814582 2000-03-31 0.575283 2000-03-31 NaN
2000-04-30 1.634858 2000-04-30 0.304205 2000-04-30 NaN
Freq: M Freq: M Freq: M

A common use of shift is computing percent changes in a time series or multiple time
series as DataFrame columns. This is expressed as

ts / ts.shift(1) - 1

Because naive shifts leave the index unmodified, some data is discarded. Thus if the
frequency is known, it can be passed to shift to advance the timestamps instead of
simply the data:

In [403]: ts.shift(2, freq='M')
Out [403]:

2000-03-31 0.575283
2000-04-30 0.304205
2000-05-31 1.814582
2000-06-30 1.634858

Freq: M

 

Date Ranges, Frequencies, and Shifting | 301

Other frequencies can be passed, too, giving you a lot of flexibility in how to lead and

lag the data:
In [404]: ts.shift(3, freq='D') In [405]: ts.shift(1, freq='3D')
Out [404]: Out[405]:
2000-02-03 0.575283 2000-02-03 0.575283
2000-03-03 0.304205 2000-03-03 0.304205
2000-04-03 1.814582 2000-04-03 1.814582
2000-05-03 1.634858 2000-05-03 1.634858
In [406]: ts.shift(1, freq='90T')
Out [406]:

2000-01-31 :00 0.575283
2000-02-29 :00 0.304205
2000-03-31 :00 1.814582
2000-04-30 :00 1.634858

Shifting dates with offsets

The pandas date offsets can also be used with datetime or Timestamp objects:

In [407]:
In [408]:

In [409]:
Out [409]:

from pandas.tseries.offsets import Day, MonthEnd
now = datetime(2011, 11, 17)

now + 3 * Day()
datetime.datetime(2011, 11, 20, 0, 0)

If you add an anchored offset like MonthEnd, the first increment will roll forward a date
to the next date according to the frequency rule:

In [410]:
Out [410]:

In [411]:
Out [411]:

now + MonthEnd()
datetime.datetime(2011, 11, 30, 0, 0)

now + MonthEnd(2)
datetime.datetime(2011, 12, 31, 0, 0)

Anchored offsets can explicitly “roll” dates forward or backward using their rollfor

ward and roll
In [412]:

back methods, respectively:
offset = MonthEnd()

: offset.rollforward(now)
: datetime.datetime(2011, 11, 30, 0, 0)

: offset.rollback(now)
: datetime.datetime(2011, 10, 31, 0, 0)

A clever use of date offsets is to use these methods with groupby:

In [415]:

In [416]:

Out [416]:

 

2000-01-31

ts = Series(np.random.randn(20),
index=pd.date_range('1/15/2000', periods=20, freq='4d'))

ts.groupby(offset.rollforward) .mean()

-0.448874

 

302 | Chapter 10:

: Time Series

2000-02-29  -0.683663
2000-03-31 0.251920

Ofcourse, an easier and faster way to do this is using resample (much more on this later):

In [417]: ts.resample('M', how='mean')
Out [417]:

2000-01-31 -0.448874

2000-02-29 -0.683663

2000-03-31 0.251920

Freq: M

Time Zone Handling

Working with time zones is generally considered one of the most unpleasant parts of
time series manipulation. In particular, daylight savings time (DST) transitions are a
common source of complication. As such, many time series users choose to work with
time series in coordinated universal time or UTC, which is the successor to Greenwich
Mean Time and is the current international standard. Time zones are expressed as
offsets from UTC; for example, New York is four hours behind UTC during daylight
savings time and 5 hours the rest of the year.

In Python, time zone information comes from the 3rd party pytz library, which exposes
the Olson database, a compilation of world time zone information. This is especially
important for historical data because the DST transition dates (and even UTC offsets)
have been changed numerous times depending on the whims of local governments. In
the United States,the DST transition times have been changed many times since 1900!

For detailed information about pytz library, you’ll need to look at that library’s docu-
mentation. As far as this book is concerned, pandas wraps pytz’s functionality so you
can ignore its API outside of the time zone names. Time zone names can be found
interactively and in the docs:

In [418]: import pytz

In [419]: pytz.common_timezones[-5:]
Out[419]: ['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']

To get a time zone object from pytz, use pytz.timezone:
In [420]: tz = pytz.timezone('US/Eastern' )

In [421]: tz
Out[421]: <DstTzInfo 'US/Eastern' EST-1 day, :00 STD>

Methods in pandas will accept either time zone names or these objects. I recommend
just using the names.

 

Time Zone Handling | 303

Localization and Conversion

By default, time series in pandas are time zone naive. Consider the following time series:

rng = pd.date_range('3/9/2012 9:30', periods=6, freq='D')
ts = Series(np.random.randn(len(rng)), index=rng)

The index’s tz field is None:

In [423]: print(ts.index.tz)
None

Date ranges can be generated with a time zone set:

In [424]: pd.date_range('3/9/2012 9:30', periods=10, freq='D', tz='UTC')
Out[ 424]:

<class 'pandas.tseries.index.DatetimeIndex'>

[2012-03-09 :00, ..., 2012-03-18 :00]

Length: 10, Freq: D, Timezone: UTC

Conversion from naive to localized is handled by the tz_localize method:
In [425]: ts_utc = ts.tz_localize('UTC')

In [426]: ts_utc

Out [426]:

2012-03-09 :00+ 0.414615
2012-03-10 :00+ 0.427185
2012-03-11 :00+ 19172557
2012-03-12 :00+ -0.351572
2012-03-13 :00+ 1.454593
2012-03-14 :00+ 2.043319
Freq: D

In [427]: ts_utc.index

Out [427]:

<class 'pandas.tseries.index.DatetimeIndex'>
[2012-03-09 :00, ..., 2012-03-14 :00]
Length: 6, Freq: D, Timezone: UTC

Once a time series has been localized to a particular time zone, it can be converted to
another time zone using tz_convert:

In [428]: ts_utc.tz_convert('US/Eastern' )
Out [428]:

2012-03-09 :00- 0.414615
2012-03-10 :00- 0.427185
2012-03-11 :00- 1.172557
2012-03-12 :00- -0.351572
2012-03-13 :00- 1.454593
2012-03-14 :00- 2.043319
Freq: D

In the case of the above time series, which straddles a DST transition in the US/Eastern
time zone, we could localize to EST and convert to, say, UTC or Berlin time:
In [429]: ts_eastern = ts.tz_localize('US/Eastern' )

 

304 | Chapter 10: Time Series

In [430]: ts_eastern.tz_convert('UTC')
Out [430]:

2012-03-09 :00+ 0.414615
2012-03-10 :00+ 0.427185
2012-03-11 :00+ 1.172557
2012-03-12 :00+ -0.351572
2012-03-13 :00+ 1.454593
2012-03-14 :00+ 2.043319
Freq: D

In [431]: ts_eastern.tz_convert('Europe/Berlin' )
Out [431]:

2012-03-09 :00+ 0.414615

2012-03-10 :00+ 0.427185

2012-03-11 :00+ 1.172557

2012-03-12 :00+ -0.351572

2012-03-13 :00+ 1.454593

2012-03-14 :00+ 2.043319

Freq: D

tz_localize and tz_convert are also instance methods on DatetimeIndex:

In [432]: ts.index.tz_localize('Asia/Shanghai' )
Out [432]:

<class 'pandas.tseries.index.DatetimeIndex'>
[2012-03-09 :00, ..., 2012-03-14 :00]
Length: 6, Freq: D, Timezone: Asia/Shanghai

 

Localizing naive timestamps also checks for ambiguous or non-existent

ta) times around daylight savings time transitions.

 

 

 

Operations with Time Zone—aware Timestamp Objects

Similar to time series and date ranges, individual Timestamp objects similarly can be
localized from naive to time zone-aware and converted from one time zone to another:

In [433]: stamp = pd.Timestamp('2011-03-12 ')
In [434]: stamp_utc = stamp.tz_localize(‘utc')

In [435]: stamp_utc.tz_convert('US/Eastern’ )
Out[435]: <Timestamp: 2011-03-11 :00-0500 EST, tz=US/Eastern>

You can also pass a time zone when creating the Timestamp:

In [436]: stamp_moscow = pd.Timestamp('2011-03-12 ', tz='Europe/Moscow' )

In [437]: stamp_moscow
Out[437]: <Timestamp: 2011-03-12 :00+0300 MSK, tz=Europe/Moscow>

Time zone-aware Timestamp objects internally store a UTC timestamp value as nano-
seconds since the UNIX epoch January 1, 1970); this UTC value is invariant between
time zone conversions:

 

Time Zone Handling | 305

In [438]: stamp_utc.value
Out[438]: 1299902400000000000

In [439]: stamp_utc.tz_convert('US/Eastern').value
Out [439]: 1299902400000000000

When performing time arithmetic using pandas’s DateOffset objects, daylight savings
time transitions are respected where possible:

# 30 minutes before DST transition

In [440]: from pandas.tseries.offsets import Hour

In [441]: stamp = pd.Timestamp('2012-03-12 ', tz='US/Eastern' )

In [442]: stamp
Out[442]: <Timestamp: 2012-03-12 :00-0400 EDT, tz=US/Eastern>

In [443]: stamp + Hour()
Out[443]: <Timestamp: 2012-03-12 :00-0400 EDT, tz=US/Eastern>

# 90 minutes before DST transition
In [444]: stamp = pd.Timestamp('2012-11-04 ', tz='US/Eastern' )

In [445]: stamp
Out[445]: <Timestamp: 2012-11-04 :00-0400 EDT, tz=US/Eastern>

In [446]: stamp + 2 * Hour()
Out[446]: <Timestamp: 2012-11-04 :00-0500 EST, tz=US/Eastern>

 

Operations between Different Time Zones

If two time series with different time zones are combined, the result will be UTC. Since
the timestamps are stored under the hood in UTC, this is a straightforward operation
and requires no conversion to happen:

In [447]: rng = pd.date_range('3/7/2012 9:30', periods=10, freq='B')
In [448]: ts = Series(np.random.randn(len(rng)), index=rng)

In [449]: ts

Out [449]:

2012-03-07 :00 -1.749309
2012-03-08 :00 -0.387235
2012-03-09 :00 -0.208074
2012-03-12 :00 -1.221957
2012-03-13 :00 -0.067460
2012-03-14 :00 0.229005
2012-03-15 :00 -0.576234
2012-03-16 :00 0.816895
2012-03-19 :00 -0.772192
2012-03-20 :00 -1.333576
Freq: B

In [450]: tsa = ts[:7].tz_localize('Europe/London' )

 

306 | Chapter 10: Time Series

In [451]: ts2 = ts1[2:].tz_convert('Europe/Moscow' )
In [452]: result = ts1 + ts2

In [453]: result.index

Out [453]:

<class 'pandas.tseries.index.DatetimeIndex'>
[2012-03-07 :00, ..., 2012-03-15 :00]
Length: 7, Freq: B, Timezone: UTC

Periods and Period Arithmetic

Periods represent time spans, like days, months, quarters, or years. The Period class
represents this data type, requiring a string or integer and a frequency from the above

table:
In [454]: p = pd.Period(2007, freq='A-DEC')

In [455]: p
Out[455]: Period('2007', 'A-DEC')

In this case, the Period object represents the full timespan from January 1, 2007 to
December 31, 2007, inclusive. Conveniently, adding and subtracting integers from pe-
riods has the effect of shifting by their frequency:

In [456]: p +5 In [457]: p - 2
Out[456]: Period('2012', 'A-DEC') Out[457]: Period('2005', 'A-DEC')

If two periods have the same frequency, their difference is the number of units between
them:

In [458]: pd.Period('2014', freq='A-DEC') - p

Out[458]: 7
Regular ranges of periods can be constructed using the period_range function:

In [459]: rng = pd.period_range('1/1/2000', '6/30/2000', freq='M')

In [460]: rng

Out [460]:

<class 'pandas.tseries.period.PeriodIndex'>
freq: M

[2000-01, ..., 2000-06]

length: 6

The PeriodIndex class stores a sequence of periods and can serve as an axis index in
any pandas data structure:

In [461]: Series(np.random.randn(6), index=rng)
Out [461]:

2000-01 -0.309119

2000-02 0.028558

2000-03 1.129605

2000-04 -0.374173

2000-05 -0.011401

 

Periods and Period Arithmetic | 307

2000-06 0.272924
Freq: M

If you have an array of strings, you can also appeal to the PeriodIndex class itself:
In [462]: values = ['200103', '200202', '200301']

In [463]: index = pd.PeriodIndex(values, freq='Q-DEC')

In [464]: index

Out [464]:

<class 'pandas.tseries.period.PeriodIndex'>
freq: Q-DEC

[200103, ..., 200301]

length: 3

Period Frequency Conversion

Periods and PeriodIndex objects can be converted to another frequency using their

asfreq method. As an example, suppose we had an annual period and wanted to convert
it into a monthly period either at the start or end of the year. This is fairly straightfor-
ward:

In [465]: p = pd.Period('2007', freq='A-DEC')
In [466]: p.asfreq('M', how='start') In [467]: p.asfreq('M', how="end')
Out[466]: Period('2007-01', 'M') Out[467]: Period('2007-12', 'M')

You can think of Period('2007', 'A-DEC') as being a cursor pointing to a span of time,
subdivided by monthly periods. See Figure 10-1 for an illustration of this. For a fiscal
year ending on a month other than December, the monthly subperiods belonging are
different:

In [468]: p = pd.Period('2007', freq='A-JUN')
In [469]: p.asfreq('M', 'start') In [470]: p.asfreq('M', ‘end')
Out[469]: Period('2006-07', 'M') Out[470]: Period('2007-06', 'M')

When converting from high to low frequency, the superperiod will be determined de-
pending on where the subperiod “belongs”. For example, in A-JUN frequency, the month
Aug-2007 is actually part of the 2008 period:

In [471]: p = pd.Period('2007-08', 'M')

In [472]: p.asfreq('A-JUN')
Out[472]: Period('2008', 'A-JUN')

Whole PeriodIndex objects or TimeSeries can be similarly converted with the same
semantics:

In [473]: rng = pd.period_range('2006', '2009', freq='A-DEC')
In [474]: ts = Series(np.random.randn(len(rng)), index=rng)

In [475]: ts

 

308 | Chapter 10: Time Series

Out[475]:

2006 -0.601544
2007 0.574265
2008 -0.194115
2009 0.202225

 

   

Freq: A-DEC
In [476]: ts.asfreq('M', how='start' ) In [477]: ts.asfreq('B', how='end')
Out [476]: Out [477]:
2006-01 -0.601544 2006-12-29 -0.601544
2007-01 0.574265 2007-12-31 0.574265
2008-01 -0.194115 2008-12-31 -0.194115
2009-01 0.202225 2009-12-31 0.202225
Freq: M Freq: B
Period('2011-06; 'M')
T 1
an || a] tr | ay] | | Aen Se | 0 | or ec
Period('2011;, 'A-DEC’)

 

 

Figure 10-1. Period frequency conversion illustration

Quarterly Period Frequencies

Quarterly data is standard in accounting, finance, and other fields. Much quarterly data
is reported relative to a fiscal year end, typically the last calendar or business day of one
of the 12 months of the year. As such, the period 201204 has a different meaning de-
pending on fiscal year end. pandas supports all 12 possible quarterly frequencies as 0-
JAN through Q-DEC:

In [478]: p = pd.Period('201204', freq='Q-JAN')

In [479]: p
Out[479]: Period('201204', 'Q-JAN')

In the case of fiscal year ending in January, 201204 runs from November through Jan-
uary, which you can check by converting to daily frequency. See Figure 10-2 for an
illustration:

In [480]: p.asfreq('D', 'start') In [481]: p.asfreq('D', ‘end')
Out[480]: Period('2011-11-01', 'D') Out[481]: Period('2012-01-31', 'D')

 

Periods and Period Arithmetic | 309

 

Thus, it’s possible to do period arithmetic very easily; for example, to get the timestamp
at 4PM on the 2nd to last business day of the quarter, you could do:

In [482]: p4pm = (p.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60

In [483]: p4pm
Out[483]: Period('2012-01-30 ', 'T')

In [484]: p4pm.to_timestamp()
Out [484]: <Timestamp: 2012-01-30 :00>

 

Year 2012
M [JAN | FeB_[ MAR | APR | MAY | JUN } JUL | AUG | seP _[ oct [ Nov | DEC |
Q-DEC
Q-SEP

Q-FEB | 201204 201301 201302 201303

 

 

 

Figure 10-2. Different quarterly frequency conventions

Generating quarterly ranges works as you would expect using period_range. Arithmetic
is identical, too:

In [485]: rng = pd.period_range('201103', '201204', freq='Q-JAN')
In [486]: ts = Series(np.arange(len(rng)), index=rng)

In [487]: ts
Out [487]:

201103 0
201104 1
201201 2
201202 3
201203 4
201204 5
Freq: Q-JAN

In [488]: new_rng = (rng.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60
In [489]: ts.index = new_rng.to timestamp()

In [490]: ts

Out [490]:
2010-10-28 :00
2011-01-28 :00
2011-04-28 :00
2011-07-28 :00
2011-10-28 :00
2012-01-30 :00

UWPWN PO

 

310 | Chapter 10: Time Series

Converting Timestamps to Periods (and Back)

Series and DataFrame objects indexed by timestamps can be converted to periods using

the to_period method:

In [491]: rng = pd.date_range('1/1/2000', periods=3, freq='M')

In [492]: ts = Series(randn(3), index=rng)

In [493]: pts = ts.to_period()

In [494]: ts

Out[ 494]:

2000-01-31 -0.505124
2000-02-29 2.954439
2000-03-31 -2.630247
Freq: M

In [495]: pts
Out[495]:

2000-01 -0.505124
2000-02 2.954439
2000-03 -2.630247
Freq: M

Since periods always refer to non-overlapping timespans, a timestamp can only belong
to a single period for a given frequency. While the frequency of the new PeriodIndex is
inferred from the timestamps by default, you can specify any frequency you want. There
is also no problem with having duplicate periods in the result:

In [496]: rng = pd.date_range('1/29/2000', periods=6, freq='D')

In [497]: ts2 = Series(randn(6), index=rng)

In [498]: ts2.to_period('M')

Out [498]:

2000-01 -0.352453
2000-01 -0.477808
2000-01 0.161594
2000-02 1.686833
2000-02 0.821965
2000-02 -0.667406
Freq: M

To convert back to timestamps, use to_timestamp:

In [499]: pts = ts.to_period()

In [500]: pts

Out[ 500]:

2000-01 -0.505124
2000-02 2.954439
2000-03 -2.630247
Freq: M

In [501]: pts.to_timestamp(how='end' )

Out[501]:

2000-01-31 -0.505124
2000-02-29 2.954439
2000-03-31 -2.630247
Freq: M

 

Periods and Period Arithmetic | 311

Creating a PeriodIndex from Arrays

Fixed frequency data sets are sometimes stored with timespan information spread
across multiple columns. For example, in this macroeconomic data set, the year and
quarter are in different columns:

In [502]: data = pd.read_csv('ch08/macrodata.csv' )

In [503]: data.year In [504]: data.quarter
Out [503]: Out[504]:

0 1959 0 )

1 1959 1 2

2 1959 2 3

3 1959 3 4

199 2008 199 4

200 2009 200 1

201 2009 201 2

202 2009 202 3

Name: year, Length: 203 Name: quarter, Length: 203

By passing these arrays to PeriodIndex with a frequency, they can be combined to form
an index for the DataFrame:

In [505]: index = pd.PeriodIndex(year=data.year, quarter=data.quarter, freq='Q-DEC')

In [506]: index

Out [506]:

<class 'pandas.tseries.period.PeriodIndex'>
freq: Q-DEC

[195901, ..., 200903]

length: 203

In [507]: data.index = index

In [508]: data.infl

Out [508]:

195901 0.00
195902 2.34
195903 2.74
195904 0.27
200804 -8.79
200901 0.94
200902 3.37
200903 3.56

Freq: Q-DEC, Name: infl, Length: 203

Resampling and Frequency Conversion

Resampling refers to the process of converting a time series from one frequency to
another. Aggregating higher frequency data to lower frequency is called downsam-
pling, while converting lower frequency to higher frequency is called upsampling. Not

 

312 | Chapter 10: Time Series

all resampling falls into either of these categories; for example, converting W-WED (weekly
on Wednesday) to W-FRI is neither upsampling nor downstampling.

pandas objects are equipped with a resample method, which is the workhorse function
for all frequency conversion:

In [509]: rng = pd.date_range('1/1/2000', periods=100, freq='D')

In [510]: ts

In [511]: ts.

Out[511]:
2000-01-31
2000-02-29
2000-03-31
2000-04-30
Freq: M

In [512]: ts.

Out [512]:
2000-01
2000-02
2000-03
2000-04
Freq: M

oooo°o

= Series(randn(len(rng)), index=rng)
resample('M', how='mean')

0.170876

0.165020

0.095451
0.363566

resample('M', how='mean', kind='period' )

- 170876
- 165020
-095451
- 363566

resample is a flexible and high-performance method that can be used to process very
large time series. I’ll illustrate its semantics and use through a series of examples.

Table 10-5. Resample method arguments

Argument Description

freq String or DateOffset indicating desired resampled frequency, e.g. ‘M’, ‘Smin’, or Sec
ond(15)

how='mean' Function name or array function producing aggregated value, for example ‘mean’,
"ohlc', np.max. Defaults to ‘mean’. Other common values: 'first', ‘last’,
‘median’, ‘ohlc', 'max', ‘min’.

axis=0 Axis to resample on, default axis=0

fi11_method=None How to interpolate when upsampling, as in 'fi11' or 'bfil1'. By default does no

closed='right'

label='right'

loffset=None

limit=None

interpolation.

In downsampling, which end of each interval is closed (inclusive), ‘right' or
‘left’. Defaults to 'right'

In downsampling, how to label the aggregated result, with the 'right' or ‘left’
bin edge. For example, the 9:30 to 9:35 5-minute interval could be labeled 9 : 30 or
9:35. Defaults to 'right' (or 9:35, in this example).

Time adjustment to the bin labels, such as '-1s' /Second(-1) to shift the aggregate
labels one second earlier

When forward or backward filling, the maximum number of periods to fill

 

Resampling and Frequency Conversion | 313

Argument Description

kind=None Aggregate to periods ("period') or timestamps (‘timestamp '); defaults to kind of
index the time series has

convention=None When resampling periods, the convention (‘start' or 'end') for converting the low
frequency period to high frequency. Defaults to 'end'

 

Downsampling

Aggregating data to a regular, lower frequency is a pretty normal time series task. The
data you’re aggregating doesn’t need to be fixed frequently; the desired frequency de-
fines bin edges that are used to slice the time series into pieces to aggregate. For example,
to convert to monthly, 'M' or 'BM', the data need to be chopped up into one month
intervals. Each interval is said to be half-open; a data point can only belong to one
interval, and the union of the intervals must make up the whole time frame. There are
a couple things to think about when using resample to downsample data:

¢ Which side of each interval is closed

* How to label each aggregated bin, either with the start of the interval or the end

To illustrate, let’s look at some one-minute data:
In [513]: rng = pd.date_range('1/1/2000', periods=12, freq='T')

In [514]: ts = Series(np.arange(12), index=rng)

In [515]: ts
Out[515]:
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
2000-01-01 :00
Freq: T

COON DURBPWNR OO

Be
Bp

Suppose you wanted to aggregate this data into five-minute chunks or bars by taking
the sum of each group:

In [516]: ts.resample('5min', how='sum' )
Out [516]:

2000-01-01 :00 0

2000-01-01 :00 45

2000-01-01 :00 40

2000-01-01 :00 11

Freq: 5T

 

314 | Chapter 10: Time Series

The frequency you pass defines bin edges in five-minute increments. By default, the
right bin edge is inclusive, so the  value is included in the  to  inter-
val.1 Passing closed='left' changes the interval to be closed on the left:

In [517]: ts.resample('5min', how='sum', closed='left')
Out[517]:

2000-01-01 :00 10

2000-01-01 :00 35

2000-01-01 :00 21

Freq: 5T

As you can see, the resulting time series is labeled by the timestamps from the right side
of each bin. By passing label="left' you can label them with the left bin edge:

In [518]: ts.resample('5min', how='sum', closed='left', label='left')
Out [518]:

2000-01-01 :00 10

2000-01-01 :00 35

2000-01-01 :00 21

Freq: 5T

See Figure 10-3 for an illustration of minutely data being resampled to five-minute.

 

closed=" tert’ [OO] 901 [ 502 | 905 | 504] 905 |

 

closed="right" [300 [901 | 502 | 905 | 504 | 985 |

label='left' label='right'

 

 

 

 

Figure 10-3. 5-minute resampling illustration of closed, label conventions

Lastly, you might want to shift the result index by some amount, say subtracting one
second from the right edge to make it more clear which interval the timestamp refers
to. To do this, pass a string or date offset to loffset:

In [519]: ts.resample('5min', how='sum', loffset='-1s')
Out [519]:

1999-12-31 :59 0)

2000-01-01 :59 15

2000-01-01 :59 40

2000-01-01 :59 11

Freq: 5T

1. The choice of closed='right', label='right' as the default might seem a bit odd to some users. In
practice the choice is somewhat arbitrary; for some target frequencies, closed='left' is preferable, while
for others closed='right' makes more sense. The important thing is that you keep in mind exactly how
you are segmenting the data.

 

Resampling and Frequency Conversion | 315

This also could have been accomplished by calling the shift method on the result
without the loffset.

Open-High-Low-Close (OHLC) resampling

In finance, an ubiquitous way to aggregate a time series is to compute four values for
each bucket: the first (open), last (close), maximum (high), and minimal (low) values.
By passing how='ohlc' you will obtain a DataFrame having columns containing these
four aggregates, which are efficiently computed in a single sweep of the data:

In [520]: ts.resample('5min', how='ohlc')

Out [520]:
open high low close
2000-01-01 :00 0 0 0 0
2000-01-01 :00 1 5 1 5
2000-01-01 :00 6 10 6 10
2000-01-01 :00 11 11 11 11
Resampling with GroupBy

An alternate way to downsample is to use pandas’s groupby functionality. For example,
you can group by month or weekday by passing a function that accesses those fields
on the time series’s index:

In [521]: rng = pd.date_range('1/1/2000', periods=100, freq='D')
In [522]: ts = Series(np.arange(100), index=rng)

In [523]: ts.groupby(lambda x: x.month).mean()
Out [523]:

i 15

2 45

3 75

4 95

n [524]: ts.groupby(lambda x: x.weekday) .mean()

I

01

0 47.5
1 48.5
2 49.5
3 50.5
4 51.5
5 49.0
6 50.0

Upsampling and Interpolation

When converting from a low frequency toa higher frequency, no aggregation is needed.
Let’s consider a DataFrame with some weekly data:
In [525]: frame = DataFrame(np.random.randn(2, 4),

wommsed index=pd.date_range('1/1/2000', periods=2, freq='W-WED'),
scone ad columns=['Colorado', 'Texas', 'New York', 'Ohio'])

 

316 | Chapter 10: Time Series

In [526]: frame[:5]
Out [526]:

Colorado Texas New York Ohio
2000-01-05 -0.609657 -0.268837 0.195592 0.85979
2000-01-12 -0.263206 1.141350 -0.101937 -0.07666

When resampling this to daily frequency, by default missing values are introduced:
In [527]: df_daily = frame.resample('D')

In [528]: df_daily

Out [528]:

Colorado Texas New York Ohio
2000-01-05 -0.609657 -0.268837 0.195592 0.85979
2000-01-06 NaN NaN NaN NaN
2000-01-07 NaN NaN NaN NaN
2000-01-08 NaN NaN NaN NaN
2000-01-09 NaN NaN NaN NaN
2000-01-10 NaN NaN NaN NaN
2000-01-11 NaN NaN NaN NaN

2000-01-12 -0.263206 1.141350 -0.101937 -0.07666

Suppose you wanted to fill forward each weekly value on the non-Wednesdays. The
same filling or interpolation methods available in the fillna and reindex methods are
available for resampling:

In [529]: frame.resample('D', fill_method='ffil1')

Out[529]:

Colorado Texas New York Ohio
2000-01-05 -0.609657 -0.268837 0.195592 0.85979
2000-01-06 -0.609657 -0.268837 0.195592 0.85979
2000-01-07 -0.609657 -0.268837 0.195592 0.85979
2000-01-08 -0.609657 -0.268837 0.195592 0.85979
2000-01-09 -0.609657 -0.268837 0.195592 0.85979
2000-01-10 -0.609657 -0.268837 0.195592 0.85979
2000-01-11 -0.609657 -0.268837 0.195592 0.85979
2000-01-12 -0.263206 1.141350 -0.101937 -0.07666

You can similarly choose to only fill a certain number of periods forward to limit how
far to continue using an observed value:

In [530]: frame.resample('D', fill_method='ffill', limit=2)
Out[530]:

Colorado Texas New York Ohio
2000-01-05 -0.609657 -0.268837 0.195592 0.85979
2000-01-06 -0.609657 -0.268837 0.195592 0.85979
2000-01-07 -0.609657 -0.268837 0.195592 0.85979

2000-01-08 NaN NaN NaN NaN
2000-01-09 NaN NaN NaN NaN
2000-01-10 NaN NaN NaN NaN
2000-01-11 NaN NaN NaN NaN

2000-01-12 -0.263206 1.141350 -0.101937 -0.07666

Notably, the new date index need not overlap with the old one at all:

 

Resampling and Frequency Conversion | 317

In [531]: frame.resample('W-THU', fill_method='ffill')
Out [531]:

Colorado Texas New York Ohio
2000-01-06 -0.609657 -0.268837 0.195592 0.85979
2000-01-13 -0.263206 1.141350 -0.101937 -0.07666

Resampling with Periods

Resampling data indexed by periods is reasonably straightforward and works as you
would hope:

In [532]: frame = DataFrame(np.random.randn(24, 4),
wees : index=pd.period_range('1-2000', '12-2001', freq='M'),
sere a columns=['Colorado', 'Texas', 'New York', ‘Ohio'])

In [533]: frame[:5]
Out [533]:

Colorado Texas New York Ohio
2000-01 0.120837 1.076607 0.434200 0.056432
2000-02 -0.378890 0.047831 0.341626 1.567920
2000-03 -0.047619 -0.821825 -0.179330 -0.166675
2000-04 0.333219 -0.544615 -0.653635 -2.311026
2000-05 1.612270 -0.806614 0.557884 0.580201

In [534]: annual_frame = frame.resample('A-DEC', how='mean' )

In [535]: annual_frame
Out[535]:

Colorado Texas New York Ohio
2000 0.352070 -0.553642 0.196642 -0.094099
2001 0.158207 0.042967 -0.360755 0.184687

Upsampling is more nuanced as you must make a decision about which end of the
timespan in the new frequency to place the values before resampling, just like the
asfreq method. The convention argument defaults to 'end' but can also be ‘start’:

# Q-DEC: Quarterly, year ending in December
In [536]: annual_frame.resample('Q-DEC', fill_method='f#fil1')
Out [536]:
Colorado Texas New York Ohio
200004 0.352070 -0.553642 0.196642 -0.094099
200101 0.352070 -0.553642 0.196642 -0.094099
200102 0.352070 -0.553642 0.196642 -0.094099
200103 0.352070 -0.553642 0.196642 -0.094099
200104 0.158207 0.042967 -0.360755 0.184687

In [537]: annual_frame.resample('Q-DEC', fill_method='ffill', convention='start')
Out [537]:
Colorado Texas New York Ohio
200001 0.352070 -0.553642 0.196642 -0.094099
200002 0.352070 -0.553642 0.196642 -0.094099
200003 0.352070 -0.553642 0.196642 -0.094099
200004 0.352070 -0.553642 0.196642 -0.094099
200101 0.158207 0.042967 -0.360755 0.184687

 

318 | Chapter 10: Time Series

Since periods refer to timespans, the rules about upsampling and downsampling are
more rigid:

¢ Indownsampling, the target frequency must be a subperiod of the source frequency.

¢ Inupsampling, the target frequency must be a superperiod of the source frequency.

If these rules are not satisfied, an exception will be raised. This mainly affects the quar-
terly, annual, and weekly frequencies; for example, the timespans defined by Q-MAR only
line up with A-MAR, A-JUN, A-SEP, and A-DEC:

In [538]: annual_frame.resample('Q-MAR', fi11_method='f#fil11')

Out [538]:

Colorado Texas New York Ohio

200103 0.352070 -0.553642 0.196642 -0.094099

200104 0.352070 -0.553642 0.196642 -0.094099

200201 0.352070 -0.553642 0.196642 -0.094099

200202 0.352070 -0.553642 0.196642 -0.094099

200203 0.158207 0.042967 -0.360755 0.184687

Time Series Plotting

Plots with pandas time series have improved date formatting compared with matplotlib
out of the box. As an example, I downloaded some stock price data on a few common
US stock from Yahoo! Finance:

In [539]: close_px_all = pd.read_csv('cho9/stock_px.csv', parse_dates=True, index_col=0)
In [540]: close px = close px_all[['AAPL', 'MSFT', 'XOM']]
In [541]: close px = close px.resample('B', fill_method='ffil1')

In [542]: close_px

Out[542]:

<class 'pandas.core.frame.DataFrame' >

DatetimeIndex: 2292 entries, 2003-01-02 :00 to 2011-10-14 :00
Freq: B

Data columns:

AAPL 2292 non-null values

MSFT 2292 non-null values

XOM 2292 non-null values

dtypes: float64(3)

Calling plot on one of the columns grenerates a simple plot, seen in Figure 10-4.

In [544]: close_px['AAPL'].plot()
When called on a DataFrame, as you would expect, all of the time series are drawn on
a single subplot with a legend indicating which is which. P’ll plot only the year 2009

data so you can see how both months and years are formatted on the X axis; see
Figure 10-5.

In [546]: close_px.ix['2009'].plot()

 

Time Series Plotting | 319

 

     

   
     

   

 

1 i 1 1 1 1 i
2004 2005 2006 2007 2008 2009 2010 2011

 

 

 

Figure 10-4. AAPL Daily Price

 

250

200

150

 

 

 

 

Figure 10-5. Stock Prices in 2009
In [548]: close_px['AAPL'].ix['01-2011': '03-2011'].plot()

Quarterly frequency data is also more nicely formatted with quarterly markers, some-
thing that would be quite a bit more work to do by hand. See Figure 10-7.

In [550]: appl_q = close _px['AAPL'].resample('Q-DEC', fill_method='ffill')

In [551]: appl_q.ix['2009':].plot()
A last feature of time series plotting in pandas is that by right-clicking and dragging to
zoom in and out, the dates will be dynamically expanded or contracted and reformat-

ting depending on the timespan contained in the plot view. This is of course only true
when using matplotlib in interactive mode.

Moving Window Functions

A common class of array transformations intended for time series operations are sta-
tistics and other functions evaluated over a sliding window or with exponentially de-

 

320 | Chapter 10: Time Series

 

 

 

365
360F
355 [one
350
345
a9 |Z ivrvceenrseninvann

325

 

 

Jan Feb Mar
2011

 

 

 

Figure 10-6. Apple Daily Price in 1/2011-3/2011

 

 

 

5O7 Q2 Q3 Q4 Ql Q2 Q3 Q4 Ql Q2 Q3 Q4
2009 2010 2011

 

 

 

Figure 10-7. Apple Quarterly Price 2009-2011

caying weights. I call these moving window functions, even though it includes functions
without a fixed-length window like exponentially-weighted moving average. Like other
statistical functions, these also automatically exclude missing data.

rolling mean is one of the simplest such functions. It takes a TimeSeries or DataFrame
along with a window (expressed as a number of periods):

In [555]: close px.AAPL.plot()
Out[555]: <matplotlib.axes.AxesSubplot at 0x1099b3990>

In [556]: pd.rolling mean(close_px.AAPL, 250).plot()

See Figure 10-8 for the plot. By default functions like rolling mean require the indicated
number of non-NA observations. This behavior can be changed to account for missing
data and, in particular, the fact that you will have fewer than window periods of data at
the beginning of the time series (see Figure 10-9):

 

Moving Window Functions | 321

In [558]: appl_std250 = pd.rolling std(close_px.AAPL, 250, min_periods=10)

In [559]: appl_std250[5:12]

Out[559]:

2003-01-09 NaN
2003-01-10 NaN
2003-01-13 NaN
2003-01-14 NaN

2003-01-15 0.077496
2003-01-16 0.074760
2003-01-17 0.112368
Freq: B

In [560]: appl_std250.plot()

 

100

        
   

 

 

i L i 1 L L i i
2004 2005 2006 2007 2008 2009 2010 2011

 

 

Figure 10-8. Apple Price with 250-day MA

 

40
35 pe

BO veo eceesceseesdess eee seeees tees efevessestsstessiesbosissseseseen feoocuersss ved
25
BO | evvesereneeeee i
15

10

2004 2005 2006 2007 2008 2009 2010 2011

 

 

 

Figure 10-9. Apple 250-day daily return standard deviation

To compute an expanding window mean, you can see that an expanding window is just
a special case where the window is the length of the time series, but only one or more
periods is required to compute a value:

 

322 | Chapter 10: Time Series

 

# Define expanding mean in terms of rolling mean
In [561]: expanding mean = lambda x: rolling mean(x, len(x), min_periods=1)

Calling rolling mean and friends on a DataFrame applies the transformation to each
column (see Figure 10-10):

In [563]: pd.rolling mean(close px, 60).plot(logy=True)

 

 

10°

10?

10°

 

 

10°

2004 2005

2006 2007 2008 2009 2010 2011

 

 

Figure 10-10. Stocks Prices 60-day MA (log Y-axis)

See Table 10-6 for a listing of related functions in pandas.

Table 10-6. Moving window and exponentially-weighted functions

Function

rolling count
rolling sum
rolling mean
rolling median

rolling var, rolling std

rolling skew, rolling kurt
rolling min, rolling max
rolling quantile

rolling corr, rolling cov
rolling apply

ewma

ewmvar, ewmstd

ewmcorr, ewmcov

Description

Returns number of non-NA observations in each trailing window.
Moving window sum.

Moving window mean.

Moving window median.

Moving window variance and standard deviation, respectively. Uses n - 1 denom-
inator.

Moving window skewness (3rd moment) and kurtosis (4th moment), respectively.
Moving window minimum and maximum.

Moving window score at percentile/sample quantile.

Moving window correlation and covariance.

Apply generic array function over a moving window.

Exponentially-weighted moving average.

Exponentially-weighted moving variance and standard deviation.

Exponentially-weighted moving correlation and covariance.

 

Moving Window Functions | 323

bottleneck, a Python library by Keith Goodman, provides an alternate
implementation of NaN-friendly moving window functions and may be
“12° worth looking at depending on your application.

 
 

 

Exponentially-weighted functions

An alternative to using a static window size with equally-weighted observations is to
specify a constant decay factor to give more weight to more recent observations. In
mathematical terms, if ma, is the moving average result at time t and x is the time series
in question, each value in the result is computed as ma, = a * ma;,_; + (a- 1) *x_,, where
a is the decay factor. There are a couple of ways to specify the decay factor, a popular
one is using a span, which makes the result comparable to a simple moving window
function with window size equal to the span.

Since an exponentially-weighted statistic places more weight on more recent observa-
tions, it “adapts” faster to changes compared with the equal-weighted version. Here’s
an example comparing a 60-day moving average of Apple’s stock price with an EW
moving average with span=60 (see Figure 10-11):

fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True,
figsize=(12, 7))

aapl_px = close_px.AAPL['2005':'2009' ]

ma60

= pd.rolling mean(aapl_px, 60, min_periods=50)
ewma60 =

pd.ewma(aapl_px, span=60)

aapl_px.plot(style='k-', ax=axes[0])
ma60.plot(style='k--', ax=axes[0])
aapl_px.plot(style='k-', ax=axes[1]
ewma60.plot(style='k--', ax=axes[1]
axes[0].set_title('Simple MA')

axes[1].set_title('Exponentially-weighted MA’)

)
)

Binary Moving Window Functions

Some statistical operators, like correlation and covariance, need to operate on two time
series. As an example, financial analysts are often interested in a stock’s correlation to
a benchmark index like the S@P 500. We can compute that by computing the percent
changes and using rolling corr (see Figure 10-12):

In [569]: spx_px = close_px_all['SPX']
In [570]: spx_rets = spx_px / spx_px.shift(1) - 1
In [571]: returns = close_px.pct_change()

In [572]: corr = pd.rolling corr(returns.AAPL, spx_rets, 125, min_periods=100)

 

324 | Chapter 10: Time Series

 

Simple MA

250

 

 

 

Bos 2006 2007 2008 2009
Exponentially-weighted MA

 

 

 

 

 

Bos 2006 2007 2008 2009

 

 

 

Figure 10-11. Simple moving average versus exponentially-weighted

 

0.9
0.8} - i
0.7

0.5

0.4

0.3

0.2

0.1

 

i n L ts js i 1 n
2004 2005 2006 2007 2008 2009 2010 2011

 

 

 

Figure 10-12. Six-month AAPL return correlation to S&P 500

In [573]: corr.plot()

Suppose you wanted to compute the correlation of the S@P 500 index with many stocks
at once. Writing a loop and creating a new DataFrame would be easy but maybe get
repetitive, so if you pass a TimeSeries and a DataFrame, a function like rolling corr
will compute the correlation of the TimeSeries (spx_rets in this case) with each column
in the DataFrame. See Figure 10-13 for the plot of the result:

In [575]: corr = pd.rolling corr(returns, spx_rets, 125, min_periods=100)

In [576]: corr.plot()

 

Moving Window Functions | 325

 

 

2004 2005 2006 2007 2008 2009 2010 2011

 

 

 

Figure 10-13. Six-month return correlations to S&P 500

 

95
a
BG sisseonemint

rie}

   

ae 2004 2005 2006 2007 2008 2009 2010 2011

 

 

 

Figure 10-14. Percentile rank of 2% AAPL return over 1 year window

User-Defined Moving Window Functions

The rolling apply function provides a means to apply an array function of your own
devising over a moving window. The only requirement is that the function produce a
single value (a reduction) from each piece of the array. For example, while we can
compute sample quantiles using rolling quantile, we might be interested in the per-
centile rank of a particular value over the sample. The scipy.stats.percentileof
score function does just this:

In [578]: from scipy.stats import percentileofscore
In [579]: score_at_2percent = lambda x: percentileofscore(x, 0.02)
In [580]: result = pd.rolling apply(returns.AAPL, 250, score_at_2percent)

In [581]: result.plot()

 

326 | Chapter 10: Time Series

Performance and Memory Usage Notes

Timestamps and periods are represented as 64-bit integers using NumPy’s date
time64 dtype. This means that for each data point, there is an associated 8 bytes of
memory per timestamp. Thus, a time series with 1 million float64 data points has a
memory footprint of approximately 16 megabytes. Since pandas makes every effort to
share indexes among time series, creating views on existing time series do not cause
any more memory to be used. Additionally, indexes for lower frequencies (daily and
up) are stored in a central cache, so that any fixed-frequency index is a view on the date
cache. Thus, if you have a large collection of low-frequency time series, the memory
footprint of the indexes will not be as significant.

Performance-wise, pandas has been highly optimized for data alignment operations
(the behind-the-scenes work of differently indexed ts1 + ts2) and resampling. Here is
an example of aggregating 1OMM data points to OHLC:

In [582]: rng = pd.date_range('1/1/2000', periods=10000000, freq='10ms' )

In [583]: ts = Series(np.random.randn(len(rng)), index=rng)

In [584]: ts
Out [584]:
2000-01-01 :00 -1.402235

2000-01-01 :00.010000 2.424667
2000-01-01 :00.020000 -1.956042
2000-01-01 :00.030000 -0.897339

2000-01-02 :39.960000

0.495530
2000-01-02 :39.970000 0.574766
2000-01-02 :39.980000 1.348374
2000-01-02 :39.990000 0.665034

Freq: 10L, Length: 10000000

In [585]: ts.resample('15min', how='ohlc')
Out [585]:

<class 'pandas.core.frame.DataFrame' >
DatetimeIndex: 113 entries, 2000-01-01 :00 to 2000-01-02 :00
Freq: 15T

Data columns:

open 113. non-null values

high 113 non-null values

low 113. non-null values

close 113. non-null values

dtypes: float64(4)

In [586]: %timeit ts.resample('15min', how='ohlc')
10 loops, best of 3: 61.1 ms per loop

The runtime may depend slightly on the relative size of the aggregated result; higher
frequency aggregates unsurprisingly take longer to compute:

In [587]: rng = pd.date_range('1/1/2000', periods=10000000, freq='1s')

 

Performance and Memory Usage Notes | 327

In [588]: ts = Series(np.random.randn(len(rng)), index=rng)

In [589]: %timeit ts.resample('15s', how='ohlc')
1 loops, best of 3: 88.2 ms per loop

It’s possible that by the time you read this, the performance of these algorithms may
be even further improved. As an example, there are currently no optimizations for
conversions between regular frequencies, but that would be fairly straightforward to do.

 

328 | Chapter 10: Time Series

CHAPTER 11
Financial and Economic Data
Applications

 

The use of Python in the financial industry has been increasing rapidly since 2005, led
largely by the maturation of libraries (like NumPy and pandas) and the availability of
skilled Python programmers. Institutions have found that Python is well-suited both
as an interactive analysis environment as well as enabling robust systems to be devel-
oped often in a fraction of the time it would have taken in Java or C++. Python is also
an ideal glue layer; it is easy to build Python interfaces to legacy libraries built in C or
C++.

While the field of financial analysis is broad enough to fill an entire book, I hope to
show you how the tools in this book can be applied to a number of specific problems
in finance. As with other research and analysis domains, too much programming effort
is often spent wrangling data rather than solving the core modeling and research prob-
lems. I personally got started building pandas in 2008 while grappling with inadequate
data tools.

In these examples, I’ll use the term cross-section to refer to data at a fixed point in time.
For example, the closing prices of all the stocks in the S@P 500 index on a particular
date form a cross-section. Cross-sectional data at multiple points in time over multiple
data items (for example, prices together with volume) form a panel. Panel data can
either be represented as a hierarchically-indexed DataFrame or using the three-dimen-
sional Panel pandas object.

Data Munging Topics

Many helpful data munging tools for financial applications are spread across the earlier
chapters. Here I’ll highlight a number of topics as they relate to this problem domain.

 

329

Time Series and Cross-Section Alignment

One of the most time-consuming issues in working with financial data is the so-called
data alignment problem. Two related time series may have indexes that don’t line up
perfectly, or two DataFrame objects might have columns or row labels that don’t match.
Users of MATLAB, R, and other matrix-programming languages often invest significant
effort in wrangling data into perfectly aligned forms. In my experience, having to align
data by hand (and worse, having to verify that data is aligned) is a far too rigid and
tedious way to work. It is also rife with potential for bugs due to combining misaligned
data.

pandas take an alternate approach by automatically aligning data in arithmetic opera-
tions. In practice, this grants immense freedom and enhances your productivity. As an
example, let’s consider a couple of DataFrames containing time series of stock prices
and volume:

In [16]: prices
Out [16]:

AAPL JNJ SPX XOM
2011-09-06 379.74 64.64 1165.24 71.15
2011-09-07 383.93 65.43 1198.62 73.65
2011-09-08 384.14 64.95 1185.90 72.82
2011-09-09 377.48 63.64 1154.23 71.01
2011-09-12 379.94 63.59 1162.27 71.84
2011-09-13 384.62 63.61 1172.87 71.65
2011-09-14 389.30 63.73 1188.68 72.64

In [17]: volume
Out[17]:

AAPL INI XOM
2011-09-06 18173500 15848300 25416300
2011-09-07 12492000 10759700 23108400
2011-09-08 14839800 15551500 22434800
2011-09-09 20171900 17008200 27969100
2011-09-12 16697300 13448200 26205800

Suppose you wanted to compute a volume-weighted average price using all available
data (and making the simplifying assumption that the volume data is a subset of the
price data). Since pandas aligns the data automatically in arithmetic and excludes
missing data in functions like sum, we can express this concisely as:

In [18]: prices * volume
Out [18]:

AAPL JNJ SPX XOM
2011-09-06 6901204890 1024434112 NaN 1808369745
2011-09-07 4796053560 704007171 NaN 1701933660
2011-09-08 5700560772 1010069925 NaN 1633702136
2011-09-09 7614488812 1082401848 NaN 1986085791
2011-09-12 6343972162 855171038 NaN 1882624672
2011-09-13 NaN NaN NaN NaN
2011-09-14 NaN NaN NaN NaN

In [19]: vwap = (prices * volume).sum() / volume.sum()

 

330 | Chapter 11: Financial and Economic Data Applications

In [20]: vwap In [21]: vwap.dropna()
Out[20] Out [21]:

AAPL 380.655181 AAPL 380.655181

INI 64.394769 INI 64.394769
SPX NaN XOM 72..024288
XOM 72..024288

Since SPX wasn’t found in volume, you can choose to explicitly discard that at any point.
Should you wish to align by hand, you can use DataFrame’s align method, which
returns a tuple of reindexed versions of the two objects:

In [22]: prices.align(volume, join='inner')

Out [22]:
( AAPL
2011-09-06 379.74
2011-09-07 383.93
2011-09-08 384.14
2011-09-09 377.48
2011-09-12 379.94
AAP
18173500
12492000
14839800
20171900
16697300

2011-09-06
2011-09-07
2011-09-08
2011-09-09
2011-09-12

INI

64.64
65.43
64.95
63.64
63.59
L

15848300
10759700
15551500
17008200
13448200

XOM
71.15
73.65
72.82
71.01
71.84,

JNJ XOM
25416300
23108400
22434800
27969100
26205800)

Another indispensable feature is constructing a DataFrame from a collection of poten-
tially differently indexed Series:

In [23]: s1 = Series(range(3), index=['a', 'b', 'c'])
In [24]: s2 = Series(range(4), index=['d', 'b', 'c', ‘e'])
In [25]: s3 = Series(range(3), index=['f', 'a', 'c'])
In [26]: DataFrame({'one': s1, 'two': s2, ‘three’: s3})
Out [26]:

one three two
a 0 1 NaN
b 1 NaN 1
c 2 2 2
d NaN NaN 0
e NaN NaN 3
* NaN oO NaN

As you have seen earlier, you can of course specify explicitly the index of the result,
discarding the rest of the data:

In [27]: DataFrame({'one': s1, 'two': s2, ‘three’: s3}, index=list('face'))

Out [27]:

one three two
f NaN O NaN
a 0 1 NaN
Cc 2 2 2
e NaN NaN 3

 

Data Munging Topics | 331

Operations with Time Series of Different Frequencies

Economic time series are often of annual, quarterly, monthly, daily, or some other more
specialized frequency. Some are completely irregular; for example, earnings revisions
for a stock may arrive at any time. The two main tools for frequency conversion and
realignment are the resample and reindex methods. resample converts data to a fixed
frequency while reindex conforms data to a new index. Both support optional inter-
polation (such as forward filling) logic.

Let’s consider a small weekly time series:
In [28]: ts1 =

In [29]: ts1

Out [29]:

2012-06-13
2012-06-20
2012-06-27

Freq: W-WED

“1s
0.
-0.

Series (np.random.randn(3),

index=pd.date_range('2012-6-13', periods=3, freq='W-WED'))

124801
469004
117439

If you resample this to business daily (Monday-Friday) frequency, you get holes on the
days where there is no data:

In [30]: ts1.resample('B')

Out [30]:

2012-06-13
2012-06-14
2012-06-15
2012-06-18
2012-06-19
2012-06-20
2012-06-21
2012-06-22
2012-06-25
2012-06-26
2012-06-27
Freq: B

21,

0.

-0.

124801
NaN
NaN
NaN
NaN

469004
NaN
NaN
NaN
NaN

117439

Of course, using 'ffill' as the fill_method forward fills values in those gaps. This is
a common practice with lower frequency data as you compute a time series of values
on each timestamp having the latest valid or “as of” value:

In [31]: ts1.resample('B', fill_method='f#fil1')

Out [31]:

2012-06-13
2012-06-14
2012-06-15
2012-06-18
2012-06-19
2012-06-20
2012-06-21
2012-06-22
2012-06-25
2012-06-26

-1

-1

0

0
0
0
0

-124801
“1.
“1.
-124801
“1.
- 469004
- 469004
- 469004
- 469004
- 469004

124801
124801

124801

 

332 | Chapter 11: Financial and Economic Data Applications

2012-06-27 -0.117439
Freq: B

In practice, upsampling lower frequency data to a higher, regular frequency is a fine
solution, but in the more general irregular time series case it may be a poor fit. Consider
an irregularly sampled time series from the same general time period:

In [32]: dates = pd.DatetimeIndex(['2012-6-12', '2012-6-17', '2012-6-18',
semen § "2012-6-21', '2012-6-22', '2012-6-29'])

In [33]: ts2 = Series(np.random.randn(6), index=dates)

In [34]: ts2

Out [34]:

2012-06-12 -0.449429
2012-06-17 0.459648
2012-06-18  -0.172531
2012-06-21 0.835938
2012-06-22 -0.594779
2012-06-29 0.027197

If you wanted to add the “as of” values in ts1 (forward filling) to ts2. One option would
be to resample both to a regular frequency then add, but if you want to maintain the
date index in ts2, using reindex is a more precise solution:

In [35]: ts1.reindex(ts2.index, method='ffill')
Out [35]:

2012-06-12 NaN

2012-06-17 -1.124801

2012-06-18 -1.124801

2012-06-21 0.469004

2012-06-22 0.469004

2012-06-29 -0.117439

In [36]: ts2 + ts1.reindex(ts2.index, method='ffill')
Out [36]:

2012-06-12 NaN

2012-06-17 -0.665153

2012-06-18 -1.297332

2012-06-21 1.304942

2012-06-22 -0.125775

2012-06-29 -0.090242

Using periods instead of timestamps

Periods (representing time spans) provide an alternate means of working with different
frequency time series, especially financial or economic series with annual or quarterly
frequency having a particular reporting convention. For example, a company might
announce its quarterly earnings with fiscal year ending in June, thus having Q-JUN fre-
quency. Consider a pair of macroeconomic time series related to GDP and inflation:

In [37]: gdp = Series([1.78, 1.94, 2.08, 2.01, 2.15, 2.31, 2.46],
ssorme index=pd.period_range('198402', periods=7, freq='Q-SEP'))

 

Data Munging Topics | 333

In [38]: infl = Series([0.025, 0.045, 0.037, 0.04],
wowed index=pd. period range('1982', periods=4, freq='A-DEC'))

In [39]: gdp In [40]: infl
Out [39]: Out [40]:
198402 1.78 1982 0.025
198403 1.94 1983 0.045
198404 2.08 1984 0.037
198501 2.01 1985 0.040
198502 2.15 Freq: A-DEC
198503 2.31

198504 2.46

Freq: Q-SEP

Unlike time series with timestamps, operations between different-frequency time series
indexed by periods are not possible without explicit conversions. In this case, if we
know that infl values were observed at the end of each year, we can then convert to
Q-SEP to get the right periods in that frequency:

In [41]: infl_q = infl.asfreq('Q-SEP', how='end')

In [42]: infl_q
Out [42]:

198301 0.025
198401 0.045
198501 0.037
198601 0.040
Freq: Q-SEP

That time series can then be reindexed with forward-filling to match gdp:
In [43]: infl_q.reindex(gdp.index, method='ffil1')

Out [43]:

198402 0.045
198403 0.045
198404 0.045
198501 0.037
198502 0.037
198503 0.037
198504 0.037
Freq: Q-SEP

Time of Day and “as of” Data Selection

Suppose you have a long time series containing intraday market data and you want to
extract the prices at a particular time of day on each day of the data. What if the data
are irregular such that observations do not fall exactly on the desired time? In practice
this task can make for error-prone data munging if you are not careful. Here is an
example for illustration purposes:

# Make an intraday date range and time series
In [44]: rng = pd.date_range('2012-06-01 ', ‘2012-06-01 ', freq='T')

# Make a 5-day series of 9:30- values

 

334 | Chapter 11: Financial and Economic Data Applications

In [45]: rng = rng.append([rng + pd.offsets.BDay(i) for i in range(1, 4)])

In [46]: ts = Series(np.arange(len(rng), dtype=float), index=rng)

In [47]: ts
Out [47]:

2012-06-01
2012-06-01
2012-06-01
2012-06-01

2012-06-06
2012-06-06
2012-06-06
2012-06-06
Length: 156

09:
09:
09:
09:

152
152
152
152

0

30:
31:
32:
33:

56:
57:
58:
59:

00
00
00
00

00
00
00
00

WNrR OO

1556
1557
1558
1559

Indexing with a Python datetime.time object will extract values at those times:

In [48]: from datetime import time

In [49]: ts[time(10, 0)]

Out[49]:

2012-06-01 :00
2012-06-04 :00
2012-06-05 :00
2012-06-06 :00

30
420
810

1200

Under the hood, this uses an instance method at_time (available on individual time

series and DataFrame objects alike):
In [50]: ts.at_time(time(10, 0))

Out[50]:

2012-06-01 :00
2012-06-04 :00
2012-06-05 :00
2012-06-06 :00

30
420
810

1200

You can select values between two times using the related between_time method:

In [51]: ts.between_time(time(10, 0), time(10, 1))

Out[51]:

2012-06-01
2012-06-01
2012-06-04
2012-06-04
2012-06-05
2012-06-05
2012-06-06
2012-06-06

10:
10:
10:
10:
10:
10:
10:
10:

00:
01:
00:
01:
00:
01:
00:
01:

00
00
00
00
00
00
00
00

30
31
420
421
810
811
1200
1201

As mentioned above, it might be the case that no data actually fall exactly at a time like
10 AM, but you might want to know the last known value at 10 AM:

# Set most of the time series randomly to NA

In [53]: indexer

np.sort(np.random.permutation(len(ts))[700: ])

 

Data Munging Topics | 335

In [54]: irr_ts = ts.copy()
In [55]: irr_ts[indexer] = np.nan

In [56]: irr_ts['2012-06-01 ':'2012-06-01 ' ]
Out [56]:

2012-06-01 :00 20
2012-06-01 :00 NaN
2012-06-01 :00 22
2012-06-01 :00 23
2012-06-01 :00 NaN
2012-06-01 :00 25
2012-06-01 :00 NaN
2012-06-01 :00 NaN
2012-06-01 :00 NaN
2012-06-01 :00 NaN
2012-06-01 :00 NaN

By passing an array of timestamps to the asof method, you will obtain an array of the
last valid (non-NA) values at or before each timestamp. So we construct a date range
at 10 AM for each day and pass that to asof:

In [57]: selection = pd.date_range('2012-06-01 ', periods=4, freq='B')

In [58]: irr_ts.asof(selection)

Out [58]:
2012-06-01 :00 25
2012-06-04 :00 420
2012-06-05 :00 810
2012-06-06 :00 1197
Freq: B

Splicing Together Data Sources

In Chapter 7, I described a number of strategies for merging together two related data
sets. In a financial or economic context, there are a few widely occurring use cases:

* Switching from one data source (a time series or collection of time series) to another
at a specific point in time

¢ “Patching” missing values in a time series at the beginning, middle, or end using
another time series

* Completely replacing the data for a subset of symbols (countries, asset tickers, and
so on)

In the first case, switching from one set of time series to another at a specific instant, it
is a matter of splicing together two TimeSeries or DataFrame objects using pandas.con
cat:
In [59]: data1 = DataFrame(np.ones((6, 3), dtype=float),
ataa i columns=['a', 'b', ‘c'],
wana i index=pd.date_range('6/12/2012', periods=6))

 

336 | Chapter 11: Financial and Economic Data Applications

In [60]: data2 = DataFrame(np.ones((6, 3), dtype=float) * 2,
ccoseve S columns=['a', 'b', ‘c'],
scoswie E index=pd.date_range('6/13/2012', periods=6))
In [61]: spliced = pd.concat([data1.ix[:'2012-06-14'], data2.ix['2012-06-15':]])

In [62]: spliced

Out [62]:

abe
2012-06-12 11 1
2012-06-13 1 1 1
2012-06-14 1 1 1
2012-06-15 2 2 2
2012-06-16 2 2 2
2012-06-17 2 2 2
2012-06-18 2 2 2

Suppose in a similar example that data1 was missing a time series present in data2:

In [63]: data2 = DataFrame(np.ones((6, 4), dtype=float) * 2,
acerece columns=['a', 'b', 'c', ‘d'],
sores index=pd.date_range('6/13/2012', periods=6))
In [64]: spliced = pd.concat([data1.ix[:'2012-06-14'], data2.ix['2012-06-15':]])

In [65]: spliced

Out [65]:

a bce d
2012-06-12 1 1 1 NaN
2012-06-13 1 1 1 NaN
2012-06-14 1 1 1 NaN
2012-06-15 2 2 2 2
2012-06-16 2 2 2 2
2012-06-17 2 2 2 2
2012-06-18 2 2 2 2

Using combine first, you can bring in data from before the splice point to extend the
history for 'd' item:
In [66]: spliced filled = spliced.combine_first(data2)

In [67]: spliced_filled
Out [67]:

2012-06-12
2012-06-13
2012-06-14
2012-06-15
2012-06-16
2012-06-17
2012-06-18

Na

NNNNRPRP PRD
NNNNRPRP RPO
NNNNRPRPRP OT
NNNNNNZQ

Since data2 does not have any values for 2012-06-12, no values are filled on that day.

DataFrame has a related method update for performing in-place updates. You have to
pass overwrite=False to make it only fill the holes:

 

Data Munging Topics | 337

In [68]: spliced.update(data2, overwrite=False)

In [69]: spliced

Out [69]:

2012-06-12
2012-06-13
2012-06-14
2012-06-15
2012-06-16
2012-06-17
2012-06-18

NNNNRPR PRD

To replace the da
but sometimes it’

In [70]: cp_sp
In [71]: cp_sp

In [72]: cp_sp
Out[72]:

a
2012-06-12 1
2012-06-13 1
2012-06-14 1
2012-06-15 1
2012-06-16 1
2012-06-17 1
2012-06-18 NaN

NNNNRPRP PRO
NNNNRPRPRPO
NNNNNNZQ

ta for a subset of symbols, you can use any of the above techniques,
s simpler to just set the columns directly with DataFrame indexing:

liced = spliced. copy()
liced[['a', 'c']] = datai[['a', 'c']]

liced

NNNNBPBBR OS
ZrPrrPRPRBREO
NNNNNDN 2D

2
mw

Return Indexes and Cumulative Returns

In a financial context, returns usually refer to percent changes in the price of an asset.
Let’s consider price data for Apple in 2011 and 2012:

In [73]: impor
In [74]: price

In [75]: price
Out[75]:

Date
2012-07-23
2012-07-24
2012-07-25
2012-07-26
2012-07-27

t pandas.io.data as web
= web.get_data_yahoo('AAPL', '2011-01-01')['Adj Close']

[-5:]

603.83
600.92
574.97
574.88
585.16

Name: Adj Close

For Apple, which

has no dividends, computing the cumulative percent return between

two points in time requires computing only the percent change in the price:

In [76]: price
Out[76]: 0.072

['2011-10-03'] / price['2011-3-01'] - 1
399874037388123

 

338 | Chapter 11: Financial and Economic Data Applications

For other stocks with dividend payouts, computing how much money you make from
holding a stock can be more complicated. The adjusted close values used here have
been adjusted for splits and dividends, however. In all cases, it’s quite common to derive
a return index, which is a time series indicating the value of a unit investment (one
dollar, say). Many assumptions can underlie the return index; for example, some will
choose to reinvest profit and others not. In the case of Apple, we can compute a simple
return index using cumprod:

In [77]: returns = pri

In [78]: ret_index = (

In [79]: ret_index[o]

In [80]: ret_index

Out [80]:
Date
2011-01-03
2011-01-04
2011-01-05
2011-01-06

2012-07-24
2012-07-25
2012-07-26
2012-07-27
Length: 396

PRPRPRB

PRPRPRB

-000000
-005219
-013442
-012623

-823346
- 744607
- 744334
-775526

ce.pct_change()
1 + returns).cumprod()

=1 # Set first value to 1

With a return index in hand, computing cumulative returns at a particular resolution

is simple:

In [81]: m_returns = ret_index.resample('BM', how='last').pct_change()

In [82]: m_returns['2012']

Out [82]:
Date
2012-01-31
2012-02-29
2012-03-30
2012-04-30
2012-05-31
2012-06-29
2012-07-31
Freq: BM

o

-127111
- 188311
- 105284
-0.
-0.
-010853
- 001986

025969
010702

Of course, in this simple case (no dividends or other adjustments to take into account)
these could have been computed from the daily percent changed by resampling with
aggregation (here, to periods):

In [83]: m_rets

= (1+

returns).resample('M', how='prod', kind='period') - 1

In [84]: m_rets['2012']

Out [84]:
Date

 

Data Munging Topics | 339

2012-01 0.127111
2012-02 0.188311
2012-03 0.105284
2012-04 -0.025969
2012-05 -0.010702
2012-06 0.010853
2012-07 0.001986
Freq: M

If you had dividend dates and percentages, including them in the total return per day

would look like:

returns[dividend_dates] += dividend_pcts

Group Transforms and Analysis

In Chapter 9, you learned the basics of computing group statistics and applying your
own transformations to groups in a dataset.

Let’s consider a collection of hypothetical stock portfolios. I first randomly generate a
broad universe of 2000 tickers:

import random; random.seed(0)
import string

N = 1000
def rands(n):

choices = string.ascii_uppercase

return ''.join([random.choice(choices) for _ in xrange(n)])
tickers = np.array([rands(5) for _ in xrange(N)])

I then create a DataFrame containing 3 columns representing hypothetical, but random
portfolios for a subset of tickers:
M = 500
df = DataFrame({'Momentum' : np.random.randn(M) / 200 + 0.03,
‘Value’ : np.random.randn(M) / 200 + 0.08,

'ShortInterest' : np.random.randn(M) / 200 - 0.02},
index=tickers[:M])

Next, let’s create a random industry classification for the tickers. To keep things simple,
Pl just keep it to 2 industries, storing the mapping in a Series:
ind_names = np.array(['FINANCIAL', 'TECH'])
sampler = np.random.randint(0, len(ind_names), N)
industries = Series(ind_names[sampler], index=tickers,
name=' industry’ )

Now we can group by industries and carry out group aggregation and transformations:

In [90]: by_industry = df.groupby(industries)

In [91]: by_industry.mean()
Out[91]:
Momentum ShortInterest Value

 

340 | Chapter 11: Financial and Economic Data Applications

industry
FINANCIAL 0.029485 -0.020739 0.079929
TECH 0.030407 -0.019609 0.080113

In [92]: by_industry.describe()

Out[92]:
Momentum ShortInterest Value
industry
FINANCIAL count 246.000000 246.000000 246.000000
mean 0.029485 -0.020739 0.079929
std 0.004802 0.004986 0.004548
min 0.017210 -0.036997 0.067025
25% 0.026263 -0.024138 0.076638
50% 0.029261 -0.020833 0.079804
75% 0.032806 -0.017345 0.082718
max 0.045884 -0.006322 0.093334
TECH count 254.000000 254.000000 254.000000
mean 0.030407 -0.019609 0.080113
std 0.005303 0.005074 0.004886
min 0.016778 -0.032682 0.065253
25% 0.026456 -0.022779 0.076737
50% 0.030650 -0.019829 0.080296
15% 0.033602 -0.016923 0.083353
max 0.049638 -0.003698 0.093081

By defining transformation functions, it’s easy to transform these portfolios by industry.
For example, standardizing within industry is widely used in equity portfolio construc-
tion:

# Within-Industry Standardize
def zscore(group):
return (group - group.mean()) / group.std()

df_stand = by_industry.apply(zscore)

You can verify that each industry has mean 0 and standard deviation 1:

In [94]: df_stand.groupby(industries).agg(['mean', 'std'])

Out [94]:
Momentum ShortInterest Value
mean std mean std mean std
industry
FINANCIAL ) 1 0 1 0 1
TECH -0 1 -0 1 -0 1

Other, built-in kinds of transformations, like rank, can be used more concisely:

# Within-industry rank descending
In [95]: ind_rank = by_industry.rank(ascending=False)

In [96]: ind_rank.groupby(industries).agg(['min', 'max'])

Out [96]:
Momentum ShortInterest Value
min max min max min max
industry
FINANCIAL 1 246 1 246 1 246
TECH 1 254 1 254 1 254

 

Group Transforms and Analysis | 341

In quantitative equity, “rank and standardize” is a common sequence of transforms.
You could do this by chaining together rank and zscore like so:

# Industry rank and standardize

In [97]: by_industry.apply(lambda x: zscore(x.rank()))
Out [97]:

<class 'pandas.core.frame.DataFrame' >

Index: 500 entries, VTKGN to PTDQE

Data columns:

Momentum 500 non-null values
ShortInterest 500 non-null values
Value 500 non-null values

dtypes: float64(3)

Group Factor Exposures

Factor analysis is a technique in quantitative portfolio management. Portfolio holdings
and performance (profit and less) are decomposed using one or more factors (risk fac-
tors are one example) represented as a portfolio of weights. For example, a stock price’s
co-movement with a benchmark (like S@P 500 index) is known as its beta, a common
risk factor. Let’s consider a contrived example of a portfolio constructed from 3 ran-
domly-generated factors (usually called the factor loadings) and some weights:

from numpy.random import rand
faci, fac2, fac3 = np.random.rand(3, 1000)

ticker_subset = tickers.take(np.random.permutation(N)[ :1000])

# Weighted sum of factors plus noise
port = Series(0.7 * faci - 1.2 * fac2 + 0.3 * fac3 + rand(1000),
index=ticker_subset)
factors = DataFrame({'f1': faci, 'f2': fac2, 'f3': fac3},
index=ticker_subset)

Vector correlations between each factor and the portfolio may not indicate too much:

In [99]: factors.corrwith(port)
Out[99]:

#2 0.402377

£2 -0.680980

£3 0.168083

The standard way to compute the factor exposures is by least squares regression; using
pandas.ols with factors as the explanatory variables we can compute exposures over
the entire set of tickers:

In [100]: pd.ols(y=port, x=factors).beta

Out[100]:

il 0.761789
£2 -1.208760
£3 0.289865

intercept 0.484477

 

342 | Chapter 11: Financial and Economic Data Applications

As you can see, the original factor weights can nearly be recovered since there was not
too much additional random noise added to the portfolio. Using groupby you can com-
pute exposures industry by industry. To do so, write a function like so:

def beta_exposure(chunk, factors=None) :
return pd.ols(y=chunk, x=factors).beta

Then, group by industries and apply that function, passing the DataFrame of factor
loadings:

In [102]: by_ind = port.groupby(industries)
In [103]: exposures = by_ind.apply(beta_exposure, factors=factors)

In [104]: exposures.unstack()

Out[104]:

f1 f2 3 intercept
industry
FINANCIAL 0.790329 -1.182970 0.275624 0.455569
TECH 0.740857 -1.232882 0.303811 0.508188

Decile and Quartile Analysis

Analyzing data based on sample quantiles is another important tool for financial ana-
lysts. For example, the performance of a stock portfolio could be broken down into
quartiles (four equal-sized chunks) based on each stock’s price-to-earnings. Using pan
das.qcut combined with groupby makes quantile analysis reasonably straightforward.

As an example, let’s consider a simple trend following or momentum strategy trading
the S&P 500 index via the SPY exchange-traded fund. You can download the price
history from Yahoo! Finance:

In [105]: import pandas.io.data as web
In [106]: data = web.get_data_yahoo('SPY', '2006-01-01')

In [107]: data

Out[107]:

<class 'pandas.core. frame.DataFrame' >

DatetimeIndex: 1655 entries, 2006-01-03 :00 to 2012-07-27 :00
Data columns:

Open 1655 non-null values
High 1655 non-null values
Low 1655 non-null values
Close 1655 non-null values
Volume 1655 non-null values

Adj Close 1655 non-null values

dtypes: float64(5), int64(1)
Now, we’ll compute daily returns and a function for transforming the returns into a
trend signal formed from a lagged moving sum:

px = data['Adj Close']
returns = px.pct_change()

 

Group Transforms and Analysis | 343

def to_index(rets):
index = (1 + rets).cumprod()
first_loc = max(index.notnull().argmax() - 1, 0)
index.values[first_loc] = 1
return index

def trend_signal(rets, lookback, lag):
signal = pd.rolling sum(rets, lookback, min_periods=lookback - 5)
return signal.shift(lag)

Using this function, we can (naively) create and test a trading strategy that trades this
momentum signal every Friday:

In [109]: signal = trend_signal(returns, 100, 3)
In [110]: trade friday = signal.resample('W-FRI').resample('B', fill_method='ffill1')
In [111]: trade_rets = trade friday.shift(1) * returns

We can then convert the strategy returns to a return index and plot them (see Fig-
ure 11-1):

In [112]: to_index(trade_rets).plot()

 

 

 

 

 

 

1.12
1.10b ++: : : : : 4
1.08} : rf '

1.06} |
1.04+ , : |
1.02} . 4
1.00} tel |
0.98 ; - ; - ;

2007 2008 2009 2010 2011 2012
Date

 

 

 

Figure 11-1. SPY momentum strategy return index

Suppose you wanted to decompose the strategy performance into more and less volatile
periods of trading. Trailing one-year annualized standard deviation is a simple measure
of volatility, and we can compute Sharpe ratios to assess the reward-to-risk ratio in
various volatility regimes:

vol = pd.rolling std(returns, 250, min_periods=200) * np.sqrt(250)

 

344 | Chapter 11: Financial and Economic Data Applications

def sharpe(rets, ann=250):
return rets.mean() / rets.std() * np.sqrt(ann)

Now, dividing vol into quartiles with qcut and aggregating with sharpe we obtain:

In [114]: trade_rets.groupby(pd.qcut(vol, 4)).agg(sharpe)
Out[114]:

[0.0955, 0.16] 0.490051

(0.16, 0.188] 0.482788

(0.188, 0.231] -0.731199

(0.231, 0.457] 0.570500

These results show that the strategy performed the best during the period when the
volatility was the highest.

More Example Applications

Here is a small set of additional examples.

Signal Frontier Analysis

In this section, I’ll describe a simplified cross-sectional momentum portfolio and show
how you might explore a grid of model parameterizations. First, I’ll load historical
prices for a portfolio of financial and technology stocks:

names = ['AAPL', 'GOOG', 'MSFT', 'DELL', 'GS', 'MS', ‘BAC’, 'C']

def get_px(stock, start, end):

return web.get_data_yahoo(stock, start, end)['Adj Close’ ]
px = DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})

We can easily plot the cumulative returns of each stock (see Figure 11-2):
In [117]: px = px.asfreq('B').fillna(method=' pad’)

In [118]: rets = px.pct_change()
In [119]: ((1 + rets).cumprod() - 1).plot()

For the portfolio construction, we’ll compute momentum over a certain lookback, then
rank in descending order and standardize:
def calc_mom(price, lookback, lag):
mom_ret = price.shift(lag).pct_change(lookback)
ranks = mom_ret.rank(axis=1, ascending=False)

demeaned = ranks - ranks.mean(axis=1)
return demeaned / demeaned.std(axis=1)

With this transform function in hand, we can set up a strategy backtesting function
that computes a portfolio for a particular lookback and holding period (days between
trading), returning the overall Sharpe ratio:

compound = lambda x : (1 + x).prod() - 1
daily sr = lambda x: x.mean() / x.std()

 

More Example Applications | 345

def strat_sr(prices, lb, hold):
# Compute portfolio weights
freq = '%dB' % hold
port = calc_mom(prices, 1b, lag=1)

daily rets = prices.pct_change()

# Compute portfolio returns

port = port.shift(1).resample(freq, how='first')
returns = daily rets.resample(freq, how=compound)

port_rets = (port * returns).sum(axis=1)

return daily sr(port_rets) * np.sqrt(252 / hold)

 

 

 

 

 

 

 

 

 

 

Figure 11-2. Cumulative returns for each of the stocks

When called with the prices and a parameter combination, this function returns a scalar
value:

In [122]: strat_sr(px, 70, 30)
Out [122]: 0.27421582756800583

From there, you can evaluate the strat_sr function over a grid of parameters, storing
them as you go in a defaultdict and finally putting the results in a DataFrame:

from collections import defaultdict

lookbacks = range(20, 90, 5)
holdings = range(20, 90, 5)
dd = defaultdict(dict)
for lb in lookbacks:
for hold in holdings:
dd[1b][hold] = strat_sr(px, 1b, hold)

 

346 | Chapter 11: Financial and Economic Data Applications

ddf = DataFrame(dd)
ddf.index.name = ‘Holding Period'
ddf.columns.name = 'Lookback Period’

To visualize the results and get an idea of what’s going on, here is a function that uses
matplotlib to produce a heatmap with some adornments:

import matplotlib.pyplot as plt

def heatmap(df, cmap=plt.cm.gray_r):
fig = plt.figure()

ax

= fig.add_subplot(111)

axim = ax.imshow(df.values, cmap=cmap, interpolation='nearest' )

ax

ax
ax

.set_xlabel(df.columns.name)
ax.
ax.
ax.
.set_yticks(np.arange(len(df.index) ) )
.set_yticklabels(list(df.index))

set_xticks(np.arange(len(df.columns) ))
set_xticklabels(list(df.columns) )
set_ylabel(df.index.name)

plt.colorbar(axim)

Calling this function on the backtest results, we get Figure 11-3:
In [125]: heatmap(ddf)

 

 

   

1.00
0.75
0.50

B 45 0.25

=

o

a

oD '

£ 0.00

3S 55

2 60
-0.25
—0.50

, -0.75
20 25 30 35 40 45 50 55 60 65 70 75 80 85
Lookback Period

 

 

Figure 11-3. Heatmap of momentum strategy Sharpe ratio (higher is better) over various lookbacks
and holding periods

Future Contract Rolling

A future is an ubiquitous form of derivative contract; it is an agreement to take delivery
of a certain asset (such as oil, gold, or shares of the FTSE 100 index) on a particular
date. In practice, modeling and trading futures contracts on equities, currencies,

 

More Example Applications | 347

commodities, bonds, and other asset classes is complicated by the time-limited nature
of each contract. For example, at any given time for a type of future (say silver or copper
futures) multiple contracts with different expiration dates may be traded. In many cases,
the future contract expiring next (the near contract) will be the most liquid (highest
volume and lowest bid-ask spread).

For the purposes of modeling and forecasting, it can be much easier to work with a

continuous return index indicating the profit and loss associated with always holding
the near contract. Transitioning from an expiring contract to the next (or far) contract
is referred to as rolling. Computing a continuous future series from the individual con-
tract data is not necessarily a straightforward exercise and typically requires a deeper
understanding of the market and how the instruments are traded. For example, in
practice when and how quickly would you trade out of an expiring contract and into
the next contract? Here I describe one such process.

First, I'll use scaled prices for the SPY exchange-traded fund as a proxy for the S&P 500
index:

In [127]: import pandas.io.data as web

# Approximate price of S&P 500 index
In [128]: px = web.get_data_yahoo('SPY')['Adj Close'] * 10

In [129]: px

Out [129]:

Date

2011-08-01 1261.0
2011-08-02 1228.8
2011-08-03 1235.5

2012-07-25 1339.6
2012-07-26 1361.7
2012-07-27 1386.8
Name: Adj Close, Length: 251

Now, a little bit of setup. I put a couple of S@P 500 future contracts and expiry dates
in a Series:

from datetime import datetime

expiry = {'ESU2': datetime(2012, 9, 21),
"ESZ2': datetime(2012, 12, 21)}

expiry = Series(expiry) .order()

expiry then looks like:

In [131]: expiry

Out [131]:

ESU2 2012-09-21 :00
ESZ2 2012-12-21 :00

 

348 | Chapter 11: Financial and Economic Data Applications

Then, I use the Yahoo! Finance prices along with a random walk and some noise to
simulate the two contracts into the future:

np.random. seed (12347)

N = 200

walk = (np.random.randint(0, 200, size=N) - 100) * 0.25
perturb = (np.random.randint(0, 20, size=N) - 10) * 0.25
walk = walk.cumsum()

rng = pd.date_range(px.index[0], periods=len(px) + N, freq='B')
near = np.concatenate([px.values, px.values[-1] + walk])

far = np.concatenate([px.values, px.values[-1] + walk + perturb])
prices = DataFrame({'ESU2': near, 'ESZ2': far}, index=rng)

prices then has two time series for the contracts that differ from each other by arandom
amount:

In [133]: prices.tail()
Out [133]:

ESU2 ESZ2
2013-04-16 1416.05 1417.80
2013-04-17 1402.30 1404.55
2013-04-18 1410.30 1412.05
2013-04-19 1426.80 1426.05
2013-04-22 1406.80 1404.55

One way to splice time series together into a single continuous series is to construct a
weighting matrix. Active contracts would have a weight of 1 until the expiry date ap-
proaches. At that point you have to decide on a roll convention. Here is a function that
computes a weighting matrix with linear decay over a number of periods leading up to

expiry:

def get_roll weights(start, expiry, items, roll_periods=5):
# start : first date to compute weighting DataFrame
# expiry : Series of ticker -> expiration dates
# items : sequence of contract names

dates = pd.date_range(start, expiry[-1], freq='B')
weights = DataFrame(np.zeros((len(dates), len(items))),
index=dates, columns=items)

prev_date = weights.index[0]
for i, (item, ex_date) in enumerate(expiry.iteritems()):
if i < len(expiry) - 1:
weights.ix[prev_date:ex_date - pd.offsets.BDay(), item] = 1
roll_rng = pd.date_range(end=ex_date - pd.offsets.BDay(),
periods=roll periods + 1, freq='B')

decay weights = np.linspace(0, 1, roll periods + 1)

weights.ix[roll_rng, item] = 1 - decay weights

weights.ix[roll_rng, expiry.index[i + 1]] = decay weights
else:

weights.ix[prev_date:, item] = 1

prev_date = ex_date

 

More Example Applications | 349

return weights

The weights look like this around the ESU2 expiry:
In [135]: weights = get_roll_weights('6/1/2012', expiry, prices.columns)

In [136]: weights. ix['2012-09-12':'2012-09-21' ]

Out [136]:

ESU2 ESZ2
2012-09-12 1.0 0.0
2012-09-13 1.0 0.0
2012-09-14 0.8 0.2
2012-09-17 0.6 0.4
2012-09-18 0.4 0.6
2012-09-19 0.2 0.8
2012-09-20 0.0 1.0
2012-09-21 0.0 1.0

Finally, the rolled future returns are just a weighted sum of the contract returns:

In [137]: rolled_returns = (prices.pct_change() * weights) .sum(1)

Rolling Correlation and Linear Regression

Dynamic models play an important role in financial modeling as they can be used to
simulate trading decisions over a historical period. Moving window and exponentially-
weighted time series functions are an example of tools that are used for dynamic models.

Correlation is one way to look at the co-movement between the changes in two asset
time series. pandas’s rolling corr function can be called with two return series to
compute the moving window correlation. First, I load some price series from Yahoo!
Finance and compute daily returns:

aapl = web.get_data_yahoo('AAPL', '2000-01-01')['Adj Close’ ]
msft = web.get_data_yahoo('MSFT', '2000-01-01')['Adj Close']

aapl_rets = aapl.pct_change()
msft_rets = msft.pct_change()

Then, I compute and plot the one-year moving correlation (see Figure 11-4):
In [140]: pd.rolling corr(aapl_rets, msft_rets, 250).plot()

One issue with correlation between two assets is that it does not capture differences in
volatility. Least-squares regression provides another means for modeling the dynamic
relationship between a variable and one or more other predictor variables.

In [142]: model = pd.ols(y=aapl_rets, x={'MSFT': msft_rets}, window=250)

In [143]: model.beta

Out [143]:

<class 'pandas.core.frame.DataFrame' >

DatetimeIndex: 2913 entries, 2000-12-28 :00 to 2012-07-27 :00
Data columns:

 

350 | Chapter 11: Financial and Economic Data Applications

MSFT 2913 non-null values
intercept 2913 non-null values
dtypes: float64(2)

In [144]: model.beta[ 'MSFT'].plot()

 

      

og
2 D S © 7 % 9 9 5 2
ov 9 ge ge Wh gh” og’
Date

a

 

 

Figure 11-4. One-year correlation of Apple with Microsoft

 

1.0
0.9
0.8F--
0.7
0.6
0.5
0.4
0.3

 

as"

as"
Date

oy®

0. i i i i
ae or aw? ao" aw? o>

1 1 L
vo) 2 9
gr gg 7

 

 

Figure 11-5. One-year beta (OLS regression coefficient) of Apple to Microsoft

pandas’s ols function implements static and dynamic (expanding or rolling window)
least squares regressions. For more sophisticated statistical and econometrics models,
see the statsmodels project (http://statsmodels.sourceforge.net).

 

More Example Applications | 351

 

 


CHAPTER 12
Advanced NumPy

 

ndarray Object Internals

The NumPy ndarray provides a means to interpret a block of homogeneous data (either
contiguous or strided, more on this later) as a multidimensional array object. As you’ve
seen, the data type, or dtype, determines how the data is interpreted as being floating
point, integer, boolean, or any of the other types we’ve been looking at.

Part of what makes ndarray powerful is that every array object is a strided view on a
block of data. You might wonder, for example, how the array view arr[::2, ::-1] does
not copy any data. Simply put, the ndarray is more than just a chunk of memory and
a dtype; it also has striding information which enables the array to move through
memory with varying step sizes. More precisely, the ndarray internally consists of the
following:

¢ A pointer to data, that is a block of system memory
¢ The data type or dtype

e Atuple indicating the array’s shape; For example, a 10 by 5 array would have shape
(10, 5)
In [8]: np.ones((10, 5)).shape
Out[8]: (10, 5)
¢ A tuple of strides, integers indicating the number of bytes to “step” in order to
advance one element along a dimension; For example, a typical (C order, more on
this later) 3 x 4 x 5 array of float64 (8-byte) values has strides (160, 40, 8)
In [9]: np.ones((3, 4, 5), dtype=np.float64).strides
Out[9]: (160, 40, 8)
While it is rare that a typical NumPy user would be interested in the array strides,
they are the critical ingredient in constructing copyless array views. Strides can
even be negative which enables an array to move backward through memory, which
would be the case in a slice like obj[::-1] or obj[:, ::-1].

 

353

See Figure 12-1 for a simple mockup the ndarray innards.

 

ndarray object

datal | | 1 1 TT TTT I~

Figure 12-1. The NumPy ndarray object

 

 

 

 

NumPy dtype Hierarchy

You may occasionally have code which needs to check whether an array contains in-
tegers, floating point numbers, strings, or Python objects. Because there are many types
of floating point numbers (float16 through float128), checking that the dtype is among
a list of types would be very verbose. Fortunately, the dtypes have superclasses such as
np.integer and np.floating which can be used in conjunction with the np.issubd
type function:

In [10]: ints = np.ones(10, dtype=np.uint16)
In [11]: floats = np.ones(10, dtype=np.float32)

12]: np.issubdtype(ints.dtype, np.integer)
12]: True

13]: np.issubdtype(floats.dtype, np. floating)
13]: True

You can see all of the parent classes of a specific dtype by calling the type’s mro method:

In [14]: np.float64.mro()
Out[14]:
[numpy.float64,
numpy. floating,
numpy.inexact,
numpy.number ,
numpy.generic,
float,
object]

 

Most NumPy users will never have to know about this, but it occasionally comes in
handy. See Figure 12-2 for a graph of the dtype hierarchy and parent-subclass
relationships !.

1. Some of the dtypes have trailing underscores in their names. These are there to avoid variable name
conflicts between the NumPy-specific types and the Python built-in ones.

 

354 | Chapter 12: Advanced NumPy

 

ann

Figure 12-2. The NumPy dtype class hierarchy

 

 

 

Advanced Array Manipulation

There are many ways to work with arrays beyond fancy indexing, slicing, and boolean
subsetting. While much of the heavy lifting for data analysis applications is handled by
higher level functions in pandas, you may at some point need to write a data algorithm
that is not found in one of the existing libraries.

Reshaping Arrays

Given what we know about NumPy arrays, it should come as little surprise that you
can convert an array from one shape to another without copying any data. To do this,
pass a tuple indicating the new shape to the reshape array instance method. For exam-
ple, suppose we had a one-dimensional array of values that we wished to rearrange into
a matrix:

In [15]: arr = np.arange(8)

In [16]: arr
Out[16]: array([0, 1, 2, 3, 4, 5, 6, 7])

In [17]: arr.reshape((4, 2))

Out [17]:

array([[0, 1],
[2, 3],
[4, 5],
[6, 7]]

)

A multidimensional array can also be reshaped:

In [18]: arr.reshape((4, 2)).reshape((2, 4))
Out [18]:

 

Advanced Array Manipulation | 355

array([{[0, 1, 2, 3],
[4, 5, 6, 7]])

One of the passed shape dimensions can be -1, in which case the value used for that
dimension will be inferred from the data:

In [19]: arr = np.arange(15) In [20]: arr.reshape((5, -1))
Out[20]:
array([[ 0, 1, 2],
[3, 4, 5],
[6, 7, 8],
[ 9, 10, 11],
[12, 13, 14]])

Since an array’s shape attribute is a tuple, it can be passed to reshape, too:

In [21]: other_arr = np.ones((3, 5))

In [22]: other_arr.shape
Out[22]: (3, 5)

In [23]: arr.reshape(other_arr.shape)
Out [23]:
array([[ 0, 1, 2, 3, 4],

[ 5» 6, 7; 8, 9],

[10, 11, 12, 13, 14]])

The opposite operation of reshape from one-dimensional to a higher dimension is typ-
ically known as flattening or raveling:

In [24]: arr = np.arange(15).reshape((5, 3)) In [25]: arr
Out[25]:
array([[ 0, 1, 2],
[3, 4, 5],
[6, 7, 8],
[ 9, 10, 14],
[12, 13, 14]])

In [26]: arr.ravel()

Out[26]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])
ravel does not produce a copy of the underlying data if it does not have to (more on
this below). The flatten method behaves like ravel except it always returns a copy of
the data:

In [27]: arr.flatten()

Out[27]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])
The data can be reshaped or raveled in different orders. This is a slightly nuanced topic
for new NumPy users and is therefore the next subtopic.

C versus Fortran Order

Contrary to some other scientific computing environments like R and MATLAB,
NumPy gives you much more control and flexibility over the layout of your data in

 

356 | Chapter 12: Advanced NumPy

memory. By default, NumPy arrays are created in row major order. Spatially this means
that if you have a two-dimensional array of data, the items in each row of the array are
stored in adjacent memory locations. The alternative to row major ordering is column
major order, which means that (you guessed it) values within each column of data are
stored in adjacent memory locations.

For historical reasons, row and column major order are also know as C and Fortran
order, respectively. In FORTRAN 77, the language of our forebears, matrices were all
column major.

Functions like reshape and ravel, accept an order argument indicating the order to use
the data in the array. This can be 'C' or 'F' in most cases (there are also less commonly-
used options 'A’ and 'K'; see the NumPy documentation). These are illustrated in
Figure 12-3.

In [28]: arr = np.arange(12).reshape((3, 4))

In [29]: arr
Out [29]:
array([[ 0, 1, 2, 3],
5» 6, 7],
» 9, 10, 11]])

ocr
of Oo
.

In [30]: arr.ravel()
Out[30]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])

In [31]: arr.ravel('F')
Out[31]: array([ 0, 4, 8, 1, 5, 9, 2, 6, 10, 3, 7, 11])

Reshaping arrays with more than two dimensions can be a bit mind-bending. The key
difference between C and Fortran order is the order in which the dimensions are

walked:
* C/row major order: traverse higher dimensions first (e.g. axis 1 before advancing
on axis 0).

¢ Fortran / column major order: traverse higher dimensions last (e.g. axis 0 before
advancing on axis 1).

Concatenating and Splitting Arrays
numpy. concatenate takes a sequence (tuple, list, etc.) of arrays and joins them together
in order along the input axis.

In [32]: arr1 = np-array([[1, 2, 3], [4, 5, 6]])

In [33]: arr2 = np.array([[7, 8, 9], [10, 11, 12]])

In [34]: np.concatenate([arr1, arr2], axis=0)
Out [34]:
array([[ 1, 2, 3],

[4, 5, 6],

 

Advanced Array Manipulation | 357

[7, 8, 9],
[10, 11, 12]])

In [35]: np.concatenate([arr1, arr2], axis=1)
Out [35]:
array([[ 1, 2, 3, 7, 8, 9],

[ 4, 5, 6, 10, 11, 12]])

pols fetal ei siel 7 |e] ej] a)

arr.reshape((4, 3), order=?)

 

C order (row major) Fortran order (column major)

 

order='C' order='F'

 

 

 

Figure 12-3. Reshaping in C (row major) or Fortran (column major) order

There are some convenience functions, like vstack and hstack, for common kinds of
concatenation. The above operations could have been expressed as:

In [36]: np.vstack((arr1, arr2)) In [37]: np.hstack((arr1, arr2))

Out [36]: Out [37]:

array([[ 1, 2, 3], array([[ 1, 2, 3, 7, 8, 9],
[4, 5, 6], [ 4, 5, 6, 10, 11, 12]])
[7, 8 9],
[10, 11, 12]])

split, on the other hand, slices apart an array into multiple arrays along an axis:

In [38]: from numpy.random import randn

In [39]: arr = randn(5, 2) In [40]: arr
Out [40]:
array([[ 0.1689, 0.3287],

[

[ 0.4703, 0.8989],
[ 0.1535, 0.0243],
[-0.2832, 1.1536],
[ 0.2707, 0.8075]])

In [41]: first, second, third = np.split(arr, [1, 3])

In [42]: first

 

358 | Chapter 12: Advanced NumPy

Out[42]: array([[ 0.1689, 0.3287]])

In [43]: second In [44]: third

Out [43]: Out [44]:

array([[ 0.4703, 0.8989], array([[-0.2832, 1.1536],
[ 0.1535, 0.0243]]) [ 0.2707, 0.8075]])

See Table 12-1 for a list of all relevant concatenation and splitting functions, some of
which are provided only as a convenience of the very general purpose concatenate.

Table 12-1. Array concatenation functions

Function Description

concatenate Most general function, concatenates collection of arrays along one axis
vstack, row_stack Stack arrays row-wise (along axis 0)

hstack Stack arrays column-wise (along axis 1)

column_stack Like hstack, but converts 1D arrays to 2D column vectors first

dstack Stack arrays “depth"-wise (along axis 2)

split Split array at passed locations along a particular axis

hsplit / vsplit / dsplit Convenience functions for splitting on axis 0, 1, and 2, respectively.

Stacking helpers: r_ and c_

There are two special objects in the NumPy namespace, r_ and c_, that make stacking
arrays more concise:

In [45]: arr = np.arange(6)
In [46]: arr1 = arr.reshape((3, 2))

In [47]: arr2 = randn(3, 2)

In [48]: np.r_[arr1, arr2] In [49]: np.c_[np.r [arr1, arr2], arr]
Out [48]: Out[49]:
array([[ 0 » 1. 1], array([[ 0 » 1. » 0. 1
[ 2 » 3. |, [ 2 » 3B , %  ],
[4 , 5 ], [4. 5, 5 5 2 J,
[ 0.7258, -1.5325], [ 0.7258, -1.5325, 3. J],
[-0.4696, -0.2127], [-0.4696, -0.2127, 4. ];
[-0.1072, 1.2871]]) [-0.1072, 1.2871, 5. 1)

These additionally can translate slices to arrays:
In [50]: np.c_[1:6, -10:-5]

Out[50]:

array([[ 1, -10],
[ 2; -9],
[ 35 -8],
[ 4, -7];
[ 5, -6]])

See the docstring for more on what you can do with c_andr_.

 

Advanced Array Manipulation | 359

Repeating Elements: Tile and Repeat

 

a,

Oo The need to replicate or repeat arrays is less common with NumPy than
“ s . it is with other popular array programming languages like MATLAB.
~*_ 448s The main reason for this is that broadcasting fulfills this need better,

 

* which is the subject of the next section.

The two main tools for repeating or replicating arrays to produce larger arrays are the
repeat and tile functions. repeat replicates each element in an array some number of
times, producing a larger array:

In [51]: arr = np.arange(3)
In [52]: arr.repeat(3)
Out[52]: array([0, 0, 0, 1, 1, 1, 2, 2, 2])

By default, if you pass an integer, each element will be repeated that number of times.
If you pass an array of integers, each element can be repeated a different number of
times:

In [53]: arr.repeat([2, 3, 4])
Out[53]: array([0, 0, 1, 1, 1, 2, 2, 2, 2])

Multidimensional arrays can have their elements repeated along a particular axis.

In [54]: arr = randn(2, 2)

In [55]: arr In [56]: arr.repeat(2, axis=0)
Out[55]: Out[56]:
array([[ 0.7157, -0.6387], array([[ 0.7157, -0.6387],
[ 0.3626, 0.849 ]]) [ 0.7157, -0.6387],
[ 0.3626, 0.849 ],

[ 0.3626, 0.849 ]])

Note that if no axis is passed, the array will be flattened first, which is likely not what
you want. Similarly you can pass an array of integers when repeating a multidimen-
sional array to repeat a given slice a different number of times:

In [57]: arr.repeat([2, 3], axis=0)
Out [57]:

array([[ 0.7157, -0.6387],
[ 0.7157, -0.6387],
[ 0.3626, 0.849 ],
[ 0.3626, 0.849 ],
[

0.3626, 0.849 ]])

In [58]: arr.repeat([2, 3], axis=1)

Out [58]:

array([[ 0.7157, 0.7157, -0.6387, -0.6387, -0.6387],
[ 0.3626, 0.3626, 0.849 , 0.849 , 0.849 ]])

 

360 | Chapter 12: Advanced NumPy

tile, on the other hand, is a shortcut for stacking copies of an array along an axis. You
can visually think about it as like “laying down tiles”:

In [59]: arr

Out [59]:

array([[ 0.7157, -0.6387],
[ 0.3626, 0.849 ]])

In [60]: np.tile(arr, 2)

Out [60]:

array([[ 0.7157, -0.6387, 0.7157, -0.6387],
[ 0.3626, 0.849 , 0.3626, 0.849 ]])

The second argument is the number of tiles; with a scalar, the tiling is made row-by-
row, rather than column by column: The second argument to tile can be a tuple in-
dicating the layout of the “tiling”:
In [61]: arr
Out [61]:
array([[ 0.7157, -0.6387],
[ 0.3626, 0.849 ]])

In [62]: np.tile(arr, (2, 1)) In [63]: np.tile(arr, (3, 2))
Out [62]: Out [63]:
array([[ 0.7157, -0.6387], array([[ 0.7157, -0.6387, 0.7157, -0.6387],
[ 0.3626, 0.849 ], [ 0.3626, 0.849 , 0.3626, 0.849 J,
[ 0.7157, -0.6387], [ 0.7157, -0.6387, 0.7157, -0.6387],
[ 0.3626, 0.849 ]]) [ 0.3626, 0.849 , 0.3626, 0.849 J,
[ 0.7157, -0.6387, 0.7157, -0.6387],
[ 0.3626, 0.849 , 0.3626, 0.849 ]])

Fancy Indexing Equivalents: Take and Put

As you may recall from Chapter 4, one way to get and set subsets of arrays is by
fancy indexing using integer arrays:
In [64]: arr = np.arange(10) * 100

In [65]: inds = [7, 1, 2, 6] In [66]: arr[inds]
Out[66]: array([700, 100, 200, 600])

There are alternate ndarray methods that are useful in the special case of only making
a selection on a single axis:

In [67]: arr.take(inds)
Out[67]: array([700, 100, 200, 600])

In [68]: arr.put(inds, 42)

In [69]: arr
Out[69]: array([ 0, 42, 42, 300, 400, 500, 42, 42, 800, 900])

In [70]: arr.put(inds, [40, 41, 42, 43])

 

Advanced Array Manipulation | 361

In
Out

71]: arr
71]: array([ 0, 41, 42, 300, 400, 500, 43, 40, 800, 900])

To use take along other axes, you can pass the axis keyword:
In [72]: inds = [2, 0, 2, 1]

In [73]: arr = randn(2, 4)

In [74]: arr

Out[74]:

array([[-0.8237, 2.6047, -0.4578, -1. 1
[ 2.3198, -1.0792, 0.518 , 0.2527]])

In [75]: arr.take(inds, axis=1)
Out[75]:
array([[-0.4578, -0.8237, -0.4578, 2.6047],

[ 0.518 , 2.3198, 0.518 , -1.0792]])

 

put does not accept an axis argument but rather indexes into the flattened (one-di-
mensional, C order) version of the array (this could be changed in principle). Thus,
when you need to set elements using an index array on other axes, you will want to use
fancy indexing.

As of this writing, the take and put functions in general have better
performance than their fancy indexing equivalents by a significant mar-
ia° gin. I regard this as a “bug” and something to be fixed in NumPy, but
* it’s something worth keeping in mind if you’re selecting subsets of large
arrays using integer arrays:

  

 

In [76]: arr = randn(1000, 50)

# Random sample of 500 rows
In [77]: inds = np.random.permutation(1000) [:500]

In [78]: %timeit arr[inds]
1000 loops, best of 3: 356 us per loop

In [79]: %timeit arr.take(inds, axis=0)
10000 loops, best of 3: 34 us per loop

Broadcasting

Broadcasting describes how arithmetic works between arrays of different shapes. It is
a very powerful feature, but one that can be easily misunderstood, even by experienced
users. The simplest example of broadcasting occurs when combining a scalar value
with an array:

In [80]: arr = np.arange(5)

In [81]: arr In [82]: arr * 4
Out[81]: array([0, 1, 2, 3, 4]) Out[82]: array([ 0, 4, 8, 12, 16])

 

362 | Chapter 12: Advanced NumPy

Here we say that the scalar value 4 has been broadcast to all of the other elements in
the multiplication operation.

For example, we can demean each column of an array by subtracting the column means.
In this case, it is very simple:
In [83]: arr = randn(4, 3)

In [84]: arr.mean(0)
Out[84]: array([ 0.1321, 0.552 , 0.8571])

In [85]: demeaned = arr - arr.mean(0)

In [86]: demeaned In [87]: demeaned.mean(0)
Out [86]: Out[87]: array([ 0., -0., -0.])
array([[ 0.1718, -0.1972, -1.3669],

[-0.1292, 1.6529, -0.3429],

[-0.2891, -0.0435, 1.2322],

[ 0.2465, -1.4122, 0.4776]])

See Figure 12-4 for an illustration of this operation. Demeaning the rows as a broadcast
operation requires a bit more care. Fortunately, broadcasting potentially lower dimen-
sional values across any dimension of an array (like subtracting the row means from
each column of a two-dimensional array) is possible as long as you follow the rules.
This brings us to:

 

(4, 3) (3, ) (4,3)

appa] * Bhs
3] a | * [ats{s|

Figure 12-4. Broadcasting over axis 0 with a 1D array

 

 

 

 

The Broadcasting Rule

Two arrays are compatible for broadcasting if for each trailing dimension (that is, start-
ing from the end), the axis lengths match or if either of the lengths is 1. Broadcasting
is then performed over the missing and / or length 1 dimensions.

 

 

 

Even as an experienced NumPy user, I often must stop to draw pictures and think about
the broadcasting rule. Consider the last example and suppose we wished instead to
subtract the mean value from each row. Since arr.mean(0) has length 3, it is compatible

 

Broadcasting | 363

for broadcasting across axis 0 because the trailing dimension in arr is 3 and therefore
matches. According to the rules, to subtract over axis 1 (that is, subtract the row mean
from each row), the smaller array must have shape (4, 1):

In [88]: arr

Out [88]:

array([[ 0.3039, 0.3548, -0.5097],
[ 0.0029, 2.2049, 0.5142],
[-0.1571, 0.5085, 2.0893],
[ 0.3786, -0.8602, 1.3347]])

In [89]: row means = arr.mean(1) In [90]: row_means.reshape((4, 1))
Out [90]:

array([[ 0.0496],

[ 0.9073],

[ 0.8136],

[ 0.2844]]

)

In [91]: demeaned = arr - row _means.reshape((4, 1))

In [92]: demeaned.mean(1)
Out[92]: array([ 0., 0., 0., 0.])

Has your head exploded yet? See Figure 12-5 for an illustration of this operation.

 

(4,3) (4,1)

 

 

 

 

Figure 12-5. Broadcasting over axis 1 of a 2D array

See Figure 12-6 for another illustration, this time adding a two-dimensional array to a
three-dimensional one across axis 0.

Broadcasting Over Other Axes

Broadcasting with higher dimensional arrays can seem even more mind-bending, but
it is really a matter of following the rules. If you don’t, you'll get an error like this:

In [93]: arr - arr.mean(1)

ValueError Traceback (most recent call last)
<ipython-input-93-7b87b85a20b2> in <module>()

 

364 | Chapter 12: Advanced NumPy

----> 1 arr - arr.mean(1)
ValueError: operands could not be broadcast together with shapes (4,3) (4)

 

(3,4, 2) (4,2) (3,4, 2)

  

 

 

 

Figure 12-6. Broadcasting over axis 0 of a 3D array

It’s quite common to want to perform an arithmetic operation with a lower dimensional
array across axes other than axis 0. According to the broadcasting rule, the “broadcast
dimensions” must be 1 in the smaller array. In the example of row demeaning above
this meant reshaping the row means to be shape (4, 1) instead of (4,):
In [94]: ary - arr.mean(1).reshape((4, 1))
Out[94]:
array([[ 0.2542, 0.3051, -0.5594],
[-0.9044, 1.2976, -0.3931],
[-0.9707, -0.3051, 1.2757],
[ 0.0942, -1.1446, 1.0503]])

In the three-dimensional case, broadcasting over any of the three dimensions is only a
matter of reshaping the data to be shape-compatible. See Figure 12-7 for a nice visual-
ization of the shapes required to broadcast over each axis of a three-dimensional array.

A very common problem, therefore, is needing to add a new axis with length 1 specif-
ically for broadcasting purposes, especially in generic algorithms. Using reshape is one
option, but inserting an axis requires constructing a tuple indicating the new shape.
This can often be a tedious exercise. Thus, NumPy arrays offer a special syntax for
inserting new axes by indexing. We use the special np.newaxis attribute along with
“full” slices to insert the new axis:

In [95]: arr = np.zeros((4, 4))
In [96]: arr_3d = arr[:, np.newaxis, :] In [97]: arr_3d.shape
Out[97]: (4, 1, 4)

In [98]: arr_1d = np.random.normal(size=3)

In [99]: arr_id[:, np.newaxis] In [100]: arr_id[np.newaxis, :]
Out[99]: Out[100]: array([[-0.3899, 0.396 , -0.1852]])

 

Broadcasting | 365

array([[-0.3899],
[ 0.396 ],
[-0.1852]])

 

Full array shape: (8, 5, 3) Axis 2: (8, 5, 1)

  

Axis 0: 6, 3)\| J L | axis 1: (8, 1, 3)
@, 5, 3) =

 

 

 

Figure 12-7. Compatible 2D array shapes for broadcasting over a 3D array

Thus, if we had a three-dimensional array and wanted to demean axis 2, say, we would
only need to write:

In [101]: arr = randn(3, 4, 5)
In [102]: depth_means = arr.mean(2)

In [103]: depth_means

Out[103]:

array([[ 0.1097, 0.3118, -0.5473, 0.2663],
[ 0.1747, 0.1379, 0.1146, -0.4224],
[ 0.0217, 0.3686, -0.0468, 1.3026]])

In [104]: demeaned = arr - depth_means[:, :, np.newaxis]

In [105]: demeaned.mean(2)
Out[105]:

array([[ 0., 0., -0., 0.],
[ 0., -0., -0., 0.],
[-0., -0., 0., 0.]])

If you’re completely confused by this, don’t worry. With practice you will get the hang
of it!

 

366 | Chapter 12: Advanced NumPy

Some readers might wonder if there’s a way to generalize demeaning over an axis
without sacrificing performance. There is, in fact, but it requires some indexing
gymnastics:

def demean_axis(arr, axis=0):
means = arr.mean(axis)

# This generalized things like [:, :, np.newaxis] to N dimensions
indexer = [slice(None)] * arr.ndim

indexer[axis] = np.newaxis

return arr - means[ indexer]

Setting Array Values by Broadcasting

The same broadcasting rule governing arithmetic operations also applies to setting
values via array indexing. In the simplest case, we can do things like:

In [106]: arr = np.zeros((4, 3))

In [107]: arr[:] = 5 n [108]:
aucthane

array([[ 5., 5., 5-],

[5.5 5.5 5],

[5.5 5.5 5-],

[5-5 5.5 5-]])

However, if we had a one-dimensional array of values we wanted to set into the columns
of the array, we can do that as long as the shape is compatible:

In [109]: col = np.array([1.28, -0.42, 0.44, 1.6])

In [110]: arr[:] = col[:, np.newaxis] In [111]: arr
Out[111]:
array([{[ 1.28, 1.28, 1.28],
[-0.42, -0.42, -0.42],
[ 0.44, 0.44, 0.44],
[1.6 , 1.6, 1.6 ]])

In [112]: arr[:2] = [[-1.37], [0.509]] In [113]: arr
Out [113]:
array([[-1.37 , -1.37 , -1.37 ],
[ 0.509, 0.509, 0.509],
[ 0.44, 0.44, 0.44 ],
[1.6 , 1.6 , 1.6 ]])

Advanced ufunc Usage

While many NumPy users will only make use of the fast element-wise operations pro-
vided by the universal functions, there are a number of additional features that occa-
sionally can help you write more concise code without loops.

 

Advanced ufunc Usage | 367

ufunc Instance Methods

Each of NumPy’s binary ufuncs has special methods for performing certain kinds of
special vectorized operations. These are summarized in Table 12-2, but ’ll give a few
concrete examples to illustrate how they work.

reduce takes a single array and aggregates its values, optionally along an axis, by per-
forming a sequence of binary operations. For example, an alternate way to sum ele-
ments in an array is to use np.add. reduce:

In [114]: arr = np.arange(10)

In [115]: np.add.reduce(arr)
Out[115]: 45

In [116]: arr.sum()

Out[116]: 45
The starting value (0 for add) depends on the ufunc. If an axis is passed, the reduction
is performed along that axis. This allows you to answer certain kinds of questions in a
concise way. As a less trivial example, we can use np. logical_and to check whether the
values in each row of an array are sorted:

In [118]: arr = randn(5, 5)
In [119]: arr[::2].sort(1) # sort a few rows

In [120]: arr[:, :-1] < arr[:, 1:]
Out[120]:
array([{[ True, True, True, True],
[False, True, False, False],
[ True, True, True, True],
[ True, False, True, True],
[ True, True, True, True]], dtype=bool)

In [121]: np.logical_and.reduce(arr[:, :-1] < arr[:, 1:], axis=1)
Out[121]: array([ True, False, True, False, True], dtype=bool)

Of course, logical_and.reduce is equivalent to the all method.
accumulate is related to reduce like cumsum is related to sum. It produces an array of the
same size with the intermediate “accumulated” values:

In [122]: arr = np.arange(15).reshape((3, 5))

In [123

Out[123

]: np.add.accumulate(arr, axis=1)
]
array({[ 0, 1, 3, 6, 10],
[
[

5, 11, 18, 26, 35],
10, 21, 33, 46, 60]])

outer performs a pairwise cross-product between two arrays:

In [124]: arr = np.arange(3).repeat([1, 2, 2])

 

368 | Chapter 12: Advanced NumPy

In [125]: arr
Out[125]: array([0, 1, 1, 2, 2])

In [126]: np.multiply.outer(arr, np.arange(5))
Out [126]:
array([[0, 0, 0, 0, 0],
[0, 1, 2, 3, 4],
[0, 1, 2, 3, 4],
[0, 2; 4, 6, 8],
[0, 2, 4, 6, 8]])
The output of outer will have a dimension that is the sum of the dimensions of the
inputs:

In [127]: result = np.subtract.outer(randn(3, 4), randn(5))

In [128]: result.shape
Out[128]: (3, 4, 5)

The last method, reduceat, performs a “local reduce”, in essence an array groupby op-
eration in which slices of the array are aggregated together. While it’s less flexible than
the GroupBy capabilities in pandas, it can be very fast and powerful in the right cir-

cumstances. It accepts a sequence of “bin edges” which indicate how to split and ag-
gregate the values:

In [129]: arr = np.arange(10)

In [130]: np.add.reduceat(arr, [0, 5, 8])
Out[130]: array([10, 18, 17])

The results are the reductions (here, sums) performed over arr[0:5], arr[5:8], and
arr[8:]. Like the other methods, you can pass an axis argument:

In [131]: arr = np.multiply.outer(np.arange(4), np.arange(5))

In [132]: arr In [133]: np.add.reduceat(arr, [0, 2, 4], axis=1)
Out [132]: Out [133]:
array([[ 0, 0, 0, 0, ol], array([[ 0, 0, 0],
[o, 1, 2, 3, 4], [1, 5, 4],
[0, 2, 4, 6, 8], [ 2, 10, 8],
[ 0, 3, 6, 9, 12]]) [ 3, 15, 12]])
Table 12-2. ufunc methods
Method Description
reduce(x) Aggregate values by successive applications of the operation
accumulate(x) Aggregate values, preserving all partial aggregates
reduceat(x, bins) “Local” reduce or “group by”. Reduce contiguous slices of data to produce aggregated
array.
outer(x, y) Apply operation to all pairs of elements in x and y. Result array has shape x. shape +
y. shape

 

 

Advanced ufunc Usage | 369

Custom ufuncs

There are a couple facilities for creating your own functions with ufunc-like semantics.
numpy. frompyfunc accepts a Python function along with a specification for the number
of inputs and outputs. For example, a simple function that adds element-wise would
be specified as:

In [134]: def add_elements(x, y):
severe o § return x + y

In [135]: add them = np.frompyfunc(add_elements, 2, 1)

In [136]: add_them(np.arange(8), np.arange(8))
Out[136]: array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object)

Functions created using frompyfunc always return arrays of Python objects which isn’t
very convenient. Fortunately, there is an alternate, but slightly less featureful function
numpy.vectorize that is a bit more intelligent about type inference:

In [137]: add_them = np.vectorize(add_elements, otypes=[np.float64])

In [138]: add_them(np.arange(8), np.arange(8))

Out[138]: array([ 0., 2., 4., 6., 8., 10., 12., 14.])
These functions provide a way to create ufunc-like functions, but they are very slow
because they require a Python function call to compute each element, which is a lot
slower than NumPy’s C-based ufunc loops:

In [139]: arr = randn(10000)

In [140]: %timeit add_them(arr, arr)
100 loops, best of 3: 2.12 ms per loop

In [141]: %timeit np.add(arr, arr)
100000 loops, best of 3: 11.6 us per loop

There are a number of projects under way in the scientific Python community to make
it easier to define new ufuncs whose performance is closer to that of the built-in ones.

Structured and Record Arrays

You may have noticed up until now that ndarray is a homogeneous data container; that
is, it represents a block of memory in which each element takes up the same number
of bytes, determined by the dtype. On the surface, this would appear to not allow you
to represent heterogeneous or tabular-like data. A structured array is an ndarray in
which each element can be thought of as representing a struct in C (hence the “struc-
tured” name) or a row in a SQL table with multiple named fields:

In [142]: dtype = [('x', np.floaté4), (‘y', np.int32)]

In [143]: sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)

 

370 | Chapter 12: Advanced NumPy

In [144]: sarr

Out[ 144]:

array([(1.5, 6), (3.141592653589793, -2)
dtype=[("x', '<f8"), (‘y', '<id')]

];
)
There are several ways to specify a structured dtype (see the online NumPy documen-
tation). One typical way is as a list of tuples with (field_name, field_data_type). Now,
the elements of the array are tuple-like objects whose elements can be accessed like a
dictionary:

In [145]: sarr[o]

Out[145]: (1.5, 6)

In [146]: sarr[o]['y']

Out[146]: 6
The field names are stored in the dtype.names attribute. On accessing a field on the
structured array, a strided view on the data is returned thus copying nothing:

In [147]: sarr['x']
Out[147]: array([ 1.5  , 3.1416])

Nested dtypes and Multidimensional Fields

When specifying a structured dtype, you can additionally pass a shape (as an int or
tuple):
In [148]: dtype = [('x', np.int64, 3), (‘y', np.int32)]

In [149]: arr = np.zeros(4, dtype=dtype)

In [150]: arr
Out[150]:
array([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, o], o)],
dtype=[('x', '<i8', (3,)), Cy', ‘<i4')])
In this case, the x field now refers to an array of length three for each record:
In [151]: arr[o]['x']
Out[151]: array([0, 0, 0])
Conveniently, accessing arr['x'] then returns a two-dimensional array instead of a
one-dimensional array as in prior examples:
In [152]: arr['x']
Out[152]:
array([[0, 0, 0],
[0, 0, o],
[0, 0, o],
[0, 0, 0]])
This enables you to express more complicated, nested structures as a single block of
memory in an array. Though, since dtypes can be arbitrarily complex, why not nested
dtypes? Here is a simple example:

 

Structured and Record Arrays | 371

I

=

[153]: dtype = [(‘x', [(‘a', 'f8"), (‘b', ‘f4")]), Cy’, np.int32)]
In [154]: data = np.array([((1, 2), 5), ((3, 4), 6)], dtype=dtype)

In [155]: data['x']

Out[155]:

array([(1.0, 2.0), (3.0, 4.0)],
dtype=[('a', '<f8'), (‘b', ‘<f4')])

In [156]: data['y']
Out[156]: array([5, 6], dtype=int32)

In [157]: data['x']['a']
Out[157]: array([ 1., 3.])

 

As you can see, variable-shape fields and nested records is a very rich feature that can
be the right tool in certain circumstances. A DataFrame from pandas, by contrast, does
not support this feature directly, though it is similar to hierarchical indexing.

Why Use Structured Arrays?

Compared with, say, a DataFrame from pandas, NumPy structured arrays are a com-
paratively low-level tool. They provide a means to interpreting a block of memory as a
tabular structure with arbitrarily complex nested columns. Since each element in the
array is represented in memory as a fixed number of bytes, structured arrays provide a
very fast and efficient way of writing data to and from disk (including memory maps,
more on this later), transporting it over the network, and other such use.

As another common use for structured arrays, writing data files as fixed length record
byte streams is a common way to serialize data in C and C++ code, which is commonly
found in legacy systems in industry. As long as the format of the file is known (the size
of each record and the order, byte size, and data type of each element), the data can be
read into memory using np. fromfile. Specialized uses like this are beyond the scope of
this book, but it’s worth knowing that such things are possible.

Structured Array Manipulations: numpy.lib.recfunctions

While there is not as much functionality available for structured arrays as for Data-
Frames, the NumPy module numpy.1lib.recfunctions has some helpful tools for adding
and dropping fields or doing basic join-like operations. The thing to remember with
these tools is that it is typically necessary to create a new array to make any moditfica-
tions to the dtype (like adding or dropping a column). These functions are left to the
interested reader to explore as I do not use them anywhere in this book.

 

372 | Chapter 12: Advanced NumPy

More About Sorting

Like Python’s built-in list, the ndarray sort instance method is an in-place sort, meaning
that the array contents are rearranged without producing a new array:

In [158]: arr = randn(6)
In [159]: arr.sort()

In [160]: arr
Out[160]: array([-1.082 , 0.3759, 0.8014, 1.1397, 1.2888, 1.8413])

When sorting arrays in-place, remember that if the array is a view on a different ndarray,
the original array will be modified:

In [161]: arr = randn(3, 5)

In [162]: arr

Out [162]:

array([[-0.3318, -1.4711, 0.8705, -0.0847, -1.1329],
[-1.0111, -0.3436, 2.1714, 0.1234, -0.0189],
[ 0.1773, 0.7424, 0.8548, 1.038 , -0.329 ]])

In [163]: arr[:, 0].sort() # Sort first column values in-place

In [164]: arr

Out[164]:

array([[-1.0111, -1.4711, 0.8705, -0.0847, -1.1329],
[-0.3318, -0.3436, 2.1714, 0.1234, -0.0189],
[ 0.1773, 0.7424, 0.8548, 1.038 , -0.329 ]])

On the other hand, numpy.sort creates a new, sorted copy of an array. Otherwise it
accepts the same arguments (such as kind, more on this below) as ndarray.sort:

In [165]: arr = randn(5)

In [166]: arr
Out[166]: array([-1.1181, -0.2415, -2.0051, 0.7379, -1.0614])

In [167]: np.sort(arr)
Out[167]: array([-2.0051, -1.1181, -1.0614, -0.2415, 0.7379])

In [168]: arr
Out[168]: array([-1.1181, -0.2415, -2.0051, 0.7379, -1.0614])

All of these sort methods take an axis argument for sorting the sections of data along
the passed axis independently:

In [169]: arr = randn(3, 5)

In [170]: arr

Out [170]:

array([[ 0.5955, -0.2682, 1.3389, -0.1872, 0.9111],
[-0.3215, 1.0054, -0.5168, 1.1925, -0.1989],
[ 0.3969, -1.7638, 0.6071, -0.2222, -0.2171]])

 

More About Sorting | 373

In [171]: arr.sort(axis=1)

In [172]: arr

Out[172]:

array([[-0.2682, -0.1872, 0.5955, 0.9111, 1.3389],
[-0.5168, -0.3215, -0.1989, 1.0054, 1.1925],
[-1.7638, -0.2222, -0.2171, 0.3969, 0.6071]])

You may notice that none of the sort methods have an option to sort in descending
order. This is not actually a big deal because array slicing produces views, thus not
producing a copy or requiring any computational work. Many Python users are familiar
with the “trick” that for a list values, values[::-1] returns a list in reverse order. The
same is true for ndarrays:

In [173]: arr[:, ::-4]

Out [173]:

array([[ 1.3389, 0.9111, 0.5955, -0.1872, -0.2682]

,
[ 1.1925, 1.0054, -0.1989, -0.3215, -0.5168],
[ 0.6071, 0.3969, -0.2171, -0.2222, -1.7638]])

Indirect Sorts: argsort and lexsort

In data analysis it’s very common to need to reorder data sets by one or more keys. For
example, a table of data about some students might need to be sorted by last name then
by first name. This is an example of an indirect sort, and if you’ve read the pandas-
related chapters you have already seen many higher-level examples. Given a key or keys
(an array or values or multiple arrays of values), you wish to obtain an array of integer
indices (I refer to them colloquially as indexers) that tells you how to reorder the data
to be in sorted order. The two main methods for this are argsort and numpy.lexsort.
As a trivial example:

In [174]: values = np.array([5, 0, 1, 3, 2])
In [175]: indexer = values.argsort()

176]: indexer
176]: array([1, 2, 4, 3, 0])

177]: values[indexer ]
177]: array([0, 1, 2, 3, 5])

As a less trivial example, this code reorders a 2D array by its first row:

In [178]: arr = randn(3, 5)

In [179]: arr[0] = values

 

In [180]: arr

Out[180]

array([{[ 5. >» 0. » 1. > 3. » 2. 1
[-0.3636, -0.1378, 2.1777, -0.4728, 0.8356],
[-0.2089, 0.2316, 0.728 , -1.3918, 1.9956]])

 

374 | Chapter 12: Advanced NumPy

In [181]: arr[:, arr[0].argsort()]

Out [181]:

array([[ 0. » 1. » 2. > 3. > 5s 1
[-0.1378, 2.1777, 0.8356, -0.4728, -0.3636],
[ 0.2316, 0.728 , 1.9956, -1.3918, -0.2089]])

lexsort is similar to argsort, but it performs an indirect lexicographical sort on multiple
key arrays. Suppose we wanted to sort some data identified by first and last names:

In [182]: first_name = np.array(['Bob', ‘Jane’, ‘Steve’, 'Bill', ‘Barbara'])
In [183]: last_name = np.array(['Jones', 'Arnold', ‘Arnold’, 'Jones', 'Walters'])
In [184]: sorter = np.lexsort((first_name, last_name))

In [185]: zip(last_name[sorter], first_name[sorter])
Out [185]:

[(‘Arnold', ‘Jane'),

(‘Arnold', 'Steve'),

(‘Jones', 'Bill'),

(‘Jones', 'Bob'),

(‘Walters', 'Barbara')]

lexsort can be a bit confusing the first time you use it because the order in which the
keys are used to order the data starts with the last array passed. As you can see,
last_name was used before first_name.

Va

‘

   
 

pandas methods like Series’s and DataFrame’s sort_index methods and
the Series order method are implemented with variants of these func-
2° tions (which also must take into account missing values)

 

Alternate Sort Algorithms

A stable sorting algorithm preserves the relative position of equal elements. This can
be especially important in indirect sorts where the relative ordering is meaningful:

In [186]: values = np.array(['2:first', '2:second', '1:first', '1:second', '1:third'])
In [187]: key = np.array([2, 2, 1, 1, 1])
In [188]: indexer = key.argsort(kind='mergesort' )

In [189]: indexer
Out[189]: array([2, 3, 4, 0, 1])

In [190]: values.take(indexer)

Out[190]:
array(['1:first', '1:second', ‘1:third', '2:first', '2:second'],
dtype='|S8')

The only stable sort available is mergesort which has guaranteed O(n log n) performance
(for complexity buffs), but its performance is on average worse than the default

 

More About Sorting | 375

quicksort method. See Table 12-3 for a summary of available methods and their relative
performance (and performance guarantees). This is not something that most users will
ever have to think about but useful to know that it’s there.

Table 12-3. Array sorting methods

Kind Speed Stable Workspace Worst-case
"quicksort' 1 NC) 0 O(n2)
‘mergesort' 2 Yes n/2 O(n log n)
"heapsort' 3 No 0 O(n log n)

 

At the time of this writing, sort algorithms other than quicksort are not
tay) available on arrays of Python objects (dtype=object). This means occa-

sionally that algorithms requiring stable sorting will require work-
arounds when dealing with Python objects.

 

numpy.searchsorted: Finding elements in a Sorted Array

searchsorted is an array method that performs a binary search on a sorted array, re-
turning the location in the array where the value would need to be inserted to maintain
sortedness:

In [191]: arr = np.array([0, 1, 7, 12, 15])
In [192]: arr.searchsorted(9)
Out[192]: 3
As you might expect, you can also pass an array of values to get an array of indices back:

In [193]: arr.searchsorted([0, 8, 11, 16])
Out[193]: array([0, 3, 3, 5])

You might have noticed that searchsorted returned 0 for the 0 element. This is because
the default behavior is to return the index at the left side of a group of equal values:
In [194]: arr = np.array([0, 0, 0, 1, 1, 1, 1])

In [195]: arr.searchsorted([0, 1])
Out[195]: array([0, 3])

In [196]: arr.searchsorted([0, 1], side='right')
Out[196]: array([3, 7])

As another application of searchsorted, suppose we had an array of values between 0
and 10,000) and a separate array of “bucket edges” that we wanted to use to bin the
data:

In [197]: data = np.floor(np.random.uniform(0, 10000, size=50))

In [198]: bins = np.array([0, 100, 1000, 5000, 10000])

In [199]: data

 

376 | Chapter 12: Advanced NumPy

Out[199]:

array([ 8304. 4181. 9352. 4907. 3250. 8546. 2673. 6152.

d a J d a J , ’
2774., 5130., 9553., 4997., 1794., 9688., 426., 1612.,
651., 8653., 1695., 4764., 1052., 4836., 8020., 3479.,
1513., 5872., 8992., 7656., 4764., 5383., 2319., 4280.,
4150., 8601., 3946., 9904., 7286., 9969., 6032., 4574.,
8480., 4298., 2708., 7358., 6439., 7916., 3899., 9182.,
871., 7973.])

To then get a labeling of which interval each data point belongs to (where 1 would
mean the bucket [0, 100)), we can simply use searchsorted:

In [200]: labels = bins.searchsorted(data)

In [201]: labels

Out[201]:

array([4, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 2, 3, 2, 4, 3, 3, 3, 3, 4,
3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4,
3, 4, 2, 4])

This, combined with pandas’s groupby, can be used to easily bin data:
In [202]: Series(data).groupby(labels) .mean()
Out[202]:
2 649 . 333333

3 3411.521739
4 7935 .041667

Note that NumPy actually has a function digitize that computes this bin labeling:
In [203]: np.digitize(data, bins)
Out [203]:
array([4, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 2, 3, 2, 4, 3, 3, 3, 35 4,
3, 3, 4, 4, 4, 3, 4, 3, 3, 35 4, 35 4, 4, 4, 4, 35 4, 3, 3, 4, 4, 4;
3, 4, 2, 4])

NumPy Matrix Class

Compared with other languages for matrix operations and linear algebra, like MAT-
LAB, Julia, and GAUSS, NumPy’s linear algebra syntax can often be quite verbose. One
reason is that matrix multiplication requires using numpy.dot. Also NumPy’s indexing
semantics are different, which makes porting code to Python less straightforward at
times. Selecting a single row (e.g. X[1, :]) or column (e.g. X[:, 1]) from a 2D array
yields a 1D array compared with a 2D array as in, say, MATLAB.
In [204]: X = np.array([[ 8.82768214, 3.82222409, -1.14276475, 2.04411587
weeee i [ 3.82222409, 6.75272284, 0.83909108, 2.08293758

],
],
soneed [-1.14276475, 0.83909108, 5.01690521, 0.79573241],
]]

wwesed [ 2.04411587, 2.08293758, 0.79573241, 6.24095859]])

In [205]: X[:, 0] # one-dimensional
Out[205]: array([ 8.8277, 3.8222, -1.1428, 2.0441])

In [206]: y = X[:, :1] # two-dimensional by slicing

 

NumPy Matrix Class | 377

In [207]: X

Out [207]:

array([[ 8.8277,
[ 3.8222,
[-1.1428,
[ 2.0441,

In [208]: y

Out [208]:

array([[ 8.8277],
[ 3.8222],

[-1.1428],

[

2.0441]])

3.8222,
6.7527,
0.8391,
2.0829,

-1.1428, 2.0441],
0.8391, 2.0829],
5.0169, 0.7957],
0.7957, 6.241 J])

In this case, the product y! X y would be expressed like so:

In [209]: np.dot(y.T, np.dot(X, y))
Out[209]: array([[ 1195.468]])

To aid in writing code with a lot of matrix operations, NumPy has a matrix class which
has modified indexing behavior to make it more MATLAB-like: single rows and col-
umns come back two-dimensional and multiplication with * is matrix multiplication.
The above operation with numpy.matrix would look like:

In [210]: Xm

In [211]: ym = Xm[:, 0]

In [212]: Xm

Out[212]:

matrix([[ 8.8277,
[ 3.8222,
[-1.1428,
[ 2.0441,

In [213]: ym

Out [213]:

matrix([[ 8.8277]
[ 3.8222]
[-1.1428]

,

,

J

3.8222,
6.7527,
0.8391,
2.0829,

[ 2.0441]])

In [214]: ym.T * Xm * ym
Out[214]: matrix([[ 1195.468]])

np.matrix(X)

-1.1428, 2.0441],
0.8391, 2.0829],
5.0169, 0.7957],
0.7957, 6.241 ]])

matrix also has a special attribute I which returns the matrix inverse:

In [215]: Xm.I * X

Out [215]:

matrix([[ 1., -0.
[, Oss: ds
[0., 0.
[0., 0.

eevee

1
o
t
rROoOO°O
es
moe ee
VY

o

BR
ve ev

 

378 | Chapter 12: Advanced NumPy

I do not recommend using numpy.matrix as a replacement for regular ndarrays because
they are generally more seldom used. In individual functions with lots of linear algebra,
it may be helpful to convert the function argument to matrix type, then cast back to
regular arrays with np.asarray (which does not copy any data) before returning them.

Advanced Array Input and Output

In Chapter 4, Iintroduced you to np. save and np. load for storing arrays in binary format
on disk. There are a number of additional options to consider for more sophisticated
use. In particular, memory maps have the additional benefit of enabling you to work
with data sets that do not fit into RAM.

Memory-mapped Files

A memory-mapped file is a method for treating potentially very large binary data on
disk as an in-memory array. NumPy implements a memmap object that is ndarray-like,
enabling small segments of a large file to be read and written without reading the whole
array into memory. Additionally, amemmap has the same methods as an in-memory array
and thus can be substituted into many algorithms where an ndarray would be expected.

To create a new memmap, use the function np.memmap and pass a file path, dtype, shape,
and file mode:
In [216]: mmap = np.memmap('mymmap', dtype='float64', mode='w+', shape=(10000, 10000) )
In [217]: mmap

Out [217]:
memmap([[ 0., 0., 0., ..-, O., 0., 0O.],

[ 0., 0., 0., ..., 0., 0O., O.],

[ 0., 0., 0., ..., 0O., 0O., O.],
wey

[ 0., 0., 0., ..., 0., 0O., O.],

[ 0., 0., 0., ..., 0., 0O., O.],

[ 0., 0., 0., ..., 0., O., 0.]])

Slicing a memmap returns views on the data on disk:

In [218]: section = mmap[:5]
If you assign data to these, it will be buffered in memory (like a Python file object), but
can be written to disk by calling flush:

In [219]: section[:] = np.random.randn(5, 10000)
In [220]: mmap.flush()

In [221]: mmap

Out[221]:

memmap([[-0.1614, -0.1768, 0.422 , ..., -0.2195, -0.1256, -0.4012],
[ 0.4898, -2.2219, -0.7684, ..., -2.3517, -1.0782, 1.3208],
[-0.6875, 1.6901, -0.7444, ..., -1.4218, -0.0509, 1.2224],

 

Advanced Array Input and Output | 379

0 5, OF 5, O yay OF 85 OF 8, 0% 6],
[0. , 0 5, OF ,.0, 0% 4» 0% 5, O |,
[0 , 0 5, OF ,.645 0% 5» O 5, O  J))

In [222]: del mmap

Whenever a memory map falls out of scope and is garbage-collected, any changes will
be flushed to disk also. When opening an existing memory map, you still have to specify
the dtype and shape as the file is just a block of binary data with no metadata on disk:

In [223]: mmap = np.memmap('mymmap', dtype='float64', shape=(10000, 10000))

In [224]: mmap

Out [224]:

memmap([[-0.1614, -0.1768, 0.422 , ..., -0.2195, -0.1256, -0.4012],
[ 0.4898, -2.2219, -0.7684, ..., -2.3517, -1.0782, 1.3208],
[-0.6875, 1.6901, -0.7444, ..., -1.4218, -0.0509, 1.2224],

0 » 0. » 0. yp eeey OD » +O. >» 0. 1
[ 0. » 0. » 0. yp eeey OD » +O. >» 0. 1
[0 5, O 5 OF yee, OF 5 O 5 0 J)

Since a memory map is just an on-disk ndarray, there are no issues using a structured
dtype as described above.

HDF5 and Other Array Storage Options

PyTables and h5py are two Python projects providing NumPy-friendly interfaces for
storing array data in the efficient and compressible HDF5 format (HDF stands for
hierarchical data format). You can safely store hundreds of gigabytes or even terabytes
of data in HDF5 format. The use of these libraries is unfortunately outside the scope

of the book.

PyTables provides a rich facility for working with structured arrays with advanced
querying features and the ability to add column indexes to accelerate queries. This is
very similar to the table indexing capabilities provided by relational databases.

Performance Tips

Getting good performance out of code utilizing NumPy is often straightforward, as
array operations typically replace otherwise comparatively extremely slow pure Python
loops. Here is a brief list of some of the things to keep in mind:

¢ Convert Python loops and conditional logic to array operations and boolean array
operations

¢ Use broadcasting whenever possible

e Avoid copying data using array views (slicing)

¢ Utilize ufuncs and ufunc methods

 

3

oo

0 | Chapter 12: Advanced NumPy

If you can’t get the performance you require after exhausting the capabilities provided
by NumPy alone, writing code in C, Fortran, or especially Cython (see a bit more on
this below) may be in order. I personally use Cython (http://cython.org) heavily in my
own work as an easy way to get C-like performance with minimal development.

The Importance of Contiguous Memory

While the full extent of this topic is a bit outside the scope of this book, in some ap-
plications the memory layout of an array can significantly affect the speed of compu-
tations. This is based partly on performance differences having to do with the cache
hierarchy of the CPU; operations accessing contiguous blocks of memory (for example,
summing the rows of a C order array) will generally be the fastest because the memory
subsystem will buffer the appropriate blocks of memory into the ultrafast L1 or L2 CPU
cache. Also, certain code paths inside NumPy’s C codebase have been optimized for
the contiguous case in which generic strided memory access can be avoided.

To say that an array’s memory layout is contiguous means that the elements are stored
in memory in the order that they appear in the array with respect to Fortran (column
major) or C (row major) ordering. By default, NumPy arrays are created as C-contigu-
ous or just simply contiguous. A column major array, such as the transpose of a C-
contiguous array, is thus said to be Fortran-contiguous. These properties can be ex-
plicitly checked via the flags attribute on the ndarray:

In [227]: arr_c = np.ones((1000, 1000), order='C')

In [228]: arr_f = np.ones((1000, 1000), order='F')

In [229]: arr_c.flags In [230]: arr_f.flags

Out[229]: Out [230]:
C_CONTIGUOUS : True C_CONTIGUOUS : False
F_CONTIGUOUS : False F_CONTIGUOUS : True
OWNDATA : True OWNDATA : True
WRITEABLE : True WRITEABLE : True
ALIGNED : True ALIGNED : True
UPDATEIFCOPY : False UPDATEIFCOPY : False

In [231]: arr_f.flags.f contiguous

Out[231]: True
In this example, summing the rows of these arrays should, in theory, be faster for
arr_c than arr_f since the rows are contiguous in memory. Here I check for sure using
%timeit in [Python:

In [232]: %timeit arr_c.sum(1)
1000 loops, best of 3: 1.33 ms per loop

In [233]: %timeit arr_f.sum(1)
100 loops, best of 3: 8.75 ms per loop

 

Performance Tips | 381

When looking to squeeze more performance out of NumPy, this is often a place to
invest some effort. If you have an array that does not have the desired memory order,
you can use copy and pass either 'C' or 'F':
In [234]: arr_f.copy('C').flags
Out [234]:
C_ CONTIGUOUS : True
F_CONTIGUOUS : False
OWNDATA : True
WRITEABLE : True
ALIGNED : True
UPDATEIFCOPY : False

When constructing a view on an array, keep in mind that the result is not guaranteed
to be contiguous:
In [235]: arr_c[:50].flags.contiguous In [236]: arr_c[:, :50].flags
Out[235]: True Out [236]:
C_CONTIGUOUS : False
F_CONTIGUOUS : False
OWNDATA : False
WRITEABLE : True
ALIGNED : True
UPDATEIFCOPY : False

Other Speed Options: Cython, f2py, C

In recent years, the Cython project ((http://cython.org) has become the tool of choice
for many scientific Python programmers for implementing fast code that may need to
interact with C or C++ libraries, but without having to write pure C code. You can
think of Cython as Python with static types and the ability to interleave functions im-
plemented in C into Python-like code. For example, a simple Cython function to sum
the elements of a one-dimensional array might look like:

from numpy cimport ndarray, floaté4 t

def sum_elements(ndarray[float64 t] arr):
cdef Py ssize t i, n = len(arr)
cdef float64_t result = 0

for i in range(n):
result += arr[i]

return result

Cython takes this code, translates it to C, then compiles the generated C code to create
a Python extension. Cython is an attractive option for performance computing because
the code is only slightly more time-consuming to write than pure Python code and it
integrates closely with NumPy. A common workflow is to get an algorithm working in
Python, then translate it to Cython by adding type declarations and a handful of other
tweaks. For more, see the project documentation.

 

382 | Chapter 12: Advanced NumPy

Some other options for writing high performance code with NumPy include f2py, a
wrapper generator for Fortran 77 and 90 code, and writing pure C extensions.

 

Performance Tips | 383


APPENDIX
Python Language Essentials

 

Knowledge is a treasure, but practice is the key to it.

—Thomas Fuller

People often ask me about good resources for learning Python for data-centric appli-
cations. While there are many excellent Python language books, I am usually hesitant
to recommend some of them as they are intended for a general audience rather than
tailored for someone who wants to load in some data sets, do some computations, and
plot some of the results. There are actually a couple of books on “scientific program-
ming in Python”, but they are geared toward numerical computing and engineering
applications: solving differential equations, computing integrals, doing Monte Carlo
simulations, and various topics that are more mathematically-oriented rather than be-
ing about data analysis and statistics. As this is a book about becoming proficient at
working with data in Python, I think it is valuable to spend some time highlighting the
most important features of Python’s built-in data structures and libraries from the per-
spective of processing and manipulating structured and unstructured data. As such, I
will only present roughly enough information to enable you to follow along with the
rest of the book.

This chapter is not intended to be an exhaustive introduction to the Python language
but rather a biased, no-frills overview of features which are used repeatedly throughout
this book. For new Python programmers, I recommend that you supplement this chap-
ter with the official Python tutorial (http://docs.python.org) and potentially one of the
many excellent (and much longer) books on general purpose Python programming. In
my opinion, it is not necessary to become proficient at building good software in Python
to be able to productively do data analysis. I encourage you to use [Python to experi-
ment with the code examples and to explore the documentation for the various types,
functions, and methods. Note that some of the code used in the examples may not
necessarily be fully-introduced at this point.

Much of this book focuses on high performance array-based computing tools for work-
ing with large data sets. In order to use those tools you must often first do some munging
to corral messy data into a more nicely structured form. Fortunately, Python is one of

 

385

the easiest-to-use languages for rapidly whipping your data into shape. The greater your
facility with Python, the language, the easier it will be for you to prepare new data sets
for analysis.

The Python Interpreter

Python is an interpreted language. The Python interpreter runs a program by executing
one statement at a time. The standard interactive Python interpreter can be invoked on
the command line with the python command:

$ python

Python 2.7.2 (default, Oct 4 2011, :09)

[GCC 4.6.1] on linux2
Type "help", "copyright", "credits" or "license" for more information.

>>> a=5
>>> print a
5

The >>> you see is the prompt where you'll type expressions. To exit the Python inter-
preter and return to the command prompt, you can either type exit() or press Ctr1-D.

Running Python programs is as simple as calling python with a . py file as its first argu-
ment. Suppose we had created hello _world.py with these contents:

print ‘Hello world'

This can be run from the terminal simply as:

$ python hello _world.py
Hello world

While many Python programmers execute all of their Python code in this way, many
scientific Python programmers make use of IPython, an enhanced interactive Python
interpreter. Chapter 3 is dedicated to the [Python system. By using the %run command,
IPython executes the code in the specified file in the same process, enabling you to
explore the results interactively when it’s done.

$ ipython

Python 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul 3 2011, :51)
Type "copyright", "credits" or "license" for more information.

IPython 0.12 -- An enhanced Interactive Python.

? -> Introduction and overview of IPython's features.
#quickref -> Quick reference.

help -> Python's own help system.

object? -> Details about ‘object’, use 'object??' for extra details.

In [1]: %run hello _world.py
Hello world

In [2]:

 

386 | Appendix: Python Language Essentials

The default [Python prompt adopts the numbered In [2]: style compared with the
standard >>> prompt.

The Basics

Language Semantics

The Python language design is distinguished by its emphasis on readability, simplicity,
and explicitness. Some people go so far as to liken it to “executable pseudocode”.

Indentation, not braces

Python uses whitespace (tabs or spaces) to structure code instead of using braces as in
many other languages like R, C++, Java, and Perl. Take the for loop in the above
quicksort algorithm:
for x in array:
if x < pivot:
less.append(x)
else:
greater .append(x)

A colon denotes the start of an indented code block after which all of the code must be
indented by the same amount until the end of the block. In another language, you might
instead have something like:
for x in array {
if x < pivot {
less. append(x)

} else {
greater .append(x)
}

}

One major reason that whitespace matters is that it results in most Python code looking
cosmetically similar, which means less cognitive dissonance when you read a piece of
code that you didn’t write yourself (or wrote in a hurry a year ago!). In a language
without significant whitespace, you might stumble on some differently formatted code
like:

for x in array

{

if x < pivot
less. append(x)
else

{
greater .append(x)

 

The Basics | 387

}
}

Love it or hate it, significant whitespace is a fact of life for Python programmers, and
in my experience it helps make Python code a lot more readable than other languages
I’ve used. While it may seem foreign at first, I suspect that it will grow on you after a
while.

a a,

    
 

I strongly recommend that you use 4 spaces to as your default indenta-
tion and that your editor replace tabs with 4 spaces. Many text editors
via° have a setting that will replace tab stops with spaces automatically (do
this!). Some people use tabs or a different number of spaces, with 2
spaces not being terribly uncommon. 4 spaces is by and large the stan-
dard adopted by the vast majority of Python programmers, so I recom-
mend doing that in the absence of a compelling reason otherwise.

 

As you can see by now, Python statements also do not need to be terminated by sem-
icolons. Semicolons can be used, however, to separate multiple statements on a single
line:

a=5; b=6; c=7

Putting multiple statements on one line is generally discouraged in Python as it often
makes code less readable.

Everything is an object

An important characteristic of the Python language is the consistency of its object
model. Every number, string, data structure, function, class, module, and so on exists
in the Python interpreter in its own “box” which is referred to as a Python object. Each
object has an associated type (for example, string or function) and internal data. In
practice this makes the language very flexible, as even functions can be treated just like
any other object.

Comments

Any text preceded by the hash mark (pound sign) # is ignored by the Python interpreter.
This is often used to add comments to code. At times you may also want to exclude
certain blocks of code without deleting them. An easy solution is to comment out the
code:
results = []
for line in file handle:
# keep the empty lines for now
# if len(line) == 0:
# continue
results.append(line.replace('foo', '‘bar'))

 

388 | Appendix: Python Language Essentials

Function and object method calls

Functions are called using parentheses and passing zero or more arguments, optionally
assigning the returned value to a variable:

result = f(x, y, z)
g()

Almost every object in Python has attached functions, known as methods, that have
access to the object’s internal contents. They can be called using the syntax:

obj.some_method(x, y, z)

Functions can take both positional and keyword arguments:
result = f(a, b, c, d=5, e='foo')

More on this later.

Variables and pass-by-reference

When assigning a variable (or name) in Python, you are creating a reference to the object

on the right hand side of the equals sign. In practical terms, consider a list of integers:
In [241]: a = [1, 2, 3]

Suppose we assign a to a new variable b:

In [242]: b=a
In some languages, this assignment would cause the data [1, 2, 3] to be copied. In
Python, a and b actually now refer to the same object, the original list [1, 2, 3] (see

Figure A-1 for a mockup). You can prove this to yourself by appending an element to
a and then examining b:

In [243]: a.append(4)

In [244]: b
Out[244]: [1, 2, 3, 4]

 

list
eee!

a ————>

b ————>

 

 

 

Figure A-1. Two references for the same object

Understanding the semantics of references in Python and when, how, and why data is
copied is especially critical when working with larger data sets in Python.

 

The Basics | 389

Assignment is also referred to as binding, as we are binding a name to
an object. Variables names that have been assigned may occasionally be
‘12° referred to as bound variables.

 
 

 

When you pass objects as arguments to a function, you are only passing references; no
copying occurs. Thus, Python is said to pass by reference, whereas some other languages
support both pass by value (creating copies) and pass by reference. This means that a
function can mutate the internals of its arguments. Suppose we had the following func-
tion:

def append_element(some_list, element):
some_list.append(element)

Then given what’s been said, this should not come as a surprise:
In [2]: data = [1, 2, 3]
In [3]: append _element(data, 4)
In [4]: data
Out[4]: [1, 2, 3, 4]
Dynamic references, strong types

In contrast with many compiled languages, such as Java and C++, object references in
Python have no type associated with them. There is no problem with the following:

In [245]: a=5 In [246]: type(a)
Out[246]: int
In [247]: a = 'foo' In [248]: type(a)

Out[248]: str

Variables are names for objects within a particular namespace; the type information is
stored in the object itself. Some observers might hastily conclude that Python is not a
“typed language”. This is not true; consider this example:

In [249]: '5' +5

TypeError Traceback (most recent call last)
<ipython-input-249-f9dbf5f0b234> in <module>()
woo 1 '5' 45

TypeError: cannot concatenate ‘str’ and ‘int’ objects

In some languages, such as Visual Basic, the string '5' might get implicitly converted
(or casted) to an integer, thus yielding 10. Yet in other languages, such as JavaScript,
the integer 5 might be casted to a string, yielding the concatenated string '55'. In this
regard Python is considered a strongly-typed language, which means that every object
has a specific type (or class), and implicit conversions will occur only in certain obvious
circumstances, such as the following:

 

390 | Appendix: Python Language Essentials

In [250]: a

4.5

In [251]: b = 2

# String formatting, to be visited later
In [252]: print 'a is %s, b is %s' % (type(a), type(b))
a is <type 'float'>, b is <type ‘int'>

In [253]: a / b
Out [253]: 2.25

Knowing the type of an object is important, and it’s useful to be able to write functions
that can handle many different kinds of input. You can check that an object is an
instance of a particular type using the isinstance function:

In [254]: a=5 In [255]: isinstance(a, int)
Out[255]: True
isinstance can accept a tuple of types if you want to check that an object’s type is
among those present in the tuple:
In [256]: a = 53 b = 4.5

In [257]: isinstance(a, (int, float) ) In [258]: isinstance(b, (int, float))
Out[257]: True Out[258]: True
Attributes and methods

Objects in Python typically have both attributes, other Python objects stored “inside”
the object, and methods, functions associated with an object which can have access to
the object’s internal data. Both of them are accessed via the syntax obj. attribute_name:

In [1]: a = 'foo'

In [2]: a.<Tab>

a.capitalize a.format a.isupper a.rindex a.strip
a.center a.index a.join a.rjust a. swapcase
a.count a.isalnum a.ljust a.rpartition a.title
a.decode a.isalpha a. lower a.rsplit a.translate
a.encode a.isdigit a.lstrip a.rstrip a.upper
a.endswith a.islower a.partition a.split a.zfill
a.expandtabs a.isspace a.replace a.splitlines

a. find a.istitle a.rfind a.startswith

Attributes and methods can also be accessed by name using the getattr function:
>>> getattr(a, ‘split')
<function split>

While we will not extensively use the functions getattr and related functions hasattr
and setattr in this book, they can be used very effectively to write generic, reusable
code.

 

The Basics | 391

“Duck” typing

Often you may not care about the type of an object but rather only whether it has certain
methods or behavior. For example, you can verify that an object is iterable if it imple-
mented the iterator protocol. For many objects, this means it has a __iter__ “magic
method”, though an alternative and better way to checkis to try using the iter function:
def isiterable(obj):
try:
iter (obj)
return True

except TypeError: # not iterable
return False

This function would return True for strings as well as most Python collection types:

In [260]: isiterable('a string’) In [261]: isiterable([1, 2, 3])
Out[260]: True Out[261]: True

In [262]: isiterable(5)
Out[ 262]: False

A place where I use this functionality all the time is to write functions that can accept
multiple kinds of input. A common case is writing a function that can accept any kind
of sequence (list, tuple, ndarray) or even an iterator. You can first check if the object is
a list (or a NumPy array) and, if it is not, convert it to be one:

if not isinstance(x, list) and isiterable(x):
x = list(x)

Imports

In Python a module is simply a -py file containing function and variable definitions
along with such things imported from other . py files. Suppose that we had the following
module:

# some_module. py
PI = 3.14159

def f(x):
return x + 2

def g(a, b):
return a + b

If we wanted to access the variables and functions defined in some_module.py, from
another file in the same directory we could do:

import some_module
result = some_module.f(5)
pi = some_module.PI

Or equivalently:

from some_module import f, g, PI
result = g(5, PI)

 

392 | Appendix: Python Language Essentials

By using the as keyword you can give imports different variable names:

import some_module as sm
from some_module import PI as pi, g as gf

x1 = sm.f(pi)
r2 = gf(6, pi)

Binary operators and comparisons

Most of the binary math operations and comparisons are as you might expect:
In [263]: 5-7 In [264]: 12 + 21.5
Out [263]: -2 Out [264]: 33.5

In [265]: 5 <= 2
Out[265]: False

See Table A-1 for all of the available binary operators.
To check if two references refer to the same object, use the is keyword. is not is also
perfectly valid if you want to check that two objects are not the same:

In [266]: a = [1, 2, 3]

In [267]: b=a

# Note, the list function always creates a new list
In [268]: c = list(a)

269]: a is b In [270]: a is not c
269]: True Out[270]: True

Out

Note this is not the same thing is comparing with ==, because in this case we have:

271]: a == Cc
271]: True

 

Out

A very common use of is and is not is to check if a variable is None, since there is only
one instance of None:

In [272]: a = None

In [273]: a is None
Out[273]: True

Table A-1. Binary operators

Operation Description

at+b Add a andb

a-b Subtract b froma

a*b Multiply a by b

a/b Divide a by b

a//b Floor-divide a by b, dropping any fractional remainder

 

The Basics | 393

Operation Description

a ** b Raise a to the b power

a&b True if both a and b are True. For integers, take the bitwise AND.

a |b True if either a or b is True. For integers, take the bitwise OR.

a’*b For booleans, True if a or b is True, but not both. For integers, take the bitwise EXCLUSIVE-OR.
a == True if a equals b

a !=b True if a is not equal to b

a <= b, a <b Trueifais less than (less than or equal) to b
a >b, a>=b _ Trueifais greater than (greater than or equal) to b
ais b True if a and b reference same Python object

a is not b True if a and b reference different Python objects

Strictness versus laziness
When using any programming language, it’s important to understand when expressions
are evaluated. Consider the simple expression:

a=b=ce#5
d=a+b*c

* iW

In Python, once these statements are evaluated, the calculation is immediately (or
strictly) carried out, setting the value of d to 30. In another programming paradigm,
such as in a pure functional programming language like Haskell, the value of d might
not be evaluated until it is actually used elsewhere. The idea of deferring computations
in this way is commonly known as lazy evaluation. Python, on the other hand, is a very
strict (or eager) language. Nearly all of the time, computations and expressions are
evaluated immediately. Even in the above simple expression, the result of b * c is
computed as a separate step before adding it to a.

There are Python techniques, especially using iterators and generators, which can be
used to achieve laziness. When performing very expensive computations which are only
necessary some of the time, this can be an important technique in data-intensive ap-
plications.

Mutable and immutable objects

Most objects in Python are mutable, such as lists, dicts, NumPy arrays, or most user-
defined types (classes). This means that the object or values that they contain can be

modified.
In [274]: a_list = ['foo', 2, [4, 5]]

In [275]: a_list[2] = (3, 4)

In [276]: a_list
Out[276]: ['foo', 2, (3, 4)]

 

394 | Appendix: Python Language Essentials

Others, like strings and tuples, are immutable:
In [277]: a_tuple = (3, 5, (4, 5))

In [278]: a_tuple[1] = ‘four'

TypeError Traceback (most recent call last)

<ipython-input-278-b7966a9ae0f1> in <module>()

----> 1 a_tuple[1] = 'four'

TypeError: ‘tuple’ object does not support item assignment
Remember that just because you can mutate an object does not mean that you always
should. Such actions are known in programming as side effects. For example, when
writing a function, any side effects should be explicitly communicated to the user in
the function’s documentation or comments. If possible, I recommend trying to avoid
side effects and favor immutability, even though there may be mutable objects involved.

Scalar Types

Python has a small set of built-in types for handling numerical data, strings, boolean
(True or False) values, and dates and time. See Table A-2 for a list of the main scalar
types. Date and time handling will be discussed separately as these are provided by the
datetime module in the standard library.

Table A-2. Standard Python Scalar Types

Type Description
None The Python “null” value (only one instance of the None object exists)
str String type. ASCll-valued only in Python 2.x and Unicode in Python 3

unicode Unicode string type

float Double-precision (64-bit) floating point number. Note there is no separate doub1e type.

bool A True or False value

int Signed integer with maximum value determined by the platform.

long Arbitrary precision signed integer. Large int values are automatically converted to long.
Numeric types

The primary Python types for numbers are int and float. The size of the integer which
can be stored as an int is dependent on your platform (whether 32 or 64-bit), but Python
will transparently convert a very large integer to long, which can store arbitrarily large
integers.

In [279]: ival = 17239871

In [280]: ival ** 6
Out[280]: 26254519291092456596965462913230729701102721L

 

The Basics | 395

Floating point numbers are represented with the Python float type. Under the hood
each one is a double-precision (64 bits) value. They can also be expressed using scien-
tific notation:

In [281]: fval = 7.243

In [282]: fval2 = 6.78e-5
In Python 3, integer division not resulting in a whole number will always yield a floating
point number:

In [284]: 3 / 2

Out[284]: 1.5

In Python 2.7 and below (which some readers will likely be using), you can enable this
behavior by default by putting the following cryptic-looking statement at the top of
your module:

from future import division
Without this in place, you can always explicitly convert the denominator into a floating
point number:

In [285]: 3 / float(2)

Out [285]: 1.5
To get C-style integer division (which drops the fractional part if the result is not a
whole number), use the floor division operator //:

In [286]: 3 // 2

Out[286]: 1
Complex numbers are written using j for the imaginary part:

In [287]: cval = 1 + 2j

In [288]: cval * (1 - 2})
Out[288]: (5+07)

Strings

Many people use Python for its powerful and flexible built-in string processing capa-
bilities. You can write string literal using either single quotes ' or double quotes ":

a = ‘one way of writing a string’
b = "another way"

For multiline strings with line breaks, you can use triple quotes, either ''' or

ez
This is a longer string that
spans multiple lines

Python strings are immutable; you cannot modify a string without creating a new string:

 

396 | Appendix: Python Language Essentials

In [289]: a = 'this is a string’

In [290]: a[10] = 'f'

TypeError Traceback (most recent call last)
<ipython-input-290-5ca625d1e504> in <module>()

----> 1 a[10] = 'f'

TypeError: 'str' object does not support item assignment

In [291]: b = a.replace('string', ‘longer string')

In [292]: b
Out[292]: ‘this is a longer string'

Many Python objects can be converted to a string using the str function:
In [293]: a = 5.6 In [294]: s = str(a)

In [295]: s
Out[295]: '5.6'

Strings are a sequence of characters and therefore can be treated like other sequences,
such as lists and tuples:
In [296]: s = ‘python’ In [297]: list(s)
Out[297]: ['p's ‘y', 't', "hl 5 ‘o'*» ‘n']
In [298]: s[:3]
Out[298]: ‘pyt'

The backslash character \ is an escape character, meaning that it is used to specify
special characters like newline \n or unicode characters. To write a string literal with
backslashes, you need to escape them:

In [299]: s = '12\\34'

In [300]: print s
12\34

If you have a string with a lot of backslashes and no special characters, you might find
this a bit annoying. Fortunately you can preface the leading quote of the string with r
which means that the characters should be interpreted as is:

In [301]: s = r'this\has\no\special\characters'

In [302]: s
Out[302]: 'this\\has\\no\\special\\characters'

Adding two strings together concatenates them and produces a new string:
In [303]: a = 'this is the first half '

In [304]: b = 'and this is the second half'

In [305]: a+b
Out[305]: ‘this is the first half and this is the second half'

 

The Basics | 397

String templating or formatting is another important topic. The number of ways to do
so has expanded with the advent of Python 3, here I will briefly describe the mechanics
of one of the main interfaces. Strings with a % followed by one or more format characters
is a target for inserting a value into that string (this is quite similar to the printf function
in C). As an example, consider this string:

In [306]: template = '%.2f %s are worth $%d'

In this string, %s means to format an argument as a string, %. 2 a number with 2 decimal
places, and %d an integer. To substitute arguments for these format parameters, use the
binary operator % with a tuple of values:

In [307]: template % (4.5560, ‘Argentine Pesos’, 1)
Out[307]: '4.56 Argentine Pesos are worth $1'

String formatting is a broad topic; there are multiple methods and numerous options
and tweaks available to control how values are formatted in the resulting string. To
learn more, I recommend you seek out more information on the web.

I discuss general string processing as it relates to data analysis in more detail in Chap-
ter.

Booleans

The two boolean values in Python are written as True and False. Comparisons and
other conditional expressions evaluate to either True or False. Boolean values are com-
bined with the and and or keywords:

In [308]: True and True
Out[308]: True

In [309]: False or True
Out[309]: True

Almost all built-in Python tops and any class defining the _ nonzero__ magic method
have a True or False interpretation in an if statement:
In [310]: a = [1, 2, 3]
weeeet ifa:
wenn ed print 'I found something! '

I found something!

In [311]: b = []
..eee: if not b:
acuea of print ‘Empty!’

Most objects in Python have a notion of true- or falseness. For example, empty se-
quences (lists, dicts, tuples, etc.) are treated as False if used in control flow (as above
with the empty list b). You can see exactly what boolean value an object coerces to by
invoking bool on it:

 

398 | Appendix: Python Language Essentials

In [312]: bool([]), bool([1, 2, 3])
Out[312]: (False, True)

In [313]: bool('Hello world!'), bool('')
Out[313]: (True, False)

In [314]: bool(0), bool(1)
Out[314]: (False, True)
Type casting

The str, bool, int and float types are also functions which can be used to cast values
to those types:

In [315]: s = '3.14159'

In [316]: fval = float(s) In [317]: type(fval)
Out[317]: float
In [318]: int(fval) In [319]: bool(fval) In [320]: bool(0)
Out [318]: 3 Out[319]: True Out[320]: False
None

None is the Python null value type. If a function does not explicitly return a value, it
implicitly returns None.
In [321]: a = None In [322]: a is None
Out[322]: True

In [323]: b=5 In [324]: b is not None
Out[324]: True

None is also a common default value for optional function arguments:

def add_and_maybe_multiply(a, b, c=None):
result = a+b

if c is not None:
result = result * c

return result

While a technical point, it’s worth bearing in mind that None is not a reserved keyword
but rather a unique instance of NoneType.

Dates and times

The built-in Python datetime module provides datetime, date, and time types. The
datetime type as you may imagine combines the information stored in date and time
and is the most commonly used:

In [325]: from datetime import datetime, date, time

In [326]: dt = datetime(2011, 10, 29, 20, 30, 21)

 

The Basics | 399

In [327]: dt.day In [328]: dt.minute
Out[327]: 29 Out[328]: 30

Given a datetime instance, you can extract the equivalent date and time objects by
calling methods on the datetime of the same name:

In [329]: dt.date() In [330]: dt.time()
Out[329]: datetime.date(2011, 10, 29) Out[330]: datetime.time(20, 30, 21)

The strftime method formats a datetime as a string:

In [331]: dt.strftime('%m/%d/%Y %H:%M' )
Out[331]: ‘10/29/2011 '

Strings can be converted (parsed) into datetime objects using the strptime function:

In [332]: datetime.strptime('20091031', '%Y%m%d' )
Out[332]: datetime.datetime(2009, 10, 31, 0, 0)

See Table 10-2 for a full list of format specifications.

When aggregating or otherwise grouping time series data, it will occasionally be useful
to replace fields of a series of datetimes, for example replacing the minute and second
fields with zero, producing a new object:

In [333]: dt.replace(minute=0, second=0)
Out [333]: datetime.datetime(2011, 10, 29, 20, 0)

The difference of two datetime objects produces a datetime. timedelta type:
In [334]: dt2 = datetime(2011, 11, 15, 22, 30)

In [335]: delta = dt2 - dt

336]: delta In [337]: type(delta)

336]: datetime.timedelta(17, 7179) Out[337]: datetime.timedelta
Adding a timedelta to a datetime produces a new shifted datetime:

338]: dt

338]: datetime.datetime(2011, 10, 29, 20, 30, 21)

339]: dt + delta
339]: datetime.datetime(2011, 11, 15, 22, 30)

 

Control Flow

if, elif, and else
The if statement is one of the most well-known control flow statement types. It checks
a condition which, if True, evaluates the code in the block that follows:

if x < 0:
print ‘It's negative’

 

400 | Appendix: Python Language Essentials

An if statement can be optionally followed by one or more elif blocks and a catch-all
else block if all of the conditions are False:

if x <0:
print ‘It's negative’
elif x == 0:
print 'Equal to zero'
elif 0<x <5:
print 'Positive but smaller than 5'
else:
print ‘Positive and larger than or equal to 5'

If any of the conditions is True, no further elif or else blocks will be reached. With a
compound condition using and or or, conditions are evaluated left-to-right and will
short circuit:

In [340]: a= 5; b=7

In [341]: c = 8; d

4

In [342]: if a< bor c>d:
werey 6 print 'Made it'

In this example, the comparisonc > d never gets evaluated because the first comparison
was True.

for loops

for loops are for iterating over a collection (like a list or tuple) or an iterater. The
standard syntax for a for loop is:

for value in collection:
# do something with value

A for loop can be advanced to the next iteration, skipping the remainder of the block,
using the continue keyword. Consider this code which sums up integers in a list and
skips None values:

sequence = [1, 2, None, 4, None, 5]
total = 0
for value in sequence:
if value is None:
continue
total += value

A for loop can be exited altogether using the break keyword. This code sums elements
of the list until a 5 is reached:

sequence = [1, 2, 0, 4, 6, 5, 2, 1]
total_until_5 = 0
for value in sequence:
if value ==
break
total_until_5 += value

 

The Basics | 401

As we will see in more detail, if the elements in the collection or iterator are sequences
(tuples or lists, say), they can be conveniently unpacked into variables in the for loop
statement:

for a, b, c in iterator:
# do something

while loops

A while loop specifies a condition and a block of code that is to be executed until the
condition evaluates to False or the loop is explicitly ended with break:

xX = 256

total = 0

while x > 0:
if total > 500:

break
total += x
x=x//2

pass

pass is the “no-op” statement in Python. It can be used in blocks where no action is to
be taken; it is only required because Python uses whitespace to delimit blocks:

if x <0:
print ‘negative!’

elif x == 0:
# TODO: put something smart here
pass

else:

print ‘positive! '

It’s common to use pass as a place-holder in code while working on a new piece of
functionality:
def f(x, y, z):
# TODO: implement this function!
pass

Exception handling

Handling Python errors or exceptions gracefully is an important part of building robust
programs. In data analysis applications, many functions only work on certain kinds of
input. As an example, Python’s float function is capable of casting a string to a floating
point number, but fails with ValueError on improper inputs:

In [343]: float('1.2345')
Out [343]: 1.2345

In [344]: float('something' )

ValueError Traceback (most recent call last)
<ipython-input-344-439904410854> in <module>()

 

402 | Appendix: Python Language Essentials

----> 1 float('something' )
ValueError: could not convert string to float: something

Suppose we wanted a version of float that fails gracefully, returning the input argu-
ment. We can do this by writing a function that encloses the call to float in a try/
except block:

def attempt_float(x):
try:
return float(x)
except:
return x

The code in the except part of the block will only be executed if float(x) raises an
exception:

In [346]: attempt_float('1.2345')
Out [346]: 1.2345

In [347]: attempt_float('something' )
Out[347]: ‘something’

You might notice that float can raise exceptions other than ValueError:
In [348]: float((1, 2))

TypeError Traceback (most recent call last)
<ipython-input-348-842079ebb635> in <module>()

----> 1 float((1, 2))

TypeError: float() argument must be a string or a number

You might want to only suppress ValueError, since a TypeError (the input was not a
string or numeric value) might indicate a legitimate bug in your program. To do that,
write the exception type after except:

def attempt_float(x):
try:
return float(x)
except ValueError:
return x

We have then:
In [350]: attempt_float((1, 2))

TypeError Traceback (most recent call last)
<ipython-input-350-9bdfd730cead> in <module>()
----> 1 attempt_float((1, 2))
<ipython-input-349-3e06b8379b6b> in attempt_float(x)
1 def attempt_float(x):

2 try:

----> 3 return float(x)
4 except ValueError:
5 return x

TypeError: float() argument must be a string or a number

 

The Basics | 403

You can catch multiple exception types by writing a tuple of exception types instead
(the parentheses are required):
def attempt_float(x):
try:
return float(x)

except (TypeError, ValueError):
return x

In some cases, you may not want to suppress an exception, but you want some code
to be executed regardless of whether the code in the try block succeeds or not. To do
this, use finally:

f = open(path, 'w')

try:

write _to_file(f)
finally:

f.close()

Here, the file handle f will always get closed. Similarly, you can have code that executes
only if the try: block succeeds using else:

f = open(path, 'w')

try:

write _to_file(f)
except:

print 'Failed'
else:

print 'Succeeded'
finally:

f.close()

range and xrange

The range function produces a list of evenly-spaced integers:
In [352]: range(10)
Out[352]: [0, 1, 2, 3, 4, 5, 6 7, 8, 9]
Both a start, end, and step can be given:
In [353]: range(0, 20, 2)
Out[353]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
As you can see, range produces integers up to but not including the endpoint. A com-
mon use of range is for iterating through sequences by index:

seq = [1, 2, 3, 4]
for i in range(len(seq)):
val = seq[i]

For very long ranges, it’s recommended to use xrange, which takes the same arguments
as range but returns an iterator that generates integers one by one rather than generating

 

404 | Appendix: Python Language Essentials

all of them up-front and storing them in a (potentially very large) list. This snippet sums
all numbers from 0 to 9999 that are multiples of 3 or 5:

sum = 0
for i in xrange(10000):
# % is the modulo operator

if x %3 == 0 or x%5 == 0:
sum += i

In Python 3, range always returns an iterator, and thus it is not necessary
to use the xrange function

 

 

Ternary Expressions

A ternary expression in Python allows you combine an if-else block which produces
a value into a single line or expression. The syntax for this in Python is

value = true-expr if condition else
false-expr

Here, true-expr and false-expr can be any Python expressions. It has the identical
effect as the more verbose

if condition:

value = true-expr
else:

value = false-expr

This is a more concrete example:
In [354]: x = 5

In [355]: 'Non-negative' if x >= 0 else ‘Negative’
Out[355]: ‘Non-negative'

As with if-else blocks, only one of the expressions will be evaluated. While it may be
tempting to always use ternary expressions to condense your code, realize that you may
sacrifice readability if the condition as well and the true and false expressions are very
complex.

Data Structures and Sequences

Python’s data structures are simple, but powerful. Mastering their use is a critical part
of becoming a proficient Python programmer.

 

Data Structures and Sequences | 405

Tuple
A tuple is a one-dimensional, fixed-length, immutable sequence of Python objects. The
easiest way to create one is with a comma-separated sequence of values:

In [356]: tup = 4, 5, 6

In [357]: tup
Out[357]: (4, 5, 6)

When defining tuples in more complicated expressions, it’s often necessary to enclose
the values in parentheses, as in this example of creating a tuple of tuples:
In [358]: nested _tup = (4, 5, 6), (7, 8)

359]: nested_tup
359]: ((4, 5, 6), (75 8))

Any sequence or iterator can be converted to a tuple by invoking tuple:
In [360]: tuple([4, 0, 2])

Out[360]: (4, 0, 2)

In [361]: tup = tuple('string')

362]: tup
362]: ('s', "th, 'r'y ‘i’, ‘n'y "g')

 

Elements can be accessed with square brackets [] as with most other sequence types.
Like C, C++, Java, and many other languages, sequences are 0-indexed in Python:

In [363]: tup[o]

Out[ 363]: 's'
While the objects stored in a tuple may be mutable themselves, once created it’s not
possible to modify which object is stored in each slot:

In [364]: tup = tuple(['foo', [1, 2], True])

In [365]: tup[2] = False

TypeError Traceback (most recent call last)
<ipython-input-365-c7308343b841> in <module>()

----> 1 tup[2] = False

TypeError: ‘tuple’ object does not support item assignment

# however
In [366]: tup[1].append(3)

In [367]: tup
Out[367]: (‘foo', [1, 2, 3], True)
Tuples can be concatenated using the + operator to produce longer tuples:

In [368]: (4, None, 'foo') + (6, 0) + (‘bar',)
Out[368]: (4, None, 'foo', 6, 0, ‘bar')

 

406 | Appendix: Python Language Essentials

Multiplying a tuple by an integer, as with lists, has the effect of concatenating together
that many copies of the tuple.

In [369]: (‘foo', 'bar') * 4
Out[369]: ('foo', 'bar', 'foo', 'bar', 'foo', ‘bar', ‘foo', ‘bar')

Note that the objects themselves are not copied, only the references to them.

Unpacking tuples

If you try to assign to a tuple-like expression of variables, Python will attempt to un-
pack the value on the right-hand side of the equals sign:

In [370]: tup = (4, 55 6)

In [371]: a, b, c = tup

Even sequences with nested tuples can be unpacked:
In [373]: tup = 4, 5, (6, 7)

In [374]: a, b, (c, d) = tup

 

Using this functionality it’s easy to swap variable names, a task which in many lan-
guages might look like:

i}
tro ll

a
b = tmp
b =

» a=a,b

One of the most common uses of variable unpacking when iterating over sequences of
tuples or lists:

seq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]
for a, b, c in seq:
pass

Another common use is for returning multiple values from a function. More on this
later.

Tuple methods

Since the size and contents of a tuple cannot be modified, it is very light on instance
methods. One particularly useful one (also available on lists) is count, which counts the
number of occurrences of a value:

In [376]: a = (1, 2, 2, 2, 3, 4, 2)

 

Data Structures and Sequences | 407

In [377]: a.count(2)
Out[377]: 4

List
In contrast with tuples, lists are variable-length and their contents can be modified.
They can be defined using square brackets [] or using the list type function:

In [378]: a_list = [2, 3, 7, None]
In [379]: tup = (‘foo', ‘bar’, 'baz')

In [380]: b_list = list(tup) In [381]: b_list
Out[381]: ['foo', 'bar', 'baz']
In [382]: b_list[1] = 'peekaboo' In [383]: b_list
Out[383]: ['foo', 'peekaboo', 'baz']

Lists and tuples are semantically similar as one-dimensional sequences of objects and
thus can be used interchangeably in many functions.

Adding and removing elements
Elements can be appended to the end of the list with the append method:
In [384]: b_list.append(' dwarf")

In [385]: b_list
Out[385]: ['foo', ‘peekaboo', 'baz', 'dwarf']

Using insert you can insert an element at a specific location in the list:
In [386]: b_list.insert(1, 'red')

In [387]: b_list
Out[387]: ['foo', 'red', 'peekaboo', 'baz', ‘dwarf']

insert is computationally expensive compared with append as references
— ta) to subsequent elements have to be shifted internally to make room for

the new element.

 

The inverse operation to insert is pop, which removes and returns an element at a
particular index:

In [388]: b_list.pop(2)

Out[388]: 'peekaboo'

In [389]: b_list
Out[389]: ['foo', 'red', 'baz', ‘dwarf’ ]

Elements can be removed by value using remove, which locates the first such value and
removes it from the last:

 

408 | Appendix: Python Language Essentials

In [390]: b_list.append('foo' )

In [391]: b_list.remove('foo' )

In [392]: b_list

Out[392]: ['red', 'baz', ‘dwarf’, 'foo']
If performance is not a concern, by using append and remove, a Python list can be used
as a perfectly suitable “multi-set” data structure.
You can check if a list contains a value using the in keyword:

In [393]: ‘dwarf’ in b list
Out[393]: True

Note that checking whether a list contains a value is a lot slower than dicts and sets as
Python makes a linear scan across the values of the list, whereas the others (based on
hash tables) can make the check in constant time.

Concatenating and combining lists
Similar to tuples, adding two lists together with + concatenates them:

In [394]: [4, None, 'foo'] + [7, 8, (2, 3)]
Out[394]: [4, None, 'foo', 7, 8, (2, 3)]

If you have a list already defined, you can append multiple elements to it using the
extend method:
In [395]: x = [4, None, 'foo']

In [396]: x.extend([7, 8, (2, 3)])

In [397]: x
Out[397]: [4, None, 'foo', 7, 8, (2, 3)]

Note that list concatenation is a compartively expensive operation since a new list must
be created and the objects copied over. Using extend to append elements to an existing
list, especially if you are building up a large list, is usually preferable. Thus,

everything = []
for chunk in list_of_lists:
everything. extend(chunk)

is faster than than the concatenative alternative

everything = []
for chunk in list_of_lists:
everything = everything + chunk

Sorting
A list can be sorted in-place (without creating a new object) by calling its sort function:
In [398]: a = [7, 2, 5, 1, 3]

 

Data Structures and Sequences | 409

In [399]: a.sort()

In [400]: a
Out[400]: [1, 2, 3, 5, 7]

sort has a few options that will occasionally come in handy. One is the ability to pass
a secondary sort key, i.e. a function that produces a value to use to sort the objects. For
example, we could sort a collection of strings by their lengths:

In [401]: b = ['saw', 'small', 'He', 'foxes', 'six']
In [402]: b.sort(key=len)
In [403]: b

Out[403]: ['He', ‘saw’, 'six', 'small', 'foxes']

Binary search and maintaining a sorted list

The built-in bisect module implements binary-search and insertion into a sorted list.
bisect.bisect finds the location where an element should be inserted to keep it sorted,
while bisect.insort actually inserts the element into that location:

In [404]: import bisect
In [405]: c = [1, 2, 2, 2, 3, 4, 7]

In [406]: bisect.bisect(c, 2) In [407]: bisect.bisect(c, 5)
Out[406]: 4 Out[407]: 6

In [408]: bisect.insort(c, 6)

In [409]: c
Out[409]: [1, 2, 2, 2, 3, 4, 6, 7]

 

The bisect module functions do not check whether the list is sorted as
ta) doing so would be computationally expensive. Thus, using them with

an unsorted list will succeed without error but may lead to incorrect
results.

 

 

 

Slicing
You can select sections of list-like types (arrays, tuples, NumPy arrays) by using slice
notation, which in its basic form consists of start: stop passed to the indexing operator

[]:

In [410]: seq = [7, 2, 3, 7, 5, 6, 0, 4]

In [411]: seq[1:5]
Out[411]: [2, 3, 7, 5]

Slices can also be assigned to with a sequence:
In [412]: seq[3:4] = [6, 3]

 

410 | Appendix: Python Language Essentials

In [413]: seq
Out[413]: [7, 2, 3, 6, 3, 5, 6, 0, 1]

While element at the start index is included, the stop index is not included, so that

the number of elements in the result is stop - start.

Either the start or stop can be omitted in which case they default to the start of the
sequence and the end of the sequence, respectively:

In [414]: seq[:5] In [415]: seq[3:]
Out[414]: [7, 2, 3, 6, 3] Out[415]: [6, 3, 5, 6, 0, 1]

Negative indices slice the sequence relative to the end:

In [416]: seq[-4:] In [417]: seq[-6:-2]
Out[416]: [5, 6, 0, 1] Out[417]: [6, 3, 5, 6]

Slicing semantics takes a bit of getting used to, especially if you’re coming from R or
MATLAB. See Figure A-2 for a helpful illustrating of slicing with positive and negative
integers.

A step can also be used after a second colon to, say, take every other element:

In [418]: seq[::2]
Out[418]: [7, 3, 3, 6, 1]

A clever use of this is to pass -1 which has the useful effect of reversing a list or tuple:

In [419]: seq[::-1]
Out[419]: [1, 0, 6, 5, 3, 6, 3, 2, 7]

2 3 5
pu fete fefofe
0 1 2 3 4 5 6
6 5 4 3 2 4

string[2:4] string[-5:-2]

 

 

 

 

Figure A-2. Illustration of Python slicing conventions

Built-in Sequence Functions

Python has a handful of useful sequence functions that you should familiarize yourself
with and use at any opportunity.

 

Data Structures and Sequences | 411

enumerate
It’s common when iterating over a sequence to want to keep track of the index of the
current item. A do-it-yourself approach would look like:

i=0

for value in collection:
# do something with value
it=1

Since this is so common, Python has a built-in function enumerate which returns a
sequence of (i, value) tuples:

for i, value in enumerate(collection):
# do something with value

When indexing data, a useful pattern that uses enumerate is computing a dict mapping
the values of a sequence (which are assumed to be unique) to their locations in the
sequence:

In [420]: some_list = ['foo', 'bar', 'baz']
In [421]: mapping = dict((v, i) for i, v in enumerate(some_list))

In [422]: mapping
Out[422]: {'bar': 1, 'baz': 2, 'foo': o}

sorted

The sorted function returns a new sorted list from the elements of any sequence:

In [423]: sorted([7, 1, 2, 6, 0, 3, 2])

Out[423]: [0, 1, 2, 2, 3, 6, 7]

In [424]: sorted('horse race’)

Out [424]: [' ‘5 ‘a', ‘G'y re’, ‘e', "h', ‘o'*, 'r', 'r'y ‘s']
A common pattern for getting a sorted list of the unique elements in a sequence is to
combine sorted with set:

In [425]: sorted(set('this is just some string'))

Out [425]: [' mS "e', "g', ‘h', ‘i’, ‘Z', ‘m', ‘n'y "Org 'r', "Ss tty ‘u']
zip
zip “pairs” up the elements of a number of lists, tuples, or other sequences, to create
a list of tuples:

In [426]: seq1 = ['foo', ‘bar', ‘baz']
In [427]: seq2 = ['one', ‘two', 'three']

In [428]: zip(seq1, seq2)
Out[428]: [('foo', 'one'), (‘bar', 'two'), (‘baz', 'three')]

 

412 | Appendix: Python Language Essentials

zip can take an arbitrary number of sequences, and the number of elements it produces
is determined by the shortest sequence:

In [429]: seq3 = [False, True]

In [430]: zip(seq1, seq2, seq3)
Out[430]: [('foo', ‘one', False), (‘bar', 'two', True) ]

A very common use of zip is for simultaneously iterating over multiple sequences,
possibly also combined with enumerate:

In [431]: for i, (a, b) in enumerate(zip(seqi, seq2)):
worewed print('%d: %s, %s' % (i, a, b))

2: baz, three

Given a “zipped” sequence, zip can be applied in a clever way to “unzip” the sequence.
Another way to think about this is converting a list of rows into a list of columns. The
syntax, which looks a bit magical, is:

In [432]: pitchers = [('Nolan', 'Ryan'), (‘Roger', 'Clemens'),
Saale s : (‘Schilling', 'Curt')]

In [433]: first_names, last_names = zip(*pitchers)

In [434]: first_names
Out[434]: (‘'Nolan', 'Roger', 'Schilling')

In [435]: last_names

Out[435]: (‘Ryan', 'Clemens', 'Curt')
We'll look in more detail at the use of * in a function call. It is equivalent to the fol-
lowing:

zip(seq[0], seq[1], ..., seq[len(seq) - 1])

reversed

reversed iterates over the elements of a sequence in reverse order:

In [436]: list(reversed(range(10) ))
Out[436]: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]

Dict

dict is likely the most important built-in Python data structure. Amore common name
for it is hash map or associative array. It is a flexibly-sized collection of key-value pairs,
where key and value are Python objects. One way to create one is by using curly braces
{} and using colons to separate keys and values:

In [437]: empty dict = {}

In [438]: d1 = {'a' : 'some value’, 'b' : [1, 2, 3, 4]}

 

Data Structures and Sequences | 413

In [439]: d1
Out[439]: {'a': ‘some value', 'b': [1, 2, 3, 4]}

Elements can be accessed and inserted or set using the same syntax as accessing ele-
ments of a list or tuple:
In [440]: d1[7] = ‘an integer'

In [441]: da
Out[441]: {7: 'an integer’, 'a': ‘some value’, 'b': [1, 2, 3, 4]}

In [442]: da['b']
Out[442]: [1, 2, 3, 4]

You can check if a dict contains a key using the same syntax as with checking whether
a list or tuple contains a value:

In [443]: 'b' in d1
Out[443]: True

Values can be deleted either using the del keyword or the pop method (which simulta-
neously returns the value and deletes the key):

In [444]: d1[5] = ‘some value'
In [445]: da['dummy'] = ‘another value’
In [446]: del d1[5]

In [447]: ret = d1.pop('dummy' ) In [448]: ret
Out[448]: ‘another value’
The keys and values method give you lists of the keys and values, respectively. While
the key-value pairs are not in any particular order, these functions output the keys and
values in the same order:

In [449]: d1.keys() In [450]: d1.values()
Out[449]: ['a', 'b', 7] Out[450]: ['some value’, [1, 2, 3, 4], ‘an integer']

If you’re using Python 3, dict.keys() and dict.values() are iterators

tay) instead of lists.

 

One dict can be merged into another using the update method:
In [451]: di.update({'b' : 'foo', 'c' : 12})

In [452]: d1
Out[452]: {7: 'an integer’, 'a': ‘some value’, 'b': 'foo', 'c': 12}

 

414 | Appendix: Python Language Essentials

Creating dicts from sequences

It’s common to occasionally end up with two sequences that you want to pair up ele-
ment-wise in a dict. As a first cut, you might write code like this:
mapping = {}
for key, value in zip(key_list, value_list):
mapping[ key] = value
Since a dict is essentially a collection of 2-tuples, it should be no shock that the dict
type function accepts a list of 2-tuples:

In [453]: mapping = dict(zip(range(5), reversed(range(5))))

In [454]: mapping

Out[454]: {0: 4, 1: 3, 2: 2, 3: 1, 4: o}
Ina later section we'll talk about dict comprehensions, another elegant way to construct
dicts.

Default values

It’s very common to have logic like:

if key in some dict:

value = some_dict[key]
else:

value = default_value

Thus, the dict methods get and pop can take a default value to be returned, so that the
above if-else block can be written simply as:

value = some_dict.get(key, default_value)

get by default will return None if the key is not present, while pop will raise an exception.
With setting values, a common case is for the values in a dict to be other collections,
like lists. For example, you could imagine categorizing a list of words by their first letters
as a dict of lists:

In [455]: words = ['apple', 'bat', ‘bar’, ‘atom’, ‘book']
In [456]: by_letter = {}

In [457]: for word in words:
sees : letter = word[0]
wenn ed if letter not in by letter:
soneed by_letter[letter] = [word]
ecomned else:
soneed by_letter[letter].append(word)

In [458]: by_letter
Out[458]: {'a': ['apple', ‘atom'], ‘b': ['bat', ‘bar’, 'book']}

The setdefault dict method is for precisely this purpose. The if-else block above can
be rewritten as:

 

Data Structures and Sequences | 415

by_letter.setdefault(letter, []).append(word)

The built-in collections module has a useful class, defaultdict, which makes this even
easier. One is created by passing a type or function for generating the default value for
each slot in the dict:

from collections import defaultdict

by_letter = defaultdict(list)

for word in words:
by_letter[word[0]].append(word)

The initializer to defaultdict only needs to be a callable object (e.g. any function), not
necessarily a type. Thus, if you wanted the default value to be 4 you could pass a
function returning 4

counts = defaultdict(lambda: 4)

Valid dict key types

While the values of a dict can be any Python object, the keys have to be immutable
objects like scalar types (int, float, string) or tuples (all the objects in the tuple need to
be immutable, too). The technical term here is hashability. You can check whether an
object is hashable (can be used as a key in a dict) with the hash function:

In [459]: hash('string')
Out[459]: -9167918882415130555

In [460]: hash((1, 2, (2, 3)))
Out[460]: 1097636502276347782

In [461]: hash((1, 2, [2, 3])) # fails because lists are mutable

TypeError Traceback (most recent call last)
<ipython-input-461-800cd14ba8be> in <module>()

----> 1 hash((1, 2, [2, 3])) # fails because lists are mutable

TypeError: unhashable type: ‘list’

To use a list as a key, an easy fix is to convert it to a tuple:
In [462]: d = {}

In [463]: d[tuple([1, 2, 3])] =5

In [464]: d
Out[464]: {(1, 2, 3): 5}

Set

A set is an unordered collection of unique elements. You can think of them like dicts,
but keys only, no values. A set can be created in two ways: via the set function or using
a set literal with curly braces:

In [465]: set([2, 2, 2, 1, 3, 3])
Out[465]: set([1, 2, 3])

 

416 | Appendix: Python Language Essentials

In [466]: {2, 2, 2, 1, 3, 3}
Out[466]: set([1, 2, 3])

Sets support mathematical set operations like union, intersection, difference, and sym-
metric difference. See Table A-3 for a list of commonly used set methods.

In

In

In
Out

In
Out

In
Out

In
Out

[467]:

[468]

[469]:
[469]:

[470]:
[470]:

[471]:
[471]:

 

[472]:
[472]:

a= {1, 2, 3, 4; 5}
: b= {3, 4, 5, 6, 7, 8}

a | b # union (or)
set([1, 2, 3, 4, 5, 6, 7, 8])

a &b_ # intersection (and)
set([3, 4, 5])

a-b # difference
set([1, 2])

a” b # symmetric difference (xor)
set([1, 2, 6, 7, 8])

You can also check if a set is a subset of (is contained in) or a superset of (contains all
elements of) another set:

In

In
Out

In
Out

[473]:

[474]
[474]

[475]:

[475]

a_set = {1, 2, 3, 4, 5}

: {1, 2, 3}.issubset(a_set)
: True

a_set.issuperset({1, 2, 3})
: True

As you might guess, sets are equal if their contents are equal:

In [476]: {1, 2, 3} == {3, 2, 1}
Out[476]: True

Table A-3. Python Set Operations

Function Alternate Syntax —_ Description

a.add(x) N/A Add element x to the set a

a.remove(x) N/A Remove element x from the set a
a.union(b) a |b All of the unique elements in a and b.
a.intersection(b) a&b All of the elements in both a and b.
a.difference(b) a-b The elements in a that are not in b.
a.symmetric_difference(b) a%*b All of the elements in a or b but not both.
a.issubset(b) N/A Txue if the elements of a are all contained in b.
a.issuperset(b) N/A Txue if the elements of b are all contained in a.
a.isdisjoint(b) N/A True if a and b have no elements in common.

 

Data Structures and Sequences | 417

List, Set, and Dict Comprehensions

List comprehensions are one of the most-loved Python language features. They allow
you to concisely form a new list by filtering the elements of a collection and transforming
the elements passing the filter in one conscise expression. They take the basic form:

[expr for val in collection if condition]

This is equivalent to the following for loop:

result = []
for val in collection:
if condition:
result.append(expr)

The filter condition can be omitted, leaving only the expression. For example, given a
list of strings, we could filter out strings with length 2 or less and also convert them to
uppercase like this:

In [477]: strings = ['a', ‘as', ‘bat', 'car', ‘dove’, ‘python’ ]

In [478]: [x.upper() for x in strings if len(x) > 2]
Out[478]: ['BAT', 'CAR', 'DOVE', 'PYTHON']

Set and dict comprehensions are a natural extension, producing sets and dicts in a
idiomatically similar way instead of lists. A dict comprehension looks like this:

dict_comp = {key-expr : value-expr for value in collection
if condition}

A set comprehension looks like the equivalent list comprehension except with curly
braces instead of square brackets:

set_comp = {expr for value in collection if condition}
Like list comprehensions, set and dict comprehensions are just syntactic sugar, but they
similarly can make code both easier to write and read. Consider the list of strings above.

Suppose we wanted a set containing just the lengths of the strings contained in the
collection; this could be easily computed using a set comprehension:

In [479]: unique_lengths = {len(x) for x in strings}

In [480]: unique_lengths

Out[480]: set([1, 2, 3, 4, 6])
As asimple dict comprehension example, we could create a lookup map of these strings
to their locations in the list:

In [481]: loc_mapping = {val : index for index, val in enumerate(strings) }

In [482]: loc_mapping

Out[482]: {'a': 0, ‘as': 1, ‘bat’: 2, 'car': 3, ‘dove’: 4, ‘python’: 5}
Note that this dict could be equivalently constructed by:

loc_mapping = dict((val, idx) for idx, val in enumerate(strings) )

 

418 | Appendix: Python Language Essentials

The dict comprehension version is shorter and cleaner in my opinion.

Va,

a Dict and set comprehensions were added to Python fairly recently in
43 Python 2.7 and Python 3.1+.
Le As

 

 

Nested list comprehensions

Suppose we have a list of lists containing some boy and girl names:

In [483]: all_data = [['Tom', 'Billy', 'Jefferson', ‘Andrew', 'Wesley', 'Steven', 'Joe'],

aaa : ['Susie’, 'Casey', ‘Jill’, ‘Ana', 'Eva', ‘'Jennifer', 'Stephanie']]

You might have gotten these names from a couple of files and decided to keep the boy
and girl names separate. Now, suppose we wanted to get a single list containing all
names with two or more e’s in them. We could certainly do this with a simple for loop:

names of interest = []

for names in all data:

enough_es = [name for name in names if name.count('e') > 2]
names_of_interest.extend(enough es)

You can actually wrap this whole operation up in a single nested list comprehension,

which will look like:

In [484]: result = [name for names in all data for name in names
setae 8 : if name.count('e') >= 2]

In [485]: result

Out[485]: ['Jefferson', ‘Wesley’, 'Steven', ‘Jennifer’, ‘Stephanie’ ]
At first, nested list comprehensions are a bit hard to wrap your head around. The for
parts of the list comprehension are arranged according to the order of nesting, and any
filter condition is put at the end as before. Here is another example where we “flatten”
a list of tuples of integers into a simple list of integers:

In [486]: some_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]
In [487]: flattened = [x for tup in some_tuples for x in tup]

In [488]: flattened

Out[488]: [1, 2, 3, 4, 5, 6, 7, 8, 9]
Keep in mind that the order of the for expressions would be the same if you wrote a
nested for loop instead of a list comprehension:

flattened = []
for tup in some_tuples:

for x in tup:
flattened. append(x)

 

Data Structures and Sequences | 419

You can have arbitrarily many levels of nesting, though if you have more than two or
three levels of nesting you should probably start to question your data structure design.
It’s important to distinguish the above syntax from a list comprehension inside a list
comprehension, which is also perfectly valid:

In [229]: [[x for x in tup] for tup in some tuples]

Functions

Functions are the primary and most important method of code organization and reuse
in Python. There may not be such a thing as having too many functions. In fact, I would
argue that most programmers doing data analysis don’t write enough functions! As you
have likely inferred from prior examples, functions are declared using the def keyword
and returned from using the return keyword:
def my_function(x, y, z=1.5):
if z> 41:
return z * (x + y)

else:
return z / (x + y)

There is no issue with having multiple return statements. If the end of a function is
reached without encountering a return statement, None is returned.

Each function can have some number of positional arguments and some number of
keyword arguments. Keyword arguments are most commonly used to specify default
values or optional arguments. In the above function, x and y are positional arguments
while z is a keyword argument. This means that it can be called in either of these
equivalent ways:

my function(5, 6, z=0.7)
my function(3.14, 7, 3.5)

The main restriction on function arguments it that the keyword arguments must follow
the positional arguments (if any). You can specify keyword arguments in any order;
this frees you from having to remember which order the function arguments were
specified in and only what their names are.

Namespaces, Scope, and Local Functions

Functions can access variables in two different scopes: global and local. An alternate
and more descriptive name describing a variable scope in Python is a namespace. Any
variables that are assigned within a function by default are assigned to the local name-
space. The local namespace is created when the function is called and immediately
populated by the function’s arguments. After the function is finished, the local name-
space is destroyed (with some exceptions, see section on closures below). Consider the
following function:

 

420 | Appendix: Python Language Essentials

def func():
a= []
for i in range(5):
a.append(i)

Upon calling func(), the empty list a is created, 5 elements are appended, then a is
destroyed when the function exits. Suppose instead we had declared a

a= []
def func():
for i in range(5):
a.append(i)

Assigning global variables within a function is possible, but those variables must be
declared as global using the global keyword:

In [489]: a = None
In [490]: def bind_a variable():
ecesene 8 global a

In [491]: print a
]

I generally discourage people from using the global keyword frequently.
tay) Typically global variables are used to store some kind of state in a sys-

tem. If you find yourself using a lot of them, it’s probably a sign that
some object-oriented programming (using classes) is in order.

 

Functions can be declared anywhere, and there is no problem with having local func-
tions that are dynamically created when a function is called:
def outer_function(x, y, z):
def inner_function(a, b, c):
pass
pass

In the above code, the inner_function will not exist until outer_function is called. As
soon as outer_function is done executing, the inner_function is destroyed.

Nested inner functions can access the local namespace of the enclosing function, but
they cannot bind new variables in it. Pll talk a bit more about this in the section on
closures.

In a strict sense, all functions are local to some scope, that scope may just be the module
level scope.

 

Functions | 421

Returning Multiple Values

When I first programmed in Python after having programmed in Java and C++, one of
my favorite features was the ability to return multiple values from a function. Here’s a
simple example:

def f():
a=5
b= 6
c=7

return a, b, c
a, b, c = f()

In data analysis and other scientific applications, you will likely find yourself doing this
very often as many functions may have multiple outputs, whether those are data struc-
tures or other auxiliary data computed inside the function. If you think about tuple
packing and unpacking from earlier in this chapter, you may realize that what’s hap-
pening here is that the function is actually just returning one object, namely a tuple,
which is then being unpacked into the result variables. In the above example, we could
have done instead:

return_value = f()
In this case, return_value would be, as you may guess, a 3-tuple with the three returned

variables. A potentially attractive alternative to returning multiple values like above
might be to return a dict instead:

def f():
a=5
b= 6
c=7

return {'a' : a, 'b' : b, 'c' : c}

Functions Are Objects

Since Python functions are objects, many constructs can be easily expressed that are
difficult to do in other languages. Suppose we were doing some data cleaning and
needed to apply a bunch of transformations to the following list of strings:
states = [' Alabama ', 'Georgia!', 'Georgia', 'georgia', 'FlOrIda',
‘south carolina##', ‘West virginia?’ ]

Anyone who has ever worked with user-submitted survey data can expect messy results
like these. Lots of things need to happen to make this list of strings uniform and ready
for analysis: whitespace stripping, removing punctuation symbols, and proper capital-
ization. As a first pass, we might write some code like:

import re # Regular expression module

def clean_strings(strings):
result = []

 

422 | Appendix: Python Language Essentials

for value in strings:
value = value.strip()
value = re.sub('[!#?]', '', value) # remove punctuation
value = value.title()
result.append(value)
return result

The result looks like this:

In [15]: clean_strings(states)
Out[15]:
[ ‘Alabama’,

‘Georgia’,

‘Georgia’,

‘Georgia’,

‘Florida’,

"South Carolina’,

‘West Virginia’ ]

An alternate approach that you may find useful is to make a list of the operations you
want to apply to a particular set of strings:

def remove_punctuation(value):
return re.sub('[!#?]', '', value)

clean_ops = [str.strip, remove punctuation, str.title]

def clean_strings(strings, ops):
result = []
for value in strings:
for function in ops:
value = function(value)
result.append(value)
return result

Then we have

In [22]: clean_strings(states, clean_ops)
Out [22]:
[ ‘Alabama’,

‘Georgia’,

‘Georgia’,

‘Georgia’,

‘Florida’,

"South Carolina’,

‘West Virginia’ ]

A more functional pattern like this enables you to easily modify how the strings are
transformed at a very high level. The clean_strings function is also now more reusable!

You can naturally use functions as arguments to other functions like the built-in map
function, which applies a function to a collection of some kind:
In [23]: map(remove_punctuation, states)
Out [23]:
[' Alabama ',
‘Georgia’,

 

Functions | 423

‘Georgia’,

‘georgia’,

"FlOrIda' ,

"south carolina’,
‘West virginia’ ]

Anonymous (lambda) Functions

Python has support for so-called anonymous or lambda functions, which are really just
simple functions consisting of a single statement, the result of which is the return value.
They are defined using the lambda keyword, which has no meaning other than “we are
declaring an anonymous function.”

def short_function(x):
return x * 2

equiv_anon = lambda x: x * 2

I usually refer to these as lambda functions in the rest of the book. They are especially
convenient in data analysis because, as you'll see, there are many cases where data
transformation functions will take functions as arguments. It’s often less typing (and
clearer) to pass a lambda function as opposed to writing a full-out function declaration
or even assigning the lambda function to a local variable. For example, consider this
silly example:

def apply to list(some_list, f):
return [f(x) for x in some_list]

ints = [4, 0, 1, 5, 6]

apply to list(ints, lambda x: x * 2)
You could also have written [x * 2 for x in ints], but here we were able to succintly
pass a custom operator to the apply_to list function.
As another example, suppose you wanted to sort a collection of strings by the number
of distinct letters in each string:

In [492]: strings = ['foo', 'card', 'bar', 'aaaa', ‘abab']
Here we could pass a lambda function to the list’s sort method:

In [493]: strings.sort(key=lambda x: len(set(list(x))))

In [494]: strings
Out[494]: ['aaaa', 'foo', 'abab', 'bar', ‘card']

Vs
x
sO One reason lambda functions are called anonymous functions is that
“ & the function object itself is never given a name attribute.

 

 

 

424 | Appendix: Python Language Essentials

Closures: Functions that Return Functions

Closures are nothing to fear. They can actually be a very useful and powerful tool in
the right circumstance! In a nutshell, a closure is any dynamically-generated function
returned by another function. The key property is that the returned function has access
to the variables in the local namespace where it was created. Here is a very simple
example:
def make_closure(a):
def closure():

print('I know the secret: %d' % a)
return closure

closure = make_closure(5)

The difference between a closure and a regular Python function is that the closure
continues to have access to the namespace (the function) where it was created, even
though that function is done executing. So in the above case, the returned closure will
always print I know the secret: 5 whenever you call it. While it’s common to create
closures whose internal state (in this example, only the value of a) is static, you can just
as easily have a mutable object like a dict, set, or list that can be modified. For example,
here’s a function that returns a function that keeps track of arguments it has been called
with:
def make_watcher():
have_seen = {}

def has_been_seen(x):
if x in have_seen:
return True
else:
have_seen[x] = True
return False

return has_been_seen

Using this on a sequence of integers I obtain:
In [496]: watcher = make_watcher()

In [497]: vals = [5, 6, 1, 5, 1, 6, 3, 5]

In [498]: [watcher(x) for x in vals]
Out[498]: [False, False, False, True, True, True, False, True]

However, one technical limitation to keep in mind is that while you can mutate any
internal state objects (like adding key-value pairs to a dict), you cannot bind variables
in the enclosing function scope. One way to work around this is to modify a dict or list
rather than binding variables:

def make_counter():

count = [0]
def counter():

 

Functions | 425

# increment and return the current count
count[0] += 1
return count[0o]

return counter

counter = make_counter()

You might be wondering why this is useful. In practice, you can write very general
functions with lots of options, then fabricate simpler, more specialized functions.
Here’s an example of creating a string formatting function:

def format_and_pad(template, space):

def formatter (x):
return (template % x).rjust(space)

return formatter

You could then create a floating point formatter that always returns a length-15 string
like so:

In [500]: fmt = format_and_pad('%.4f', 15)

In [501]: fmt(1.756)
Out[501]: ' 1.7560'

If you learn more about object-oriented programming in Python, you might observe
that these patterns also could be implemented (albeit more verbosely) using classes.

Extended Call Syntax with *args, **kwargs

The way that function arguments work under the hood in Python is actually very sim-
ple. When you write func(a, b, c, d=some, e=value), the positional and keyword
arguments are actually packed up into a tuple and dict, respectively. So the internal
function receives a tuple args and dict kwargs and internally does the equivalent of:

a, b, c = args

d = kwargs.get('d', d_default_value)

e = kwargs.get('e', e default_value)

This all happens nicely behind the scenes. Of course, it also does some error checking
and allows you to specify some of the positional arguments as keywords also (even if
they aren’t keyword in the function declaration!).
def say hello then_call f(f, *args, **kwargs):

print ‘args is', args

print 'kwargs is', kwargs

print("Hello! Now I'm going to call %s" % f)

return f(*args, **kwargs)

def g(x, y, z=1):
return (x + y) /z

Then if we call g with say hello then call _f we get:

 

426 | Appendix: Python Language Essentials

In [8]: say_hello_then_call_ f(g, 1, 2, z=5.)

args is (1, 2)

kwargs is {'z': 5.0}

Hello! Now I'm going to call <function g at ox2dd5cf8>
Out[8]: 0.6

Currying: Partial Argument Application

Currying is a fun computer science term which means deriving new functions from
existing ones by partial argument application. For example, suppose we had a trivial
function that adds two numbers together:

def add_numbers(x, y):
return x + y

Using this function, we could derive a new function of one variable, add_five, that adds
5 to its argument:

add_five = lambda y: add_numbers(5, y)
The second argument to add_numbers is said to be curried. There’s nothing very fancy

here as we really only have defined a new function that calls an existing function. The
built-in functools module can simplify this process using the partial function:

from functools import partial
add_five = partial(add_numbers, 5)

When discussing pandas and time series data, we'll use this technique to create speci-
alized functions for transforming data series
# compute 60-day moving average of time series x

ma60 = lambda x: pandas.rolling mean(x, 60)

# Take the 60-day moving average of all time series in data
data.apply(ma60)

Generators

Having a consistent way to iterate over sequences, like objects in a list or lines in a file,
is an important Python feature. This is accomplished by means of the iterator proto-
col, a generic way to make objects iterable. For example, iterating over a dict yields the
dict keys:

In [502]: some dict = {'a': 1, 'b': 2, 'c': 3}

In [503]: for key in some_dict:
acerece ed print key,

When you write for key in some dict, the Python interpreter first attempts to create
an iterator out of some_dict:

In [504]: dict_iterator = iter(some_dict)

 

Functions | 427

In [505]: dict_iterator
Out [505]: <dictionary-keyiterator at 0x10a0a1578>

Any iterator is any object that will yield objects to the Python interpreter when used in
acontext like a for loop. Most methods expecting a list or list-like object will also accept
any iterable object. This includes built-in methods such as min, max, and sum, and type
constructors like list and tuple:
In [506]: list(dict_iterator)
Out[506]: ['a', 'c', 'b']
A generator is a simple way to construct a new iterable object. Whereas normal func-
tions execute and return a single value, generators return a sequence of values lazily,
pausing after each one until the next one is requested. To create a generator, use the
yield keyword instead of return in a function:
def squares(n=10):
for i in xrange(1, n+ 1):
print ‘Generating squares from 1 to %d' % (n ** 2)
yield i ** 2

When you actually call the generator, no code is immediately executed:

In [2]: gen = squares()

In [3]: gen
Out[3]: <generator object squares at 0x34c8280>

It is not until you request elements from the generator that it begins executing its code:

In [4]: for x in gen:
print x,

Generating squares from 0 to 100
149 16 25 36 49 64 81 100

As a less trivial example, suppose we wished to find all unique ways to make change
for $1 (100 cents) using an arbitrary set of coins. You can probably think of various
ways to implement this and how to store the unique combinations as you come up with
them. One way is to write a generator that yields lists of coins (represented as integers):
def make_change(amount, coins=[1, 5, 10, 25], hand=None):
hand = [] if hand is None else hand
if amount ==
yield hand
for coin in coins:
# ensures we don't give too much change, and combinations are unique

if coin > amount or (len(hand) > 0 and hand[-1] < coin):
continue

for result in make_change(amount - coin, coins=coins,
hand=hand + [coin]):
yield result

The details of the algorithm are not that important (can you think of a shorter way?).
Then we can write:

 

428 | Appendix: Python Language Essentials

In [508]: for way in make_change(100, coins=[10, 25, 50]):
women print way

[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]

[25, 25, 10, 10, 10, 10, 10]

[25, 25, 25, 25]

[50, 10, 10, 10, 10, 10]

[50, 25, 25]

[50, 50]

In [509]: len(list(make_change(100) ))
Out[509]: 242

Generator expresssions

A simple way to make a generator is by using a generator expression. This is a generator
analogue to list, dict and set comprehensions; to create one, enclose what would other-
wise be a list comprehension with parenthesis instead of brackets:

In [510]: gen = (x ** 2 for x in xrange(100) )
In [511]: gen
Out[511]: <generator object <genexpr> at 0x10a0a31e0>
This is completely equivalent to the following more verbose generator:

def _make_gen():
for x in xrange(100):
yield x ** 2
gen = _make_gen()
Generator expressions can be used inside any Python function that will accept a gen-
erator:

In [512]: sum(x ** 2 for x in xrange(100) )
Out[512]: 328350

In [513]: dict((i, i **2) for i in xrange(5))
Out[513]: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}

itertools module

The standard library itertools module has a collection of generators for many common
data algorithms. For example, groupby takes any sequence and a function; this groups
consecutive elements in the sequence by return value of the function. Here’s an exam-

ple:
In [514]: import itertools

In [515]: first_letter = lambda x: x[o]
In [516]: names = ['Alan', ‘Adam', 'Wes', 'Will', ‘Albert’, 'Steven']
In [517]: for letter, names in itertools.groupby(names, first_letter):

arma ef print letter, list(names) # names is a generator
A ['Alan', 'Adam']

 

Functions | 429

W ['Wes', ‘Will']
A ['Albert']
S ['Steven' ]

See Table A-4 for a list of a few other itertools functions I’ve frequently found useful.

Table A-4. Some useful itertools functions

Function Description

imap(func, *iterables) Generator version of the built-in map; applies Func to each zipped tuple of
the passed sequences.

ifilter(func, iterable) Generator version of the built-in Filter; yields elements x for which
func (x) is True.

combinations(iterable, k) Generates a sequence of all possible k-tuples of elements in the iterable,
ignoring order.

permutations(iterable, k) Generates a sequence of all possible k-tuples of elements in the iterable,
respecting order.

groupby(iterable[, keyfunc]) Generates (key, sub-iterator) for each unique key

Va,

s In Python 3, several built-in functions (zip, map, filter) producing
lists have been replaced by their generator versions found in itertools
“3° in Python 2.

 

ae
ww

se

 

Files and the operating system

Most of this book uses high-level tools like pandas. read_csv to read data files from disk
into Python data structures. However, it’s important to understand the basics of how
to work with files in Python. Fortunately, it’s very simple, which is part of why Python
is so popular for text and file munging.

To open a file for reading or writing, use the built-in open function with either a relative
or absolute file path:

In [518]: path = 'ch13/segismundo.txt'
In [519]: f = open(path)

By default, the file is opened in read-only mode 'r'. We can then treat the file handle
f like a list and iterate over the lines like so

for line in f:
pass

The lines come out of the file with the end-of-line (EOL) markers intact, so you’ll often
see code to get an EOL-tfree list of lines in a file like

In [520]: lines = [x.rstrip() for x in open(path) ]

In [521]: lines

 

430 | Appendix: Python Language Essentials

Out [521]:
['Sue\xc3\xb1a el rico en su riqueza,',
"que m\xc3\xa1s cuidados le ofrece;',

d
"sue\xc3\xb1a el pobre que padece',
"su miseria y su pobreza;',

z
"sue\xc3\xb1a el que a medrar empieza,',
‘sue\xc3\xb1a el que afana y pretende,',
‘sue\xc3\xbi1a el que agravia y ofende,',

wt
d

"y en el mundo, en conclusi\xc3\xb3n,',
‘todos sue\xc3\xb1an lo que son,',
‘aunque ninguno lo entiende.',

"]

If we had typed f = open(path, ‘w'), a new file at ch13/segismundo.txt would have
been created, overwriting any one in its place. See below for a list of all valid file read/
write modes.

Table A-5. Python file modes

Mode Description

r Read-only mode

w Write-only mode. Creates a new file (deleting any file with the same name)

a Append to existing file (create it if it does not exist)

r+ Read and write

b Add to mode for binary files, that is ‘rb’ or 'wb'

U Use universal newline mode. Pass by itself ‘U' or appended to one of the read modes like 'rU'

 

To write text to a file, you can use either the file’s write or writelines methods. For
example, we could create a version of prof_mod.py with no blank lines like so:

In [522]: with open('tmp.txt', 'w') as handle:
worewed handle.writelines(x for x in open(path) if len(x) > 1)

In [523]: open('tmp.txt').readlines()

Out [523]:

['Sue\xc3\xb1a el rico en su riqueza,\n',
"que m\xc3\xa1s cuidados le ofrece;\n',
"sue\xc3\xb1a el pobre que padece\n',

‘su miseria y su pobreza;\n',
‘sue\xc3\xb1a el que a medrar empieza, \n',
‘sue\xc3\xb1a el que afana y pretende,\n',
‘sue\xc3\xb1a el que agravia y ofende,\n',
'y en el mundo, en conclusi\xc3\xb3n, \n',
‘todos sue\xc3\xb1an lo que son,\n',
‘aunque ninguno lo entiende.\n']

See Table A-6 for many of the most commonly-used file methods.

 

Files and the operating system | 431

Table A-6. Important Python file methods or attributes

Method

read([size])

readlines([size])
readlines([size])
write(str)
writelines(strings)
close()

flush()

seek(pos)

tell()

closed

Description

Return data from file as a string, with optional size argument indicating the number of bytes
to read

Return list of lines in the file, with optional size argument
Return list of lines (as strings) in the file

Write passed string to file.

Write passed sequence of strings to the file.

Close the handle

Flush the internal 1/0 buffer to disk

Move to indicated file position (integer).

Return current file position as integer.

True if the file is closed.

 

 

432 | Appendix: Python Language Essentials

 

Symbols

! character, 60, 61, 64

!= operator, 91

!cmd command, 60
"two-language" problem, 2-3

# (hash mark), 388

$PATH variable, 10

% character, 398

%a datetime format, 293

%A datetime format, 293
%alias magic function, 61
%automagic magic function, 55
%b datetime format, 293

%B datetime format, 293

% bookmark magic function, 60, 62
%c datetime format, 293

%cd magic function, 60
%cpaste magic function, 51-52, 55
%d datetime format, 292

%D datetime format, 293

%d format character, 398
%debug magic function, 54-55, 62
%dhist magic function, 60
%dirs magic function, 60

%env magic function, 60

%F datetime format, 293

%gui magic function, 57

%H datetime format, 292
%hist magic function, 55, 59
%I datetime format, 292
%logstart magic function, 60
%logstop magic function, 60
%lprun magic function, 70, 72
%m datetime format, 292

Index

%M datetime format, 292

%magic magic function, 55

%p datetime format, 293

% page magic function, 55

% paste magic function, 51, 55

%pdb magic function, 54, 63

%popd magic function, 60

%prun magic function, 55, 70

%pushd magic function, 60

%pwd magic function, 60

%quickref magic function, 55

%reset magic function, 55, 59

%run magic function, 49-50, 55, 386

%S datetime format, 292

%s format character, 398

%time magic function, 55, 67

%timeit magic function, 54, 67, 68

%U datetime format, 293

%w datetime format, 292

%W datetime format, 293

%who magic function, 55

%whos magic function, 55

%who_ls magic function, 55

%x datetime format, 293

%X datetime format, 293

%xdel magic function, 55, 59

%xmode magic function, 54

%Y datetime format, 292

%y datetime format, 292

%z datetime format, 293

& operator, 91

* operator, 105

+ operator, 406, 409

2012 Federal Election Commission database
example, 278-287

We'd like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.

 

433

 

 

bucketing donation amounts, 283-285
donation statistics by occupation and
employer, 280-283

donation statistics by state, 285-287

== operator, 393

>>> prompt, 386

? (question mark), 49

[] (brackets), 406, 408

\ (backslash), 397

_ (underscore), 48, 58

___ (two underscores), 58

{} (braces), 413

| operator, 91

A
a file mode, 431
abs function, 96
accumulate method, 368
add method, 95, 130, 417
add_patch method, 229
add_subplot method, 221
aggfunc option, 277
aggregate method, 260, 262
aggregations, 100
algorithms for sorting, 375-376
alignment of data, 330-331
all method, 101, 368
alpha argument, 233
and keyword, 398, 401
annotating in matplotlib, 228-230
anonymous functions, 424
any method, 101, 110, 201
append method, 122, 408
apply method, 39, 132, 142, 266-268, 270
apt package management tool, 10
arange function, 82
arccos function, 96
arccosh function, 96
arcsin function, 96
arcsinh function, 96
arctan function, 96
arctanh function, 96
argmax method, 101
argmin method, 101, 139
argsort method, 135, 374
arithmetic, 128-132

operations between DataFrame and Series,

130-132
with fill values, 129-130

arrays
boolean arrays, 101
boolean indexing for, 89-92
conditional logic as operation, 98-100
creating, 81-82
creating PeriodIndex from, 312
data types for, 83-85
fancy indexing, 92-93
file input and output with, 103-105
saving and loading text files, 104-105
storing on disk in binary format, 103-
104
finding elements in sorted array, 376-377
in NumPy, 355-362
concatenating, 357-359
c_ object, 359
layout of in memory, 356-357
replicating, 360-361
reshaping, 355-356
r_ object, 359
saving to file, 379-380
splitting, 357-359
subsets for, 361-362
indexes for, 86-89
operations between, 85-86
setting values by broadcasting, 367
slicing, 86-89
sorting, 101-102
statistical methods for, 100
structured arrays, 370-372
benefits of, 372
mainpulating, 372
nested data types, 371-372
swapping axes in, 93-94
transposing, 93-94
unique function, 102-103
where function, 98—100
arrow function, 229
as keyword, 393
asarray function, 82, 379
asfreq method, 308, 318
asof method, 334-336
astype method, 84, 85
attributes
in Python, 391
starting with underscore, 48
average method, 136
ax argument, 233
axes

 

434 | Index

broadcasting over, 364-367
concatenating along, 185-188
labels for, 226-227
renaming indexes for, 197-198
swapping in arrays, 93-94
AxesSubplot object, 221
axis argument, 188
axis method, 138

B

b file mode, 431
backslash (\), 397
bar plots, 235-238
Basemap object, 246
.bashrc file, 10
-bash_profile file, 9
bbox_inches option, 231
benefits
of Python, 2-3
glue for code, 2
solving "two-language" problem with, 2—
8
of structured arrays, 372
beta function, 107
defined, 342
between_time method, 335
bfill method, 123
bin edges, 314
binary data formats, 171-173
HDF5, 171-172
Microsoft Excel files, 172-173
storing arrays in, 103-104
binary moving window functions, 324-325
binary search of lists, 410
binary universal functions, 96
binding
defined, 390
variables, 425
binomial function, 107
bisect module, 410
bookmarking directories in IPython, 62
Boolean
arrays, 101
data type, 84, 398
indexing for arrays, 89-92
bottleneck library, 324
braces ({}), 413
brackets ([]), 406, 408
break keyword, 401

broadcasting, 362-367
defined, 86, 360, 362
over other axes, 364-367
setting array values by, 367
bucketing, 283-285

C

calendar module, 290
casting, 84
cat method, 156, 212
Categorical object, 199
ceil function, 96
center method, 212
Chaco, 248
chisquare function, 107
chunksize argument, 160, 161
clearing screen shortcut, 53
clipboard, executing code from, 50-52
clock function, 67
close method, 220, 432
closures, 425—426
cmd.exe, 7
collections module, 416
colons, 387
cols option, 277
columns, grouping on, 256-257
column_stack function, 359
combinations function, 430
combine_first method, 177, 189
combining
data sources, 336-338
data sources, with overlap, 188-189
lists, 409
commands, 65
(see also magic commands)
debugger, 65
history in IPython, 58-60
input and output variables, 58-59
logging of, 59-60
reusing command history, 58
searching for, 53
comment argument, 160
comments in Python, 388
compile method, 208
complex128 data type, 84
complex256 data type, 84
complex64 data type, 84
concat function, 34, 177, 184, 185, 186, 267,
357, 359

 

Index | 435

 

 

concatenating
along axis, 185-188
arrays, 357-359
conditional logic as array operation, 98-100
conferences, 12
configuring matplotlib, 231-232
conforming, 122
contains method, 212
contiguous memory, 381-382
continue keyword, 401
continuous return, 348
convention argument, 314
converting
between string and datetime, 291-293
timestamps to periods, 311
coordinated universal time (UTC), 303
copy argument, 181
copy method, 118
copysign function, 96
corr method, 140
correlation, 139-141
corrwith method, 140
cos function, 96
cosh function, 96
count method, 139, 206, 212, 261, 407
Counter class, 21
cov method, 140
covariance, 139-141
CPython, 7
cross-section, 329
crosstab function, 277-278
crowdsourcing, 241
CSV files, 163-165, 242
Ctrl-A keyboard shortcut, 53
Ctrl-B keyboard shortcut, 53
Ctrl-C keyboard shortcut, 53
Ctrl-E keyboard shortcut, 53
Ctrl-F keyboard shortcut, 53
Ctrl-K keyboard shortcut, 53
Ctrl-L keyboard shortcut, 53
Ctrl-N keyboard shortcut, 53
Ctrl-P keyboard shortcut, 53
Ctrl-R keyboard shortcut, 53
Ctrl-Shift-V keyboard shortcut, 53
Ctrl-U keyboard shortcut, 53
cummax method, 139
cummin method, 139
cumprod method, 100, 139
cumsum method, 100, 139

cumulative returns, 338-340
currying, 427

cursor, moving with keyboard, 53
custom universal functions, 370

cut function, 199, 200, 201, 268, 283
Cython project, 2, 382-383

c_ object, 359

D

data aggregation, 259-264
returning data in unindexed form, 264
using multiple functions, 262-264
data alignment, 128-132
arithmetic methods with fill values, 129—
130
operations between DataFrame and Series,
130-132
data munging, 329-340
asof method, 334-336
combining data, 336-338
for data alignment, 330-331
for specialized frequencies, 332-334
data structures for pandas, 112-121
DataFrame, 115-120
Index objects, 120-121
Panel, 152-154
Series, 112-115
data types
for arrays, 83-85
for ndarray, 83-85
for NumPy, 353-354
hierarchy of, 354
for Python, 395-400
boolean data type, 398
dates and times, 399-400
None data type, 399
numeric data types, 395-396
str data type, 396-398
type casting in, 399
for time series data, 290-293
converting between string and datetime,
291-293
nested, 371-372
data wrangling
manipulating strings, 205-211
methods for, 206-207
vectorized string methods, 210-211
with regular expressions, 207-210
merging data, 177-189

 

436 | Index

combining data with overlap, 188-189
concatenating along axis, 185-188
DataFrame merges, 178-181
on index, 182-184
pivoting, 192-193
reshaping, 190-191
transforming data, 194-205
discretization, 199-201
dummy variables, 203-205
filtering outliers, 201-202
mapping, 195-196
permutation, 202
removing duplicates, 194-195
renaming axis indexes, 197-198
replacing values, 196-197
USDA food database example, 212-217
databases
reading and writing to, 174-176
DataFrame data structure, 22, 27, 112, 115-—
120
arithmetic operations between Series and,
130-132
hierarchical indexing using, 150-151
merging data with, 178-181
dates and times, 291
(see also time series data)
data types for, 291, 399-400
date ranges, 298
datetime type, 291-293, 395, 399
DatetimeIndex Index object, 121
dateutil package, 291
date_parser argument, 160
date_range function, 298
dayfirst argument, 160
debug function, 66
debugger, [Python
in IPython, 62-66
def keyword, 420
defaults
profiles, 77
values for dicts, 415-416
del keyword, 59, 118, 414
delete method, 122
delimited formats, 163-165
density plots, 238-239
describe method, 138, 243, 267
design tips, 74-76
flat is better than nested, 75
keeping relevant objects and data alive, 75

overcoming fear of longer files, 75-76
det function, 106
development tools in IPython, 62—72
debugger, 62-66
profiling code, 68-70
profiling function line-by-line, 70-72
timing code, 67-68
diag function, 106
dicts, 413-416
creating, 415
default values for, 415-416
dict comprehensions, 418-420
grouping on, 257-258
keys for, 416
returning system environment variables as,
60
diff method, 122, 139
difference method, 417
digitize function, 377
directories
bookmarking in IPython, 62
changing, commands for, 60
discretization, 199-201
div method, 130
divide function, 96
.dmg file, 9
donation statistics
by occupation and employer, 280-283
by state, 285-287
dot function, 105, 106, 377
doublequote option, 165
downsampling, 312
dpi (dots-per-inch) option, 231
dreload function, 74
drop method, 122, 125
dropna method, 143
drop_duplicates method, 194
dsplit function, 359
dstack function, 359
dtype object (see data types)
“duck” typing in Python, 392
dummy variables, 203-205
dumps function, 165
duplicated method, 194
duplicates
indices, 296-297
removing from data, 194-195
dynamically-generated functions, 425

 

Index | 437

 

 

E

edgecolo option, 231
edit-compile-run workflow, 45
eig function, 106
elif blocks (see if statements)
else block (see if statements)
empty function, 82, 83
empty namespace, 50
encoding argument, 160
endswith method, 207, 212
enumerate function, 412
environment variables, 8, 60
EPD (Enthought Python Distribution), 7-9
equal function, 96
escapechar option, 165
ewma function, 323
ewmcorr function, 323
ewmcov function, 323
ewmstd function, 323
ewmvar function, 323
ExcelFile class, 172
except block, 403
exceptions
automatically entering debugger after, 55
defined, 402
handling in Python, 402-404
exec keyword, 59
execute-explore workflow, 45
execution time
of code, 55
of single statement, 55
exit command, 386
exp function, 96
expanding window mean, 322
exponentially-weighted functions, 324
extend method, 409
extensible markup language (XML) files, 169-
171
eye function, 83

F

fabs function, 96
facecolor option, 231
factor analysis, 342-343
Factor object, 269
factors, 342
fancy indexing

defined, 361

for arrays, 92-93
ffill method, 123
figsize argument, 234
Figure object, 220, 223
file input/output
binary data formats for, 171-173
HDF5, 171-172
Microsoft Excel files, 172—173
for arrays, 103-105
HDF5, 380
memory-mapped files, 379-380
saving and loading text files, 104-105
storing on disk in binary format, 103-
104
in Python, 430-431
saving plot to file, 231
text files, 155-171
delimited formats, 163-165
HTML files, 166-171
JSON data, 165-166
Ixml library, 166-171
reading in pieces, 160-162
writing to, 162-163
XML files, 169-171
with databases, 174-176
with Web APIs, 173-174
filling in missing data, 145-146, 270-271
fillna method, 22, 143, 145, 146, 196, 270,
317
fill_method argument, 313
fill_value option, 277
filtering
in pandas, 125-128
missing data, 143-144
outliers, 201-202
financial applications
cumulative returns, 338-340
data munging, 329-340
asof method, 334-336
combining data, 336-338
for data alignment, 330-331
for specialized frequencies, 332-334
future contract rolling, 347-350
grouping for, 340-345
factor analysis with, 342-343
quartile analysis, 343-345
linear regression, 350-351
return indexes, 338-340
rolling correlation, 350-351

 

438 | Index

signal frontier analysis, 345-347
find method, 206, 207
findall method, 167, 208, 210, 212
finditer method, 210
first crossing time, 109
first method, 136, 261
flat is better than nested, 75
flattening, 356
float data type, 83, 354, 395, 396, 399
float function, 402
float128 data type, 84
floatl6 data type, 84
float32 data type, 84
float64 data type, 84
floor function, 96
floor_divide function, 96
flow control, 400—405
exception handling, 402-404
for loops, 401-402
if statements, 400-401
pass statements, 402
range function, 404—405
ternary expressions, 405
while loops, 402
xrange function, 404-405
flush method, 432
fmax function, 96
fmin function, 96
fname option, 231
for loops, 85, 100, 401-402, 418, 419
format option, 231
frequencies, 299-301
converting, 308
specialized frequencies, 332-334
week of month dates, 301
frompyfunc function, 370
from_csv method, 163
functions, 389, 420-430
anonymous functions, 424
are objects, 422—423
closures, 425—426
currying of, 427
extended call syntax for, 426
lambda functions, 424
namespaces for, 420-421
parsing in pandas, 155
returning multiple values from, 422
scope of, 420-421
functools module, 427

future contract rolling, 347-350
futures, 347

G

gamma function, 107
gcc command, 9, 11
generators, 427-430
defined, 428
generator expressions, 429
itertools module for, 429-430
get method, 167, 173, 212, 415
getattr function, 391
get_chunk method, 162
get_dummies function, 203, 205
get_value method, 128
get_xlim method, 226
GIL (global interpreter lock), 3
global scope, 420, 421
glue for code
Python as, 2
gov domain, 18
Granger, Brian, 72
graphics
Chaco, 248
mayavi, 249
greater function, 96
greater_equal function, 96
grid argument, 234
group keys, 268
groupby method, 39, 252-259, 297, 316, 343,
377, 429
iterating over groups, 255-256
on column, 256-257
on dict, 257-258
on levels, 259
resampling with, 316
using functions with, 258-259
with Series, 257-258
grouping
2012 Federal Election Commission database
example, 278-287
bucketing donation amounts, 283-285
donation statistics by occupation and
employer, 280-283
donation statistics by state, 285-287
apply method, 266-268
data aggregation, 259-264
returning data in unindexed form, 264
using multiple functions, 262-264

 

Index | 439

 

 

filling missing values with group-specific
values, 270-271
for financial applications, 340-345
factor analysis with, 342-343
quartile analysis, 343-345
group weighted average, 273-274
groupby method, 252-259
iterating over groups, 255-256
on column, 256-257
on dict, 257-258
on levels, 259
using functions with, 258-259
with Series, 257-258
linear regression for, 274-275
pivot tables, 275-278
cross-tabulation, 277-278
quantile analysis with, 268-269
random sampling with, 271-272

H

Haiti earthquake crisis data example, 241-246
half-open, 314
hasattr function, 391
hash mark (#), 388
hashability, 416
HDFS5 (hierarchical data format), 171-172,
380
HDFStore class, 171
header argument, 160
heapsort sorting method, 376
hierarchical data format (HDF5), 171-172,
380
hierarchical indexing
in pandas, 147-151
sorting levels, 149-150
summary statistics by level, 150
with DataFrame columns, 150-151
reshaping data with, 190-191
hist method, 238
histograms, 238-239
history of commands, searching, 53
homogeneous data container, 370
how argument, 181, 313, 316
hsplit function, 359
hstack function, 358
HTML files, 166-171
HTML Notebook in IPython, 72
Hunter, John D., 5, 219
hyperbolic trigonometric functions, 96

icol method, 128, 152
IDEs (Integrated Development Environments),
11, 52
idxmax method, 138
idxmin method, 138
if statements, 400-401, 415
ifilter function, 430
iget_value method, 152
ignore_index argument, 188
imap function, 430
import directive
in Python, 392-393
usage of in this book, 13
imshow function, 98
in keyword, 409
in-place sort, 373
inld method, 103
indentation
in Python, 387-388
IndentationError event, 51
index method, 206, 207
Index objects data structure, 120-121
indexes
defined, 112
for arrays, 86-89
for axis, 197-198
for TimeSeries class, 294-296
hierarchical indexing, 147-151
reshaping data with, 190-191
sorting levels, 149-150
summary statistics by level, 150
with DataFrame columns, 150-151
in pandas, 136
integer indexing, 151-152
merging data on, 182-184
index_col argument, 160
indirect sorts, 374-375, 374
input variables, 58-59
insert method, 122, 408
insort method, 410
int data type, 83, 395, 399
int16 data type, 84
int32 data type, 84
int64 data type, 84
Int64Index Index object, 121
int8 data type, 84
integer arrays, indexing using (see fancy
indexing)

 

440 | Index

integer indexing, 151-152
Integrated Development Environments (IDEs),
11,52
interpreted languages
defined, 386
Python interpreter, 386
interrupting code, 50, 53
intersectld method, 103
intersection method, 122, 417
intervals of time, 289
inv function, 106
inverse trigonometric functions, 96
-ipynb files, 72
IPython, 5
bookmarking directories, 62
command history in, 58-60
input and output variables, 58-59
logging of, 59-60
reusing command history, 58
design tips, 74-76
flat is better than nested, 75
keeping relevant objects and data alive,
75
overcoming fear of longer files, 75—76
development tools, 62-72
debugger, 62-66
profiling code, 68-70
profiling function line-by-line, 70-72
timing code, 67-68
executing code from clipboard, 50-52
HTML Notebook in, 72
integration with IDEs and editors, 52
integration with mathplotlib, 56-57
keyboard shortcuts for, 52
magic commands in, 54-55
making classes output correctly, 76
object introspection in, 48-49
profiles for, 77-78
Qt console for, 55
Quick Reference Card for, 55
reloading module dependencies, 74
%run command in, 49—50
shell commands in, 60-61
tab completion in, 47-48
tracebacks in, 53-54
ipython_config.py file, 77
irow method, 128, 152
is keyword, 393
isdisjoint method, 417

isfinite function, 96

isin method, 141-142

isinf function, 96

isinstance function, 391

isnull method, 96, 114, 143
issubdtype function, 354
issubset method, 417
issuperset method, 417
is_monotonic method, 122
is_unique method, 122

iter function, 392

iterating over groups, 255-256
iterator argument, 160

iterator protocol, 392, 427
itertools module, 429-430, 429
ix_ function, 93

J

join method, 184, 206, 212
JSON (JavaScript Object Notation), 18, 165—
166, 213

K

KDE (kernel density estimate) plots, 239
keep_date_col argument, 160
kernels, 239
key-value pairs, 413
keyboard shortcuts, 53

for deleting text, 53

for IPython, 52
KeyboardInterrupt event, 50
keys

argument, 188

for dicts, 416

method, 414
keyword arguments, 389, 420
kind argument, 234, 314
kurt method, 139

L

label argument, 233, 313, 315
lambda functions, 211, 262, 424

last method, 261

layout of arrays in memory, 356-357
left argument, 181

left_index argument, 181

left_on argument, 181

legends in matplotlib, 228

 

Index | 441

 

 

len function, 212, 258
less function, 96
less_equal function, 96
level keyword, 259
levels
defined, 147
grouping on, 259
sorting, 149-150
summary statistics by, 150
lexicographical sort
defined, 375
lexsort method, 374
libraries, 3-6
IPython, 5
matplotlib, 5
NumPy, 4
pandas, 4-5
SciPy, 6
limit argument, 313
linalg function, 105
line plots, 232-235
linear algebra, 105-106
linear regression, 274-275, 350-351
lineterminator option, 164
line_profiler extension, 70
Linux, setting up on, 10-11
list comprehensions, 418-420
nested list comprehensions, 419-420
list function, 408
lists, 408-411
adding elements to, 408-409
binary search of, 410
combining, 409
insertion into sorted, 410
list comprehensions, 418-420
removing elements from, 408-409
slicing, 410-411
sorting, 409-410
just method, 207
load function, 103, 379
load method, 171
loads function, 18
local scope, 420
localizing time series data, 304-305
loffset argument, 313, 316
log function, 96
log1p function, 96
log2 function, 96
logging command history in IPython, 59-60

logical_and function, 96
logical_not function, 96
logical_or function, 96
logical_xor function, 96
logy argument, 234
long format, 192

long type, 395

longer files overcoming fear of, 75-76
lower method, 207, 212
Istrip method, 207, 212
Istsq function, 106

Ixml library, 166-171

M

mad method, 139
magic methods, 48, 54-55
main function, 75
mainpulating structured arrays, 372
many-to-many merge, 179
many-to-one merge, 178
map method, 133, 195-196, 211, 280, 423
margins, 275
markers, 224
match method, 208-212
matplotlib, 5, 219-232
annotating in, 228-230
axis labels in, 226-227
configuring, 231-232
integrating with IPython, 56-57
legends in, 228
saving to file, 231
styling for, 224-225
subplots in, 220-224
ticks in, 226-227
title in, 226-227
matplotlibre file, 232
matrix operations in NumPy, 377-379
max method, 101, 136, 139, 261, 428
maximum function, 95, 96
mayavi, 249
mean method, 100, 139, 253, 259, 261, 265
median method, 139, 261
memmap object, 379
memory, layout of arrays in, 356-357
memory-mapped files
defined, 379
saving arrays to file, 379-380
mergesort sorting method, 375, 376
merging data, 177-189

 

442 | Index

combining data with overlap, 188-189
concatenating along axis, 185-188
DataFrame merges, 178-181
on index, 182-184
meshgrid function, 97
methods
defined, 389
for tuples, 407
in Python, 389
starting with underscore, 48
Microsoft Excel files, 172-173
mil domain, 18
min method, 101, 136, 139, 261, 428
minimum function, 96
missing data, 142-146
filling in, 145-146
filtering out, 143-144
mod function, 96
modf function, 95
modules, 392
momentum, 343
MongoDB, 176
MovieLens 1M data set example, 26-31
moving window functions, 320-326
binary moving window functions, 324-325
exponentially-weighted functions, 324
user-defined, 326
-mpkg file, 9
mro method, 354
mul method, 130
MultiIndex Index object, 121, 147, 149
multiple profiles, 77
multiply function, 96
munging, 13
mutable objects, 394-395

N

NA data type, 143
names argument, 160, 188
namespaces
defined, 420
in Python, 420-421
naming trends
in US baby names 1880-2010 example, 36-
43
boy names that became girl names, 42—
43
measuring increase in diversity, 37-40
revolution of last letter, 40-41

NaN (not a number), 101, 114, 143
na_values argument, 160
ncols option, 223
ndarray, 80
Boolean indexing, 89-92
creating arrays, 81-82
data types for, 83-85
fancy indexing, 92-93
indexes for, 86-89
operations between arrays, 85-86
slicing arrays, 86-89
swapping axes in, 93-94
transposing, 93-94
nested code, 75
nested data types, 371-372
nested list comprehensions, 419-420
New York MTA (Metropolitan Transportation
Authority), 169
None data type, 395, 399
normal function, 107, 110
normalized timestamps, 298
NoSQL databases, 176
not a number (NaN), 101, 114, 143
NotebookCloud, 72
notnull method, 114, 143
not_equal function, 96
.npy files, 103
.npz files, 104
nrows argument, 160, 223
nuisance column, 254
numeric data types, 395-396
NumPy, 4
arrays in, 355-362
concatenating, 357-359
c_ object, 359
layout of in memory, 356-357
replicating, 360-361
reshaping, 355-356
r_ object, 359
saving to file, 379-380
splitting, 357-359
subsets for, 361-362
broadcasting, 362-367
over other axes, 364-367
setting array values by, 367
data processing using
where function, 98-100
data processing using arrays, 97-103

 

Index | 443

 

 

conditional logic as array operation, 98—
100
methods for boolean arrays, 101
sorting arrays, 101-102
statistical methods, 100
unique function, 102-103
data types for, 353-354
file input and output with arrays, 103-105
saving and loading text files, 104-105
storing on disk in binary format, 103-
104
linear algebra, 105-106
matrix operations in, 377-379
ndarray arrays, 80
Boolean indexing, 89-92
creating, 81-82
data types for, 83-85
fancy indexing, 92-93
indexes for, 86-89
operations between arrays, 85-86
slicing arrays, 86-89
swapping axes in, 93-94
transposing, 93-94
numpy-discussion (mailing list), 12
performance of, 380-383
contiguous memory, 381-382
Cython project, 382-383
random number generation, 106-107
random walks example, 108-110
sorting, 373-377
algorithms for, 375-376
finding elements in sorted array, 376—
377
indirect sorts, 374-375
structured arrays in, 370-372
benefits of, 372
mainpulating, 372
nested data types, 371-372
universal functions for, 95-96, 367-370
custom, 370
in pandas, 132-133
instance methods for, 368-369

0

object introspection, 48-49
object model, 388

object type, 84

objectify function, 166, 169
objs argument, 188

offsets for time series data, 302-303

OHLC (Open-High-Low-Close) resampling,
316

ols function, 351

Olson database, 303

on argument, 181

ones function, 82

open function, 430

Open-High-Low-Close (OHLC) resampling,
316

operators in Python, 393

or keyword, 401

order method, 375

OS X, setting up Python on, 9-10

outer method, 368, 369

outliers, filtering, 201-202

output variables, 58-59

P
pad method, 212
pairs plot, 241
pandas, 4-5
arithmetic and data alignment, 128-132
arithmetic methods with fill values, 129—
130
operations between DataFrame and
Series, 130-132
data structures for, 112-121
DataFrame, 115-120
Index objects, 120-121
Panel, 152-154
Series, 112-115
drop function, 125
filtering in, 125-128
handling missing data, 142-146
filling in, 145-146
filtering out, 143-144
hierarchical indexing in, 147-151
sorting levels, 149-150
summary statistics by level, 150
with DataFrame columns, 150-151
indexes in, 136
indexing options, 125-128
integer indexing, 151-152
NumPy universal functions with, 132-133
plotting with, 232
bar plots, 235-238
density plots, 238-239
histograms, 238-239

 

444 | Index

line plots, 232-235
scatter plots, 239-241
ranking data in, 133-135
reductions in, 137-142
reindex function, 122-124
selecting in objects, 125-128
sorting in, 133-135
summiaty Statistics in
correlation and covariance, 139-141
isin function, 141-142
unique function, 141-142
value_counts function, 141-142
usa.gov data from bit.ly example with, 21-
26
Panel data structure, 152-154
panels, 329
parse method, 291
parse_dates argument, 160
partial function, 427
partial indexing, 147
pass statements, 402
passing by reference, 390
pasting
keyboard shortcut for, 53
magic command for, 55
patches, 229
path argument, 160
Path variable, 8
pcet_change method, 139
pdb debugger, 62
.pdf files, 231
percentileofscore function, 326
Pérez, Fernando, 45, 219
performance
and time series data, 327-328
of NumPy, 380-383
contiguous memory, 381-382
Cython project, 382-383
Period class, 307
PeriodIndex Index object, 121, 311, 312
periods, 307-312
converting timestamps to, 311
creating PeriodIndex from arrays, 312
defined, 289, 307
frequency conversion for, 308
instead of timestamps, 333-334
quarterly periods, 309-310
resampling with, 318-319
period_range function, 307, 310

permutation, 202
pickle serialization, 170
pinv function, 106
pivoting data
cross-tabulation, 277-278
defined, 189
pivot method, 192-193
pivot_table method, 29, 275-278
pivot_table aggregation type, 275
plot method, 23, 36, 41, 220, 224, 232, 239,
246, 319
plotting
Haiti earthquake crisis data example, 241—
246
time series data, 319-320
with matplotlib, 219-232
annotating in, 228-230
axis labels in, 226-227
configuring, 231-232
legends in, 228
saving to file, 231
styling for, 224-225
subplots in, 220-224
ticks in, 226-227
title in, 226-227
with pandas, 232
bar plots, 235-238
density plots, 238-239
histograms, 238-239
line plots, 232-235
scatter plots, 239-241
.png files, 231
pop method, 408, 414
positional arguments, 389
power function, 96
pprint module, 76
pretty printing
and displaying through pager, 55
defined, 47
private attributes, 48
private methods, 48
prod method, 261
profiles
defined, 77
for IPython, 77-78
profile_default directory, 77
profiling code
in IPython, 68-70
pseudocode, 14

 

Index | 445

 

 

put function, 362
put method, 362
‘py files, 50, 386, 392
pydata (Google group), 12
pylab mode, 219
pymongo driver, 175
pyplot module, 220
pystatsmodels (mailing list), 12
Python
benefits of using, 2-3
glue for code, 2
solving "two-language" problem with, 2—
3
data types for, 395-400
boolean data type, 398
dates and times, 399-400
None data type, 399
numeric data types, 395-396
str data type, 396-398
type casting in, 399
dict comprehensions in, 418-420
dicts in, 413-416
creating, 415
default values for, 415-416
keys for, 416
file input/output in, 430-431
flow control in, 400-405
exception handling, 402-404
for loops, 401-402
if statements, 400-401
pass statements, 402
range function, 404-405
ternary expressions, 405
while loops, 402
xrange function, 404-405
functions in, 420-430
anonymous functions, 424
are objects, 422—423
closures, 425—426
currying of, 427
extended call syntax for, 426
lambda functions, 424
namespaces for, 420-421
returning multiple values from, 422
scope of, 420-421
generators in, 427-430
generator expressions, 429
itertools module for, 429-430
IDEs for, 11

interpreter for, 386
list comprehensions in, 418-420
lists in, 408-411
adding elements to, 408-409
binary search of, 410
combining, 409
insertion into sorted, 410
removing elements from, 408-409
slicing, 410-411
sorting, 409-410
Python 2 vs. Python 3, 11
required libraries, 3-6
IPython, 5
matplotlib, 5
NumPy, 4
pandas, 4-5
SciPy, 6
semantics of, 387-395
attributes in, 391
comments in, 388
functions in, 389
import directive, 392-393
indentation, 387-388
methods in, 389
mutable objects in, 394-395
object model, 388
operators for, 393
references in, 389-390
strict evaluation, 394
strongly-typed language, 390-391
variables in, 389-390
“duck” typing, 392
sequence functions in, 411-413
enumerate function, 412
reversed function, 413
sorted function, 412
zip function, 412-413
set comprehensions in, 418-420
sets in, 416-417
setting up, 6-11
on Linux, 10-11
on OS X, 9-10
on Windows, 7—9
tuples in, 406-407
methods for, 407
unpacking, 407
pytz library, 303

 

446 | Index

Q

qcut method, 200, 201, 268, 269, 343
qr function, 106

Qt console for IPython, 55
quantile analysis, 268-269
quarterly periods, 309-310
quartile analysis, 343-345
question mark (?), 49
quicksort sorting method, 376
quotechar option, 164
quoting option, 164

R

r file mode, 431
r+ file mode, 431
Ramachandran, Prabhu, 249
rand function, 107
randint function, 107, 202
randn function, 89, 107
random number generation, 106-107
random sampling with grouping, 271-272
random walks example, 108-110
range function, 82, 404-405
ranking data
defined, 135
in pandas, 133-135
ravel method, 356, 357
rc method, 231, 232
re module, 207
read method, 432
read-only mode, 431
reading
from databases, 174-176
from text files in pieces, 160-162
readline functionality, 58
readlines method, 432
readshapefile method, 246
read_clipboard function, 155
read_csv function, 104, 155, 161, 163, 261,
430
read_frame function, 175
read_fwf function, 155
read_table function, 104, 155, 158, 163
recfunctions module, 372
reduce method, 368, 369
reduceat method, 369
reductions, 137
(see also aggregations)

defined, 137
in pandas, 137-142
references
defined, 389, 390
in Python, 389-390
regress function, 274
regular expressions (regex)
defined, 207
manipulating strings with, 207-210
reindex method, 122-124, 317, 332
reload function, 74
remove method, 408, 417
rename method, 198
renaming axis indexes, 197-198
repeat method, 212, 360
replace method, 196, 206, 212
replicating arrays, 360-361
resampling, 312-319, 332
defined, 312
OHLC (Open-High-Low-Close)
resampling, 316
upsampling, 316-317
with groupby method, 316
with periods, 318-319
reset_index function, 151
reshape method, 190-191, 355, 365
reshaping
arrays, 355-356
defined, 189
with hierarchical indexing, 190-191
resources, 12
return statements, 420
returns
cumulative returns, 338-340
defined, 338
return indexes, 338-340
reversed function, 413
rfind method, 207
right argument, 181
right_index argument, 181
right_on argument, 181
rint function, 96
rjust method, 207
rollback method, 302
rollforward method, 302
rolling, 348
rolling correlation, 350-351
rolling_apply function, 323, 326
rolling_corr function, 323, 350

 

Index | 447

 

 

rolling_count function, 323
rolling_cov function, 323
rolling_kurt function, 323
rolling_mean function, 321, 323
rolling_median function, 323
rolling_min function, 323
rolling_mint function, 323
rolling_quantile function, 323, 326
rolling_skew function, 323
rolling_std function, 323
rolling_sum function, 323
rolling_var function, 323

rot argument, 234

rows option, 277

row_stack function, 359

rstrip method, 207, 212

r_ object, 359

S

save function, 103, 379

save method, 171, 176

savefig method, 231

savez function, 104

saving text files, 104-105

scatter method, 239

scatter plots, 239-241

scatter_matrix function, 241

Scientific Python base, 7

SciPy library, 6

scipy-user (mailing list), 12

scope, 420-421

screen, clearing, 53

scripting languages, 2

scripts, 2

search method, 208, 210

searchsorted method, 376

seed function, 107

seek method, 432

semantics, 387-395
attributes in, 391
comments in, 388
“duck” typing, 392
functions in, 389
import directive, 392-393
indentation, 387-388
methods in, 389
mutable objects in, 394-395
object model, 388
operators for, 393

references in, 389-390
strict evaluation, 394
strongly-typed language, 390-391
variables in, 389-390
semicolons, 388
sentinels, 143, 159
sep argument, 160
sequence functions, 411-413
enumerate function, 412
reversed function, 413
sorted function, 412
zip function, 412-413
Series data structure, 112-115
arithmetic operations between DataFrame
and, 130-132
grouping with, 257-258
set comprehensions, 418-420
set function, 416
setattr function, 391
setdefault method, 415
setdiffld method, 103
sets/set comprehensions, 416—417
setxorld method, 103
set_index function, 151
set_index method, 193
set_title method, 226
set_trace function, 65
set_value method, 128
set_xlabel method, 226
set_xlim method, 226
set_xticklabels method, 226
set_xticks method, 226
shapefiles, 246
shapes, 80, 353
sharex option, 223, 234
sharey option, 223, 234
shell commands in IPython, 60-61
shifting in time series data, 301-303
shortcuts, keyboard, 53
for deleting text, 53
for IPython, 52
shuffle function, 107
sign function, 96, 202
signal frontier analysis, 345-347
sin function, 96
sinh function, 96
size method, 255
skew method, 139
skipinitialspace option, 165

 

448 | Index

skipna method, 138
skipna option, 137
skiprows argument, 160
skip_footer argument, 160
slice method, 212
slicing
arrays, 86-89
lists, 410-411
Social Security Administration (SSA), 32
solve function, 106
sort argument, 181
sort method, 101, 373, 409, 424
sorted function, 412
sorting
arrays, 101-102
finding elements in sorted array, 376-377
in NumPy, 373-377
algorithms for, 375-376
finding elements in sorted array, 376—
377
indirect sorts, 374-375
in pandas, 133-135
levels, 149-150
lists, 409-410
sortlevel function, 149
sort_columns argument, 235
sort_index method, 133, 150, 375
spaces, structuring code with, 387-388
spacing around subplots, 223-224
span, 324
specialized frequencies
data munging for, 332-334
split method, 165, 206, 210, 212, 358
split-apply-combine, 252
splitting arrays, 357-359
SQL databases, 175
sql module, 175
SQLite databases, 174
sqrt function, 95, 96
square function, 96
squeeze argument, 160
SSA (Social Security Administration), 32
stable sorting, 375
stacked format, 192
start index, 411
startswith method, 207, 212
statistical methods, 100
std method, 101, 139, 261
stdout, 162

step index, 411
stop index, 411
strftime method, 291, 400
strict evaluation/language, 394
strides/strided view, 353
strings
converting to datetime, 291-293
data types for, 84, 396-398
manipulating, 205-211
methods for, 206-207
vectorized string methods, 210-211
with regular expressions, 207-210
strip method, 207, 212
strongly-typed languages, 390-391, 390
strptime method, 291, 400
structs, 370
structured arrays, 370-372
benefits of, 372
defined, 370
mainpulating, 372
nested data types, 371-372
style argument, 233
styling for matplotlib, 224-225
sub method, 130, 209
subn method, 210
subperiod, 319
subplots, 220-224
subplots method, 222
subplots_adjust method, 223
subplot_kw option, 223
subsets for arrays, 361-362
subtract function, 96
sudo command, 11
suffixes argument, 181
sum method, 100, 132, 137, 139, 259, 261, 330,
428
summary statistics, 137
by level, 150
correlation and covariance, 139-141
isin function, 141-142
unique function, 141-142
value_counts function, 141-142
superperiod, 319
svd function, 106
swapaxes method, 94
swaplevel function, 149
swapping axes in arrays, 93-94
symmetric_difference method, 417
syntactic sugar, 14

 

Index | 449

 

 

system commands, defining alias for, 60

T

tab completion in IPython, 47—48
tabs, structuring code with, 387-388
take method, 202, 362
tan function, 96
tanh function, 96
tell method, 432
terminology, 13-14
ternary expressions, 405
text editors, integrating with [Python, 52
text files, 155-171
delimited formats, 163-165
HTML files, 166-171
JSON data, 165-166
Ixml library, 166-171
reading in pieces, 160-162
saving and loading, 104-105
writing to, 162-163
XML files, 169-171
TextParser class, 160, 162, 168
text_content method, 167
thousands argument, 160
thresh argument, 144
ticks, 226-227
tile function, 360, 361
time series data
and performance, 327-328
data types for, 290-293
converting between string and datetime,
291-293
date ranges, 298
frequencies, 299-301
week of month dates, 301
moving window functions, 320-326
binary moving window functions, 324—
325
exponentially-weighted functions, 324
user-defined, 326
periods, 307-312
converting timestamps to, 311
creating PeriodIndex from arrays, 312
frequency conversion for, 308
quarterly periods, 309-310
plotting, 319-320
resampling, 312-319
OHLC (Open-High-Low-Close)
resampling, 316

upsampling, 316-317
with groupby method, 316
with periods, 318-319
shifting in, 301-303
with offsets, 302-303
time zones in, 303-306
localizing objects, 304-305
methods for time zone-aware objects,
305-306
TimeSeries class, 293-297
duplicate indices with, 296-297
indexes for, 294-296
selecting data in, 294-296
timestamps
converting to periods, 311
defined, 289
using periods instead of, 333-334
timing code, 67-68
title in matplotlib, 226-227
top method, 267, 282
to_csv method, 162, 163
to_datetime method, 292
to_panel method, 154
to_period method, 311
trace function, 106
tracebacks, 53-54
transform method, 264-266
transforming data, 194-205
discretization, 199-201
dummy variables, 203-205
filtering outliers, 201-202
mapping, 195-196
permutation, 202
removing duplicates, 194-195
renaming axis indexes, 197-198
replacing values, 196-197
transpose method, 93, 94
transposing arrays, 93-94
trellis package, 248
trigonometric functions, 96
truncate method, 296
try/except block, 403, 404
tuples, 406-407
methods for, 407
unpacking, 407
type casting, 399
type command, 156
TypeError event, 84, 403
types, 388

 

450 | Index

tz_convert method, 305
tz_localize method, 304, 305

U

U file mode, 431

uint16 data type, 84

uint32 data type, 84

uint64 data type, 84

uint8 data type, 84

unary functions, 95

underscore (_), 48, 58

unicode type, 19, 84, 395

uniform function, 107

union method, 103, 122, 204, 417

unique method, 102-103, 122, 141-142, 279

universal functions, 95—96, 367-370
custom, 370
in pandas, 132-133
instance methods for, 368-369

universal newline mode, 431

unpacking tuples, 407

unstack function, 148

update method, 337

upper method, 207, 212

upsampling, 312, 316-317

US baby names 1880-2010 example, 32-43
boy names that became girl names, 42-43
measuring increase in diversity, 37-40
revolution of last letter, 40-41

usa.gov data from bit.ly example, 18-26

USDA (US Department of Agriculture) food

database example, 212-217
use_index argument, 234
UTC (coordinated universal time), 303

V
ValueError event, 402, 403
values method, 414
value_counts method, 141-142
var method, 101, 139, 261
variables, 55

(see also environment variables)

deleting, 55

displaying, 55

in Python, 389-390
Varoquaux, Gaél, 249
vectorization, 85

defined, 97

vectorize function, 370
vectorized string methods, 210-211
verbose argument, 160
verify_integrity argument, 188
views, 86, 118
visualization tools

Chaco, 248

mayavi, 249
vsplit function, 359
vstack function, 358

W
w file mode, 431
Wattenberg, Laura, 40
Web APIs, file input/output with, 173-174
week of month dates, 301
when expressions, 394
where function, 98-100, 188
while loops, 402
whitespace, structuring code with, 387-388
Wickham, Hadley, 252
Williams, Ashley, 212
Windows, setting up Python on, 7-9
working directory
changing to passed directory, 60
of current system, returning, 60
wrangling (see data wrangling)
write method, 431
write-only mode, 431
writelines method, 431
writer method, 165
writing
to databases, 174-176
to text files, 162-163

X

Xcode, 9

xlim method, 225, 226

XML (extensible markup language) files, 169—
171

xrange function, 404-405

xs method, 128

xticklabels method, 225

Y

yield keyword, 428
ylim argument, 234
yticks argument, 234

 

Index | 451

 

Z

zeros function, 82
zip function, 412-413

 

452 | Index

 

About the Author

Wes McKinney is a New York-based data hacker and entrepreneur. After finishing
his undergraduate degree in mathematics at MIT in 2007, he went on to do quantitative
finance work at AQR Capital Management in Greenwich, CT. Frustrated by cumber-
some data analysis tools, he learned Python and in 2008, started building what would
later become the pandas project. He's now an active member of the scientific Python
community and is an advocate for the use of Python in data analysis, finance, and
statistical computing applications.

 

Colophon

The animal on the cover of Python for Data Analysis is a golden-tailed, or pen-tailed,
tree shrew (Ptilocercus lowii). The golden-tailed tree shrew is the only one of its species
in the genus Ptilocercus and family Ptilocercidae; all the other tree shrews are of the
family Tupaiidae. Tree shrews are identified by their long tails and soft red-brown fur.
As nicknamed, the golden-tailed tree shrew has a tail that resembles the feather on a
quill pen. Tree shrews are omnivores, feeding primarily on insects, fruit, seeds, and
small vertebrates.

 

Found predominantly in Indonesia, Malaysia, and Thailand, these wild mammals are
known for their chronic consumption of alcohol. Malaysian tree shrews were found to
spend several hours consuming the naturally fermented nectar of the bertam palm,
equalling about 10 to 12 glasses of wine with 3.8% alcohol content. Despite this, no
golden-tailed tree shrew has ever been intoxicated, thanks largely to their impressive
ethanol breakdown, which includes metabolizing the alcohol in a way not used by
humans. Also more impressive than any of their mammal counterparts, including hu-
mans? Brain to body mass ratio.

Despite these mammals’ name, the golden-tailed shrew is not a true shrew, instead
more closely related to primates. Because of their close relation, tree shrews have be-
come an alternative to primates in medical experimentation for myopia, psychosocial
stress, and hepatitis.

The cover image is from Cassel’s Natural History. The cover font is Adobe ITC Gara-
mond. The text font is Linotype Birka; the heading font is Adobe Myriad Condensed;
and the code font is LucasFont’s TheSansMonoCondensed.


: Yeah, we Yup. 0:00 : Okay again. Welcome everybody. It's 6, 32 now, and it's still Wednesday, April the twelfth. And this is pretty much one of the last classes that we are doing. So. 0:02 : Let me pull up 0:19 : me share the screen. 0:21 Brandon Vuong: bye. 0:23 : and let me pull this up. 0:26 : So we are right here. 0:30 We we talk about machine learning with Python, and we will 0:34 : spend some time on the the final. How is going to be structured? What are going to be the topics and a few other things, and I will present some of the 0:40 : oh, the previous fine Also, just to give you an idea of how things should be done. 0:57 : There will be no assignment for next week. That's good news. So 1:05 : but I really want you to focus on on the final. So stop thinking about the final. So, Don't, ask me about the extensions when you started the 3 days before the due date, so the due date is going to be 1:13 : on the 20 first. 1:33 : We need to have some time to review your finals and to 1:36 : ere 1:45 : that means that we will need the some time to review them and to decide who is going to present, and if we will present them 1:53 : in terms of presentation, just to start with the finals. 2:06 : Some of you, let's say 5, 6 of you will be asked the to present day. Find out. 2:12 : Find out so our individual. 2:20 : if you will be asked to present. You are supposed to present, so there will be points of if you don't 2:23 : if you can. I mean the 2:32 : number of days we have is not big. That means 2:39 : that we will let you know 1, 2 days in advance. If you are going to present or not, so presume that you May 2:47 : D selected for representing, and it's Random. 2:57 : Excuse me. 3:05 : final Sara individual, as all the other assignments that we did so far. 3:09 : and then we will go into the details on how to structure them up there. Some examples so, and what they find out so are going to be in terms of topics. 3:17 : So before we 3:32 : continue. 3:36 : let me spend a little bit of time on the the assignment. So the assignment was about the extracting 3:38 : headlines so pretty much from a a website and then do some analysis. 3:51 : So we use the this: I presented this very basic Excuse me. 3:58 : Script, just to have a base for you to use to build on top of it for screening content in this case from the New York Times 4:08 : and for the assignment You have been asked to do something a little bit more complex than that 250, 4:24 : meaning calculating or creating the the word cloud. 4:33 : calculating the sentiment. And that's why it was a assignment, and not just 4:41 i'm in that a snipet to to be incorporated somewhere else. 4:50 : anyway. So I 4:56 : imported 4:59 : the libraries that they are required. 5:02 : as in many other cases. 5:11 : I'm using a a a a function for a cleaning the text. So this is again I I said, min of clients, so it's something that I strongly encourage you to do. 5:17 : You may want to create your own. If you plan to use a natural language processing in the future. 5:29 : you may want to create your own text cleaning function. So in this case this function is is taking up the a string of words, so sometimes it's, at least, sometimes it's a string. It really depends on how you create it. 5:41 : It takes the minimum length of of of the world that will be acceptable. This is based on the assumption that if a word 5:59 : is a smaller than a certain number of characters, then as no semantic meaning is this true? It is wrong for 6:12 : Hmm. English language. Yes, probably for others. Not so much. But 6:21 we're using a. 6:26 : And then you have the list of of software 6:28 : So it's doing it's transforming the string into a list is looping into the list. It's transforming into lower case, checking. If there's a alphabetical or not, if is a bigger than the minimum line. 6:32 : if it's not in the so-called list or not. If we'll pass all those. Ifs, then we'll add the the word to the list of clean words 150, 6:51 : and we retire not the least of clean boards then. 7:03 : and connecting 7:09 : my script to the web page. So that's the URL. 7:11 : Get in the body. 7:18 : then passing it through a beautiful super 7:20 initialize the string that will hold the words in the headline. 7:26 : printing it. 7:32 : Now you don't have to do that, but because of that I was getting quite a lot of lines, so ending with the mean red like 3 min, or read the 3 min. Read the 7:34 : 4 min, Read the or things like that. I decided to keep them. 7:50 : You'll not need to do it, and I mean for another. 7:56 : the website. This would not 8:02 : make much sense, so 8:06 : I mean, if you don't have that will just, I mean pass for each one of them. But in this case. If it's not ending with the mean read. 8:09 : then we will print the the text, I mean the the line. Now that i'm reading it's keeping one line, and then adding to the 8:22 : the the the single worse then creating. 8:34 : So once I have the string. I'm. Extending 8:39 : the software, list at this point, this may not make sense, but that's fine 8:48 : Setting the minimum length. 8:56 : leaving the text, sir. 8:57 : And then i'm tracking diagrams. So extra team diagrams. 9:01 : So again, that's a very basic way to to extract diagrams and taking the 7 most common could be any number. 9:07 : Then i'm 9:19 : i'm checking for the most. Call, Mona. 9:29 : Oh, my! That's kind of lived on that. But that's okay. 9:34 : appending to the list. 9:37 : generating the work. Cloud. 9:44 : Then go into the sentiment analysis, using father calculating positive negative, a new role and and then printing it. 9:49 : So when I run it you have the the word cloud here. : and you have a : the headlines. : You have a : the seventeenth. : It's kind of common. Again, we said 1 million times the the new drilla is a way much more than the other 2. : Most of what we say doesn't really have any positive or negative connotation? : Hmm. It could be interesting to see : over the days, over the months, the years. What is the : the fluctuation of a positiveness and negativeness : all the headlines so, and see if we can determine some sort of correlation with the leading events or things that can be somehow correlated : erez agmoni. The work cloud in this case is not telling us much. Most likely you may want to eliminate some other words like what the said 101 : because it's not really doing much. Again, we have a : still quite a lot of trump related that. So this Donald, the most likely it's Donald Trump. But investigation could be about the Donald Trump investigation. But that's what we have : just to let you know. I : also play with another : script that they call web mining, but it's not redoing the web mining. But just. I want to go through that to share with you a form of development that's not necessary is better. : but it's it's it's more clear. So : this script is not doing anything, because I mean it's all a definition of functions just : printing. : I'm: in that. Thanks so. : But what is interesting is is the fact that each logical step of your possible script is a function. : Some people are more familiar, a more comfortable working this way. : It will give you a a logical decomposition of your script in that logical steps. : So loading the files instead of having in the main program is a function. So you pass the name of the file, and we'll open it, and we what you would do just with the opening. 5 obviously : things like that that may have more sense. If you have a I mean, there are 2 reasons: one, if it's something that you do 1 million times so, and it's long enough. Then you want to create your function once for all, and then you will reuse it, or if you have a mouth. : people like in this case finds to open up. : and then, instead of writing the same, 2, 3 lines, you're right. Just one line th that that's an option : for a removing punctuation. I mean, that may be more justified because it's bigger, or it can be just a a part of a larger text cleaning. That's another option. : Remove the alpha numerical lot, same thing, remove so forth, so remove what it less than a given number of characters : remove a irrelevant words. So you will create a list of the 11 what! You you and even have a a file with the relevant words, and then read the file, the load, the the words into a list, and then do the cleaning : sentiment pretty much same thing by Graham, sir. Work, cloud. : So again it it it a collection of all the snipets of code around the natural language processing. So you may want to have this approach. You may not, but I just wanted to share with you an option. : All right, so let me stop sharing for a second. Let me check if you have a any question. : All right. : So let me go to the main content. : All right. So we will. : We will talk about machine learning, data, mining, and we will do in in class, so exercise on that. So in the next, how we also, I will talk about machine learning. I will give you some examples in in in Python. : and then we will do an in class that I would really like. : Hmm you to spend a little bit more time. : Then we will. We will discuss the solution of the assignment up. : and we will talk about the the final, and that could be the end of the class. Again, there would be no assignment for next week. : All right, so : let me shut the screen again : and let me go here. : Okay. So we would talk about machine learning in general, and we would talk about a python in particular. : What is machine learning? So, generally speaking, machine do not learn now. We said several times so, but they have different behaviors based on the different data. : So that's pretty much the main characteristic of all those systems. So, generally speaking, they are models. So have 2 components, Teda. : more complex machine learning systems like Chat Gpt, more than a model. They have a a pipeline that is, including the algorithm : but it's not just data, an algorithm but it's data. : And now got it as part of it. Process. : There is no machine learning without data. Pretty much : Hmm. : Erez agmoni mostly, if not all, what we today call machine learning is based on a neural networks on of different sizes, different shapes, different uses 101. : But the core is : linear algebra. : Apply to the meta for of neural networks. So that's basically what they are now. Activual. Neural networks became somehow used the in the second half of the eighties. : but because of it, it's a very computational intensive. Algorithm They didn't become : widely used till the second half of the Ninetys. Actually. : I : I mean that : I started working in artificial intelligence many years ago at that time machine learning that wouldn't be possible at all for 2 reasons. One: we didn't have a enough data, because everything was a analog and no digit. : and the sake on the the computers we had the : they didn't have the power to process I mean meaningful : neural networks, so that could generate a results that would have been acceptable. So now, with the evolution, our the : we can run that : i'll gr it's a little more complex. We have more data, and at the very end we are reaching so : neural networks. So they have a an input layer, an output layer, and in between you have a layers that are called the hidden layers, so that can be many. So when you have something like Chat : G Gpt, then really they don't even know how many layers are there. : Yeah, but what is kind of a a measure for the complexity is what they called parameters : for the parameter in neural network you have input at each layer you have the inputs and you have weights. So the weights are the parameters and TV got it. There is one parameter each input per each layer. : So in the current mit ctl, and even Gpt. 2. That is the base for a chat, gpt as a several 1 billion of those parameters one. : So that's an indication of the complexity. : When we talk about the explainable AI. : There is quite a lot of confusion on what the explain a ball can be. It can be how the AI is present in the results in a : intelligible : or a human like : erez agmoni, where your presentation, like a child, gpt that is interacting in a natural language, or can be explain how they work. If we say 150 : on that, the explaining how they work, we we, we will probably never know how they work. The main reason, is because if you want to know what the : lay, your number I don't know 1,333 is doing. : We need to have a sort of a a probe that is getting the data, and there is nothing like that because of the calculation is ongoing. There is no memory stage, meaning that we cannot really know what is happening at each layer. : We know what is happening as input we know what is happening as output, what is happening in between is unknown. : hey? : The only thing we can do is basically to have a system so checking one the other. But it's not : much more than that. : anyway. So that's machine learning. Machine learning is using : pretty much all the algorithms originally developed for data mining and : eventually expand like a neural networks. : the at the very beginning they had not many. He then lay. If so, now again. : we don't even know how many layers we have, but is : several 100,000; : that when we talk about : how we handle the how we learn. : there are 2 main categories of of learning supervised the unsupervised. : If I want to do predictions I need to have a : the equivalent of the experience, so I cannot predict. The : I don't know results or or estimator will be in that case the the result of any event, if I don't have experience in those types of event, whatever event is, can be remaining and raining : could be one team winning or another. So I need to have some experience. So in terms of computing, experience is data meaning. I need to have data about the similar cases : happening in the past. : and that's the supervised learning. Then we use the past experience : to do predictions. : Now a few shorter can never be predicted. So there are a famous cases there. There is a book that is called the the Blacks, One, highly recommended, written by : the mathematician. The I think he's. From then he became. : I was to it. Broker made a lot of money, wrote this interesting book and another one again. So all gods : that are based on the fact that everything is consistent with the past, still is not : the name of the black One now : is from an example that he used the that everybody was thinking that all this once we're white. Then they went to New Zealand, and they realized that over there they were swans that they are black. : So the rule : we have a swan is white : was through till the moment that this call and that that was not true anymore. : So other TV gala example is with the the targets. So before Thanksgiving, they think that you months are great taking big care of you. They feed you, they treat you well. : And then Thanksgiving happened that and that think so. : change quite dramatically for them. : So everything is through till there is the moment that is not true. So that's the limitation of the predictions that that we we can do. Then we can create system that can learn now somehow from the experience. : But again. : even if we learn from experience, so there could be something different that that we will : make the predictions wrong. : But that's the way we do so every time you see any system predicting the future is predicting the future or future events, even at a very small scale, based on the past experience. If it will change. The system, is not going to work : unsupervised, that is, when you do not have a the supervision. When you do not have this experience. Think about. You have a information about your clients, and you want to create an off or a addressing that needs : you. Don't know what the needs can be, but you can the cluster your clients in a category, so that they are as much homogeneous as possible. So that's an an an example of all the unsupervised learning. : So what you are learning is basically the singularity they have one with the other. : There is a a a a third way that is called reinforcement learning. That is, basically when you have a reward, you generate a : cases, and then you measure the score that each case is getting : by collecting all the different cases you are generating your supervision. So reinforcement. Learning is actually a a self-generated the supervised learning. : So those 2 plus one are the 3 ways, for a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, to, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, to, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a : in supervised learning. There are 2 steps. So you have your data set with the all the cases. : You split, the the data set into 2 portions, one that you use to create a model. Again, a model is a a combination of data plus. Algorithm : So you take this a subset. That is typically the largest of of the 2 splits. You apply out the the proper algorithms, and you get the model. : Once you have the model, you want to test the model to test the model, you use the the remaining portion of the supervised, the data that you have to evaluate the accuracy of your predictions or classifications. : So the accuracy is measured by dividing the number of classifications at the right by the total number of cases. : So it it's pretty intuitive. : When you do this type of supervised learning, you are assuming that : the training and the testing our representations of the same thing. : So if you have a data about the apparel. : then you take for training the data in Samara and for testing the data in winter. You may not have a a a good accuracy in terms of the the current classification that the system can generate, because the the 2, the training and the testing are different. : Same thing. If you want to create, to create a a system that will predict the the selling of our in the summer, and we use all the data from winter, both the training and testing : a your model, even if we'll have a in a cure that that could be high in the model creation. But when you bring the the model into real life, the the the model we not perform well. : So staying in the : in the homogeneity or more genetic, whatever is the term between, or the similarity between training and testing when you split the the original data set between training and testing. : You want to do a split that is as much random as possible to keep the diversity, all the : all, the classification. So the same in training and testing the think about the raining. No raining you want. And there are, I don't know. : 5% of days of rain. You want to have a those 5% or similar in the training subset and in a testing subset 150. : If you use in training a subset with a 0% or raining, and you are using a testing with 10% or raining. Then the accuracy will be low. : and also not going to be a much represented. : So the distribution of values of has to be the same between training and testing and model a reality. : We mentioned about a reinforcement Learning again, it's pretty much a similar to a supervised learning, with the main difference that you have the : supervision, the experience about the the past, the data about the past that our self-generated I will skip all of those. : and I will go. : Genetic calories are kind of similar to enforcement learning. : So the the system is generating : groups : cases that are similar to generation. So you create a generation of input and you get the results : then based on the results. You adjust the input that you provide, but to something else. : and you move on a a functional. It's called the fitness function that will tell you how good you are for each generation all the data that you are creating : deep learning. We mentioned briefly. : and machine learning. It's all about neural networks. And right now. It's all about : neural networks with a lot of those he delay. Yes. So in this case this is what it's called the neural network. You have one input, layer, one output layer, and you have one hidden layer. : When you have a multiple in the layers, and then this mountain is a big number. Then it's normally called deep learning. : I mean, it's all about playing on the names it that depth of of the learning. It's basically the number of : there is nothing more than that. So deep learning. It's a an artificial neural network with a high number of he. Then lay it. : Let me skip. Yeah. that one. So : one of the things that we already realized the in the assignments that we did so far. : the data collaboration, it's really essential. Whatever you do in a data science. In a broad sense. : if the quality of the data is low, the quality of the results will be no : machine learning models. They have a. As I was saying, before, any model, I mean data based models. : data and algorithm : the algorithm can be as good as you want. But if the data is not really representing the reality : that you want to model. the result would be a poor anyway. : Representing means 2 things: one on the quote, You decide that you need to have enough cases to create the knowledge of of the domain. : and they say on the the quality. So they really need to be representative of the actual condition or the reality that you are modeling. : So the data is essential. So keeping in mind that that when you collect data from real world. There is always quite a lot of preparation to do, because data can be incomplete, no easy, inconsistent. So all of those are : things that you really need to address : the pre-processing. It's basically basically Nina. So you want to remove things. So that seems to be : not proper for the context you are considering : that can be improper content in a straight sense. So you are doing a a chat, Gpt. And a. This is going to be something that that people will use. You want to remove from : the mountain the ocean what text you have all the text with inappropriate content. and I mean because he's from the open : web. You have a everything possible. So you need to remove that to avoid the that people will be negatively affected by the results that the system would present. That's a : an example of cleaning, but cleaning can be. : You have a one rule, one column in your data set with more than 50% or missing values. There is no need to keep it, because it's not telling you much. You have a the the : you are creating a system to predict the the the whether it is raining or not. All the the data points with the no value for rain, no rain : as no meaning, because you cannot use the data because you don't know what is going to happen under those circumstances. So those are examples of data cleaning. : And another example is the correlation. So you want to eliminate the : variable, so that 2 correlated, if you have one variable with value and another variable, that is, 3 times the the first one. There is no need to to keep them both. You keep one, and you have the behavior of that variable, anyway. : So that's the Greening integration is, the more sources you have, the more you know about the the domain. : Think about what market that's know about us. They know a lot because they are getting data from multiple sources. So they have a data from Google equivalent about : the that we visited the sites we clicked on. So not just visiting. But what are the elements that are more relevant to us : Erez Agmoni, but also they have the information about the credit Cats companies. They have information from social media. : Think about Facebook. Facebook is also you on a list for Gram and Whatsapp, meaning that they can provide the market that with the information up to the phone number with the phone number. They can go to your name : now. In some cases there are privacy laws preventing them from doing that. But you can recreate using a machine learning algorithm so or a basic data mining algorithm : the identity so that's the : sanitizing. It's always a a a a 3 business. But but anyway, you have multiple sources. You want to integrate the sources when you integrate the sources. So you have a 1 million issues. : technical issues. One source is in one for another, another for matter. But that's an easy issue to fix. But then there are things that are more complex and the same variable, named in different ways across the different sources. : E. C. So when you do integration that you need to do all of that. : and then you may want to do transformation. You want to put together variables. You want to eliminate variables that are not relevant for your particular case. So : those 3 steps. Cleaning integration and transformation are the 3 steps that are more relevant in that the in data preparation : we mentioned unsupervised learning. And so advice learning is TV gala through clustering : clustering is is a process we you generally use. K means for that. You create a K. Is a number. So K. Cluster, and then you place your objects in those classes : and in those buckets, and then you move objects from one back to to another to have a buckets with elements that they are as close as possible, one to the other. And then you also want to have a : the : buckets that are as much different as possible, one from the other. It's in a I data. They process. So you keep moving objects, since there is no better place to move them than that the one that they already are, and that's the end of the process. : So the algorithm it's pretty basic. It can be done manually, but we fortunately have libraries for doing that. : This can be used for clustering and for a meaning for unsupervised and supervised learning. : Typically, they'd use the for supervised learning because they are very easy to explain, and they are kind of similar to the way we do months in some condition under some conditions. : That's an example. You want to go a restaurant. You check if there are a : people there, if none, you presume that the restaurant is not particularly good, and you work away. If some meaning that there are some tables available. : then you say : it's full. You ask, what is the waiting time? Who? Yeah, for getting a table? And then you have your own threshold. So if it's very long, you say you know what i'm not going to wait. : If it's something in the middle. Do you say, okay, is there anything else in the nearby : things like that? So when you put together all of this, you basically have a the the the choices that you have at each single step like when you play chess. : The goal of this process is to go from a a condition with maximum uncertainty, meaning. If you are doing a system predicting a rain or rain. : you have a a data set with all the days, with some case of rain, some days of of no rain. : Then applying filters like, what is the level of humidity? If the level of humidity is above a certain number, it's more likely that there will be rain. So applying rules like that that again, like : humidity above a given number, you reduce the the uncertainty, the impurity in your data set creating a subsets somehow, and then you keep refining the results. : Still, you reach a level of impurity that is acceptable. : So sometimes can be : the maximum meaning that you have all the days with rain on in one back at the and all the days without rain on another bucket. Some other times you can define that the how you are your data I could be : to measure that. We use a entropy or a gene in the excel. There are 2 methods just to measure this. In period : we mentioned neural networks. So our TV show and neural networks are. Are we making the human a new run? So they, those activities are new runs. So : they have inputs weights. : You have a summation of those inputs and weights. And then you have another function giving the output. : So that's basically what it is. : In this processing element or new run. You have a a function that is doing the summation and a function that is doing the the transfer to the next layer : that it's, and each layer it's : a matrix. and that : what it's doing is multiplying a matter so TV show neural networks is an application. : Whatever is the model. You may want to have, a a, a a way to measure the results. So one of the most common way for doing it is the so called : er error, madrics or confusion matrix, where you compare the true positive faults, positive or negative, false negative. : So in this case the system was trying to predict. If buying a a computer, yes or no, you have a You have 10,000 cases. : Your model is saying Yes, by computer for about 7,000 cases, and : I mean saying yes it, saying yes, for 7,300, and then change, and all those less than 7,000 are actual. Yes, from the testing subset. : and that the meaning that they are 2 positive and 412 are a false, positive. So the system is saying that yes, it's positive and reality is no, it's negative. : So similarly, for the No is saying no, and he's no for 2020, 2,600, the cases. So this is a case where the system is is predicting well, like both the yes and no meaning. The accuracy overall is really good. : Sometimes you are not so lucky that when you have a : a case with the data set that this queue, the in a way or in the other. You may not have a a good prediction for the one where you have less cases. If you think for a second on the it's pretty much like you months. So if you do not have cases, so that means you will not have experience. If you have been asked the things where you don't have a experience. : Okay, you are a capability to interpret. Results will be low. : A. : And that's exactly what what is happening. So give you an example. You want to predict the the cases on the general of population, of all of : cancer. : So the number of cases of people with cancer in the general population is low. : a meaning in a data set collecting all the data. You are going to have. A very few yes, cancer and way much more, no cancer. : meaning that your system is not going to predict the no cancer, the the yes, cancer as well as the no cancer. : So, and and there is no way to that. So that's the data you have. : Then there are ways to balance in the : but you are changing the data. So the model is a data driven. If you change the data, the the model will not be. : I mean that it will not work well. So : is using any a reality that is not real. : So, anyway. : you have the error, madrics, or confusion. That is what we normally use to evaluate the accuracy. Another way is the receiver operating the characteristics curve. That is a measuring the area under the car. The meaning. : the main that you are gonna look is a : meaning can be yes, can be. No. If you go above. your system is predicting better than a flipping the going. : if it's too high, there there is something wrong, because no prediction can be a 100% right unless you are predicting something : using variables that already have a the solution. : So you are predicting the the weather, and you have the millimeters of rain. : Obviously, if you needed, so rain is more than 0. Then there is rain. So obviously those 2 variables are highly correlated, and if you keep the millimeters of rain in the model. : the model will predict the 100% right. : But basically what are you getting? You are getting that if there is a you bring in on the split, it's. : You already knew that. : So you want to eliminate the variable, so that to correlate. It is part of the data. : and when you mind text things are quite different. So we had a another class on that 120. You need to find a metaphor somehow to let : your system your model dealing with text, because text cannot be interpreted as it is by any system. : because languages for humans is not for computers. So you need to have a : again a made up for representing a the the language : mit ctl. And one of the possibilities is to transform a text into numbers. Once you transform a text into numbers, then computers are pretty good in dealing with numbers 150. : But then the way you transform the text into numbers will impact heavily the quality of the results that you have. : So in in this slide. I'm mentioning what to back. That is a a method : to transform words into vectors. : but we will not spend too much time on that. But that's an example. So we generally use : back towards the that 200 or 300, the elements, or a dimension so : meaning they are : points in a in a n dimensional space, meaning, if you imagine a a a to the dimension, then you have a points and the distance between those points, we tell you how close a works can be. : Let me skip that. : So now, how we do data science, we do data, science, using tools. So in particular, if we do machine learning. We cannot do : machine learning manually, so it wouldn't work. That is too much complexity, too many data. and there is no way that you can do it manually. : So in the past the people use the more : erez agmoni of statistical tools, like Sas informatic spss. So those are our tools from different Vandals then had the the majority of the market one : now more and more. We have people using languages like Python, and our : Arra was very popular up to 7, 8, 10 years ago, but definitely now tied on is more popular than that. : Obviously, if you want to analyze the data, you have no idea how to code. You are : a business administrator all the due respect, but they may not know how to code the majority of that : or the traditional ones : meaning. You use tools. So you use a tools like S. All the others that they was mentioned. But when you go with people that are more : technical in any sense. The number of people using those tools is going definitely down and definitely not close up to 0, but not too far from that. : Then, when you develop a : your system you can do a 1 million different ways. : Many years ago we didn't have a a python. We didn't have a those tools that are quite high level with a lot of libraries, and we started from scratch. So we built our our models, creating i'll go. It's literally from scratch. : So each time you need it, either you had the code in your text study or somewhere, or there is no place where you can go and get it. : So that's the level 0. I don't know, as a several 1,000 libraries all those libraries really helping a lot in developing systems is probably the most popular in data oriented libraries. But then there are libraries that are more on the machine learning side. I would say that the vast majority of the machine learning running today are using either a tensorflow or pie torch : wine. Tensorflow was developed by Google. Python was developed by Menda Slash, Facebook. : They have advantages and disadvantages. I was using a tens of flow, but unfortunately, is not running on the new Max : with the the apple microprocess of the apple civicon. and : at this point I : I mean kind of converting my scripts from using a tensorflow to using a pi touch. : Nevertheless, there are other libraries very popular, even if they may not be intrinsically : machine learning and us. So you may want to have a data structures and is what is doing : quite efficiently. : I was a hesitating while I was saying, because now there are other libraries that are able to be more efficient with the larger data set, keeping the data set in memory. So they have a embedded, some, a compressing compression algorithm that we make in the in memory, working with larger data set. 1: : But pandas is what it 1: : use the most of the time 1: : noon pie. Those are the other libraries that are very popular, and those are embedded in in the, as he learn. That is 1: definitely one of the most popular for common tasks. 1: : I did the this search this morning just to compare a tensorflow with. 1: : So that's basically what I was saying 1: : for a long time, so this is from 2,018. For for long time tens of flow was the most popular. 1: Now Pi Torch is becoming more popular. 1: : Yeah, I know the big fun of a Facebook, but I have to say that it's more more than it's more easy to use. It's more flexible. 1: : I mean. The first release of Python was really complicated to use. Then they they moved the to an integration with another component that it's called Kras, and then the combination of the 2 made the the tens of flow 1: : easier. But still 1: : I mean, obviously, if you come for so you are. I mean breaking the ground. 1: : Python came sag on the and they leverage the on one 1: : a 1: : tens of full was doing, and they built on top of it. They are both open source meaning. They are not getting direct value out of that. And we thank somehow. 1: : Google and met. 1: : Okay, so that's basically what I had to say in terms of 1: : the theor. Let me go now into some practical applications. and let me go into 1: : some examples. So the first one is an example of how to generate a a decision 3. 1: : That's a super basic example. 1: : So you have a. So the goal is to 1: : predict the if the subject is a a male or a female that based on a height. 1: : a length and voice beach. 1: : And you have your training data. When, where you have a 1: : some heights. 1: : some hey, Lansa? 1: : And you have a 1: : men of woman. So basically. 1: : that's the X. So you 1: : and then one, the decision 3 is being generated, you will do the prediction. So if I run it. 1: : so, if you want to predict 1: : if someone with the height 133 1: : headline to 37. So presuming is a a 1: : and then that you know. 1: : and voice pitch, 0 is a woman. 1: : So again you have those values Hi! 1: : Voice speech. and then the I mean that 1: : other variable. So this is this is this is man, and so on. 1: : That's 1: basic application with no visualization, No, nothing. 1: : So let's do the same 1: : with the 1: : visual representation. 1: : so let me run it. 1: : So in this case is a generating. 1: : What is the name of 1: : is generating a 3 dot png that should be somewhere. 1: : There we go. Let me open it. 1: : So does the decision 3. So you have a 1: voice speech. 1: : So that's the root. You have fivex, and also. 1: : and then, if Hera is less of I well, 28, 1: : then you have a ginny indexes measuring the 1: the Entropy 1: : Gin index was created to measure the wealth. distribution. 1: : and how 1: : the world is distributed, meaning pretty much the same concept as entropy as a as the impurity that we mentioned before. So you have a genie index 0 when you have a all the samples that are on one type. 1: : the and so on. So that's the visualization for the same. 1: : Let's do something similar with the K-means. 1: : So I imported the the values i'm using. 1: : Yep, sure. 1: : Okay, let's go here. 1: : So in this example, the input didn't change much. You have the same X and Y, 1: : the names of the features. 1: : the prediction. So the main difference is the generation of the of the graph i'm using this library that is called the 1: : then it's called pi dot plus 1: : that I mean for that particular. It is not the only choice. You have, I think, in another case I'm using something different. 1: : I'm using. 1: : Yeah, i'm using the plane. I'm not plotting, but in another example. 1: : There 1: : I mean that I 1: : creating a the data. 1: : Keep in mind that that 1: : this library, in order to run a needs to have a graph we's installed. 1: : So you may want to use probably the other example. 1: : Well. 1: : you have a 1: : the classification. Not that you created the Let me go back here. so you have a 1: : from skill or not, you are importing 3. So a sub-type of 3 is a decision. 3 classifier. So in the client you have the classifier. 1: : and then you use X and and Y as the input for the classifier. So at the end, in the classifier 1: you will have a 1: : You are model to classify. It is the model. So in this case it's called classified. 1: : And then 1: : this is the prediction. So the values that you want to classify 1: : and you pass those values 1: : to the prediction component of your classified, of your model. 1: : and you will get the result based on 1: : that. Why, that is the class that you are considering. 1: : and then you have the prediction. So. 1: : and it's basically a way to represent it. So 1: : I mean, I wouldn't spend too much time on that it it's for more of a visualization. 1: : Let me go here and let me go through the same example, or a senior one with a different method. 1: : So in this case i'm doing both okay means and 1: : and decision 3. So and i'm using a data set that I downloaded the from a a library. That is a a 1: : so in skill or not that our data sets that you can use for training purposes. 1: : So in particular, this is so is one of the most. We use the data set in the history of data science, probably, and it's typically used for classification purposes for a clustering purposes, but can be used for classification as well. 1: : and it's about predicting. The type of irr is based on the length and the with the of the 1: : and 1: : that's for the K-means 1: : defining the size, defining the color map. This is a scatter plot. 1: : and you will have a discounted plot. I will run it in a moment. 1: : Decision. 3, you have a I copied the all the the parameters so that you can pass. and you can go through that. The I mean is from the official library. 1: : So in this case 1: : there is no need that we import I this. I did it just to have a 2 independent paths 1: : training 1: : the 3. 1: : The confusion matrix 1: : graph is again, is a library. That will let us do the visualization loading the the data set they already have 1: : loading the classifier. applying the data. 1: : I mean that you have a target and data from the source. 1: : Then i'm working now on creating the graph. 1: : Those are the parameters that that you are passing you. 1: : and then you do that. 1: : The evaluation you split, the training and testing. 1: : and you calculate the confusion matrix, and then you visualize it 1: : 500. 1: : So does the visualization on the decision 3 that is obviously 1: : similar Erez. 1: : and that's the 1: : the confusion magic. So. 1: : and that's the distribution of the values for the class setting here. 1: : So in the classroom I use the 3 plus that, sir. and that that's how they distribute it. 1: : anyway. So 1: : that's basically it. Let me 1: : stop sharing for a moment and let me 1: : make sure that you have a 1: : all the material available. 1: : Okay. 1: : you should have everything. 1: : Let me go now with the in-class exercise. So again that 1: : Don't leave the class, please, because after the the exercise, so I would talk about the the final, and you may be, you may want to say in the class to get what I'm going to say on the final. 1: : All right, so let me share the screen again. 1: : And let me go here. That's the in class so exercise. 1: : So using the file, a diabetes data dot csv create a model using a decision-three algorithm to predict the class that is representing the Poly DVD to the diabetes. 1: : You can reuse the machine learning sample dot pie that is posted on canvas to the data set is derived 1: : from this data set here with the following replacement. 1: : A male one, female, 2, 1: : yes, one 1: : no 0, positive, one negative 0. 1: : So you will go into the data set. You will do a little bit of preparation, and then you will apply the algorithms. 1: : So let me create the breakout rooms. 1: : So we have 7 breakout rooms. I opening the room, so I will pose the the recording. I will give you 1: : a good 20 min, so to work on that, because can be a little bit complex, and they really want you to spend time on it. 1: : Okay. So the rooms, I think, created. 1: : I'm opening them. 1: : See you in about 2025 min. I will. I mean, I I will send you a reminder 1: my broadcast. And the reminders 1: : resume in the recording. 1: : Okay, anyone want to say something about the assignment. 1: Vilan Kvyat: I guess I can shine in if you okay, Absolutely sure. So I 1: Vilan Kvyat: ran out of time a little bit, but I think I got something to run with the results. But it was all I was only able to get, maybe 1: Vilan Kvyat: 5 of the lines going before I kind of started to figure it out. 1: : Go ahead. 1: Vilan Kvyat: Oh, you want me to share. 1: : Yeah. 1: Vilan Kvyat: all right. Why not? 1: Vilan Kvyat: Okay, Here it is. So 1: Vilan Kvyat: basically these are my results here. 1: Vilan Kvyat: I think I was kind of getting on to something here, but I did. I decided to do it differently than the file that you sent. So I imported the same libraries. 1: Vilan Kvyat: But this, but I thought it would be easier for me personally just to define what the Csv. Was, and use it that way rather than the data frame, because I think I was starting to struggle with that, and it wasn't really working. 1: Vilan Kvyat: So, you know. And I just, you know, define data, data, set length, shape, etc., you know, created the target variables. I think maybe this is probably the reason why it was a one to 5. I had to kind of check the the number of rows I had. 1: Vilan Kvyat: but I kind of again ran out of time. but moving on. So 1: Vilan Kvyat: So I split the data set to train and test. Set up the Genie Index here, as you did in Yoda file did the entropy. I think there was an issue. I'm not sure if this is an issue here with the x-test I noticed that before, but and get a chance to test it out. 1: Vilan Kvyat: So once the I set up the entropy, I created a function to make. The predicted values 1: Vilan Kvyat: created another another function to well few more functions to calculate the accuracy with the confusion, matrix, accuracy, score classification report. 1: Vilan Kvyat: Then this is the code, the driver code, and the you know, with X Y. The train and a test. 1: Vilan Kvyat: And 1: Vilan Kvyat: here is just prediction using genie and just the results using the entropy. And I called the main function just 1: Vilan Kvyat: main. Just gave it a name, and this is 1: Vilan Kvyat: what I came out with 1: Vilan Kvyat: so far. But I kind of towards the end started to get something going there. 1: : Okay. So there are a good elements and not totally clear why you're using both the end to be in the Genie Index. So, generally speaking, we use one or the other, because both are kind of measuring the confusion or the impurity that is in the data set. 1: : Okay, it's it's good. 1: : Okay, so let me go through. 1: : Let me share the screen and let me go 1: : through another option. That is this one. 1: : So again. 1: : if you use a skill or not. The default for measuring an impurity is the genie. Not necessarily. You need to use that. You can change it, but that's what I 1: use. 1: : So you have importing all the libraries that you need 1: : setting the names of the 1: : reading, the the 1: : the the so of 1: : data into a Pandas structure. 1: : naming the features, the variables 1: : defining the target variable. So you have a 1: : class. 1: : Then it Yep. 1: : Yeah, the confusion. Politics is good. You want to do that. 1: : So you have a exa 1: : is a the values. Why is the names? 1: : So it's the categories. 1: : So you want to split the 70 30. That's what you normally do. 1: : and then 1: : you create the decision 3. You can define what is the depth of the depth of decision. 3 is a number of layers that that you have. When you create a decision 3, you can define what is the level 1: : best level of, or the lowest level of impud that you want to reach and the depth of the 1: decision 3, 1: : whatever will be reached the first, the algorithm, we will stop. 1: : But in this case I set the the maximum depth to 5 1: : training at the decision. 3 with the portion. 1: Again. 1: : why is the class and x of the features or the independent variables? 1: : Generally speaking, another way to call them. You have a dependent variable, that is the target, and in the event and variables that are all the other. 1: : So then, that 1: : predict the response 1: : export in the 3 for a visualization. visualizing. aval, waiting. 1: using confusion, metrics graph running it. 1: : So 1: : it's 5 level, 1 2 3 4 5, because that's what I said. 1: : as you can see, not everybody. The Genie Index reach the 0 1: : meaning of If I 1: : change the the theft increasing, probably I I may have something better than this. 1: : but that's what it is. So that's the 1: : the decision. 3 1: : does the error? Mavericks in a visual terms, so that is not telling much. 1: : so 1: it's definitely better when you have numbers. So you have a 1: : I mean in this case you have a the true positive and true negative. 1: : So 54 against one and 97 against, for so the model is relatively good. 1: : anyway. So just 1: let me stop sharing for a second on that, and 1: : and let me. 1: : Okay, i'm publishing 1: : the 1: : script that you just saw. and let me go now into the final so couple of things on the final. So first of all. 1: : I want to review with you 1: : the template that you will eventually, that you may use 1: : or your final close this let me share the screen again. 1: : So 1: : we already discuss that. But I want to be sure that that is clear, and you are going to use it. So you do not need to 1: : create a Powerpoint, but you may want to use a a table of content, or your document that will be kind of similar to this one. So you want to have a the goals and conditions. You want to have a 1: : Erez agmoni, a methodology or a method for the analysis. So you want to cover those 4 steps. You can call them whatever you like, 101. 1: : But the very end that you need to have one step by defining what is your goal? Once that by defining the data that you are using to get the goal, then you want to do the preparation, meaning all the faces that we discussed before. 1: : So the 3 phases you want to clean the data you want to merge the data if required, the N. And you want to. 1: : and 1: the the Google is probably better 1: : with the visualization. 1: : So let me 1: cool. 1: : What is it? 1: : Okay, I cannot find it. So I Here we go sorry about that. So you want to do the cleaning, the integration and the transformation. So that's 1: : what you are going to do in the the data. 1: : Then, once the data is prepared, then you will do your representation that it can be. Tables 1: : can be visualization. So it can be. Whatever is the method that you are going to use eventually. If you are doing a 1: : Nlp. It will be all up the different things that we did with text and 1: : including. 1: : I don't know diagrams, sentiment, work, cloud all those things. 1: : If it's numbers, you can do a decision, 3, a class setting, and then 1: : write an explanation on the the results. So whatever is the format that you are going to use, you will cover those for faces, and you want to have something like a table of content, resembling that somehow, what you have in your screen now. 1: : So for each one of the faces you will describe, but what the face means, and what, in your particular case is the business understanding. So why you are doing the the type of analysis that you are doing same thing with the data. What is the data you have? You want to do the exploratory data analysis? We went through a couple of options for that. We use the one script. 1: : the for the correlation analysis, and one for analyzing the old data, keeping in mind that when you do correlation on this is a correlation, analysis is going to work only on numerical data. 1: : So and then you do data preparation. You do the representation, and then you will. 1: : E. C. 1: : Now, in terms of what is going to be. 1: : There are a 4 tracks. So again, in the past we had the opportunity to give a students in this course 100% freedom to do whatever 1: analysis they want to be. After being approved by us. 1: : This became a complicated to get the data set. Some of the students may change the data that might not send the data set the meaning that we need to change students to get the actual data to run the code. 1: : And then Kaggle happened, and at that point we couldn't really be sure that you didn't use a problem that was in cargo, and eventually the solution that was in Cardiff. 1: : So at the very end the old thing was not manageable, and and I decided to give students a specific tasks they can choose from. 1: : So there are 4 tasks you can pick anyone you want out of those 4. So one is a and is individual. Analyze people migration data, and I mean, migration is always an issue 1: : when you have conflicts. So we have a the the war in Ukraine, and you have a quite a lot of migration from Ukraine to other countries. You have the migration from people from the South 1: : in Central America to United States You have the migration from North and Africa to Europe. So all of those are a continues continue, I mean, in continuous streams of people going from one place to another place. 1: : Fortunately there is also something that that is a a more driven by 1: : choice, and not by necessity. So I migrated from Italy to United States, and not because I didn't have a fortunately anything I mean a tweet, or I didn't have a job. But just because they wanted 1: : think about the the situation, the economical conditions. So people had in China 30 years ago or 40 years ago. Compare that to what is the current economic condition in China? Some people is is going back to China. 1: : So that's a migration back somehow. So. But whatever is the reason, there is a continuous migration since the very beginning of humankind. 1: : So the question that you may have. What are the major flows of migration where people from a more developed regions are migrating to where people from less developed regions are migrating to what are the dynamics by income and geographic region? 1: : And then you can agree down on on what region and and get more insights. And you have a data set for that. 1: : The second is analyzing a research projects. So the naval postcard with school is a graduate university, and they have a 1: : material that is available in terms of research, and you can drill down. What are the key areas? Slash topics over time? What that? The network so researchers that are more active. What are the institutions that are more active? 1: : What is the collaboration between institutions? 1: : So you have a I mean that that can be more complex because you. You need to download the the data meaning you need to write a code that will be a crawler getting the data from the website. 1: : You can get any subset, because there are a lot of. But you just download the subset the way you want. The crawler manually is up to you, and then you will create a document, and you will set the documents, and you will analyze it. 1: : Coronavirus. So it's analyzing Coronavirus. So there are 4 files. One is a list of cases of, and and that's by county and data. 1: : You have a the 1: the annual estimates of housing units per county. 1: : You have the Gdp for counties. You have a 1: : the the I mean all the data from. and all They estimate the combination. And is it least of county level from census? So 1: : so the study it's focus on allies in the files and getting inside. So that's basically what you have 1: : That's a new one for this year, and is analyzing a people. Perception of a AI AI is becoming more and more the common topical compensation, both in the scientific and non scientific complexes. 1: : In particular, after a chat, Gpt was announced in November. Everybody is talking about it. So the perception about AI changed the all over time from considering 1: : the possible future of this topic Society ruled by an AI being a sort of a big brother, big sister to the 1: you'd faith in the AI that will become a sort of a fix it all 1: : erez agmoni. So what people is thinking today? What are the trends in their opinion? You may want to extrapolate the trend and see what could be possible scenarios in the future 150. 1: : This task this project would be Mlp: so you want to collect data, and that's an additional complication that you have a for this task. You want to use the sources that you think are more appropriate. 1: : It could be news, reports, plugs, social media papers, a pro corona. You probably remember that  as a one on the topics, on what people is thinking about. AI. 1: : Then you can get. I mean, those are will be all text based on top of that you may want to. W. You may have access to number related number based data set that can provide additional viewpoints 1: : is a time analysis. It's not as nap short of what is happening today. So again, is a time analysis. You need to analyze things in time. Not just today. 1: : Again, all the requirements for all the the possible projects for the final. We'll stay the same in this case. So you want to have a data set that is big enough 1: : to create that right complexity. You want to write a script that will have more or less the same size, or at least the same size as as the last scripts that you wrote for the the assignments. 1: : You want to have a a report that is not shorter than let's say, 18 page. So something like that. 1: : In this case you have an additional level of complexity. That is the collection of the data text that you need to do 1: : this telephone analysis. Like all the other, the source data needs to be submitted along with the report and the script. 1: : Just one thing that I want to share with you. That's one of the best. 1: : and i'm not going to publish it, because it's about one of the tasks. So, but I just want to share with you the and then, Joshua, I created the last year. 1: : and he was on a global migration dynamics. It's it's a big rep of that. It's 30 page. So just going very fast. So you have a table of content. 1: : You have a table of figures and tables. 1: : abstract 1: : introduction with the sort of taxonomy, with the terms and definitions. 1: : factors, driving migration. 1: : research questions. 1: : data, description. preparation. 1: : representation. 1: : So that's a a good view representing the migrant migratory close. 1: : for migration flows 1: : a table representing a source and destination. meaning from, let's say, central and and Southern Asia. 1: : this the source destination. Many people going to this region. 1: : other representations. 1: : dynamics 1: : that's using the Sun key diagram that is really giving you an idea. All the who is going where 1: : this one was a for a 1: : the 1: : I mean for the development status. This is for the less developed, the to more developed countries. 1: : This is based on the income 1: : friends 1: : conclusions. So you want to have something like that. So the the fact that 1: : that people in less developed the lower income region are migrating to more developed countries. That is something that we already knew. 1: : But something was not so obvious. People in more developed regions do not always migrate to more developed region. 1: : they may actually migrate back. So that's what I was mentioning before. So I mean that that's an example. I'm not saying that you need to do something exactly like that. But keep in mind that that that's 1: : a goal somehow. It's it's a a target somehow. 1: : Very briefly. 1: : i'm publishing. Let me share again, and that's the last thing I want to share with you today. 1: : So i'm going to publish a all. All of this you are going to have a 1: : Erez agmoni quite a lot of information. You are going to have a the description that I was presenting to you the template. There is this document, 150. 1: : That is a how to write a paper. If you want to write, not in terms of in the powerpoint of playing a Pdf. You can write an academic paper, so the form, the structure will be abstract introduction 1: : background, where 1: : background that you will have a the lead ratio review results 1: : a critic discussion, a conclusion. 1: : references. So that's a an example of how to write your paper. 1: : Then you have a 1: : some examples that you can use 1: : the data files. 1: : and that's basically it. 1: : Okay, so it's almost 90'clock. So sorry again for keeping you so late. That's basically it feel free to send 1: : mit Ctl. And me, and see you questions you may have. I'm going to publish what you so before in a moment 250, 1: : and so next week will be soft of presenting some applications. So, and then I will introduce the last assignment to. 1: : and that would be pretty much the final class before your presentation of finance. 1: : Okay. So have a good night and yeah, sure. Good. 1: Tanmayi Soma Shekar: Hi, Professor. So 1: Tanmayi Soma Shekar: this final the report. It is due on the 20 first. 1: : Yeah, I will update the due date. 1: Let me check for a second. 1: : Yeah. I will probably stretch it to the 20 third, meaning the end of the weekend. 1: : because then at 26 that will be the presentation, and we really need to have at least a couple of days to decide who is going to present what? 1: Tanmayi Soma Shekar: Okay? No. I was just wondering, because doing the assignments by themselves takes me the entire week to do, and this seems like an awfully lot of work to finish in like a week's time, especially since I have other final. You know all the subjects finals to do as well. 1: Tanmayi Soma Shekar: and I was. I was wondering why we're finishing off this zone because our semester goes all the way up to May 16. 1: Tanmayi Soma Shekar: So I was wondering why we can't have a little more time to do it. 1: : Well, I mean we. We published the the schedule since the very beginning, so the schedule as a 13 weeks. 1: : considering. Yes, we do have some more time. We can skip one week giving you a a an additional week. 1: : so that's something that we can definitely do. I mean, we have a a room for doing that. We can do that. 1: Tanmayi Soma Shekar: Okay. 1: Tanmayi Soma Shekar: Yeah. So 1: Tanmayi Soma Shekar: I i'm sorry i'm confused. So does that mean? I we will try to finish the final by 24, or say 23. But you're saying that you could extend it if you okay, so let me check. What is the 1: : the the final period of the that Stevens is giving to us? I don't want to take any 1: : options out of? I mean, if we have more time I will give you more more time. The only thing that we need to consider is that we need to 1: : submit to post the the final grades within 72 h from the final meaning. We need to go back, and we need to be sure that we will be on time. 1: : So let let me review what is the final period of the that Stevens is giving to us, and if it's later than the end of the Webinar, as it's probably is, I will extend to 1: : whatever is going to be the latest moment possible. So 1: : check canvas, and I, I will change the due date for the final, not based on that 1: Tanmayi Soma Shekar: right? Okay, that's why I asked. Because I did check. 1: Tanmayi Soma Shekar: All right.
SYSTEMS
ENGINEERING

RESEARCH CENTER

 

Extracting decision-making metrics from text

 

Using the “Room Theory”

by

Dr. 
clipizzi@stevens.edu

This material is based upon work supported, in whole or in part, by the U.S. Department of Defense through the Systems Engineering Research
Center (SERC) under Contract H98230-08-D-0171. The SERC is a federally funded University Affiliated Research Center (UARC) managed by
Stevens Institute of Technology consisting of a collaborative network of over 20 universities. More information is available at www.SERCuarc.org

March 2020

SYSTEMS Natural Language Processing today

ENGINEERING

RESEARCH CENTER

¢ Communications today are becoming shorter, less structured, more “chopped”, even
more context dependent

¢ Some of the media have intrinsic limitations in terms of length but even those with no
such limitations tend to be used for short, fragmented, multi threads or even single
message communications. This implies less syntactic structure, less rhetorical elements

¢ Traditional methods to extract knowledge from text are based on the existence of a
predefined structure, that can be in the syntax, in writing patterns, in preset
taxonomies/ontologies

¢ The lack of structure makes the traditional media either less or not effective anymore

March 2020 2

were Limits of Supervised Learning

ENGINEERING

RESEARCH CENTER

e Language is evolving continuously. Rate of change is increasing

¢ Different group of population have different jargon. Rate of change of those jargons is
even faster that overall language

¢ Language has 2 unavoidable bias: from the writer/author and from the reader/recipient.
Supervised learning add a third bias, that is in the interpretation algorithms. This bias
may be out of our control and can determine the final interpretation

¢ In order to control the algorithm related bias, we need to be able to determine and
model it ex-ante in a dynamic way, to accommodate the language evolution and the
jargons presence

March 2020 3

SYSTEMS Text -> Metrics

ENGINEERING

RESEARCH CENTER

e To reduce the risk of wrong/subjective interpretations when taking
decision based on text, we need to extract metrics out of it

e How to get numbers from text? Statistical methods do not work, lacking
semantic evaluation and requiring large corpora

e If we use semantics, how can we deal with subjectivity/contextuality of
the interpretation in text that can or cannot be in given semantic
structures (such as ontologies)? Plain use of a generalized reference
corpus does not provide any subjectivity

e Valuing the relativity of the interpretation is essential. For example, if we
classify emotions in a text, “ecstasy” may have different meanings for a
narcotics officer, a Vatican scholar or a psychologist

March 2020

rE Text -> Metrics: the “room theory”

ENGINEERING

RESEARCH CENTER

e The ”room theory” is addressing the relativity of the point of view providing a
computational representation of the context we want to use to evaluate the text. The non
computational theory was first released as “schema theory” by Sir Frederic Bartlett
(1886-1969) and revised for Al applications as “framework theory” by Marvin Minsky
(mid ‘70)

e When we enter a physical room, we instantly know if it’s a bedroom, a bathroom, a living
room

e Rooms/schemata/frameworks ...

e Are mental framework that an individual possesses
e A mental framework is what humans use to organize remembered information

e Represent an individuals view of reality and are representative of prior knowledge
and experiences

e We create computational ”rooms” by processing large corpora from the specific
domain/community generating embeddings tables. We consider a table as a knowledge
base for the context/point of view. In mathematical terms, rooms are Euclidian spaces

e The “room” method makes the whole approach easy to be moved to a different domain

March 2020 5

| From words co-occurrence to
ENGINEERING embeddings

RESEARCH CENTER

e Word co-occurrence measure how often a word occurs with another, within a
given number of words of separation

e Using co-occurrence we can create a word-word co-occurrence matrix, where
rows and columns are the unique words in the source text

e Each word is represented this way by a vector, that is sparse and with most of
the values being zero

Leveraging on word co-occurrence, Mikolov, Chen, Corrado, & Dean created in
2013 word2vec, that is a group of models that are used to produce word
embeddings

e We are exploring different options — besides Word2Vec - to create embeddings

2Vi
| = NN = Embeddings

March 2020 6

  

SYSTEMS Word2Vec to generate embeddings

RESEARCH CENTER

@ Published by Google in 2013
Python implementation in 2014 (gensim library)
@ Generates distributed vector representations of words (“word
to vec”) using a neural net
@ In those distributed vector representations of words
o each word is encoded as a vector of floats
© WOCg ren (0.2, -O.3, 7, Oy ose » od)
©  V@Cyoman = (0.1, -0.2, .6, 0.1, ... , .2)
o length of the vectors = dimension of the word representation

o key concept of word2vec: words with similar vectors have a
similar meaning (context)

March 2020

Why this approach is relevant & how
ENGINEERING is structured

RESEARCH CENTER

Zellig Harris (1954):
—“oculist and eye-doctor ... occur in almost the same environments”

—“If A and B have almost identical environments, we say that they are
synonyms.”

Firth (1957):

—“You shall know a word by the company it keeps!”

Intuition for algorithm:

—Two words are similar if they have similar word contexts

e The meaning of a word is a vector of numbers

—Vector models are also called “embeddings”

March 2020

SYSTEMS
ENGINEERING

RESEARCH CENTER

2
compared with

“Room”: Domain-specific
Knowledge base

using

Documents
to analyze

 

“Benchmarks”:
Keywords defining
what we are looking for
Proximity of the
document(s) to each
keyword

March 2020

How the “room theory” works

“Room theory” enable the use of
context-subjectivity in the analysis of
the incoming documents. To work, it
needs:

1. Acriteria for the analysis (the
”benchmark”). This is represented
by a list of words/chunks with
possible attributes/weights

2. A point of view for the
comparison (the “room”). This is
represented by an embeddings
table extracted from the specific
domain




a Adding elements to the “room theory”

RESEARCH CENTER

 Optimizing the process

 Single words may not represent a concepts (“school of business”, “weapon of
mass destruction”) => “chunking”

 Creating a standard “room” generation process/script, running on GPUs
 Creating a standard 3-way weighted comparison engine

 Using the theory for prediction
 Future cannot really be predicted, but evolutions can be estimated

 Estimate future vectors from past positions in a multidimensional space, then
from vectors estimate the closest words, reverse engineering the process

 Using the theory to estimate people reactions/moods
 Sentiment analysis is not working, providing a limited view of the reactions

 Using emotion classification from psychology literature (Plutchik) as
benchmark document on proper “rooms”

March 2020 11

 

Room theory at work:
Processing Information

The larger bucket can be indefinitely
filled up or regularly purged

“Larger bucket” for
generating rooms oes to” weeks


” Room theory at work:
ENGINEERING Using Information


Numeric value of

interest (metric)

March 2020

2%,
°e@ @e¢
®e°

SYSTEMS
ENGINEERING

RESEARCH CENTER

STEVENS

INSTITUTE of TECHNOLOGY
THE INNOVATION UNIVERSITY®

 

Thank you!

Dr. 
clipizzi@stevens.edu

Okay. 0:01 : All right. So let's slowly start 0:03 : slowly to lose some time to the other students to join. 0:09 : So 0:15 : it's much 20, s, it's, 6, 32, and we will do. I mean the the the for use of the class today is going to be visualization. But we will also talk about 0:17 : the midterm, and we will do an in class except size, and we will introduce the next assignment. 0:36 All right. So let me start the sharing the screen 0:49 : and let me start as usual from here up. 0:55 : So we are right here. So once they march 20 s, we we talk about the 1:01 : visualization. We'll talk a little bit about what to do with data, but we'll be kind of the everybody what the visualization in this case, and then I will introduce a exercise Number 6. 1:10 : So 1:26 : let me start with the at the mid term. 1:31 : So we all know what the midterm was about. 1:36 So let me start with the 1:40 : the code, the to be checked. So the initial code that was doing something like this. So let me run it. 1:45 : So that's what? 1:59 : Yeah, I will talk about the final project that we do have a a little bit of time for that. But I I will start introducing the the what are what are going to be the rules of the game. 2:07 : So 2:24 : the output for the way it was initially 2:27 : was something like that. That's not what was supposed to do. 2:36 : So the way we changed it, it's basically we converted the the original string into a list calculated the length. 2:41 : A, and then for each one of the characters that at that point will be an element of the list. We multiplied it by a 4, 2:54 : and then we joined the the 3:07 : string, the at least creating a a string. and then we printed it. 3:13 : So if I run 3:21 : this 3:24 : so the same one 3:32 : at this point is doing the right way. 3:35 : So that was the first one, the second one. I cannot run it right away, because if I run a section and there is a file, it's not recognized in the file. 3:41 : But I mean there were several issues here. What I did was basically initialize an empty list. 3:51 : Then, looping into the words in the file. 4:02 : eliminating the blanks and the special characters that are right and and checking if it's not in the list, if it's not in the list, i'm appending the the result. I mean, this is kind of a red or not, but that's fine. 4:08 : Then i'm getting and sorting by length and taking the last in the list, and then i'm printing them. I could print right away this way. It's kind of a nicer because you have one after there. 4:27 : The third one is very straightforward. So they were again. Several issues, including some else, misplaced the and things like that 4:48 : so imported the 5:01 library string as it was not originally. But that's right. 5:06 : and then enter the string 5:11 : the check. 5:15 : then the list of our wells, and then. 5:16 : and then checking. If the world in lowercase is in the list of our well, if not 5:23 : a 5:31 : I mean that if yes, so, then 5:33 : I mean I I i'm asking if is in the list, if yes, meaning is about. Well, it's not a consonant otherwise is a consonant. 5:39 : So if I run it. 5:50 : let's say, our 5:59 : is a console. So there. There was no request to do. And I mean, in this case it was yeah, I mean, when you run this way you don't have a much of a loop. 6:02 : but that's fine. 6:16 : So that's basically for the first part part of the assignment. The second part of the assignment was analyzing. 6:17 : analyzing 6:34 : a file with the words and counting and doing some other statistical analysis on the words in the file 6:38 : erez agmoni. So I open the the 2 files, the the file, and the the file with the so forth. So I initialize the list of of unique words, 101, 6:54 : a string that will contain eventually the longest world and a a a counter for the total length. and then I also initialize with an empty list the the collection awards, and it called the bag awards, and the list of so forth. 7:11 : starting the looph, populating the list of software similar thing for the the list of words. And in the process I eliminated the the words that 7:30 : a are in the software. 7:48 : Then over here. I'm, i'm, i'm, i'm, i'm i'm, i'm i'm, i'm, i'm, i'm, i'm, i'm i'm i'm looping into the list of words, and then 7:51 : for each one I calculating the length, If the length of the current word is a 8:10 : the bigger is, is a more than the length of the longest word, and that initially will be not. I will replace it in that the 8:18 : variable that will contain the longest world, the the current world. Then i'm asking if a is a unique word. If not, I will, I mean if it's in the list of a unique words, it now they will append. 8:32 and I will increase the the 8:50 : length ofwards. So then i'm calculating after the loop the number of unique words 8:54 : using the line commander. 9:05 : And then at this point I already have some of the information to be printed. So there are this number of unique words, and this is the list you were not required to to print the list, but I mean I have it, and I printed. 9:08 : Then, calculating the frequency. So frequency is the counter or the beg awards. 9:24 : then initialize to 0 or the occurrence. and then 9:33 : and looping into a frequency that is this counter here 9:40 : and I'm. 9:46 : Adding it is 9:48 : additionally, i'm. Adding the frequency of the current board. Then i'm printing the most frequent 9:52 : and calculating the relevance of the 5 most frequent words : you don't need to do the rounding with the certain number of of this amount so, but it would look nicer. : and then those that we are not required. But I mean just to have some more statistics printing the average what length and the average accordance. And that's basically : So let me run it. : So you have a : Those are again. You are not required to bring the list of unique words, meaning that you could stop. There are 386 unix works. Period. : The 5 most frameworks are the following: this is the relevance average length of required. : So that's Basically, it. Again, there are a few things that we are not required like the rounding I did with the 3 days emails : in one case, 2 days emails in the other. Doesn't really matter. : The last 2 were not required, and that's basically, it. So let me. So for a second on that. : And before we proceed, I want to be sure that there is no question from you. : All right. So : she you posted the most of the grading. So for the midterm. If you don't have your grade yet : chances are it it could be a potential case of a I really don't know why you are doing that. So most likely no one, the 25 of you did it. But in the class : some students cheated. So i'm spending a really a lot of time to : explain how we evaluate the the assignments, how we compare the assignments last class, I presented a Powerpoint describing the entire process. So : so it is pretty clear that that the chances to be quote are very high. : So each time we have achieving a we will apply penalties. : When we apply penalties along the road. You will, may not have enough points to pass the the course : at that point, because we have the rings, all everything that happen during the the the semester. We will not give you a second chance. : meaning, if you lost points from cheating, and you will not have points enough to to pass the course you will fail period. : So : again, I I really don't know why some of the students in the class are doing so. I really really strong, encourage you not to do so. : I'm also analyzing class that so : to see if is an occasional cheating, or, if is a a systematic cheating. : So for the occasional achieving that we can be a little bit more flexible because there could be a special circumstances. But in most of the cases there are classes. So we don't really have a any flexibility in those cases. : So and : oh, yeah, okay, I don't have it right away. But while you would do the in class assignment I will pull a exercise 5, and I will review it with you. : So, Anyway, that's the story on the the : We will discuss with you tomorrow the the courts of action for those of you, with no grading on on the mid term : a few words about the final I mean I I will talk about the final more next week. But just as a an anticipation. : so the final is going to be a project. I would be something that you will develop individually, like all the other assignments that you did so far, and : it will be on a a given problem with a given a data set. The class. It's a relatively big, and I I cannot give any one the opportunity to pick a. They own Us data set and problem. : because when we have a quite a lot of data sets there are a way, much more issues. So you may not use the data set that you said you should have used the the link to the data set may not work, or we need to go back to you. Then you did modification to the data set. : So we don't have the the bandwidth that to deal with the 15 change different data sets and problems. So we will give you 4, 5 options and you will pick one of them : again. It could be a project. So you will have a : yeah, a data set and a problem. And you will do the data exploration on a, on the the data set. And you will come up with a a report that will be something the around. : all included, we are all included, will be with the graphs, with the tables and on the rest. : and you would submit the the report and the python script that you will use to create the visualization of the tables. The metrics then, will be needed for the for the final. : How big the script could be. Well, I mean that that's a fine Allah. Keep in mind the that I mean that : the scripts that we are right now are roughly around the a 100 lines. : So for the final, you want something more or less that length, so not shorter than, let's say 60 70. : The number of lines is is not a a full indication of the complexity, but it's kind of difficult to to have a a complex analysis of all of a data set with the 20 lines of code. : So lines so obviously are not all created igual meaning. You may have a lines that are comments, lines so that are repetition of the other lines. You can have : blank lines so realign. So it should be around 100 : the complexity of the analysis. Again, if you have a and visualizations that are pretty much the same with different data that doesn't mean that there is more complexity. So you are just repeating the same lines of code. : So again 100 roughly or more lines of code : a around the 10 page. : I think I shared with you the a. A template for a presentation. You don't need to do a a powerpoint it? It's either a a dog Pdf or a Powerpoint, or all good. It really depends on you. : When it was a : the consultant I was used to think in Powerpoint terms, meaning to me, was more natural to write a Powerpoint document instead of a word document. : Now working in Academy, I writing more papers, and I : kind of more familiar at this point in that type of for much, but but it's up to you. : keeping in mind the the writing. A Powerpoint has some limitations, meaning that you cannot really explain match in a Powerpoint unless you do a Powerpoint that it is so dense. There shouldn't be a powerpoint that should be a a word document. : So I mean presentations should have a a more visual so than what so? And you may not have enough to. I mean visual. So it just to do as far what point the meaningful power for : you don't need the to do the presentation, so most of the presentations will be done by us so primarily myself. I I will present some on the finance. : but I will ask some of you, and the day before the last class I will let : people know the day, and been selected for presenting in class. : If you are selected for the presentation, you need to do the presentation. : meaning the day on the presentation. So you will do it. Why, if you cannot do for any reason, you are travelling, you are seek. You have a problem of any kind that doesn't mean that you are excused. : So it only means that that we need to work with you on in another moment. : So that's basically it. Anyone has a other questions on the final. Again we will talk again about the final. We do have time. : So let me share the screen again. : So you will start the the final by mid of April. So right here. keeping in mind that that you will have time. But you will not have a a lot of time, and we will not have a lot of time for grading it. So. : by Stevens regulation we need to post the final grades within 3 days after the final : meaning. We don't have a a lot of time. : So be sure that that once we are approaching the final, all the questions issues that you may have on past : assignments has been settled, because once I post the the final grades on the register on our website changing it. The is a process can be done, but it's a process and can be done only for exceptions. : So it is. It's not normal. So : you have a spreadsheet where you can trace all the the grades that you had. So far you can do an extrapolation. You have a the statistical skills for doing it. You can do an extrabolation, and see what would be the final grade. : So if you see that there is something wrong, meaning that you are failing, do something now because you still have time to correct the situation. : So again I know that I will have people the day before the last reaching out to me, saying, i'm failing the courts, so can I do something? : So the answer 99% of the time is No, because I I mean i'm telling you now, and we are a for 5 weeks from the end of the course, so you have time to plan in the proper way. : But exceptions can happen, and we can definitely handle, but not everybody who is raising a an issue that is a legitimate one or an issue that couldn't be added. I can. We could be addressed before right like right now. : all right. So that was a a longer, almost a half an hour before the current topics. So let me go now to the current topics that are related to visualization. : Let me share the screen : and let me go here. : All right. So visualization is a an essential portion of of the data exploration. And when we explore a data, the certain point, we need to present the results. : So presenting the results, meaning, calculating the proper metrics and express present the the metrics in a way that can be helpful for the the ringer. : Obviously : the visualization is something that is very strictly quarterly, and with the capabilities on the hardware : where the the programs are running on : begging time, the seventies, the eighties. So the was not particularly power. Tool. We didn't have a : display. I mean most of the computers. We are a character based meaning apart from very specific applications. So, like : all the applications where based on characters that is not much in terms of visualizations that you can do with the character based interface. : So, going fast forward with the evolution of : our computers with the introduction of : graphic Oriented, the operating systems. And you know, Gpus graphic processing units, things changed a lot. And right now we can really do a lot in in terms of using the visualizations for presenting the results or analyzing the : results. : But one of the thoughts that is a kind of a growing pushed by Van Nuts is the term visual analytics. : So reality is a : the visualizations are the the the tip of the iceberg. If you don't have a enough data enough metrics in particular, you are not going to get much out of the visualization. : So, no matter what vendors can say, the real goal of the visualizations is to represent the right metrics. If you don't have those metrics, there is no visualization that can really help you match. : So there is no intrinsic visual analytics. : but you need to do quite a lot of homework or preparation before going into the visualization. So the visualization is super useful. It needs to be addressed in the proper way. : keeping in mind that that, anyway, is something that is a subject to the quality of the method of of the data, of course, but on top of the data of the metrics use the to. I mean : stress 1 one particular aspect of the analysis that you are doing : erez agmoni. When we do visualizations, we can do visualizations for a medium different reasons for analyzing relationships between whole and part for discovery and relationships, 250 : for doing a a sort of a exploratory and a confirmatory analysis : on multiple screens. We can have a different data types combined somehow in : visualizations just to get a sense, and we have some tools where the different visualization. So it can be. : I mean a connected meaning. You change something on one visualization, and you have the changes affecting the adults. : You can use visualizations for a a temper or you for a analyzing a time series : you can use for supporting your reasoning. So I mean there are a quite a lot of applications in a visualization. So. : and I don't know it's really helping a lot in all of that. We do have a a in : visualization, that is em 622. : I mean. This is just to say that visualization is more than just one class, because it's it's a it's a complex topic with the implications in terms of economics, psychological impact, and things like that : am 622 was originally developed. Using ara. We are slowly but kind of surely moving towards pies. On us Initially, a python was not particularly good in a visualization : with the Mt. Properly, but being a at that time, I probably the only the library available. But now, between : See Bonnet Bouquet plotly. There are definitely better options than was a primarily Gg. Plot there. There is a Gg. Plot for python, but not particularly great. : so no no one is, it's pretty much using a Gg plot on python. So we use booky, and we use a plugin : Seborn. It's : a library on top of my plot, lib, and he is also integrated with the Pandas and Pandas are integrated with with noon pie. : The old combination is making this library quite powerful, so you can do quite a lot of things using a few lines of code with the : bouquet. It's another interesting library created by continuum analytics. : One of the main advantages is the that now is on nothing blockly as well is the possibility to create HTML directly from the library. : That's a big advantage, because with that you can use your visualization in a a a webpage. You can send the HTML to your client. There is a little bit of a of processing that that you can do on that. : So : And then there are a quite a lot of options with the book in, so you can. : Really. : those are some of the examples, so you can link different plots as I was mentioning before. : and you have a a sort of a : tool box for doing few things, and again that in the past that you had to provide the : HTML to have a your graph in HTML code with the bouquet. This is from the random. You don't need to do it. : Plotley is a a combination of open source and commercial software, and it's very powerful, and we use it for creating a a complex dashboards. : Dashboards are integrated the visualization. So where you have more visualizations on the same screen, linked or not together, but the all combination is giving you : much more insights than individual : individual visualization. : So let me go now into the : some code. : So let me close what we had before, and let me go into. : and the visualizations. So that's an example of a very basic visualization with, okay. : So you import the library and the components that you will need in : you name the file that you are going to generate an HTML. Then you define the conditions for the that you are creating : X and Y are the data you define the type of plot you want, and then you show it. : So if I run it up : so that's what you have it. It's a real. You can move it. You can. I mean, I have a a it zoom in. : You can : to set it if you want, and so on so is a an HTML for all instant. And : so and it's this one is a separate file. So a few more examples. So in this case it's a : I mean a little bit more complex visualization of same principle. So you have the dots in each one of the intersection of the yeah intersection X and Y on the curb on the line. In this case. : and : some other examples. : I mean, you are going to have a all those examples in canvas. So this is a little bit more complex. Let me close the other : more room. : So in this case, i'm defining a function up, and in representing the the Lorentz function in particular using bouquet. : and if I run it. : that's what they have. : all right. So this is pretty much it for a bookie again. : all of those would be on on canvas : just a couple of examples. : So in this case. : as usual, i'm importing the the libraries again. See, Bona is using pandas and importing the pandas, and is on top of M. Plotterly by importing Macloply as well, reading a file and then : setting parameters, and show it. : I mean, when you do things that are relatively sample. Simple life is easy. : But when you want to do a visualization of the more complex, then things will become a : ere : I mean explorer, even partner. : Another example. : See? But now again. : So those are : in this case it's generating 2 different visualizations. So you have a : analyzing by : genre is music. and this is platform. : So did it not he's not using his game so sorry about that. : So let's go to Plotley. : That's a a basic example of plotting. : I mean, that's working with properly can be complex because it is more powerful. Again, you can create the dashboard. So when you create dashboards, things are a more structure, more complicated. : meaning that there are more parameters to play with. : So in this case, let me run it, and then we would go back to the code. : So in this case i'm analyzing : different universities by a set of parameters. So those are the universities. : and those are the parameters, the the values. In this case the parameter is the number of students, and you have a : the different, I mean the distribution for the different schools. So in this case you have a You are analyzing citations and teaching for the : I mean different schools. and in this case you have another view with what rank and citation and teaching. : So : that's plly. So those are the 3 : visualizations that we created. : That is one that I want to show you. That is a quite interesting. That is the Sun key diagram, the same key diagram that was created to analyze the hydraulic flows. : You will see why, once I run it. But : so you basically have the starting : and the ending of the flow. So you have this flow that basically was divided in in 3 sub flows. Then there is a merger, and you can eventually have a intermediate stage. : So : for the fluid the analysis is, it's quite obvious, but can be done for a processes. It can be done for a a analyzing. I mean : one of the applications that it pretty common during the elections is to see how the the boats change the from : one political party to another, a political party. So you have a the previous elections. And what is the dynamic of those flow. : so in general is representing flows. : Obviously hydraulic flows is the original purpose. But processes or : evolution in time of events, all good examples. : We use that for analyzing the relation between the : different topics in a conversation. So how the conversation involved in time. So you have the initial topics, and how they change in time. : It's pretty cool approach. So : again, I mean in this case it's it's pretty basic. So you have a the original stage as the final stage. Just so you have a so star. Get the and values. And that's basically what you do. : all right. So : there are a couple of additional things that I want to share with you on a visualization. So one is a related to a a book that is, of knowledge : that is quite the atlas of knowledge. I'm sorry that is a a quite an interesting book. It is not a recent one. It's probably 5 years old. : That was originally, I mean. The original scope of the order was to analyze the : the geographical of research. So how the different topics are moving around the globe. But then the scope of kind of a broken up. : and became a sort of point of reference for a visualization. : This slide it's. : hey? : Kind of useful, because when you have a problem you need to define what is the the visualization, and it is going to work better for you. : So what are the the the the the metrics that you are going to use to decide which will work better for you, and which, without : so Theodora defines the the type of analysis that you are doing, and the level of analysis that that you are doing. And then in the intersection you have a an optimal form of visualization. : So the types, statistical analysis. When we're what with whom the level negro and based on where you are, you can have a a graph of one type or or another type. : So it it. It's kind of a a systematic approach, and then the other goes into the details of each one on the different level. I cannot share the book, I on a canvas. I I bought it on : Amazon in a kinder format it. It. It's a good book with great visualization. So I have a : a Pdf. From one of the presentations that you order gave all over the years, and in that one of the page it : page 16, and this Pdf. Is available on on. So you will have the different visualizations, and you have the same slide that they was using there. So with the the types and the level. : I also share the on campus. This : che it on what is the best visualization for what? So is a flow chart. So you basically have a : the options you navigate the in the chart, and you will pick whatever is the visualization that seems to be more appropriate. You have 2 versions. This version is a a one page with 2 page in it that, and then there is the one that is us more dates. So that is basically this one that's split into. : So : that's pretty much it on the visualization of that. I just want to go back for a moment on : on a pi charm to share with you 2 more : the scripts. the Dara : on the Mmm. Analyzing the data. : So when you analyze the data, you really want to be sure that you do a set of steps analyzing missing values, analyzing your outliers. But the single most important part on the analysis is analyzing the correlation. : So correlation is important, because if you have a one or more variables that are highly correlated, there, there is no real need to keep them in the analysis. : because they are telling the same story. : So if you have one variable saying length of all of an object, and then on another variable. I don't know a millimeter of length of the object. They are clearly the the same thing. I mean. This is very obvious, but and they would be highly correlated. Or if you are measuring a brain or no rain, you are the variable saying, Ring : no rain, and you have another variable saying, the so rain, obviously, if is not, 0, is raining, meaning that there is an eye correlation between the millimeter or rain and rain or rain. : so there is no need for you to keep both of them up that you want to remove 1 one of them. : So correlation analysis is important for cleaning, but it's also important to get insights, because it will give you an idea. What are the variables that are more depending one from the other. : and this will give you a good insights of what's going on. So there are 2, I mean. There are many ways to do the correlation analysis. : We are talking about linear correlation. One is growing, the other is growing. One is decreasing, the other is increasing. Obviously the not all the correlation are. Linear it can be any function, and there are other methods. But for the time being we are focusing on the linear correlation. : So this case, I mean, we are using 2 different methods. So this met on the is a pretty straightforward what is using a seaborn. : and the data set is the Nfl. Census in a Csv file. : and I i'm calculating the correlation again. Keep in mind that the C. Born as a embedded pandas and my plot lab. : So i'm reading the the file into a pandas, a data structure. And then i'm applying the correlation between the data and as one of the functions that is available in the Pandas, and then I : representing it as an heat map. That is a pretty much equivalent to a correlation matrix, and then I show it, and eventually save it as a Png. : So if I run it, it will take a little bit of time, because it's a big data set. : so that's what you have. So I mean, the subject again, is a Nfl. Census, and you have a for example, the salary. : So in this representation you have the the darker values that are positively correlated the the lighter values that are on the negative correlation side. : But then you have also the number. So is each cell is giving you the same information twice. : So you have a the visual with the call order, and the number that is, it's pretty much the the same story. So along the demand that you are going. All everything is one because each variable it's a 100% positively correlated to itself. : So if you go to the salary, this salary is more correlated with. If they are pro bowler or not, the experience, the age is less correlated to the number of teams. : So that's an example. So you are getting some insights just looking at the the : and that's one way to to do it the other way it using : this. Why, data profiling : that it's working on a pandas. So same thing. You read the the same file in a Pandas data structure, and then with one line pretty much you create the report. : So the right part is going to be a nhtml. So if you run it : so it's doing a set of steps : analyzing the the file. You would see the results of : in a second. : So it's finished. That's the output. Let me open in a browser. : So that's what you have. So is a full HTML record with the a menu on the top that will drive you in the different parts of the so you have the overview. : Where do you have the number of variables, so number of observations, missing sales, and all the rest. : So all of those are really valuable information. What are the variables that are categorical. What are numerical keeping in mind that you can do correlation analysis only for numerical variables. So we said that that correlation means one is growing, the other is growing, or vice versa. : if it's not numerical, how can you say if it is growing or not? : So? If you have a a file with only categorical variables. Or if you want to represent categorical variables as a as a : you, if you want to analyze, if categorical variables are a are a correlated that you need to transform at the category of variables into the medical level, and then do the correlation analysis. : So this is giving you a list of variables with all the analysis : for some of them, you will have the distribution. That's obviously only for the : so the way you have the distribution is selling something, because because you can say what is the most common in this case. : So a. And I, : when you have a to your Gaussian distribution. It : is is what is called the normal distribution. When you have something like in this case, where it is not a normal distribution, you see that there are a 2 seeks somehow, and you can say why there are those 2 peaks. Why, there is a peak, a little bit more than £200 and a peak a little bit more than £300. So : chances are it's because there are different play. Yes, and the role. It's kind of a associated to the : then the age is is queued, as you can see, is queue the to the the the 2423, and that makes sense, and that's another indication. : So from a wrap up to like this one you can really get a lot in terms of interpretation. You need to write the narrative. You need to do this explanation, but you have the number so for doing it. : Then, again, you don't have the distribution for the : then you have : correlation that is pretty much the same as the one that's been generated before. The : obviously, again, is only for the numerical variables. The representation is a little bit different, but the meaning that is exactly the same. : So you have a go into the blue being a positively correlated going to the red being negatively correlated. 1: : So in this case there is pretty much no one that is negatively correlated with this is something that is pale red, that is, number of team and high, but I mean to we just 1: : so 1: : you can read the into that, because you can see again. We 1: : mentioned the salary, but there are, I mean position, probably, and wait. 1: : You can see that there is a correlation, as we imagine. 1: : I mean that position, way in height are correlated more way than height 1: : and then missing values. You have all the analysis and assembles. 1: : So again, with the basically 2 lines of code, you have a a with the several page 1: : that you can use, and you can attach to your presentation if needed. 1: : Okay, so let me stop sharing for a moment. It's the 7, 1: : and I would introduce. Now 1: : it's a in class exercise. 1: : If there is no question. Obviously, if you have questions, feel free to ask. 1: : All right, so let me share again and let me go here up so we will talk about the Us. Baby names. So the reason why i'm using this 1: : it's because of the certain point quite a lot of students. We are doing their final project on us. 1: : I really don't know why, but that's what's log of students did the at the point in that they decided to move it to an in class exercise. 1: : taking a a dead option out of all the options. For you know the final, because, as you already did it in the in in class, ex at sites. 1: : So you want to read the this: Us. Baby names file into a Pandas data structure. You want to pre instead of basic information. 1: : You want to delete the call on that unnamed the 0. And a Id you want to determine if there are more female or male. The the the top 5 in tens of number of 1: : bring the number of names and the data set between the standard deviation of the name of core answer. meaning this: I mean the the average just under deviation on the name of, and then print basic descriptive analytics on the data set. 1: : Okay, so let me make sure that that all of those are are published and 1: : give me 1 s. 1: : Okay, you have all the slides, the in class exercise. 1: : And 1: okay. okay. 1: : we'll be talking about this data exploration after 1: : that. 1: : All right. So let me 1: : create the breakout rooms and 1: : and let you work on them. and then so we are creating a 1: : 9 breakout rooms with the 2 3 participants probably better if we do 8 breakout rooms, so we 3, 4 participants per room. 1: : So the rooms are open. 1: : Please join them. I will post the recordings, and we will be talking in the 20 min around the 80'clock to discuss the results. 1: : Resuming the the recording. So rooms would be closed in 5 s. They're almost there. Okay. They are over closed. 1: : all right. So how was it 1: : anyone want to share what you did? 1: : All right, so let me share a 1: : if possible solution. 1: : They share the screen 1: : go here 1: : all right. So 1: : importing the 1: : the library that they need the the library. reading the the file into the 1: it into a data structure. 1: : printing, playing info on the 1: : on the the data structure. I mean it's going 1: : it. It's going to be not particularly nice looking, but this is what both in the requirements. 1: : deleting the the 2 columns that we wanted to delete 1: : printing the first names in the structure. 1: : then i'm counting those by gender. 1: : and then I grouping 1: : the 1: I mean, I i'm creating a new data structure with a group by name 1: : and removing where I don't have either the blank, the 1: : that there is no automatic values. Then printing the 5 observations I mean, I did in Here 1: : we have the you have the first. Whatever is the number in the parentheses. 1: : sorting from the biggest to the smallest. 1: : with names printing the had the 1: : then a number of unique names. So 1: : with the 1: : A. 1: : What is interesting is that when you do 1: : the group by it will keep only the unique names meaning I do not need to. I mean, I do a loop, so it's feeling a variable I mean it's super straightforward, but it's compass to 1: : so, and then names with the highest, so calculating the the I mean just as lying the sound that the 1: : and then a little bit of a descriptive statistics. If I 1: keerthi: that's what I get. 1: : So that's the initial that information. What's names in the structure distribution of my female 1: : 5 names in the all the unique names. 1: : not I mean that sorted 1: : the top 5 names. 1: : total, unique names, names with the highest accordance, standard deviation 1: : and the descriptive statistics. 1: : So it was a just an exit size on Pandas. Pretty much 1: : so on exercise 5. I'm sorry if your except size. 5 is not being graded yet. I send an email to see you. It will be graded between tomorrow and a day after. 1: : So import in the libraries. 1: : Open the file 1: : again. In this case i'm not using a pandas. 1: : I'm 1: : creating this. Get index that will get the data set. 1: : and we will not 1: : generate a an index from 0 to 7, based on the value on 1: : the age. 1: : I have another version with, but that's fine. 1: : Then I initialize a a, a, a a counter with multiple values. Each one will count the the the number over for each one of the different age category. 1: : So initializing a a dictionary initializing the the 1: : number of max, the maximum number of that. So the name of the 1: : doing 1: : a loop into the file again. I could have been done, not using pandas. I didn't use pandas in this case, but that's fine. I'm looping into it 1: : for each. I'm getting the the index. So if the index is 10,000 so going here 1: : so meaning has not been changed, meaning that there is something wrong in the value, so it is not a 0 to 24, or whatever it is, but it is either a blank or a wrong value. So if this is the case. 1: : they next will be 1,000, and then that point I will pass. So we continue the the iteration. So otherwise i'm getting from that particular line. I'm getting the the number of that. 1: : and I will add the the number of that's to the of that particular counter, because in in at that point in the index, I have the position within this 1: : multiple counters counter that I initialized. 1: : and then you will iterate. 1: : and at the very end you will have a the the counting of all the debts for each one of the h category. 1: : then on the condition 1: : identifying the condition, and then i'm checking. If the condition is already in the the dictionary. If is not I initializing a key and value. 1: : and if it is, I am adding the number of that to that particular condition, and then I will iterate 1: : then printing the the results 1: : and counter by age, printing the condition 1: : with name of the specific culmability and the number. 1: This is the plotting. 1: : So i'm I mean X will be the x-axis 1: : and i'm. Calculating this subplot, this is the bar chart, calculating all the components. 1: : i'm adding labels to X and y the title 1: : the Rotation, just to have a a better reading on the chart by chart seeming, I think. 1: : then, that 1: : I mean sorry this is a misspelling here. So i'm using 1: : siding where i'm taking this. and then I showing it 1: : so. If I run it. 1: : you have the counter by age, the condition with more Covid that says respiratory diseases with the that that much that's for the identified age. 1: : and then on the charts you have the bar chart that you have the pie shot. 1: : So that's basically it. This solution has been posted already. You will have it. Let me take another a few minutes 1: : on something that you would use on the 1: : a final presentation of final project. So 1: : briefly. 1: : when you do data exploration, you want to be sure that you use a a methodology to cover all the needs that you have. When you do the exploration. 1: : we we use a a a modified version of a methodology that was originally developed for a data mining. 1: : The the middle of allergy was called the Priest Vm. Crossing the 1: : the process for data mining. So it's it's vm. That I modified the increase. The where the is is data exploration. 1: : So there are 4 faces, one you initially defined before the 4 phases. What are the goals. Then you define the 1: : what is in the 4 faces. So the first phase is business understanding. You define what you are going to do, because if you don't say what you are going to do. There is no way that the exploration I can be successful. 1: : So 1: : you define the reason, the goal, the research question or questions, so that you have then a data understanding. You define. You 1: : explore the data you have, and you evaluate it. If the data you have. It's good enough to address the questions that you have in the business understanding. 1: : Once you are good with the data you have. But it was the the questions you have. Then you set the data preparation 1: : where you do all the exploratory analysis that is pretty much in the I mean the correlation analysis and the the script generating the HTML had that piece of before. 1: : Once you have all the preparation meaning exploratory, but eventually even changing things. So you are doing a correlation analysis. You realize that 1: : there are variable, so that 2 correlated that you eliminate one of them to have a more lean 1: : representation of your a problem. 1: : Once you have the data in the best shape possible. At that point, you do the data representation that can be 1: : visualization, stables or calculating 1: : so. And then in the Powerpoint. You have all the details on the different phases. 1: : so you do not need to do for the final, the Powerpoint like this one, but you may want to have a a table of content that will resemble this one. You want to be sure that you are covering the 4 faces. 1: : So then, you can call whatever you like. But you need to be sure that those 4 points are fully addressed. 1: : So defining the research questions, analyzing in a critical way the data you have 1: : to be sure that the data can actually address the questions you have doing. Then the preparation on the data meaning, analyzing the data and eventually reducing the variable. So combining variables. 1: : eventually combining data set. If this is the case. 1: : once you have the final data set or data sets in the right shape. Then you start working on extracting metrics, doing the visualizations, and do whatever you may need to have a 1: : the proper representation that can be visualization, or whatever else. 1: : When you finish that that you draw your conclusions, you are tidying together all the pieces, and then eventually you will have a attachments, meaning all the tables, so there are not, I mean, so relevant to be in the main document, but they can be useful. You don't want to use the attachments as a sort of a trash bin. 1: : so you will post in the attachments only those elements that are all some sort of of use for the the better understanding of of the insights that you are providing. 1: : So that's basically. It. And you had that. So let me go now into the next assignment. I'm: not sure if I have it here. Yeah. 1: : So the assignment. 1: : Yeah. One of the 1: : I don't know if it's a it's a critic, or it just the consideration that some of the students did the in some of the previous editions. So was why Don't, we have a more exit sizes, and they are more on the 1: : management side. So is engineering management. Why, don't we do something on management? 1: : So one of the things that that I did the in the last a couple of years, and now for 1: : it's a and it's going to do by himself. Who the vast majority of it is a managing courses, managing the scheduling, managing the the load for the faculty. 1: : So I put together in this excel spreadsheet 1: : a simple, I mean. 1: : I I it it's a not real data, but it's a reasonable data. So you have a a faculty 1: : from one to 27. You have the program. We are those faculties to the most. And then you have for a 1: : certain number of years 3 components, the load. 1: : the target and the balance. So the load is basically the number of courses that that instructor actually thought 1: : the target is what they were supposed to teach, because at the beginning of each academy here each faculty as a a load. There is a 1: I mean 1: : there is a a certain number of courses, so that we need to teach by contract. But then we can have a reduction based on a 1: : research project that we have, or activities services that we provide the to and or to the school of systems and enterprises 1: : so based on that you have a reduction in terms of you're ready a load, and that's your target. 1: : So you have the load. The number of courses that you to in that particular year what was the target and the balance? So that is the difference between that load and and target. 1: : Sometimes you can be like in this case you are overloaded some other times like in this case your target was 5, but you told only 4 courses. You are under loaded. 1: : So you have a from from 2,019 to 2,023. So that's the data set that that you have Again, some of the elements may be me singer. 1: : maybe missing, because, for example, the faculty number 2 was high in the in 2,021 meaning before 2,021. You have an a. 1: : So before working on the number, so you may want to do some pre-processing eliminating the in a and a replacing them with the 0 or something that that would make more sense to you. 1: : So that's the data set that you have 1: : on this data set. You want to do some statistical analysis and some visualization. 1: : So you want to calculate the number of courses by each program at each academy. Here the average number of courses per faculty per academic year, and this will be from the load. 1: : The the number of the underloaded faculty pretty much academic here. This load that is a less than the target number or overloaded faculty for each academy here, meaning 1: : load the that is more than the target. 1: : Then, using a bouquet, you want to create 4 plots. 1: : One, that is a a courses per program per academy here, and this will be a a line plot, and you will add the a legend for the programs. 1: : The average number of courses per faculty over the years, and this is from the load. Each faculty will have one value. That is. The average, of of course, is total. In that year 1: : a number of overloaded faculty over the year. Each year we'll have one value that is, the number under loaded I mean the courses by under loaded faculty. 1: : and and then a pie chart with a courses by program in a 2,022, 2,023, 1: : using the names, so em Ssw. And as well as as a labels for the wedges. 1: : Once you have all of those meaning those numbers, so their metrics and those visualizations. You will write that 1, 2 page record incorporating the visualizations and highlighting the the key facts that those visualization will show, and you will submit the the python script and the the the slash. Pdf report. 1: : So that's basically, it 1: : comments 1: : all right. So 1: : if there is no comment. That's the end of the class. I will make sure that everything will be on canvas. 1: and and that we will complete the green by the end of the week. 1: : So I'm, stopping the recording. 1:
All right. So this is January 18th, 631. 0:01 And as the first class of the semester, four 624, I'm happy you joined this class. 0:08 So this class is one of the most popular we have at the School of Systems and Enterprises. 0:19 And that most of the time, one of the classes with the highest number of the students registered at Stephens, the class is going to run on Wednesday. 0:26 So if you have issues with Wednesday, send me an email. 0:44 We can work around that. If a large number of you cannot make it on Wednesday, we will try to find the other options. 0:51 But again, classes will be recorded, meaning if you miss a class. 1:05 So that's another major problem. 1:10 Eventually, I mean that again, classes are recorded, the lights will be available and we can have office hours either in person or online. 1:12 Let me start sharing the screen. And let me go first to the campus for the cause. 1:31 So that's basically your Campbell's minimizes that's your Campbell's so so far. 1:43 But you will find the time. That's my office, Barb. 1:50 Bill, five or seven email showing from Shoe Line. 1:55 So the information on myself will go back to that. 2:02 We do have a part time V.A. She, you, she, you is a one of my students. 2:09 So. We will. 2:20 I mean that we will talk about the syllabus. 2:27 Will we talk about the distribution of content within the semester? 2:33 The distribution of grades? 2:40 Again, there's the link for the zoom and I'm in that the class today could be shorter because it's pretty much an introduction. 2:42 We will go through the main elements of the class. 2:57 Recently the class was. Was a. A reader nominated. 3:02 So initially was a informatics for engineering management. 3:08 That doesn't make much sense anymore. 3:13 So now is data exploration and informatics for engineering management. 3:17 The data exploration is definitely more in line with the goals that we will try to achieve. 3:23 So I will open doors with lots of content during the following weeks. 3:31 So for this week you will have orientation and exercise zero that I will introduce later on today. 3:46 So you will have your grace discussion. 3:55 And let's start with discussion and quizzes. 3:59 So if you click on quizzes, you will go here to. 4:05 And am I prepared to quizzes? 4:10 So quizzes are to determine what is the level of knowledge you have in coding and you to please the mute yourself. 4:15 Thank you. So for this class there is no prerequisite. 4:29 Meaning we will start from very zero and you will end up writing 100 203 on the line. 4:36 So code by the end of the semester seems to be for some of you, mission impossible. 4:50 But the reality is not so. Again, the code is one of the most popular we have and they had over the years. 4:58 So the course was originally developed by another professor who revived it completely. 5:09 I am teaching it since, I don't know, 40 years, five years, 5:17 and most of the time the students didn't have any experience in coding and any experience in coding in either. 5:22 So there would be two questions. Question one Do you have previous coding experience any language? 5:34 Zero is no knowledge. Ten is a professional level, so you will write down the number between at zero and then. 5:42 Question to see me to. Question one buddy on Python. 5:54 Do you have previous experience in coding using Python? 5:59 No experience. A professional grade. So and you will write down what is up. 6:03 You are a level. I strongly encourage you to do those quizzes. 6:08 There is no grading, but it's very useful for me to address the class in the proper way. 6:15 Just to give you an idea of the average in the past several semesters for question one that was around three for the average for question two was one. 6:24 Two, where the mode, the meaning the most popular answer for question two was zero. 6:40 So if you have no knowledge of Python, don't worry. 6:48 If you have no knowledge or coding, that's absolutely fine. 6:54 Again, I had people from different backgrounds say that the majority and nothing to do with coding. 6:58 Coding is essential. So on the other side, let me stop sharing for a second. 7:10 So coding is becoming more and more essential and a little bit of background about about myself. 7:17 So I my academic background originally I had a master equivalent degree in mathematics from the University of Rome, 7:27 Italy, and I Sapienza a million years ago at that time we coded in a Fortran. 7:43 So that's my thesis was on eigenvalues and eigenvectors calculated on large matrices using Fortran. 7:51 At the time we didn't have any user interface, so we used the, the, the punching cards. 8:04 So it was a nightmare, but it was Fortran and it was a. 8:18 Me. Daddy's around, ladies. Then I went to industry. 8:25 So initially, as a, uh, I'm a developer, so I was coding in Kabbalah. 8:32 That was another language that was pretty popular at the time. 8:43 So Fortran was formula translator. COBOL is a common business oriented language. 8:46 I mean, those languages are still around somehow because there are legacy systems. 8:56 So Fortran that is actually part is embedded in R but as language is defined for all intents and purposes. 9:04 So then I developed my career in information technology, telecommunication and management consulting the age of 50. 9:16 I decided to go back to Academy because the type of consulting that I was doing was kind of difficult to do because it was on strategies, 9:27 new products, new business models, and it's really difficult to demonstrate that you are better than adults. 9:43 And at the very end, that was all about the people, you know, within the organization and more than what you are able to do. 9:53 So I decided to go back to academy at that point, the age of 50 again. It was around between ten and 15 years ago. I'm 65 now, and at that time, social media, we are kind of starting becoming relevant for society somehow. And my idea was to go back to the math departments and do a research on applying mathematical models and my business experience to get the insights on the social media. I applied the for Ph.D. programs to different universities, but then I realized that I mean, that obviously as a mathematician, I would have loved to go to Princeton. So the head of the Department of Math at Princeton replied to me, saying, We would be happy to work with you on this topic. But obviously Princeton is a highly competitive. So Stevenson has had that and has a great reputation. I live in Hoboken now, so it was the natural choice to be at and Stevens. I had a great conversation with. What was the head of the math department. The. At the time that he passed away, he was a kind of a mentor to me in my academic life. Charlie Hustle. I was admitted and I started working at the very end that I got. I mean, my original idea was to do it, to do I mean, math, sociology and a little bit of computer science, but not much. So that was the original idea to redirect. Somehow my consulting activity is more in line with the quantitative approach that was definitely emerging. During the period of my GFC, I continued working as a consultant for about a year. I was a CTO, co-CEO of a technology company and I did other consulting. So it was part time. It took me about five years. During the last period, I started developing courses in data science for the School of Systems and Enterprises. They offered me a job. I stayed in the loop and so I'm in the last five years and change I am in and Stephen says a full time non tenure track professor during the development. So I am telling you the whole story just to give you a little bit of background. During my research activities, I didn't want to code, so I thought I'm too old for going back to coding. I definitely want to be more on using the results. But then I realized that if you want to deal with data, the only way to deal with data is to work. Hands on, on. There is no middle ground. You cannot just ask someone to do the job because it doesn't work this way. So you really need to get your hands dirty and work with data. So at the very end, I started studying Python and then teaching Python developing courses in Python. Right now it's kind of a form of relaxation and meditation to me. So I have my ideas. So my research focus is on a machine learning and natural language processing and natural language processing in particular. So I have ideas of models so that I want to develop and coding those models to me is it it is relaxing so I don't pretend that you will be and that point lately that on that you may continue to dislike eventually according but is a skill that you definitely want to have because you can apply that to whatever you are going to do if you will be managing a team, doing the coding. You definitely want to have a real experience in coding. So firsthand experience because otherwise you will never see the problems they will have. So M 624 is a core course for engineering management and is becoming a core course for systems analytics as well, because the course is basically teaching how to use Python for doing things. So is not on teaching Python but is using Python and as a byproduct teaching Python for doing things primarily data exploration. So that's a long thought. Let me go back in sharing the screen and giving you some more background information. So let me start with the content of the course. So the course is over 13 weeks. So there would be some weeks that we will keep because on the academic calendar, we will do the orientation today, introduction next week. And then we will continue each week. Pretty much you have one assignment. The initial assignments are easy, so but each assignment is built on top of the previous one. If you fall behind the catching up, it's really complicated. So in this course, in the past, we had quite some or several cases of cheating. Cheating primarily is either having someone else doing the assignment for you or sharing the assignment with other fellow students. All the assignments are individual, so all the assignments, meaning the regular from 0 to 9, the midterm and the final. So. They are individual meaning if two of you will do the same. So with would submit the same exercise. Let's say any of you are doing exactly the same and these same is well done. So it worth a hundred. The grade for each one of the end will be hundred divided by an. And there is no ifs or buts to that. The only exception is if one of the N will come up saying I was the original owner and then the remaining n minus one will tell, yes, we copied them and they point the the one will get less than hundred the because you are supposed to protect your assignment. We get around the 80, between 80 and 90, and all the adults will get ten, 15, 20, just because they kind of are being honest in not acknowledging the fact that they cheated. If you look at the numbers. If you start losing chunks, there are chances that at the end you will not make it so. And then at that point, there is not much that they can do or want to do for you because you lost points by cheating. And I mean. The rules are clear. So if you don't follow the rules, you will be penalized. So the way you will be penalized can be up to the point that you will not reach that. And let me go to here. You will not reach that minimum grade pass. The courts. So there's a distribution of that and you do have both the previous spreadsheet and this one. So you have all the exercises. So the participation is based on the metrics that they get from what they what they is. Tracing the page that you visited, the time that you spend on the online, on canvas, on the course. And what they do is basically calculate the average and then define I mean, how much from the average is to be considered to give the full 30 order portion of it? It's sort of a standard deviation from the average. So those are the points that you can use this spreadsheet as a dashboard to have an idea of what is going to be your final grade. So the formula is the formula that you have here. I mean, there is no secret in that there's a formula that I am going to use. And if you don't get to at least 73%, you will fail. So. If he's more I'm sorry, if he's more than a 64%, you will get a C. So between a 73 between 64 and 73, there is a C below. That is an F. So you can eventually do some sort of extrapolation, meaning you have grades from exercise zero to exercise, let's say five. You can calculate an average and you can extrapolate the points and you will get for the following and you will get an idea of if you continue the same way, what is going to be your final grade? If you come to me by the end of the course saying I'm not getting a I'm not going to pass at that point, it's going to be too late. So use this spreadsheet as a sort of a tableau, the board dashboard, for understanding what is going to be based on what you have so far your final grade. Use it. Use it a lot. So that's something that you have. Assuming that you have thought in the backseat in the participation. All right, syllabus. So the syllabus is the main then on the syllabus. So again, my name and my email address. My office. Wednesday, 630 is generally till 830, 9:00, sometimes maybe longer or sometimes like today may be shorter. In the first editions of this course, I had a textbook. So then I realized that students do not really read much. The textbooks so is required. But you do have a version. So if you go all the way down, you have those as a PDFs. Later on, I will post some more readings. So use the MOOCs as a sort of a reference. So the slides should be enough to let you do the assignments. The assignments are a learning opportunity, so you are forced to study some elements because you want to do that. You want to pass or get a good grade in the assignment. When you called the when we called that at any level, sometimes we do not remember how to do things or we do not know how to do one specific task. Google is the source, so google the information. Go to StackOverflow. That is kind of a. The place to go to get supported in your coding and you will get answers. So answers that are obviously not for your specific case but are in the class of what you are doing somehow and then you use it for whatever is the goal again. Sometimes in the assignments you will be asked to do things that are not that were not in the slides. That's intentional because that's something that when you write code is happening all the time, meaning that you are kind of push the beyond your comfort zone and you need to go online and get some help from the community. Again. We use Python. So is one of the most popular. At this point is the most popular computer language and there are up in the medium versions. So we use 3.6, 3.7, that is OC 3.8. The 3.9 is a little bit of a stretch and there is a 3.10. But the main problem is one of the main advantages of Python is the fact that there are quite a lot, several thousand libraries that you can use to add the functionalities to the basic python. If you use versions of Python that are very recent 3.10, those libraries may not be compatible with those releases. So if you are anywhere between a 3.6 and 3.9, you would be okay to be on the safe side, the 3.6 at 3.8. So if you go above that, the libraries may not work. We use the. Adds These integrated development environment, you can use any idea you want that we recommend to use by China. That is probably the most common idea in Python developers or in Python development community that we do not use what is called notebooks. So notebooks are great for proof of concept or things like that, but they are not good for developing products and programs that will be part of a product that you are developing. We will go back on these concepts, but if some of you are already familiar with the concept of a notebook, please don't use it, use any etc. or any idea, but not notebooks like that. There are a few of them. I mean, at this certain point, I will spend a portion of my class on how to use those tools, but not now. In a sense I am revising those tools and in particular those that are online collab. In particular I have a mac. My Mac is latest generation with Apple silicon. The Apple silicon the Apple microprocessor is not compatible with some of the machine learning libraries. So because of that, I'm using an online environment that is called collab in itself, but it is not the only one until there are a few of them. I'm using this collab that is by Google and is probably one or the most common. So, Campbell, we went briefly into canvas that it is your responsibility to check the assignments, to check the new material up. I'm not going to send you reminders. So you know that by the end of each class, I will force the new material to new material on an average includes both the slides. Eventually some reading material. And in most of the cases, all the cases but one where cases are a class is the assignment for the following week. We don't use much the discussion board either, but you can open one if you like. Submissions. So. To me is more important that you do an assignment. Well, more than you do. Exactly. Within by the minute, by the deadline, I will apply some late submission. Just to be fair. So when we teach, one of the things that we really care a lot is to be fair across the class. So if someone is doing their best to be on time, it wouldn't be fair with those people. If I accept the submission that is one day late, then obviously there are two days maybe. There are exceptions. So you are working and you are traveling. You have an emergency. So, I mean, emergencies are emergencies. Exceptions can always happen. But the rule is that you will submit your assignment before the beginning of the following class. So the reason for that is because most of the time I will present the solution at the beginning of the following class. So at that point, I cannot be sure that you need to use my solution for your submission. So the deadline is the beginning of the following class. I think is a fair way to approach it. As I mentioned, how relevant is the ethical component of being in these courts? We come from different cultures, so we may consider cheating more or less impactful on that ethical code. But the reality is, in this case, the rules are clear and the rules are what you see in a on your screen in the syllabus. So, again, all the assignments are individual. So if you share code, you will share the grade. So again, you have ten students doing in 100% a grade assignment. Each one will get ten. So if you are losing a 90% of your grades or even 30% of your grades by the end of the course, you will not reach the minimum to pass the courts. There is no problem using online resources. So I mention that StackOverflow that is the most popular site for getting support in some critical steps in your development. So if you do so, you need to cite the source that you use that if you don't say if you if you don't cite the source, then I cannot tell if you shared the code with someone else or you just shared the source. So you need to cite the source. If there is no citation of the source, I have to assume that you are sharing the code with a fellow student. In theory, when that is a case of cheating, I should report it to the Ethical Committee. Once you are in the ethical committee, then things can go pretty bad, meaning you can be expelled by the students at that point. Finding another university would be really challenging. You don't want to go that way, so please don't cheat. I'm doing this course again since quite a while with our. We are using a tool to help detecting the level or similarity between assignments. But cheating without being caught is is difficult. So if you cheat the you need to cheat while cheating. Well, it's most likely as much complicated as doing the job yourself. So I mean, those are the outcomes. And again, you have here the content. Generally speaking, the up to the meter is pretty much basic based on an introduction. So if you are already very familiar with Biden, the first half is not going to be very challenging for you. So but you will review some of the concepts after the midterm. So the midterm in on campus classes. So the midterm is on campus. In this case, obviously would be online. I still not sure if we will do in real time. Meaning you will have a two in an hour. Fireworks like in the regular. On campus or if you will have one day or two days to develop it. After that, you have more of the application, so you have better visualization, the text processing, web mining and things like that. All right. So let me stop sharing for a second the questions around the. Are you all good? All right. So. Let me keep sharing a. And let me go through the first the OP's. Set those lights up. So again, we are going to do an introduction. So we did the introduction to the formal aspects of the courts. Now we would do an introduction to the materials. So users versus builders. Though we are. In what we do each day, sometimes use ups, sometimes builders. So in our job, what they want is the job. We may be builders, so we may actually build things. You are an engineer. You are building bridges or whatever. You are a software developer. You are writing software. But we are also user. So we use the television, but we don't build a television. We use computers. We don't build Mozilla, so we don't build computers. So we are in this range all the time. When you are on the coding side, we are both users. So code the we all use software, everyone using PowerPoint and Zoom right now. But we are also builders, meaning we write code. So when you are on the coding side, the developer is a programmer. So the demarcation line can be kind of thin because if you're using a spreadsheet, you can have quite a lot of developments in the spreadsheet as a building can tell. Meaning you can have formulas. You can have the. I mean, even some coding embedded in the chip. But generally speaking, when you are on the program side, you use a computer language, whatever the language is going to be. Languages is a code software, so is a sequence of instructions that you write one after the other. Most of the time, the sequence you write, the code is the sequence. The program will be executed. Generally speaking, a computer is something providing an input output. So is the portion that is the entry point, the element for the user to use the computer. Then you have the computer itself that is a processing unit and a memory and software up to a user. Both the central processing unit and the memory. And then eventually you have an external memory, like a hard drive or a thumb drive or a cloud drive. So that's the general schema of a traditional computer. We are all bombarded by the concept of the AI machine learning, deep learning, things like that. What is machine learning? So machines do not be loaded. So in strict terms, what is learning? Learning is getting skills. Machines do not really get new skills. They get more data. So based on data, they apply the same skills to the new data. So is this a learning? I mean, if you look at the system as a black box is kind of acting as it was learning but is not really learning. Same thing when we call up a artificial intelligence. Are computers intelligent? We cannot really define the concept of natural intelligence, so we cannot measure intelligence. So the IQ is America, but has a lot of criticism and it is very biased from the cultural standpoint. It more is working better if you are in line with the current. The common culture meaning is not really measuring the root. The intelligence that we have, if you cannot measure not can be in that you cannot know meaning. If we cannot know what intelligence and national intelligence is, we cannot really define artificial intelligence. So we define artificial intelligence. Or are we we consider a system as an A when the behavior is intelligent or we consider it as comparable to what a human would have would do. That is another definition to. So there are a lot riding on the application of AI and machine learning that I will share with you down the road by the very end. There is no real learning in machines and there is no real artificial intelligence. So there are more sophisticated ways to get answers. You probably heard about the chat GPT. So we will we will talk about this board that is based on algorithms, is based on machine learning. So machine learning as to make I mean, systems based on machine learning, they have two main components. One, the data. One, the algorithms. If you have good algorithms, crappy data, the system will have a crappy behavior. If you have a lot of data, an okay algorithm, it will be smarter than you expect. So bottom line, what is really driving machine learning is the data inside the system and the how fast the data are processed. At the very end, the way they are processed. It's a statistical approach. Even the most advanced systems they are actually rooted in statistics is basically statistics on steroids. By the very end, the. What they are doing is recognizing partners in the data and matching those partners with your request. So it's part of recognition that is a correlation on steroids. And that's what machine learning is. So it seems to be a little bit on the destructive side, but that's the way it is. So we don't have a real models to represent the knowledge yet that we can use within a better use of data. So there is no real hybrid machine between the knowledge based systems and the machine learning systems. We will go back to that down the road. So but we use this kind of machine learning because sometimes it can provide the answers to solutions. So if we have a. A topic, a domain that is highly documented and we have quite a lot of cases. Then we can create a system that can detect the patterns and match the patterns with the requests. Machine learning. At the very end, more machine learning. I would say A.I. has quite a lot of competencies, so some of the competencies are typically in the computer science, so information theory and databases. So those are typically computer science, but you have statistics and you have data mining. There is a debugger machine learning algorithm, so you should have cognitive science, psychological models and neuroscience, all of those. It's kind of like creating the magical mix for having systems with an intelligent behavior. Just to have a little bit of taxonomy. So. A.I. is a general part of automation. So not all the automation is has intelligent behavior. But if you have something that has an intelligent behavior, is trying to automate something that can be a process, can be just one particular task. Excuse me. So robots can have a. An intelligent behavior can not. So if you don't see that I don't know the robotic for tightening bolts, that's not much intelligence about the automatic process. It's automation. It's a robot. Autonomy can be a part of artificial intelligence is part of it. The artificial intelligence may be robotic or not artificial intelligence. It's a broader term. Machine learning is a part of it. So machine learning, generally speaking, we have two different approaches to artificial intelligence, one that is based on data and is machine learning. One that is based on the equivalent of urine sticks. So you have behaviors that are represented by rules taxonomies. So those are forms of representation of those risks. Data science is a combination of tools, techniques that can sort of artificial intelligence and machine learning, probably serving more machine learning than artificial intelligence, because that or that non machine learning artificial intelligence because the machine learning being based more on data needs more work on data. So data science is definitely more synergetic with machine learning. So we brought you some python y python and break now. Biden is the most popular language. One of the reasons why is so popular is because it's relatively easy to get. So the learning curve is not too steep and there are several thousand libraries that can have the functionalities to Python's. I mentioned StackOverflow. StackOverflow is providing support for all the like most of the languages. So but if you look at how much is growing by doing so, the number of the items items can be answered questions, interaction in a built sense over the years on StackOverflow on python is driving python to the leading position. Job posting up again. And within that the jobs where you have some coding that there is quite a lot of a python. So and that's why we are teaching Python as a core course and that's called on systems and enterprises. So again, why it's so popular at its high level is the interpreted. So there are two ways to do computer languages. One is a compiler, one is a interpreted compiler means you write your code, then you have a tool called the compiler that is translating your code into machine to machine language. Machine language is a sequence of zero one. Obviously for a computer working on that zero one is faster than interpreting the statements one at the time. So the interpreted languages like Python that are executed top to bottom left. So write the one statement at the time. Compiler languages, kabbalah with one another up so they are up faster. C Plus classes compiled can be compiled. So those are faster because at the very end they are like one block that would be executed that in the fastest way possible. But they are more complex to debug because if you have an error, you need to go back to the source code, compile it again, run it again and see what's going on. Because I mean, in the past that was more of an issue because processing was really slow. Right now, our computers are very fast, so it's kind of difficult to see the differences between interpreting and compile them unless you do very complex tasks. So we mentioned the difference between Compiegne and by them. So the name by. So the language was created in 1989 by Guido Rosen and. The name is not from Disney but is from Monty Python. There is a group of individuals who did several movies that were kind of defining the period and the good. Guido was most likely a big fan of them and named the language after them. Installing Python. So that's an example for installing. There is a 3.7. 11, but obviously you can have a different the last numbers for different releases. So. You may want to develop. Do use an I.D., an integrated development environment. Again, we use a. Yeah. It's kind of funny the fact that it is named after the movie. So that's an example up. But it's probably easier if we stop here and we go directly to. Bigshot. So that's by Sharma. You have on the left side, you have your files and they are organized by projects. So you may want to create the. A full draft and you will address by owned by Sharma to that folder like in this case that I have within this directory, this folder EMC X 24 and I have all the files here. So that's pretty much like finder in your Mac or equivalent in in Windows. Then over here you have the editor. So in the editor you can write whatever you want, you will write your code. Basically, I mean, is an editor like, I don't know what. So you can write whatever you want. Then you can save it in the proper way. Here you have a multi functional area with all those features. So in this case is the output of this program. You have eventually a python console. And let me move this up. So the version of Python that I'm using is 3.9 and you can interact the here doing things like two plus four at the end and you have the answer. You can create variables. So variables are like containers. So I can say a equals three. Be equal to six, and then I can do A plus B. And they can get the result. Keeping in mind that implied or not, more letters and capital letters are different. Meaning if they do capital a plus. Capital B. I will get an error. So I mean that it would it be for both for A and B but is basically stopping the first road. So. I can do multiplication, I can do three multiplied by five. I can do Division three or a 33 divided by seven. So the outcome in this case, in this case was an integer 15. In this case, A is a floating in the latest version of Python. There is not much of a difference. You do not need to define invariable as one or the other in a three point in to point something. They were a different variables. So all of those are some of the examples of ways to do things. In theory, I can write everything in this interactive window, but obviously it is not particularly, I mean, a friendly. And that's why we write programs like this one. And we will go back to this one in a moment. 1: So let me go back here. That's pretty much the example that we had. 1: Uh, let me go back again here. 1: I mean, in this case, you not really need that. So you can do a class B. 1: And you will get the result. Or you can do print. A. 1: Class B. And you will get the same result. 1: A little bit nicer. So you don't have out of that actually the number. 1: So in Python three point whatever you need to write the parentheses. 1: So when you want to print something up, if like in this case are just variable to you, just write inside of it whatever it is. 1: Variables can also contain. Things like you and say. 1: Name. Equal. 1: You can have a single or double quotation. 1: And then if I do print. Name. 1: I will have the value of that assigned here. So all of those are super basic. 1: But any statements that you've can write in the interactive Python console, as it's called, the great. 1: So let me go back here to. 1: Again, those are some of the examples that we use the. 1: We name lists. Don't worry about that. 1: So we will stay with the integer strings for a while. 1: But there are other types of variables. You can do all the operations up, as I mentioned. 1: There are some that are kind of obvious plus or minus multiplied divided some. 1: Probably less power is the double sara modulo that is the remainder. 1: Meaning if you divide the nine more by two, you will get a remainder of two. 1: By seven you will get the remainder or two. So the result will be one with the remainder or two. 1: So modulo giving you the remainder. 1: You can assign again names to variables. 1: The names can change. Meaning if now I do name. 1: Equal. Do. 1: Oops. And then I do print. 1: Name. So it was Carlo. 1: Now his job, meaning those variables can be the content of the variables can be changed that these characteristics, 1: this characteristic is called being mutable. 1: And so the variable by that the content on their variable can change is mutable. 1: Not all the variables in Python are mutable. 1: We will go back to that down the road. Okay. 1: So names you can assign any name to the variables. 1: So keeping in mind that names are Ricky's sensitive. 1: So we saw the example with a small a small B capital A cabinet would be that 1: python that didn't recognize the capital A because we didn't define it yet. 1: You want to use names for the variables that are meaningful for the use of the variable you have. 1: So if you want to calculate the BMI or someone's BMI, 1: the body mass index is a weight in meter in kilograms, so divided by high in a meter to the power of two. 1: So you can call the variable ABC. 1: But then if down the road you want to do something, you may not remember one the variables. 1: What was the value or the meaning of the variables? 1: So you want to use names, the variables that are mnemonic so you can remember. 1: So when you have a code that is few lines, life is easy, 1: but when you have code that is hundreds of lines and probably you are not even the order or the code or someone else is looking at your code. 1: Then you definitely want to have your code with the mnemonic names for the variables. 1: You can. Use the interactive. 1: Console under the Python console, or you can write programs in the editing window and then you can save it and run it all together. 1: When you run the program all together, Python is reading, as I said, from top to bottom, from left to right. 1: There are some rules, so the syntax is not super heavy in Python but is something. 1: So you want to be sure that you will follow the rules and we will discover the rules. 1: Why? We will use it. We move on to programs. 1: We mentioned what this could be. Those are the links and that's the end. 1: So let me go back to here and let me introduce the oh, let me stop for a second and check if there is any question. 1: All right. So there is no question. 1: So let me continue sharing. 1: And let me go. Here and I'd be glad to accept side zero. 1: So except say zero. As you may deduct from the number of points is very basic is only 20 points. 1: So on an average I mean that most of the assignments are 50 points after the midterm because they are more complex. 1: So they will be 100 points. The midterm is 200. 1: And the project that is also the final, there is no final. 1: So the final the project is the final is 300 points. 1: So what you are asked to do is to set up a pie chart and python. 1: So you go to a jet plane brains dot com website to download and install a pie chart either at the free or the academy version. 1: You want to set up a folder with your current and future files from this course and you want to have it. 1: Right here. So like in this case, you have em. 1: 624 And if you go to the actual organization is probably easier to do like this. 1: So you have the highest level. You have a directory with all the projects. 1: One of them is this M 624. 1: That is what you have here. So again, you place it, whatever you want. 1: So in this case, I had all of them together. 1: But then you need to go into file open. 1: And then you want to go to. The folder where you have your folder or directly the folder. 1: And then you take, for example, this one you open and you have all the files that are in it. 1: This is a project. Then I'm developing for the Department of Defense. 1: Okay. So. One more thing on page Obama. 1: If you go into settings, you are going to have quite a lot of information. 1: So then in particular, you will have information about the project, the meaning this M 624 in this case. 1: Recommendation in the previous one. And you have the structure, meaning exactly what you have here and your interpreter. 1: One of the good things all about by Sharma is that you can have more than one python on environment, two different projects. 1: So you can have one project that you do with Python 3.6 and one project than you do with a 3.9. 1: And you associate what it's called a virtual environment for each one on the. 1: And then within the projector you can have fiber. 1: And so those are all the libraries that are associated to these 624 project. 1: You can eventually change the interpreter. 1: Okay. So those are I mean, I have a 3.7. 1: 3.9. And again, in this case, I'm using a 3.9. 1: And for a while I was using 3.7 and 3.8. 1: Recently I migrated to 3.9. I still having a little bit of compatibility issues. 1: So my recommendation would be to use a 3.8 and eventually 3.9. 1: So again, you can change the appearance. 1: You can have different user interfaces, a different color schema. 1: You can have a plug in. So there are a quite a lot of I mean and on and options that you can set in settings. 1: All right, so that's by Sharma. 1: And let me go back here. 1: So you want to run some of the comments in the by them we know like we did we did here so those comments here and you want 1: to save it with a screenshot so you take a screenshot of things like this one and and you save it and you will submit them. 1: So then you will download the access site zero. 1: That will be this one. And. 1: You will have that. Right here. 1: So that's the one that you can see here. 1: Now you don't need to know exactly what is the meaning of each one of the lines that are here. 1: So just take it the way it is and do what you are asked. 1: So in this case. You open up. 1: And before the statement that withdrew, you want to add those lines? 1: So those lines up will add your name. 1: So this character here is one of the special characters in Python. 1: When you have a backslash and that's an indication that the characters are following it, that is a special character. 1: The N stands for a new line of meaning before printing ground by your name by Cardinal Eaton, you are asking the interpreter to keep a line. 1: The same result could be achieved using a printer with nothing inside. 1: You can do either one. I generally use this notation because it is more compact. 1: So again, you open the access site zero from Camus. 1: I mean, you download from Camus in the. 1: The folder that you created up. You need to save it. 1: Into the folder and then you change it this way. 1: Then you save it and run it. What is run it, run it. 1: You should have this here. 1: A name of the program and this arrow meaning run the exercise or you can right click go to run and then hit run. 1: So now the program is running and is basically doing what is called the loop, meaning is staying here until there is a break. 1: So in this case, A is asking me the first integer, let's say 22. 1: Enter saying an integer 44 or whatever 24 and I'd say. 1: And you have a. So. The first one. 1: It's right here. I'm assigning to the variable of first name, the value from the input. 1: Then. If the value is done, we break, meaning we go out of the loop. 1: Elsa Meaning if he's not done. 1: He's asking for a second number, going to the variable signal number. 1: Then is doing the calculation while is printing. 1: So printer is keeping one line the sum of both. 1: Now that is the first number. In this case, was it 22 plus? 1: And this is I mean, not a variable, but the actual value. 1: Second number and it is the 44. Is equal to is equal to. 1: And then you had the operation. So with those statements, you don't get the enigmatic value. 1: You have no numerical value. 1: So you need to transform the non numerical value into a numerical. 1: And that's what you do with this statement float. 1: So it's transforming in floating for tsunami, meaning from being non numerical will become numerical. 1: And at that point that I can do the operation. 1: And then, I mean, in this case, I'm using a combination, but doesn't really matter much. 1: So the first one is transformed into a floating of the second, then integer. 1: But whatever I could have used to transform the string into a number, both the floating or the integers. 1: And then when you eat that, I mean, when you type down, you go break, meaning you go out of the loop and you will execute this one, this print. 1: So if over here I do. Done. And there. 1: At this point, it's leaving the loop and is going to print. 1: So again, don't worry if you got just a fraction up or the description of the code other than giving you at this point is not essential. 1: We will go into each one of those in the following classes. 1: One thing that worth mentioning is the fact that Biden is using a lot the indentations. 1: So when you have a loop like this one, everything that is invented will be part of the. 1: Or when you have a conditional statement like in this case the if true as to be indented. 1: So all of this block from here to here will be executed that as part of the. 1: This one will be executed only when you have that done for first name. 1: If not, you will go into Elsa and then all of this will be executed in case of ends. 1: Again, you don't need to really understand that what's going on. 1: Another element when you use the numbers, sign up everything following the number of sign is that comment. 1: There are other ways to comment. 1: So when you have a like in this case, that is not the best way to do it, but is clear. 1: So you can have a. 1: The characters at the beginning and the end and in between you can add all the comments you want and they will be not executed by Python. 1: So that's basically it. Again, let me go back to here on. 1: So. You want to set up by Sharma, including downloading Python. 1: And by the way, you have a video here. 1: Get getting started with Pi Sharma. You have the link to pay Sharma. 1: This is the dock that we are viewing. This is the the script to this script here. 1: And the due date is next week. 1: So you go back here again, you will install Pi Sharma and PI. 1: Then you will create a folder where you will place this person map and the following assignments. 1: Then you want to run you by. Don't call monster like we did that. 1: Look right here and you want to take a screenshot of the results and submit that in a. 1: Right here. When you will do the submission, then you download the access side zero. 1: You save it in the folder that you created here. 1: Then you add that before the while through those lines. 1: Then you save the program. I mean, I shama is saving automatically the program, but you can. 1: Also do pile up. I'm in save up. 1: So eventually I'm in this case. And because there is no change and Biden didn't pay, Shaman didn't allow me to do the same because there is no change. 1: Then. You execute the program? 1: Not again to execute it right click run exercise. 1: You can also do Run, Run. You can also go here and run, but make sure that the name of the program is here. 1: The first time you open it, the the name is not there. 1: Meaning at this point, the only way is either through that. 1: The menu or to the right click. 1: You want to execute the loop a couple of times entering two numbers and then enter in. 1: Done take the screenshot and. 1: Rename eventually the screenshot the file and submit it. 1: Again, it's individual. If you use external sources, you'll need to cited. 1: Most likely you don't need any external source. 1: So because it's kind of straightforward. Questions. 1: All right. So. That's basically it. 1: So the majority of our classes will be longer than this one. 1: I already mentioned that this class is a sort of an introduction, so let me be sure that I published all the material. 1: So you have. All the lights. 1: And the assignment. Everything has been published and you will be you should be in good shape.
 

ft STEVENS

lw INSTITUTE of TECHNOLOGY

Working in Teams

Le <i kd aA a ll



clipizzi@stevens.edu

SSE

 

software Development Organization S

 

¢ Software Develooment can be a complex task and needs
Q multi-skills, multiple person effort, meaning that an
organization must put in place

¢ Organization seeks to construct a software development,
support, and service organization based on the project
plan

¢ Activities include:
— Acquiring various skilled individuals needed for the project

— Obtaining the tools to support the process and
methodologies

— Creating a set of well-defined metrics to track and gauge the
project

_
STEVENS INSTITUTE of TECHNOLOGY | 2

General Software Project Organization \@



STEVENS INSTITUTE of TECHNOLOGY | 3

ae. th
ep

Mairix vs. Hierarchical Orientation — &

¢ The software develooment structure Is flexible based on the size of the
project

¢ The organization structure may be represented either as a hierarchy or
as a matrix

— Hierarchy org.: all the people associated with a project are
grouped into functional departments that report directly within the
vertical line of command of the organization

— Matrix org.: people are grouped based on the functions they
perform

¢ Functions may be performed by non-members of official project
organization

¢ Less function duplication, better focus on specialized skill.
¢ What about team loyalty and confusion?

STEVENS INSTITUTE of TECHNOLOGY | 4

Functional Orientation ¢

¢ The general organizational structure may be further refined to
show a more precise structure

¢  It is important that the organization be defined down to a level
where each individual can see her/his name



_
STEVENS INSTITUTE of TECHNOLOGY | 5

I )Sh —3slU«Ui

Highly Specialized Organization S


STEVENS INSTITUTE of TECHNOLOGY | 6

  
 

Software development specialization
Pee

 

af,

Software Support Structures je

¢ Support managers must set Up an extensive
Customer interface group, such as call service dept.
that handles the following duties:

— Answer calls
— Analyze each problem
— Respond to the customer if a possible solution exist

— Generate a problem report when an immediate
solution does not exist

— Track problem resolution activities
— Report and deliver solutions to the customer
— Close problems

_
STEVENS INSTITUTE of TECHNOLOGY 
 
   
 
   

 
   
 
   

           
      
   
   

 

    
 
 
 

 
  
 

  


STEVENS INSTITUTE of TECHNOLOGY |

 

Recruiting and Hiring Software Personnel \@

¢ Once the organizational positions are outlined, the
software project management needs to fill open
positions

¢ The actual hiring of the employees starts with
having a clear definition of the open position in
terms of the skills, training, and character of the
candidates required for each position

Recruiting:

¢ It is Usually not sufficient fo provide a general
description of the position title to the HR
department

STEVENS INSTITUTE of TECHNOLOGY | 9

 

Project Team Life Cycle

¢ Very few projects can be completed by
individuals.

¢ Group becomes a team through proactive
efforts by members and project manager.

¢ Typical project team life cycle goes through
three stages:
— Team formation
— Team develooment
— Team maintenance

STEVENS INSTITUTE of TECHNOLOGY | 10

 

Project Team Life Cycle S

¢ Teams need forming, developing and maintaining

¢ Amount of management activity differs at different
stages of the team life cycle



Team Stages
ZL

STEVENS INSTITUTE of TECHNOLOGY | 11

 

Project Team Life Cycle

¢ Team building activities center on education and
training on areas like:

— Building trust

— Negotiation skills

— Listening skill

— Responding to pressure

Project manager must ensure that there is enough
time in the project schedule for such training

_
STEVENS INSTITUTE of TECHNOLOGY | 12

 

Team Formation lw

Having the best people does not guarantee success
unless experts work effectively as a team

Project might be delayed or fail due to personnel
conflicts

Tasks may be independent but interrelated

STEVENS INSTITUTE of TECHNOLOGY | 13

 

Team Formation S

¢ Project manager will review tasks and decide on the
Skills required to complete them

¢ Team members should possess other behavioral
Characteristics or “soft skills”

¢ No perfect person exists, managers should not look
for mythical candidates

STEVENS INSTITUTE of TECHNOLOGY | 14

 

Soft skills and personal traits S

¢ Traditionally, managers tend to focus on technical
Skills and experience

¢ Managers should look for other characteristics, many
of which are “soft skills”

¢ Soft skills

— Anon-technical skill that can be utilized on multiple
occasions and Is not restricted to any specific domain

STEVENS INSTITUTE of TECHNOLOGY | 15

Soft skills and personal traits

 

¢ These personal traits might include:
— Personal ambition
— Interoersonal communication skill
— Sense of urgency
— Strongly held likes and dislikes
— Attention to detail
¢ Some team member may have negative personal
traits
— E.g., Personal ambition over team goals

STEVENS INSTITUTE of TECHNOLOGY | 16

Team Development lw

Team members behavior need to be continuously
monitored

Project managers should perform conscientious socializing

— Informal data gathering to pick up any signs of team disorder
(e.g., Non-return of emails)

With remote and virtual teams
— Communication is d major source of team related problems

Members may need counseling on basic working
etiquette

STEVENS INSTITUTE of TECHNOLOGY | 17

 

Team Development S

Team or personal Issues

One or more people are upset about something and
Its negatively impacting the team

— E.g., personal (“Il can’t stand working with Fred”)
— E.g., systemic (“Il hate how we do code reviews’)
Start by talking one-to-one To people involved
Ask what Is going on and what can be done?
Seek Out Causes not just symptoms

STEVENS INSTITUTE of TECHNOLOGY | 18

 

  
: All right. So let me start with some general considerations. So I extended that by 2 days the deadline for the previous assignment I sent everybody an email. 0:02   : The main reason is because several of you requested extension. So for all good reasons. So, but because the number was significant, I decided that that it probably is more fair to extend that for everybody 0:21   : for 2 days. 0:44   : So we have today's extension, so that doesn't mean that that is kind of the moving down the the due date for the next assignment. 0:46   : So it will be the same meaning that there would be an overlap, and just giving you a 2 more days for this one. 0:56   : Pay yourself for the next assignment that I will introduce at the end of this class. 1:03   : Cheating is is still a problem. So we had the 1:11   : some students cheating in more than an assignment. 1:20   : Again, I really try 1:25   : not to go in the formal way. I go into the the, the, the the editor committee, whatever it is, and the whole or board. 1:31   : because when you go to the Honor Board you will know what is going to happen 1:41   : so it could end up with the expelling the student. So that's a very extreme conclusion. 1:47   : and I want to avoid that. We want 1:58   : avoid that for you. 2:01   : But when I see students cheating in more than 3 assignments, so then I mean at that point is too much so I would refer to the on the board. 2:04   : So be aware that when we go that way things can go all over the places. So, going there with the case or 3 for cheating. 2:20   : You're not going to see benevolent people on the other side. so the chances are to be expelled 2:34   about high. 2:45   : so please make sure that you do everything by yourself, and if you use it there not sources site the sources. 2:46   : All right. So let me share the screen now. 3:01   : and let me start working on this. So that's where we are. We are over here, March 29, 3:05   : and we will talk about text. So next week we we talk about the web mining. I will introduce excess size Number 7. That will have a different 3:16   : types of complications. Let's say, compare to excess. Size 6. 3:29   : It's, I mean it will be on the dealing with texts or courts. I will not. 3:36   srivatsav: Could you please? 3:46   : Okay, so I will not present the solution for accept Size 6, because I gave you 2 more days, and I want to be sure that that no one will use or would be inspired by the by the solution that i'm going to present. So 2 more days. 3:55   : no presentation in class of the solution. 4:14   : All right. So I changed the the something that I mentioned in my email I changed the 4:19   : a little bit. I mean, I added the some details to the the specs, the the the the requirements for the the exercise 6. 4:30   : I didn't specify on this point number of courses per each program for each academic here, where you take the quote the number because I didn't specify. It was okay, whatever you use it. But the actual course is what is in the load, the meaning, the courts that actually ran for that particular program. 4:46   : Same thing for this one for the last one, not the courses will be calculated using the load. But again, if you did in a different way, that's fine, because in the original specs it was not specified. 5:12   : and 5:32   : I i'm not sure that that was necessary. So when I asked the average number of courses per faculty is per faculty, not per program. So I just 5:36   : clarify that there is no program involved in this calculation or in the following, to 5:48   : so it's for faculty, not per program. So those are the the changes that I made. 5:57   Sai Mohit Bekkem Mani Prasad: Please mute yourself 6:09   Sai Mohit Bekkem Mani Prasad: what I mean. 6:12   : Could you please mute yourself? 6:14   : Okay. So 6:18   : let me 6:23   : go here again. 6:24   : Yes, each. Id is a a faculty. So just to go back here. 6:29   : So that's what you have, instead of having the now the the name. So the faculty you have, an Id 6:39   : So it it's just a progressive number, but it's a a unique identifier for the a 5. Okay. So 6:46   faculty number one to those courses, and so on. 6:55   : All right. So we we talk about the 7:02   : texts we we talk about text the in a 3 different ways this evening. 7:08   : So one would be using the basic python functionalities. One will be doing a an actual, let's say text processing. 7:15   : and the third would be from a seminar that they gave 7:30   : this morning for the Phd series. 7:37   : I want to stress a few points out of the presentation, and we will spend about 10 or 15 min of that. 7:46   : All right. Okay. So what is the text? The part singer? It's a kind of a common task when you deal with the string. So when you deal with with text 7:58   : erez agmoni, meaning taking a pieces of a string and doing something that can be useful for the type of analysis that you are doing 150. 8:11   : There are 2 different ways to do it. The 8:22   : one is using the basic string functions so that are embedded into 8:25   : the basic python without the additional libraries, and the second is using a regular expression. So that is a 8:33   : a formal coding that is common to different languages, so there is a regular expression in our regular expression, invite on any other languages. 8:43   : The regular expressions are very powerful, but they are with quite low readability, meaning the the the the syntax is not intuitive. We will not spend too much time on that. Probably just a couple of slides. 8:54   : They're not. I mean the main part of the program. But eventually you can spend more time if needed, the 9:14   : going online or just checking the rest of the slides. 9:26   : So let's start the with the managers rings. So we already so few things about managing. So let's recap and do a little bit more 9:30   : So if you have a string like in this case 9:44   : the the word banana. You know that each character, each letter, can be addressed using a pointer. So the point the pointers start with 0 and a, and at the end of the string. 9:48   : So in this case, if the name of this variable is a fruit, then, if you want to get the the second value, the first a the   : character that is the letter a. Then you do fruit the square square bracket.   : one. So one is this element here.   : Then.   : if you want to do operations in that, you you can do that, you can assign a variable, I think, about a loop, scanning the the entire thing.   : So if you have a   : the   : it   : a variable in the pointer like in this case you assign   : the value 3 to the variable, and and then, if you do n minus one, you have a 3 minus one. That is 2. Meaning is the third element that is N.   : And you can do all kind of operations inside those square brackets.   : If you pull into too far you will get an error. So meaning the index is out of range.   : You can calculate the line I mean the length of of the string, and we already use that   : you can do loops. As I was mentioning, keeping in mind that when you loop inside the of a string you will get the the individual up, that let us that are   : part of the string   : you can obviously count. You can do whatever you want. Inside the loop   : you can slice strings. So you have this string, Monty Python, with the blank separating the 2 words.   : So if you do in square brackets, 0 4, that that means that you are starting from the first character labeled 0   : til, for that is the I mean before, and the the the number 4. That is a T. So you have the sub stream, the O. And t   : you can slice from a beginning to the end, and all from the very beginning 6 7, meaning that is starting from Number 6 up to 7, excluded. The meaning is one letter, and it's a letter. P:   : Yeah. Same thing. If you do   : 6 to 20, I mean, this could get an error. That's not what you want to do, but you can do 6 to 11, and you will get it.   : You can do concatenation so meaning you can attach a strings together.   : So with the plus over a Dora you are, I mean, speaking one to the other. So if you do, the first that is hello, and then you say B is I? Well, the previous ring, plus there   : you will have the 2 strings, with no separation between the 2, because we didn't say that it is any. So if you have the space, so then you will have the space, so you can concatenate any number of a string. So creating a a new string   : you can do. You can explore if one letter, or if a substring is a inside the a given string.   : so you can ask, and in that fruit, and you will get through. This can be just like that, or can be part of the conditional statement like in this case. If a is in fruit, then print found it right.   : And in this case, because the value of the variable fruit is a banana.   : I will find the first a, and that will get found it.   : You can do comparison so you can compare strings.   : I mean. Sometimes you may want to do that. You are in a loop, and you are a a set of that you want to compare with the given string and do something.   : So that's the way it would be.   : You can have a quite a lot of a subfunction or a functions that are part of a.   : So those are some examples lower. You will lower the value of the I mean. If it's capital will become more. Let us, If it's more, let us say the same.   : You can   : transformer again, one a string in all lower. There are a bunch of other functionalities that you can use a capitalized center, and with   : th this is something that in a loop or can be useful. You are analyzing a bunch of things so, and you want to do something if the string is ending with a given value.   : and then all the rest, and if you go online you would see all the lease of a libraries or   : functionalities that you can add.   : You can search for a a given element of the string.   : I'm. Saying Elementa, because it's threeing. It can contain a letters, but also non alphabetical characters.   : So you can do find the Na. And I will give you the position of the beginning of the this substring. That would be the position of number 2.   : If you try to find something that is not there. You will get a negative value.   : You can do, search, and replace so you can. so we. So what the   : so sure can be replaced, you can replace one substring with something else using replace.   : You can eliminate the wide spaces and special characters at the beginning or at the end of the string, using the strip   : command. So if you do a 3, me, so that you are going to eliminate the   : widespread spaces and special characters on the left are means on the right, if nothing is specified, that is, both the left and side, left and right   : so, and that's an example of application. This is particularly useful when you are reading a Csv files, so that we eliminate the and the line that you have at each line   : an example. You have a this long string. and I mean an example of application, or some all the things that we did so far. So you have a   : This is the data. This is the string that you want to analyze. You want to find the at symbol   : using the at pulse and the did I mean calling at this position at possible using find, and if you print the the value will be 21. That is the position where that is.   : Then you want to find the   : a space starting from the 21, meaning that you will get the this space here, the the first one.   : and then you can extract the data from this initial position, and the   : position of the at symbol plus one till this 31 excluded, meaning that you are slicing this piece. I mean.   Yeah, there is a piece missing here   : this piece here   : regular expressions, or I guess you can go on a Wikipedia, because again, it's not something that is available only in pied on that, but is available in a many different languages.   : It's kind of a cryptic the way it is shaped. So those are the call months that are making the regular expressions.   : So, as you can see it, it is really crypt gala, so it is not particularly easy to use it.   : and that's why we have not.   : I'm. I'm. I'm. Not asking you to use it, but sometimes so the basic functions, so that are in Python or may not be enough, and you may need the some more power to slice your a text, and then you use the regular expressions.   Spurthi Raj Nagaraj: So I would keep all of them, but you will have the slides.   : Now we talked about the passing. Let's talk now about we are mining   : so mining text. It's either a a natural language processing a text mining, whatever you want to call it.   : Why, we want to do a text mining what we want to mine a text because there is a lot of it   : and text. It is an expression of language, and you must use language to communicate meaning if we are able to get some form of insight from the text.   : we would get knowledge about the domain the text is related to. So if you want   : to know what people is doing in a given field. You collect the data on that particular field, and you do some   processing of the text   : we mentioned Med on time, so that there is a sort of an explo. All the data, but most of the data that is exploding is not structured is is data that can be. Text can be. Videos can be featured.   : The size in terms of 5, so obviously is no comparison. Not so text.   : It's more files. Nothing compared to any much or or a video. But the number of those small files   : think about a text message or a a tweet. So those are a small fights carrying a quite a lot of information eventually.   : So I mean.   : you read about the the Silicon Valley Bank that probably will, and the crack they had. That was probably originated by a tweet, by a leading investor.   : So one tweet with the power of igniting sort of a disaster in the financial industry.   : So that's an extreme case. But again, mine in text, the can really give us an advantage in the analyzing a given domain.   : Examples of obligations of text mining, analyzing a court of that's analyzing a research articles, quarterly reports, customer con comments. All of those are our basic examples.   : You can analyze emails to eliminate spam, to create a sort of a prioritization or a categorization of the inmates generating a automatic response. So this is just to focus on the emails.   : So there are really a lot of of applications.   : But dealing with text the is not exactly easy, because   : Texas language has been created by few months for you months, and we have a much more knowledge of the domain that what is in the specific text. So if we say apple is apple, the company or apple, the fruit.   : we know by the context, we know by a portion of what is the common sense.   : So when you, when you talk about the vehicle, so it can be addressed as a as a specific brand or a automobile. But it's the same thing   : when you read the one company merge the with another. But you know that one of the 2 is bigger and in a in a best financial shape, you know that is not a merger, but is an acquisition.   : So those things are not, they are.   : If you have something on a table, you have a a stapler. You have a notpad and things like that. Then you say, okay, what is on the table is related to office.   : But there is nothing saying office in the objects in the things that they are on the table.   : You know it, because you know it. So that's the common sense ere   : it's an unsolved problem as today.   : So some of the terminology that we use in the national language Processing will not go through each one of those, but I will jump from one to the other   : ere   : tibi gala examples. So our articles pronounce. So. They have a a a grammatical role, a a syntactical role, but not a semantic role.   : And most of the time you want to eliminate them.   : The part of speech. Tagging is a. When you have a phrase, you have a a a role that each word or set of words are playing in the phrase. So if you have a a phrase like   : I'm going somewhere, the I is. The subject going is the verb, but somewhere is I'm going to the movie theater is a noun.   So   : subject the adjectives birds are part of speech.   : So when you analyze a a phrase when the phrase is fractured or like that. Then you may want to tag each one of those words or a group of words.   : for up   : the grammatical role that they play in the phrase, and this process is called the part of speech stacking.   : So there are a   quite a lot of other terms that we use up.   : We will go through some of them.   : So when you deal with the a text, there are several things that you didn't do, so let's focus on one of those we we talked about other   : later on during this class   : the information extraction. So it's a generic term. And what does it mean? Extracting information? It depends on the information that you want to extract   : can be just some statistical values. The number of people with a master degree within a a group of people, the the people liking chocolate in a a group of consumers   : things that that so you think that is 100, but it's not. I don't like chocolate. Just   : name one.   : So it really depends on the type of information that you want to extract. But before doing the extraction of information, you may want to do some preparation.   : So preparing a text. So it's more complex than preparing a regular data.   : because it could be misspelling. It could be synonyms. So things like that. So when you say Stevens, you can call the University Stevens Steven, Since you top technology, S. I. T.   : E. C.   : So if the goal is to count, you have a bunch of people. How many of them went to Siemens? So you want to be sure that you count as one. Either is the way that   : the Institute was a I mean input it.   : So let's go again. Let's say with the text preparation. And again, there is some basic cleaning. That is pretty much what we said the sometimes most of the time you want to do a so called removal.   : Again, if you want to get the semantic sense, you really want to eliminate those elements so that they are not adding semantic value. But the things are kind of 3. G.   : So one of the things that you normally do is to   : create what are called the engrams. So from 2   : I mean an iga 2. That is a diagram system engineering project management. So those are   : 2 words with one unique semantic meaning. So project as some meaning management as a some meaning project management as a specific meaning.   : So this process is called the and Gramminga.   : that can be from 2 to more weapon of massive fraction.   : So it's basically one logical entity with multiple words.   : if you remove the so forth. So then, let's say school of business. So it's a it's a 3 gram. If you remove all the you don't have the 3 gram school of business anymore.   : So you may want to remove the the so forth, so later on in the process. But at the certain point you may want to remove them.   : Stemming limitization. That's another step. But that sometimes is is essential. So when you use a words for the same meaning, you want to count them as one.   : So we mentioned. If you want to measure the number of vehicles that are mentioned in a text, then you want to count as one Kara, because things like that   : there are 2 ways to do that one is more a statistical, so that is called the sending. So you take. And and this is what the automatic correct or in our phones are doing. So you start typing, and then you see a Bo something telling you what it could be the entire what that you are typing.   : So it's basically taking the character so that you typed. And then I get all the the words, starting with the those characters, and then he's adding a, and this would be already staming.   : But in this case he's, adding a a another, algorithm that is a suggesting over those words, so that you use more frequently   : in some other cases. When you have a things that are previous mart, they are suggesting that those that are semantically connected to the phrase that you are writing. But there are very few of those automatic, correct or that are in place.   : So that's an example of all that standing limitization. But I mean the the resulting war. The may not be a word. It it's really a common denominator.   : All the all the words that you have. So if you have words with 10 characters, World W. What with 3 characters, chances are the resulting, will be award with 3 characters that may or may not have a a a semantic meaning.   : Limitedization is more on the semantic value. So again, if you have a car automobile vehicle, you have a vehicle   : so. But at that point, when you have a semantic approach, a semantic problem and a semantic solution, you need to have a semantic approach, we will go there in a moment.   : so I will keep all of that, and I will go. I will go here   : to talk about the natural language toolkit. That is one of the first libraries for a natural language processing   : I not a a big fan or a an Mtk. The main reason is because it was created that many years ago.   : A. And it does   : all the basic tasks. But those tasks are done the way we did the 10 years ago.   : meaning. All the portions that are more related to the semantic side are based on the on   : some   : training information that our   : not active anymore. So   : language is changing in time. Language is domain-specific when you use a met on the. That was a working fine when we wrote the letter, so not let uss characters, but the last.   : and then we are riding very short text messages or other, eventually using emoji or using not   : in traditional languages.   : then the metal that's used before or that it's based on a detecting patterns. So in terms of the structure of this, all of the phase.   : part of speech talking is playing a big role, then that that that is not good anymore.   : So the traditional and Ltk is not really effective anymore. The reason why i'm still teaching is because it has all the elements that will help you understanding what are the challenges that you have in in national language processing.   : It's a library, so you can import the just like you do for any libraries.   : In this case I'm importing, I mean. The name is from the inventor of all of the printing machine.   : and   : probably the right pronunciation. Be good America, and it is a project for a collecting all the books that they've not covered the by copyright anymore.   : So it. It. It's a collection of text, and we use it for our practicing.   : So last text that you can download and and you can eventually play with it.   : You have stop words, so I generally don't use the store ports so from a an itk. But I have my own file for a so the reason for that is because   : I want to have control of the list awards. And then, considering, stop what? So? Because eventually I I can replace some of the words, I can add the some of the words depending on the context where I am.   : So let me skip all of that, and let me go directly.   : That is probably easier. Not if I do it any way.   : Okay, let me go here. So as part of a an Mtk.   : There is a a   : library that is called whatnet. That is a   it   : intelligent dictionary. So you have all the words structured by the   : so. In this case, as an example, you have the animal kingdom, and you have birds, fish, and many other, and then bird the can be a bunch of options, for example, a Kennedy eagle, fish, bunch of options, for example, trout shark, and you can go all the way down.   : So if you want to analyze the similarity between 2 elements, what you do is basically measure the distance, the degrees of separation between those elements. So, for example, a trout and a shark or more similar.   : then I don't know   : a a, a, a trout, and a bald eagle. So   : proud. Shark they are! One degree of separation. Route and moldy go are 3 degrees of separation.   : so they are less similar. So measuring the distance in this kind of 3 is use the using an algorithm that is part of a an Mpk: that is, the lean Alli, and a similarity.   : It is actually measuring just the distance industry. So   : this 3 is a taxonom is a an organized the dictionary.   : and can be useful if it's related, if it's exactly related to your domain.   : meaning that you cannot use the the taxonomy for the animal kingdom to analyze plans because the names of the plans are not there.   : So all for any other thing. So there are. Taxonomy is a for a categories of me. So there is a taxonomy for the news within a natural language to get. There is a taxonomy for other things.   : but those taxonomy is at the near so, and language again changed a lot. creating a new taxonomy. It's really complicated, and it's something that that needs to be done with the human intervention.   : So meaning it's a lengthy process is expensive. meaning that you cannot   : replace a taxonomy each time you use it   : or each week.   : But unfortunately, language is changing fast, and domains can be very specific domain in general. But then we have a jargon that we use in different   : conditions. Circumstances so. But anyway. that's one of the ways. Knowledge is represented. So this for motor presentation is called the symbolic representation.   : meaning in this case. You are representing a form of knowledge of the animal kingdom   : that will tell you how the animals are out related the one to the other. So you are representing this kind of knowledge. There are other ways so to rather than the knowledge in that this   : kind of a formal, symbolic way.   : One way is again at taxonomy. So   : another way is using rules. If this, then, that the concept of taxonomy is, it can be expanded to a ontologies.   : One topic can be part of multiple taxonomy. So a taxonomy is like a 3, and ontology is like a forest.   : So   : Egola can have a a role in the animal kingdom, can have a role in a rock music, and so on. So each time depending on what you are analyzing, you may want to use a one element, I mean   the element in one context or another context.   : but it's kind of easy to understand how complex is to navigate into all all of that.   : So let me jump   : to   : here for a second.   : so one that package that I use the in one of my courts is that is a no forwarding, so it's called what the Joe or Dj. That is, and   : a Java script that is, analyzing texts   : and is a creating networks out of the text. Networks are created based on the the concept of call quorants   : who core answer means words according to together, or a one to the other   : in the text. then the way you do it it. It really depends on on those 3 parameters.   : So you want to consider a only work, so that the appeting more often than that a certain number of times you want to consider a pairs a couple of words that are ping up more than a certain number of times.   : and you want to consider related words so that that are appearing within a window of a awards, meaning the degrees of separation.   : If they are one next to the other, they are diagrams. If the window is too large, then you will have a a a very large number of words, so that they are putting together, and the the network that you will create will be less readable and usable   : cool.   : We want to have a a network of words because the network awards. So it's a kind of a first example of knowledge graph you can create class that's out of that. And those class that's will be   : topics within that the text that you are analyzing.   : So if you analyze in time. A text like you are collecting information from social media or whatever is the source, and you want to see how the opinion of people is changing. You can have those knowledge crafts.   : You can have a date class setting and see how those classes are changing. So class. That's a a a not labeled, meaning that you cannot say this, this topic is ready to. I don't know a specific   : basketball team.   : There. There is no label on that, but you may see that there are the play. Yes, of that team. So, meaning okay.   : this cluster is about. I don't know the nets or so on. So again, using the the   : graph metaphor to understand language. It's really important, because it is a way to recreate the somehow the structure, then the text may not have   : so.   : but not all the graphs are created in well, because in this case we are using the the metaphor of the call quorance that may or may not work.   : So the some Shauna is a wars that are appearing together are related.   : but maybe not so. That's something that we may want to consider. anyway. So that's   : what the J. Just a word the on a sentiment, analysis.   : sentiment, analysis is a it's one of those analysis that became a very commonly used, but they really don't have a much of a of theoretical ground. So what is sentiment   : it's really difficult to define. So I it might   : emotional analysis. I really use a sentiment. I rather use the emotion so   : based on a, on a, on a classification of emotions. So I measure the distance between the words in my document, and the entire document to those seeks a. The motion so depending on the classification you have.   : then how you measure the   : But   : sentiment, analysis is, it's pretty common. There are a different ways to do it. So most of the time is a a supervised meaning. You apply techniques from machine learning, and in particular the most commonly uses a support vector machine.   : A, and then you classify given text, or better, you create a model   : on some training data, and then based on the the behavior on in the training data, you do the classification for the rest of the of your document.   : You can use a a Lexig on a approach meaning, You can have a huge madrics with all the words and the sentiment   : associated to those words, and then you can do a look up and get in the sentiment for each word. Add all the values   : for positive, negative and neutral, and you will have that after a normalization you will have a the sentiment for the entire document.   : It is a sort of a supervised, even if someone is calling that unsupervised.   : So   : that's   : pretty much on a text mining, we pied on. Let me go now on using a python for some examples.   : So let me go   : all the way here and let me do a little bit of recap over this natural language toolkit.   : So keep in mind that that natural an ntk is not part of all the standard libraries, so meaning you need to install it. So just to be sure that to remember how to do it when you go to   : you, go to settings.   : you go to a python interpreter, and you have all the libraries that are part of the of the   : that you are doing.   : Yes, ma'am.   : so if you don't have the package that that you want. Are you just to plus   : this case and   : L. T. K. Actually, you already have it, but eventually you select it. You still package, and you are good to go   with Python. If you use a a library that is not there, I will be   : underlying in the dotted red, and then, if you who were on it.   : you will have the option to install it. So, without going all the way on a settings, you can do. Just write the import. If it's not part of the existing libraries, then you will have the option to install it.   : But anyway, once you have a installed the the library in your python project.   : Then you can do the importer. This is a text that I copied and paste from the New York times few years ago. and then   : see   : how that some of the the Ntk. Comments so it can be used. So this this one is a is is a token is a tokenizing a text that means splitting the text into words.   : I mean you can do without the token eyes, or just splitting, transforming the string into a list, and the result would be the same.   : But anyway, with an Mtk. You have a word to organizer. You pass the text to the function, and you will get the results.   : This is a yeah measuring the most frequent works or tokens. There are other ways to do it without an Mtk: but that's fine.   : So i'm taking the 2 most common.   : This is for printing the part of speech for the text I have.   : This is to calculate the bygrounds, keeping in mind that the the way an Ltk is doing it is based on a a, an existing list of diagrams. So it's matching the words the one that next to the other.   : with the list of diagrams, so that are embedded in the in a an ntk.   : That means if you're doing something that is a a no sound, that you have your own jargon, or it's a domain specific diagram.   : I don't know cardiovascular, or you have a   : something different. That there is very specific one domain that that diagram I cannot be in the list of diagrams, so that an Mtk. Has, and you will not get it.   : I not using the diagramming from a an Fdk of my own?   : This is a extracting the stop words and   : printing it   : in particular, in printing the words from the text that they are not so. and this is the least, all the elements. So from the   : If I run it   : so, that's what I have I have the list of token. So again that was the text in the list of token. So there is no distinction between a regular war, I mean a semantically relevant words, and so forth. So you have all the words.   : then the most frequent with the frequency. the part of pitch tagging. So if I see   I don't know. Sarcastic in is an adjective.   and so on.   : Those are the diagrams again. It's a it's a very basic approach. It's taking   : in this case   : I mean even the so forth.   : Those are the non-soft word words in the text. and this is an initial list of of the   : So that's an example of how to use a an Lt.   : We mentioned sentiment. So sentiment   : i'm using a library that is called Vader. That is the best you can get it's more, I mean with the an ntk. You can do a sentiment analysis, but the way to get it is more convoluted.   : You need to do a part of speech tagging first, because it has a different tables for different   : end of the   : meaning that you have a sentiment I mean. Most of the sentiment is on adjectives, but but there is some sentiment in some words, so there are 2 different tables for a sentiment for denounce, and and for the vote   : with. Bother. You don't have the distinction that you just pass the text, and you will get the the sentiment.   : In this case I use the a function to clean the text. I generally stronger encourage you, if you plan to work with text, to create the some, all those functions that you will use continuously.   : So you have the text cleaning. That is one of those cleaning means eliminating the so forth. So it means eliminating a so that they are too short.   : presuming that if a word that is too short, it will. There are no semantic meaning.   : and that's basically it. And then, if a is a numerical, you want to eliminate them. So I mean that   : this cleaning is very basic, but it's just an example.   : So you have a   a file of the that you are reading   : you have a the soapore file you. You will pass through the cleaning function at the minimum length for the the to be considered acceptable.   : and then you start grieving the 2 files and populating the this.   : Then   : you lean the text, passing all the elements to the function. So the file, the content on the file, the minimum length, the for the word, and the list of so forth. So   : so, and the end in a clean text. You will have a a clean text. then you call the the analyzer, for the sentiment. Analysis   : is reading strings. Not least my text is on a list, so I want to transform at the least into a string   : us the string to the analyzer and getting the positive and negative, and then printing it.   : So that's an example of how to use a for sentimental.   : So you have a positive negative, a new roller.   : keeping in mind that most of the time   the   : high Yes, value is on the new draft, because I mean not many words really bare a positive or negative value. The same word can have a a little bit of both. So if you think about the exception, i'm not. It's all positive it's 100% positive.   : But if you say cool, it's some positive and some negative 1:   : depends on the contest, but this is too sophisticated, for those are great, so 1:   : so it's it's a little bit of both, and this is the same for the entire text. So this is working on on all the texts, meaning, doing for each word, and then for the entire text. 1:   : the normalize, the summation. 1:   : What cloud is is another, I think, that is commonly used as a way to to have a a sort of a presentation of a text. 1:   : So the word cloud is made in a way that works so that our more frequent will appear bigger, less frequent. As a As it's a graphic representation of the frequency, all the words. 1:   : So this library there is again another library that you need to import in your python. So you import the the work cloud, and eventually the so forth. 1:   : The library needs a plot lead to work. 1:   : So i'm reading. A text again, is from the New York Times, but doesn't really matter. 1:   : and I I reading the text, I'm replacing the the end of line with the space. 1:   : Then i'm changing and removing all the characters. So the special characters and the space, and i'm creating a list award. 1:   : then that 1:   : this is I mean it. Could 1:   : I could have done with the cleaning function, but I did the on the fly. So in this case i'm. Making sure that the the word is a in a in alpha medical. 1:   : and i'm lowering the the characters. 1:   : And then what cloud? Just like a sentiment? Analysis is not taking list, but it's taking strings, meaning and transforming the the list into a string. 1:   : adding some elements to the so forth. 1:   : So those elements are elements so that they know are very popular, and that they would be very big in the work. Cloud, and I don't want that because it it would hide the words that could be more insightful. 1:   : Then I, passing and creating a the work cloud with some of the parameters so. 1:   : and generating the work cloud. I'm. Storing the work cloud in a 1:   : graphical file and Png format whatever. And then I'm. Showing the cloud on the screen. 1:   : So if I run it. 1:   : that's what you have. 1:   : I mean that, considering the name of the file, I could have eliminated the trump, and because was the debate, the Presidential debate. It bad name. Hillary Clinton should go when you remove those words, so that's so big. The other words will become bigger 1:   : so, and then at that point that to me would be more insightful. 1:   : All right. So that's pretty much it 1:   : let me go with the with the last piece 1:   : I I mentioned that I gave this presentation this morning for the Phd Seminar. So 1:   : that is a shorter version, or what I gave last week at the C. Sara. That was 1 h. So in this case was a 25, me 2020, 25 min. 1:   : I i'm also working on entire courts, so on a a natural language processing that we'd be off at the hopefully a nextms that are most likely in the following one. 1:   : So 1:   : I will go relatively fast, because I just want to call her a little bit more. 2 3 elements that we didn't 1:   : discuss in the view slides so. and right now there is a sort of an hype on a natural language processing. 1:   : Since the lounge of a chat gpt in the middle of November. Everybody is talking about national language processing, and it's becoming one of those 4 topics that you read about pretty much on a any newspaper, any blog, any journal, the TV you name it 1:   : So it's creating some sort of an IP. So those are from a. 1:   : then user 1:   : 77% increase spending over a 1218 months in the natural language processing. 1:   : That's the the distribution of growth. So 39% between one and 10, more than 11% to 38%. So 77 total is this number here. 1:   : This is basically how both manage those. I mean. 1:   : this increase can be quantified in terms of money. So we are talking about right now. It's 12 billionI mean, I consider that just the the money that 1:   : Microsoft invested in open AI was 10 billionSo I mean, I don't know if this has been accounted. I mean, it's 2,021. So it was not accounted, but just to give you an idea. Only that one investment is a double on that match 1:   : for what this Nlp. 1:   : In terms of business. 1:   : This is what we had before a chat gpt. So data, production and governance, knowledge, management and classification chat boats, including our CD lakes and things like that. 1:   : What is what are the technologies or the 1:   : technological domains that we cover? The mosques name, entity, recognition. This is like saying award is related to people. It's related to countries. It's it's ready to industry. Things like that 1:   : chat bots again, creating chat about so natural language generation. This is something 1:   : that is growing, and I see quite a lot of potential for that. So you can generate language as a for all presentation. So one of the next features that the blue may have in the future it could be. Instead of a 1:   : giving you a chart, I will give you a a plain English representation description of the results. That's pretty cool, and this can be done generating language based on 1:   : it. I mean that not the conversational description of it. 1:   : What is natural language processing? So we mentioned, that is a subset of artificial intelligence, and is using linguistics and machine learning models to let computers process. Human language is a combination of all the computer science. 1:   : A. I human language, and it's kind of a a rolling in a in a different areas from a generic natural language processing to a natural language, understanding. 1:   : dialogue, management, and a natural language, a generation. 1:   : Why, we do not a a. Nlp. Now we have a quite a lot of a digital text. Now, in the past we never had that much. 1:   : So when I started working in this area. It was a mid eighties, so we didn't have a much of digital information. There was no Internet and things we are not. 1:   : There was quite a lot of 1:   : the hardware we have is way much more powerful than what we had before. 1:   : for some of the processing hardware is really essential. So we will go into the numbers for a chat gpt. That will give an idea of what a lot of processing could mean. 1:   : Then the languages, so in the past. We didn't have python. 1:   : So we had the languages with no libraries, meaning. When you want to create an algorithm you need to write it from scratch, and then you can create your own library. But it's going to be your own library, and you I mean the the the the language didn't have the concept of library. 1:   : meaning that all you have to do is to save those lines of code, the copy and paste into your code. 1:   : So the type, the the languages that we use for things like natural language processing. We are a primarily list. We use the 1:   : a language that was created by by the now the fact, the digital equipment that was rule base, that was called the O. Ps. 5. That was pretty cool. But again I I could do only rules. 1:   : At that time I was developing what we call the knowledge based system, so our expert system. and that the assumption was the experts. It could be represented by rules. 1:   : So we created the rules that we're representing the knowledge of the expert. 1:   : and that the O. Ps. 5 was a a relatively easy way to do this. Roots writing 1:   : up again it for all manual. 1:   : We mentioned the challenges, so how we can use a an endp we can use in a direct way, because we can extract the valuable information from text and and use it 1:   : as a results that that can be used. 1:   : I mean selling the the analysis can be used to do 1:   : better services to understand what customer are thinking about our products, analyzing a new products. So all all of those are our direct values of an Lp. And then there is also an in direct value of Nlp. So you can use an Lp. To do something 1:   : that there can be a sort of a a way to interact with the a system. If the system is not enough. 1:   : the type of instruction so that you are providing are creating a sort of a customization of the service, so that the system will give you. So that's an indirect value of of an Mp. 1:   : What is language. Language, again, is for a human being, so as being created by human beings for human beings, and is an expression of knowledge. So we acquired knowledge, using our 5 senses. 1:   : and then we express the the knowledge through language. But language is a form of expression, is not the knowledge, but is is a way to express the knowledge 1:   : in our brain. The the areas so dedicated to a language are relatively small compared to the areas that are representing the knowledge. 1:   : The knowledge is a toward the in the areas handling the different functions. 1:   : So 1:   : again, 5 census, we have areas for each one of those 5 senses. 1:   : and the language is basically topping into this knowledge, collecting pieces and presenting it when you have a like a large language model, so they are a whole language. So the knowledge is embedded in the system. But this is not the way human beings are using their knowledge. 1:   : If we go back into how knowledge has been representing the over the years. 1:   : You have 2 schools of total one, the rationalists. 1:   : They believe that the knowledge is coming from a reason. The reason is somehow embedded in our brain, and we 1:   : leave the reality based on 1:   those programs that we have in our brain. 1:   : Up to some extent this can be understandable. 1:   : Other people think that was not 1:   : so. They told the that actually we were born as a black slate, and we get the knowledge through the experience. So by the circumstances that we are leaving, we get the knowledge. 1:   : So 1:   : now moving forward to current time. So the rationalists are becoming the traditional AI supporters. So the symbolic reason 1:   : I mentioned the 1:   the taxonomy is the rules. All of those are representation of a symbolic reason. 1:   : The in are now the machine learning people, so they are using the data 1:   : to get the the knowledge. 1:   : So again, there is a a street, one to one 1:   : correlation between the philosophical approach and the current approach to a AI machine learning. 1:   : If you want to implement a natural language, processing or or understanding, you can follow either one or the 2 approaches, or eventually a combination Of the 2. 1:   : There are pros and cons of each one, so the first one that we mentioned that each domain as a different taxonomy. 1:   : Logical taxonomy is a changing, constantly, meaning that you cannot really create that taxonomy and use it for the rest of your life. But you need to update the the taxonomy. You need to change eventually a taxonomy. If you change domain that there are 1:   : space in terms of domain, there are space. If you see these in terms of jargon meaning in theory it could work. If you have a an army of people constantly updating the symbolic approach. 1:   : There are companies that that are leaders in a natural language. Processing using a taxonom is only a and they are publicly traded and they are big. Some some of them I 1:   : I know personally them in several years, and they are doing great. 1:   : But again, there is a quite significant limitation in the way they are doing 1:   : on the other side the machine learning. So it's based on data 1:   : sometimes. So there are several limitations in this approach. So the first one is the data you have really represent in the reality that you want to express opinion upon. 1:   : and that's the first. And you can now 1:   : really be sure that you have all the the text representing that reality. 1:   : the sag on the is a 1:   : the algorithm that you are using To get, then to express the knowledge appropriate for what you are doing, and that's another big. If 1:   : so, each one of the approaches some. Yeah. As a a pros and cons, the large language models that we are using a chat. Gpt is definitely the most known of them is in this category. We we talk in a moment about that. 1:   : So when you build a. An you need to. You can extract the information, or you can create a conversational system. 1:   : or you can just do some text manipulation like a translation, a summarization. 1:   : If you do information, abstraction, you actually build a a pipeline for what you are doing. That is, from a ingest in the document to do whatever is the information that you I mean the the goal of your information instruction. 1:   : Sometimes you can stop at the certain point. I'm. Okay, with the statistical analysis some other time. So you want to have visualization and so on. 1:   : So it's a pipeline, and you need to follow the different sets in just the document, that tokenizing, cleaning, removing the so forth, and or doing the programming, and so on. 1:   : When you do statistical analysis, you need to keep in mind that that you are dealing with text. 1:   : You are dealing with language meaning. You need to see the metrics that you are extracting from this standpoint, heading a semantic meaning. 1:   : So the most frequent words, for for example, are the most relevant words in the text. 1:   : So being relevant and being frequent, is not the same thing. So that's the semantic interpretation of a statistical value. 1:   : The entropy is a measure of the diversity and unpredictability of the language. 1:   : Obviously these more elements, when you compare values because you you have a number, and then number, let's say, is not telling you much. When you compare to documents you can see the differences. 1:   : the number of unique words can be interpreted as a measurement, or the richness, diversity, or specificity of the document. So again. 1:   : the common denominator of those points, when you do. Statistical analysis is, is not like statistical analysis. So to get a number, but to get a meaning out of the number. 1:   : When you detect topics. Things are becoming complex, because again, topics may not be in the list of objects that you are considering in your text 1:   : meaning. If you use a 1:   : a plain, unsupervised machine. Learning approach. You may be disappointed because 1:   : the topic is is more a collection of a 1:   : different words or N. Grams, with no one being the leader. 1:   : When you move to the semantic space ending topic detection. You are kind of both the line. You can do a topic the detection just for using on those conglomerate, or you can go 1:   : on the semantics. But when you want the semantic side that you need to understand what is in the 1:   : a text that you are analyzing when you go in. The understanding is about knowledge. So the TV gala way for a representing the knowledge is a supervised or unsupervised. 1:   : supervised the means that you have knowledge about the past. We briefly thought about that when we told 1:   : briefly about machine learning. So you know about the past, and you use the path to classify the future unsupervised, that you know nothing about the past, and you extract the whatever meaning you can extract for what what you have. 1:   : The third way is reinforcement learning where the system is generating outcomes, and then that, based on the success or unsuccess of the outcome, meaning. You need to have a score scoring system 1:   : based on the success you use, the configuration that you use to to generate that result as a information that you will use in a supervised learning. 1:   : So reinforcement learning is basically a self-generated supervised learning. 1:   : hey? 1:   : E. C. 1:   : Either 1:   : a pipeline with humans in the loop you months. So in Chat Gpt does more than one thing. We will have a a chat on that. 1:   : One of the largest se segments in AI is a a ginger topics. 1:   : So you have a of people spending all the days reading text and labeling the text with the tags. 1:   : It's a huge market borderline slavery. They are underpaid most of the time, even underdeveloped the countries. 1:   but the entire industry somehow rely on them. 1:   : Even a chat. Gpt is using a you chunk of them. We're talking a moment 1:   : so. and 1:   : when you do information, extraction with the semantic approach you need to decide that what is the way you want to go 1:   : so, and it really depends on the type of text you have. When you have social media you can use a the social network to kind of replace the lack of a conversational structure. 1:   : When you don't have that, then you need to recreate the structure in a different way, and the way people is doing now is transforming the text into back to us. So this process, that is text vectorization. As a long as we will spend one slide on that. 1:   : or you can create graphs. But I mean how you create the graph. So it's a different story. 1:   : 2 libraries that are more most commonly used for for information, instruction, and Ntk: we spoke about that more recent spacey. That is a more reach. It does pretty much the same functionalities as a as an ntk. 1:   : plus 1:   : all the portion that is more on the machine learning. So you have the vectorization, and you have a a bunch of things on top of the characterization. 1:   : So again, an SDK is kind of outdated, but it's still doing some job. But if you are analyzing the tapes, so it is well structured that you are analyzing a book. Chances are that an Fdg. Can do the job 1:   : again. In real world communication today is different from what it was years ago. The conversations are more chopped or more are a shorter, less structured. 1:   : So 1:   : creating a graph is a way to recreate the structure. 1:   : You, my Phd. I developed the An algorithm to extract knowledge from a social media that was based on that in this case was 1:   : the tweets tweets. They have several parts. I consider the sender timestamp, and wars. 1:   : and I created the the 1:   : network of work so, and people so 1:   : ascend, or is related to award, if it's using the word in it's in in that tweet. 1:   : So it's like in this chart. That's all the black people or the red that they are using. 1:   : So that's called the bypass that network bipartite, because the 2 sets are a large degree. This, Jo: You have people, and you have words. 1:   : Then from that you can create a a network. That is a what's only so 2 words are related. If you use the by the same person. 1:   : then because you are using a unique words. This will become a network more articulated like this one. So. but it's a network with only words is 1:   : semantic networks somehow, and then you can create a class that's I use the drove in the community, the detection method that is similar to a clustering. 1:   : Algorithm and then you have those clouds that are topics because of the elements all that those clouds are. 1:   : And I had the student validating that with the I mean quite good returns. 1:   : When you don't have a a, a, a social structure that can help you. Then you need to create a network in a different way. So we mentioned the what. 1:   : What did you want to J with the call? Qu. On the next step? Was that 2,013? That what to that? That is based on conditional pro probability of award 1:   : being related to another, because 1:   : so it is the condition of probability of one word, the appearing in the text because of another. 1:   : So this will generate a sort of matrix. You reduce the matrix, using a different methods. 1:   : and then you have a one word or one, and grammar that could become a a sequence of number. So that is a vector So that was a 2,013. What to back what is called the shallow neural network. There is a network with one hidden layer, so input layer. 1:   : hidden layer of output layers. What to back as one hidden layer. 2,017. Attention is what you all you need is based on the attention algorithm. 1:   : And it's basically considering not only the proximity, but also the relevance of the word within the text 1:   : the neural net worker became a deeper neural network with a lot of he delay it. So that means that for 1:   : creating the model with what to back on our computer was a a matter of hours in that. But it is a matter of day. So Gpt. Chapter Gpt. In particular, Gpt to. That is the base for a chat, G gpt, or a Gpt, for that is the latest are all based on the the transport mass approach. 1:   : So the complexity is growing. So if you can see that that that it was mentioned before. Yeah, the 340 medium parameters. 1:   : I mentioned that it is all about the neural networks. 1:   : so 1:   : neural network. So they have an input and output in the input they have weights. So the parameters are actually the weights in the neural network. 1:   : So. having a 17.2 billionThat means that those are the weights, all the network that means that those are the notes of the neural network. 1:   : Just imagine how big that can be. 1:   : That's an example of how a chat gpt was trained so. 1:   : And just to be clear what those a large language models do is basically 1:   : they they have a a mountain of data. 1:   : They analyze the data, tagging the data for their content meaning. They are defining patterns in the 1:   : the that they have. 1:   : Then you have your request. Your request has a some patterns. and what they do is to match the the patterns in your request with the partners that they already have. 1:   : and then they put the 2 together, and then they will present it as a a, a conversational approach. It's basically like Google with the conversational layer and compiling the the answers 1:   : that's all they do. So I. 1:   : They are not intelligent in a strict term. They are just a pattern matching pretty much what Google is doing in a conversational way 1:   : to train this system that's meaning to to determine the the pattern, so 1:   : meaning that open AI, that is the company. I will develop the chat. G Gpt. Spend that 4.6 million in that energy usage. 1:   : It's a the same amount of energy that would be used by more than 30,000 American households for a day. 1:   : So that's how expensive is to train those systems. 1:   : keeping in mind that the the human mind is using a fraction of a fraction of a fraction of that amount of energy 1:   : for the entire lives. So 1:   : meaning chat, gpt is brute, for it is not an efficient system. So if we want chat, gpt to do more, we need to rethink the entire way. It's a a handling knowledge. 1:   : So the way it was training them was in part automatic, and in passing all. So you have this amount of a a massive amount of data. That is a that is pretty much what is in this slide. 1:   : So it's a a 1:   : 45 TB of a of text in the 90 languages, but primarily English. So 93% is English. That's an indication of how by as the is the system. 1:   : But nevertheless, so you have this m 45 TB of text. So they took a a subset of it, and then they had the you months going into the text and doing the the dirty job, or removing in appropriate content. 1:   : Then the second round was to tag part of this text 1:   : for topics. Then they use the this 1:   : this part of text. We then now go it to expand that somehow to a large amount of text. 1:   : and then having a an approach based on a recurrent neural network to get 1:   : I mean the answer. I mean to match the answer from the You months so expanding the size of the training data set that. And then, once you have a a a data set that is large enough for training. 1:   : using that for the entire data set. 1:   : So it's not unsupervised that you have humans in the loop in 2 states, actually in 3 States. So the first stage that is the cleaning up, and that's okay for inappropriate content, and there is no automatic solution for that. 1:   : The second is for tagging, and the to the is for creating this core or evaluating the the answers from the recurrent neural network. 1:   : So it is an an expensive process, and it's basically working only in a bad way. So you don't have a a chat Gpt. Real time. 1:   : So for 4 months, using a A. Of the that was created in 2,016, using a data set to evaluate the the semantic results. 1:   : It's working. Okay. So you see 0 shock model 0 shot, meaning no pre-training that 0 shot not performing. Well, so you need to have a you once in the loop 1:   : limitations again, is brute force is not really knowledge. 1:   It's a limited to 2,021. If you ask questions that are beyond that is not going to be there. 1:   : We use the a combination of a supervised and unsupervised with a more than that, I created that I named 1:   : erez agmoni room theory. I mean, we would talk about that sometimes in the future. I don't want to spend too much time on it. 250. 1:   : But we will talk about that 1:   : A, and we use it in several cases. So the most recent one is for a Siemens financial services in a project for financing sustainability. 1:   : That was a going into the Internet and getting a project so that could be financed by seen as financial services and then present to them with some visualization, and I mean expanding their portfolio. 1:   : Obviously there are a 1 million things that needs to be revised. One of the things 1:   : is on a last documents. When you have a large document, the same content that the same a piece of information can be scattered in different parts of the document, so it can be page one page, 22 footnot somewhere, but it's the same concept. 1:   : meaning. If you want to do an analysis of the document that is large. You cannot just say it's good or bad, that because there could be some good, some bad. But you will never know 1:   which is which. And where is what you want to focus on. 1:   : That's why I created a method to disassemble the the text and reassemble in a, in a visual paragraphs that are semantically related. 1:   : and then analyze the semantic related paragraphs, and then present the results so based on the those feature paragraphs or a highlighting the results within the original document 1:   : leveraging on large language models. So we will go back to that. Because so i'm talking, since so what i'm not even sure if you are still there, or you are sleeping somewhere. 1:   : and it's probably 1:   : a lot of information. So again i'm creating a one semester long on. 1:   So, having it in 1 h. It is definitely not 1:   : covering all the basis. 1:   : But anyway, we are combining those large language models with domain specific systems like the one that I developed that we want to incorporate that 1:   : A. And we are also building a domain, specific, large language models. From next week we will try to hire a 1:   : few students as a research assistance deal at the end of the Academy here to the end of a 1:   in June, actually a bit longer than that 1:   : for creating a a 1:   : school of systems and enterprises, a large language model. So we'd be initially for use that on this court. So, because it is the one I have more information on, and we will create a sort of a 1:   : automatic online tube or for em, 6, 24 1:   : so i'm not sure that we would succeed. But we will try to do that. 1:   : So again, leveraging on those large language models. 1:   : we are going through a sort of a dec atomic view of the job market. On one side the there is a sort of new job that is coming out. That is what is called the prompt, the engineering. 1:   : When you have those large language models. As I was saying before, they are working on pattern recognition. They match the patterns they have with the patents. In your query. 1:   : If your query has not many patterns, it not going to be 1:   : much better than doing a equity in in Google, apart from the compensation of my portion. But if you provide more context, if you give details on your query. 1:   : then there would be more elements for the system to match. 1:   : meaning that creating the right query is becoming critical. It's like a stupid user let's say 1:   : you need to know how the system is working. You need to be an expert in in the domain, and creating a sort of a a conversation that will give the system more elements to match 1:   : to that point. If you are good enough you will get the answer to the same query that someone else will not have. So that's the new 1:   : job of a prompt engineering. So that's one level, a a new type of jobs. So. But then, on the opposite side. 1:   : we allies, and and we discuss some of the elements that that those large language models are have a quite a lot of limitations, so they are ruthful, so they are not representing the knowledge. They are just 1:   : adding a factor recognition, they using pattern recognition, adding a a conversational layer. We need to work more on the on the the commission 1:   : part of it. So combination of design systems around the a better idea or representing knowledge, but using a mathematical models, cognitive science, software, engineering. 1:   : and then talking about software engineering, not just computer science, because it's really a designing pipeline. So it's more complex than just writing code. So writing code, the in a sense, is the easy part, but the more complex part is to design the pipeline. 1:   : creating the mathematical representation of it. 1:   : So that's the the call to me that I was mentioning that you have a a sort of simplification compared to coding that we do now, and you have a novel complication on the other side with the cognitive engineer, and you, whatever you want to call it. I couldn't find a better term for it 1:   : low hanging fruits, so we 1:   : I mean, that's what we are doing. You can do all of those we pied on quite easily. So you can do what frequency number of unique words so entropy. You can do the end gramming. You can do sentiment, analysis. You can create graphs in different ways. Once you have the graph, you can calculate the the 1:   : centrality that can be degree centrality between a centrality. Those will tell you what are the words or the classes, so that they are more relevant, more essential for the conversation. You have a page rank, so page rank. 1:   : It's a a metric created by pager. That is one of the 2 creators of Google measuring the relevancy of the page I mean page with the small P. But can be used 1:   : to measure the relevancy of a word or a a cluster awards within a larger documents. 1:   : So another element is the average path line that is measuring how distant to concepts it can be based on the distance in the graph. 1:   : Hmm. 1:   That is measured by the degrees of separation. They have. 1:   : So all of those are metrics that are that I've been created for different purposes, but can be reinterpreted and use the in a N IP. 1:   Analysis in a broad set. 1:   : Okay, so that basically all I have to say. And i'm sorry if it was a lot again. It's almost 2 h of talking. 1:   : and 1:   : I apologize for that. 1:   : So we don't have time for the in class exercise. But I will not publish the in class, so exercise anyway, and I will give you a few days to practice on it, and then after that I will post a a possible solution. 1:   : So the in class exercise is on analyze articles. So from a a newspaper. 1:   : So next class we will do web mining for this class. You don't do Webinar scrapping or a web mining. But you just copy and paste. So you go to a a web page. Whatever it is. 1:   : you highlight. You select a text, you copy it, you paste into a a a tech side. You go creating a Txc file, and then you start analyzing the Dx. Define 1:   : that you will open the file. You will use the software file. You will read that both this file that you created and the soapore file. 1:   : You do some cleaning, removing the so forth and eliminating the the non alphabetical words, things like that, and you will calculate and pre into the 1:   : 10 top words, and using the the cloud, the the py script that they attached in this section of of canvas, generating a meaning, printing and saving the the what cloud from the text. 1:   : So that's the in class exercise that they strongly encourage you to do it for a practicing 1:   : for up next week we will work on analyzing texts. 1:   : So what you will do will be to go to use this pro corona website. That is a kind of interesting website where they consider a a controversial topics 1:   : socially controversial topics. Those are some of the topics. So if you can see that I don't know artificial intelligence. 1:   : so you have a brief description of the context. Then you have a list of pros and a list of So you have a people saying it's good that people saying it's bad. 1:   : and there are a few of them. 1:   : So what you want to do is to put together a 1:   : the pros on one side, the the on on the other side. Analyze them. 1:   : So let me give you a little bit more details. so you will for use on the following controversial issues. So I don't want you 1:   : just to pick whatever you want, because it could be more difficult for us to control. 1:   : You won't 1:   : to focus on one of those 4 that are topics that are 1:   : over here. So it is in the new topics like this one is one of those 1:   : you have a pros and cons 1:   : cool. 1:   : What you do. You copy the pros and the 1:   : separate, the Dxy files. The goal is basically to analyze the 2 and write a report on the differences. 1:   : So you clean the text just like we did in the in class. I explain how to do in the in in class using Father, you will calculate the sentiment you will extract diagrams. 1:   : you will create the word cloud. 1:   : and then you will right, or the that will be based on the results. That would be 3 page so, or a bigger 1:   : must be human, readable meaning. You do not want to describe the process. We don't care about the process we want to see at this point the results. So what are the insights. What did you get out of the comparison that you do 1:   : again? It's not an explanation of the code 1:   : needs to be in plain English, if it is not in plain English. You will get a low-grade. 1:   : You will 1:   : work individually, I mean. Again, please, don't let me report to the on or board it. It's a pain in the neck for everybody, and it may have a terrible consequences for you. So don't go that route because it's not good for you. 1:   : So if you have problems, we we have a ta. You have a some of my time, so use that you are paying $6,000 for this education. 1:   : So for this assignment that we will evaluate the similarity based on the most similar of the 2 1:   : between the script and records. So even if in the model that they share with you. 2 weeks ago I I had the 2, and then there was a combination. 1:   : because we still have quite a lot of cases the similarity will be based on the most similar or the 2. 1:   : So it's the script. It's more similar of the rep or the that will count 0, and we will consider the similarity on the script only, and vice versa if it is on the right. 1:   : So you will submit the report and the Python script in 2 separate files. 1:   : Final script. 1:   : So that's basically it. I'm stopping the sharing, and I will publish all the content 1:   : again. My apologies for the long talking. 1:   : Natural language Processing is the core of my research, and I can become a passionate, and 1:   : I can carry on probably too much my apologies. But I I think it is 1:   useful. 1:   : and I mean I chat, gpt. We are all talking about it so. 1:   : anyway, so 1:   : I really thank you for being around for about 2 h. Questions. 1:   : Okay, If not, I'm stopping the record.
  
: All right so again. It's a April the fifth, and it's 6, 31, and 0:01   : this is the m 6 24 Ws. So as Well, yes, 6, 24 Ws. And let me start sharing the screen 0:09   : and let me go here first. 0:24   : So we are over here, Class Number 10, and we we talk about web mining we introduce except size 8. That is another heavy one. 0:29   : We didn't discuss expert size, 6, and for sure, like exercise 7. We will discuss both of them 150. 0:44   : I will spend a little bit of time on that. 0:55   : recapping a little bit what to do for the final, just, brief introduction. The formal introduction would be next week. 1:00   But I just want to give you some directions. 1:15   : I I would talk about Chat Gpt, because this is the right moment to to do that. 1:20   : All right, so let me start with the 1:29   : so we'd accept, say, 7, so excess I 7 was on 1:38   : some procon text processing. 1:46   : So let me go here. 1:52   : No, except size. Okay, let's go with that. That's I 6 first, except, say 6 was a probably a little bit complex. So so basically 1:56   : the data set that was a 2:09   : ere 2:13   : issues somehow. So again, data set is a Ssc faculty. Csv. And we want to deal with it, using 2:29   : So 2:42   : because Pandas is using numpy, we want to have noon pipe that we also give us a a little bit of more options so in terms of working with numbers. 2:44   : So i'm reading the file up. 2:57   : replacing the non-available with the 0 we want to live in place through meaning. We are not 3:00   : replacing lines or adding lines. 3:12   : Then I'm. Calculating and printing the number of courses for each program up per each academic here, meaning I did the group by program. 3:15   : and I 3:27   : calculated the summation somehow, for the and that's was basically a new data frame called the Df: One and i'm printing it. 3:31   : I mean, I I it is always some amazing 3:46   : and how a powerful the that could be 3:52   : Erez agmoni. So with one line, you basically do something that if you should have done with the loops, it would take 250, 3:59   : a decent number of the lines. So this is the calculation for a the same thing. 4:09   : Then a number of courses per faculty overloaded. So i'm basically creating a a a new data frame 4:23   : with index. Id 4:34   : then 4:37   : i'm 4:40   : getting those I mean the the balance is for the 3, the for for the 4 different areas dropping them on available 4:42   : calculating and printing 4:56   : number of courses per faculty under loaded pretty much the same 4:59   : again. Obviously. 5:09   : this case is a greater than 0 in this case it's less than 0, so i'm focusing on the balance. 5:11   : I could have done a function so 5:22   : passing this string 5:25   : that could have been a possibility. 5:31   : then create the the list of values for the 3 programs. 5:37   : I created the list because i'm going to pass the list to a bookie for the graph. So so 5:44   : em Ss. W. As well. Yes. 5:54   : calculating the loads. 5:58   : Then that starting the graphs. 6:04   : So X-axis 6:09   : would be the year. So 6:12   : call on, sir. whatever you decide. 6:15   : Then that creating the line graphs so Epsilon Epsilon, one at 1, 2. 6:19   : Why, Why, why 1 1 2, Whatever is that? 6:27   : How you want to call it so? Again? Those are the values that they define here. That was no needed to create additional variables. But that's fine. This is more clear somehow. 6:33   : Then creating a initializing the plot. 6:46   : naming the the file. creating a the site in the parameters plotting happy legends 6:51   : bar graph for for average load, the restractor. Similar things, so obviously different 7:03   : floating type 7:12   : under loaded. 7:15   : If you are plotting time 7:18   : courses by program 7:22   : bar graph. 7:25   : calculating the the summation for all the values that's where no pie is coming to play. the percentage 7:28   : overloads. 7:41   : So when you do the pie chat. You need to calculate the angles, and that's why i'm doing 7:46   : all those calculations here. Legend using the pie shot. and that's here. I run it. 7:53   : So let me go here for the second. You have the the values 8:07   : that are printed for the different loads for the different programs. Number of courses per faculty. 8:13   : 5 will be overloaded 8:24   : for the different years. 8:29   : I got the underloaded 8:34   : what is different Yet again. 8:37   : that's the end. In terms of printing. You have a 8:41   : programs over here 8:48   : that's 8:51   : call it 8:53   : average loads for distractors over the years 8:57   : under loads over the years 9:04   : courses per program. 9:10   : the pie shot, and that's it. 9:13   : So 9:18   : that was a exercise Number 6, and the one on the Ssc. Faculty. It was complex. So 9:20   : why I gave you this? There are couple of reasons. So one and I mentioned that when I presented that when I introduced the the the assignment 9:31   : in some of the past semester, so some students we are kind of a complaining that we didn't do enough in terms of 9:46   : application to managing the situation. So in a course that is part of engineering management. So I wanted to give you the opportunity to do something on the management side, obviously, or from no one, is managing the Ssc. Courses.   : but it's a   : a way to work on that.   using data   : to get elements to take decisions.   : So if you look at just the at the charts, so you basically, have a engineering management. That is a   : the largest program is more than 50%. What is the message here? Well, I mean.   : if like, in my case, you are the the, the the program director for engineering management. Then you can say as a   : promotional item, you are in the program. That is the largest in our school.   : or probably you may want to have a sort of a over the years. That means   : you can create a a a different story. I think one of the shards was on that area. So   : this chart is basically telling you what is the the I mean. It is the courses and not the students. So but what you see. So one of the things is that software engineering is growing a lot.   : It it's. I mean that   : then what is growing a lot. So I went from a very little to a bigger than a system engineering.   : So again i'm the organ director for engineering management. I'm happy. Probably this angle it's a little bit smaller than this angle.   : Probably not. So. The angle is telling you what is the increase, meaning how fast is growing.   : So i'm seeing that it's growing a lot of pretty much the same rate as the other program that is growing fast.   : If I was the program director of system. Engineering this going down is something that you don't want to see, meaning I need to do something to revert the the trend that that is down   : it.   : If I was the the program director for a software engineering because he's growing so much, I need more instructor. And we are hiding actually modest factors in a software engineering because we have more courses and we need more people. So those are the values for those visualizations.   : the values for the numbers.   : So if you I mean this is not adding match to the visualization. So number of courses for faculty over here. What you can say is that the   : there are some instructors, so that they are teaching way much more than others. So why? So I mean that one of the reasons is because we have tenure track and non- tenure. The tenure track tend to teach less, so we we can eventually discriminate the the the faculty by being tenure taraka or non- tenure, crack, and then do another round of analysis. But this is just an indication.   : So we don't have an even a distribution of all the courses   : a faculty overloaded. So you see that there are some faculty that are really overloaded. I think one of those   : meaning in this case Number 14 is teaching 4 courses in her overload   : and underload that. This is something you don't want to see. So why Number 13 and Number 18 are so under loaded.   : So you want to dig into it that there could be good reasons. So there is one with 5.   : If i'm not wrong. In this case it was kind of easy.   : It was a it was a personal leave, and that's the reason. But those are elements for you to take actions. So it's really something that is, on the management side.   : So I, who put that they gave you an idea why we did the   : this exercise. And obviously the second reason is to be sure that you are using Pandas the best way possible.   : This library is really powerful, and can really replace in many cases loops that could create a a 1 million issues. So I said that to me on time   : the python is not great. It's not very efficient in managing loops, if you can avoid loops, and in particular, if you can avoid the loops in loops, meaning that nested the loops.   : those are highly inefficient. That would be great   : erez agmoni. So you want to have a as much as possible operations done in Madrid sees, and this is what Pandas are doing 150,   : So it is. It's not part of this program to just give you the details on a linear algebra between.   : I mean on on the differences between doing loops and doing a multiplication multiplication between Madrid sees. But just keep in mind   : that, using Pandas and using all the attributes that you can have with the Pandas. So all the operations that we can do with Pandas so. and that are why they load all the   : sub libraries or components of that can be of great help.   : So use them as much as you can.   : So those 2 reasons, one on the management side, one on the technique outside. So if you want to use a python for management reasons. Most likely you will deal with a   : tables. In a broad sense. That means that most likely Pandas is what you will use to crunch the numbers so, and get the the insights that you may need for your management.   : So that was a exercise 6 again. It was challenging. If you look at the number of lines. So we are talking about the 150 lines. So that is quite something.   : Exercise 7 was definitely lighter than that. So it was a on a comparing pros and cons on a given topic   : ere   : the pro corona legalization or recreation, or marijuana.   : But it could be anyone so.   : The process was just the same that you did, going to the website, coping the   : pros on one side, the cones on the other side, and doing and and paste the value into 2 txes files, and then working on it   : so imported the all the variables so they needed them.   : Then I use the a function for cleaning text. We already said that meaning on time. So   : if you will work, if you plan to work somehow on a text in the future, create your own functions for a specific tasks.   : and in particular for most common tasks, and probably the most common tasks in a text processing text, mining, natural language, processing is cleaning text.   : So in this case i'm using a a function that is very basic. If you do so, you want to have a the right amount of comments, because it's something that you we reuse in the future meaning.   : You may not remember what what is inside the something like that where you specify what are the parameters that the the the function is taking, and what the what is, what are the out, the outputs generating generated by by the function   : and the type of a parameter in and out.   : So in this case i'm using a I mean again, very basic functions.   : You can do better than that the the the parameters, so that I mean that the input values they then taking in the text cleaning is that this the word that they want to clean the minimal length of assuming that what's that? That? A smaller than a certain number of characters are not semantically relevant.   : And then the list of software. So with toast with those 3 elements, I do all the cleaning, and then I will return at least   : a list of team words   : opening the 3 files. One is pro one is con, and one is the soapore file   : initializing the the list that I need   : erez agmoni. So I use the at least for the the soapboard, and then 2 for each one pro cones one with the lines and one with the words, You would see how they are used. 101   : initializing the parameter that I will use for the cleaning with the the minimum for words to be accepted.   : reading the the soap or file, adding.   : what's that? They know that they are very call Mona, and I do not want them to be too relevant in my analysis, because it wouldn't add any knowledge to the analysis that i'm doing.   : because it's pro call marijuana for sure the name. It's a of the words marijuana, cannabis, drug legal legalization will be a very common, and I want to eliminate that   : to give more room to the other.   : Then I I could have done that.   : A function for the reading files. I did. I mean I for the rest of the script. The the 2 pro call now are separated. That's not. Again. Could it be done with functions.   : and we would be more read Ebola, and more efficient.   : So i'm   : reading the file, so i'm, taking only   those with the a certain length.   : so that should be more than a dozen characters.   : If so, i'm appending to the list of lines.   : Same thing for the corner.   : Then i'm cleaning up.   : I'm taking the top 10,   : or whatever the number   : printing it.   : Same thing   : calculating the diagrams. So the way in calculating the backgrounds it's what is called the brute force, so there is No.   : we are not met on that. Just taking the the words the one next to the other, and then i'm taking the the most common.   : using the the first 1515, or whatever is the number we you want to have.   : And   : obviously   : you need to be aware that if you take one and the next, if you are at the last one, and you are looking for the next, you will get an error.   : meaning You   : really need to have a try, except because at the certain point the last element of of the what. So you will get an aurora. And at this point you will not skate it.   : Same thing for the corner.   : and I'm. Taking the the 5 most common printing them. calculating the sentiment, analysis, and doing the sentiment, analysis.   : sentiment, analysis, and work cloud, they take shrinks. Not least. That's why i'm creating a string. I'll go the list.   : and then I   : passing the string to the library getting a positive negative, a new roll up   : same thing for the corner, and then printing it   : so again I could have done half not half of W. 1 73, but probably I could have saved the I got the dirty lines some roughly, probably when 40 lines creating functions for dealing with both.   : So   : over here and generating the work cloud again. This is already been down. I could have used the the the same, but that's fine setting the parameters for the work Cloud   : generating the work cloud eventually saving it to a file. I just commented it because I don't need the to have another file in my directory.   : and then show in the cloud, and then end up processing   : if I run it.   : That's the the work cloud.   : and that's the the not the I mean that that text. the metrics. So you have the top 10 pro words.   : Probably you may want to eliminate state, but I don't know I mean what is the interpretation here?   : State? It could be. The States needs to take control of the process, or they being charged for doing the laws for the liberalization having Washington.   : If you see States 12 Washington, then it's kind of controversial. Someone is more pretty much the same number of people.   : I mean that logically it is a for a decentralization of the decision process, and the another portion significant portion is for a centralization meaning, he said. Oh, he, being at the State level, more at the Federal level.   : united probably part of United States. So   probably   : I mean that   : we should do the cleaning in a better way when we see things like that.   : probably eliminating a users using.   : anyway. So the most common diagram, so United States economic activity.   : all things that we expect to be there.   : Sentiment, Analysis, as a usual.   : is the majority.   : and most of what we say as a a large component of a non-negative known positive.   : And that's where the new.   And   : so if you look at the pro.   : and I would say in the board, the cases you have the negative being a bigger bigger than a positive.   : You need to read into it, because and probably the the the, the the work cloud, could help. What you see here is that people like even pro are   : leveraging more on the negative aspects. I mean   : erez agmoni. Let's say, I improve marijuana, because if we don't legalize it, then there will be more crime. There will be more illegal drugs, 250,   : and that's I mean using a negative argument to support a a a positive opinion   : on the negative side. If you have more negative, you you can say there will be more. I mean, there will be more criminality.   : That could be pretty much the same arguments. But   I mean that you should read the into that.   : All right. So that was a pretty much it for   : those 2. Let me stop sharing for a second and check. If anyone has a any question.   : and if not, we will move on to   : the next topic. That would be the topic of the day. That is a web mining.   : So let me share the screen again. Let me go here and let's start talking about the web mining.   : So what is a web mining web? Mining means going into a website and getting the content.   : Oh, that's in a nutshell that what web binding is   : getting the content could be   : It could be, it could be videos. So whatever is the content that we we want to explore the possibility of getting it   : now? Some of the questions that they had over the years. So one why we do that we already had Google   : or something like that   : second question. Is it even legal. Well, on the first one. Yes, we do have Google. But if you do a Google search, you basically have a the link to the page. You don't have the content. You may want to have the content that to do something. So you want to do an analysis like we did the   : with the pro call. So instead of coping and pasting, if we have a multiple sources, you may want to have a an automatic way to go into the website getting the pros and the calls and then use it.   : So Google cannot do that. Google will eventually just give you the URL or the pro phone, and then we'll be up to you.   : Things can be done in a more complex way. You can get metrics, you can get numbers, and then you can process the numbers. So that's the why we are doing it on a being legal. Well, if Google built a an empire   : on going into the websites and getting data. That means that it has to be legal. So if Google can do it, and a planet that a. Sc. That we can do, and in our Miniscule domain.   : Then there are a kind of a grey area, so I generally use a an example that at this point is kind of a outdated, but it's still real. A few years ago   : a small company was a mining linkedin to get   : indication of employees looking for jobs.   : a and then a a adding, some processing and reselling to companies like another. You have a those people that are   : conducting a sort of a abnormal for   : jobs. That means that they are thinking about leaving you.   : So it was a kind of a valuable service. Then, at the certain point this company received the a letter from a, the Linkedin low years, saying that you cannot do that; but if you continue doing it, that we will sue you.   : So that was a kind of a shock for them   : the the motivation for a link in that. Apart from the obvious economic reason, they said that that yes, what is on the Internet is public.   : But this is a a a, a breach of the intent of use of the information that we publish.   : So it   : star New Year decided to to side the this a small company, and the litigation went on for a while quite a while means   : appropriate. So, in the meantime, several of the employees of this small company left some of the customers left, because they so kind of at least what they we are doing.   : Bottom line day one. But the company closed the because they couldn't survive anymore. So that's an example of the grey area on web mining.   : Another major issue of with mining is related on you are mining a a website. I am the owner of the website. I can change my website the way I want, because it's my domain.   : So if you write the code, the picking specific elements in my website. If I change the the architecture on the website, your script will not work anymore.   : So that's how unstable is a web mining.   : Nevertheless, several times it makes sense, because we may need some type of information that we cannot really get in in a different way.   : So Obviously, there is quite a difference between that web mining and data mining. So when you do, I mean they have mining in common, but that's it.   : So when you have a data mining, you have numbers. So you have a a structured set of of information with the web mining is it's definitely. I mean.   : it is structured, but each website has a their own structure and structures within the same website, as I was saying before, can change at any time.   : So   : it is really different. Web mining is more challenging to begin with. And then what are you going to get? You're going to get text, and then there is a the text mining component. So web mining a is 2 portions, the web scrapping. So you are getting the the text that you want. But then you need to process it   : So it's. It's definitely more complex than the data mind.   : What is the web? So webminding web? But what is the web? It it. It's a 63 on page, so we know no one really knows how many page are in the web. A lot.   : some duplications meaning duplicated page. Some are not indexed when they index it. That means Googleized or Google Index it.   : Yup. No, not all the page are available. When you do a Google search.   : If I write my page in a way that   : is not going to be indexed by the by, the search engines, it will not be possible to find it   : so. those page at the page so creating what is called the dark web, so the dark web but doesn't necessarily mean that the that is in the gala, but just that is not indexed.   : So then I could be, I mean, a a legit business. But just for any reason I don't want my page to be public.   : What we do with the web mining. Again, we can do exploration. We can create models, we can create services, we can do predictions.   : I mean, there are a quite a lot of things that that we can do. We can use a web mining for advertising, getting advertising information.   : Indeed, the is one of the 2 largest job, such platforms. So they started with no   : jobs that we are their own   : Erez agmoni. So what they did there was, basically they went into the web, going to the different sites, posting jobs and 101   : scrapping the data and aggregating them, adding a layer of analysis, like the one that that you see on the left side the salary company, location, job, type, and other things. So they told what they were up.   : and then that that was the first version of indeed. Then they grew.   : The business was successful. Some companies that gave the jobs so directly to indeed, that became either a sponsored jobs or a a exclusive jobs on, indeed.   : But they started with the web mining.   : so they build an entire business out of that they we are integrate, or somehow all the information.   : So hmm. Sometimes they didn't use just the scraping they use the Api. We will go there in a moment.   : So let's go a little bit more into the web mining business.   : So, Web search is a for all the web mining. So you have the the webpage, and you have an agent that is a web crawler going into the page and getting a   : the Hmm.   : The keywords that are defining the page so it could be the location. It could be the most common wars. So those are the   : the elements that they use to time Good. The page creating a sort of a an index for the page.   : So the webcroller is basically creating a a cross-reference table with the the URL on the page, and the tags that that are related to to the page does the indexer.   : So the crawler as 2 components, one scroll in the data, and they say on the indexing.   : When a user is asking for something, then the search engine is matching the keywords from the user   : with the the tax that are that that have been indexed by the roller.   : and there's the match when there is a match, and then   : eventually it could be an algorithm to a sort based on   : the quarter of the match.   : Eventually, if   : if people is paying for being higher in the presentation of the results than the other element that is, consider is a how much 150   : the companies or people is paying for each key word. So it's kind of a bit process, and then the highest will go on top, and the other will follow up.   : If the model contains also placing ads, then   : you have another component that is managing the advertisement portion where you have the name of the advertiser.   : and eventually the message that can be the URL. What other messages and the tags.   : And basically at that point   : the the search engine on top of all the presenting the results is is also matching the tags in the query with the tags in the the the database, and presenting the results.   : So this is basically how a search engine it is working   : need a spider, an indexer and a query process.   : So if you look in any page. So you have a I mean, this is a a an old one, but the the concept is is still the same as you. You have a a list of a words that are links or hyperlinks to a page   : when you   : mine into a site that you basically jump from one page to another, using those hyperlinks.   : Each page is a a file in a a format that is called HTML Iber text markup language   : that it's the the standard for a browser, so to transform the content in the eventually beautiful layouts that we see from a web page.   : So again.   : hypotheses mark up language. That's an example, is a sort of a 3 like structure, where you have a in a   : angle, brackets the the the tags for what is following. So you have a HTML beginning and and and then you have a a header in this case.   : I mean because we call it the the the developer of the page. Call it head, but could have called it in any other way. Then you have tied all you have a a body, and so on.   : When you send a request the request, I mean, when you, through your browser, send the request. Your browser is sending a a a bunch of other information.   : So for sure, the text all your query, but also some additional information that we'd be used by the server primarily to know more about you. The   : no more can be finalized to   : advertising or just optimizing the traffic. So you have a what is the host? What is a   in the   : the the the browser that you use? And what is the language? Eventually, in some cases you also have the the IP address the response. Yes, we get the page. We get the HTML, and we see the page. But actually your browser is getting a little bit more. So it's getting a   : I know of a similar information. Then information that we send, with the additional Meta information that are used by by the the browser to present   : in in the best way possible the information that they are received.   : the correct that set is one of them having a Php. No Php or other elements. So what is the the version of the Http? So all those things are are useful for the browser or essential for the browser to present the the results in the most appropriate way.   : Our browser may or may not use those elements. So if is using those I mean apart from functioning.   : But what other purposes, unless you have   : what is called the cookie? That is a piece of software installed in your browser   : spying on your activities some of the activities. It could be   : unreached somehow by the the characteristics on the sides that you. Basically   : So again, that's another example.   : Spiders   : plain pieces of software. We will do some examples. So very low level examples. You will work on something slightly more complex. Not much as an assignment for next week.   : Obviously a   it's I vertex.   : meaning you can go. You can search in different ways, so you can go completing an horizontal level, or go in vertical in the end, and then go back.   : Each one of the strategies has a pros and codes.   : so as you can imagine if you do something vertical and is a site that is a let's say the New York Times site, or whatever is a a news outlet. So you go   : in depth in one. Then you emerge, you go up, and then you go down to another one. But chances are in the meantime the first line that you explored.   : I mean the results that you collected the are outdated because there is something new.   : So when you move this way there are chances that you will keep cheating the last one without getting a   : that much   : much new.   : When you send your request you use your browser is using a protocol. So, Http. You know that we know that is a HTML, but they also I mean it's a dialogue between your browser as a the client, and the server we are the information   : reside.   : So get is the common to get something. But it's not the only one. So in theory you can do with the same probable way, much more if the server will allow it.   : So let's go for a moment that step out of that, and let's go to some examples.   : So in Python there are several libraries that we can use, so request it's a pretty straightforward. It's basically going to a URL that you specify, and it's retarding the the HTML   : HTML that can be a little bit convoluted. But there is a a library called the Beautiful Soup name is from Alice in the wonderland that is transforming   : is parsing the HTML structure in a tree like structure.   : So   : in this case i'm importing a Bs, for that is a beautiful Super, i'm porting a request. So that is the library that will get the HTML.   : Then i'm   : getting using requests the HTML for   that page.   : Then I   : applying a beautiful soup to transform the structure.   : then i'm printing the the title from Super.   : and then i'm printing the the paragraphs.   : But if I run it.   : that's what you have. So your time. So I mean, if we go here.   : So this is the page of a New York Times. and it's basically what you get here.   : So this is to some Democrats and Republic, and the charge again. Don't   : something that   : that's what we have in the page.   : Unfortunately, even this portion a 6 min thread. The 5 min thread are a tag, the as a title, and that's why we are getting them.   : I mean they can tongue with whatever they want.   : Most likely they do it intentionally to   : make our lives as a as a scrap. It's more difficult.   : I I mean that that's pretty much. I think we can.   : I remember. Oh, we can.   : this one. Yeah.   : So there is a way to see. I never remember in safari how it is. Yeah.   : so does the HTML   : again. The HTML is not exactly very readable.   : so let me show you how the soup could be.   : and let me comment this one   : I just want to.   : So this will.   : So that's the way it looks. It's a big fine, but you have a tags that can help you.   : and the tax is what we use to extract the those elements.   : So, anyway.   : So this is   : a way to do   : scraping content.   : That's very basic. There are several libraries that it can do that. keeping in mind that that sometimes things can be complicated. So if you go into   : a   : into page that are distributed on multiple page. Then you need to have something that will be able to   : you, for so   : so request is not going to do it right away. You need to write the code in in a different way, or use a different library.   : There are libraries that are for a meeting. User They tend to be more efficient. But again, they not work on all the sources.   : So we mention what is either request or or you are a lib.   : That's for getting the data.   : You can save eventually. The HTML beautiful super is what is doing.   : Let me keep that, and I can do here. So we mentioned that   : things can change in a page. So sometimes you may want to establish a a direct relationship with the the the server   : so, or with the administration of the server   : to do that some so versa. They have what is called the application program interface. That is a sort of a protocol establishing the rules of the game for downloading data.   : So instead of just scraping the content from the page, you basically go to the server using the Apis.   : the vast majority or Api's, they have a a first step, but that is a a an authentication step we are. You show your credentials, and then the server will let you in for the privileges that you have.   : Think about the a news service. So you pay for getting the news and you get the news connecting your your application to the server. When you do that, you basically   : send that an authentication, saying, Hi, i'm Carolly pizza. And I. I want the news from this date to this date.   : Then the the server will check   : if my credentials are good for what i'm asking, and then we will. It will send me back the news.   : So that's what the Api's do. So when you use an Api, you are a kind of a a, a more sure. The the results are appropriate   : when you do scraping. It's kind of a last results you have, because   : things can change. Content can be on multiple page. Content can be in boxes. I mean that everybody is trying to have you paying for the content instead of giving it for free.   : So some Api's are free, not all are free.   : I would say some of the valuable Api's up not free. I was mentioning the use at the certain point. We did the project extracting news of pad and some papers   : Erez agmoni. So with patents we were okay, because the Talent Office is allowing users to download the the pardons 150.   : What news? Initially, we thought. You know what we will and   : create our own Webcra, and we will download the all the news didn't work well, because we had a with Google news that you have a limitation in number on our   : elements, that you are downloading with the New York Times. So same thing you cannot play with those things, but when you use a in a professional way, you really need to establish a a contract with them, and the Api's are are the way they contract it's on order somehow, and get the information in a more structured way. 1:   : So sometimes there are limitations meaning 1:   : for the news. When we did the the the program, and we ended up. It was a a a large project for the Dod. We had money, and we paid for the news. 1:   : and we had a certain number of of news that we could download the the timing interval, and we did that 1:   : we there as a a free Api. But you have a limitation in terms of a number of tweets that you can download the or a number of queies that you can do for a time interval. 1:   : Again, this example to either. 1:   : When you download tweets tweets, Are we much more complex than we think in terms of structure? Yes, you have the same that you have the timestamp. You have the text, but you have quite a lot of additional information information about the user the geolocation 1:   : and things like that. We may not be interested in that or the majority of the searches on the projects or the researchers that they did on that we that I use the only the sender the timestamp and the text. But sometimes 1:   : you may 1:   want to get more than that. 1:   : When you write your script I mean that you can use, or you cannot use a a library that will make your life easier. You will pass, anyway, those elements so that are part of the authentication. 1:   : So you have a consumer key and cigarette a. You th token and cigarettes. 1:   : So those are our characteristics or credentials that you get when you create your when you establish the relationship with. 1:   : So you need to register your application. When you register your application you will get those 4 numbers. 1:   : So that's basically it. That is not much more that I can tell you about web mining. So it's even some that we offer a courses on web mining. 1:   : I'm reading up all that issue, or that you really need a courts on a web mining. 1:   : But I mean that it may have a some merits so 1:   : for engineering management that I really don't see why we should do that. But anyway. 1:   : you can explore. You can do additional exercises. You will have one for a next week. That will be 1:   on the easy side. 1:   : All right. So let me go now on that something else. 1:   : the the portion strictly on 1:   web mining it is basically 1:   : over. So there is not much more than I need to tell you on that that should be. But we are not going that way. 1:   : What I want to to talk now is about the larger language models like Chat Gpt. 1:   : So 1:   : it's kind of the elephant in the room. 1:   : When they announced the chat Gpt was made on November. It was a huge we all know 1 million user in the 1:   : one week something crazy like that. It was a planet that the phenomena 1:   it was a planetary success in a sense. 1:   : So 1:   : why we didn't talk about it since the very beginning. 1:   : Well, there are quite a lot of reasons I 1:   : I wrote about the user our chat. We 1:   gpt in education. I've been interviewed by 1:   : Do not magazine so on that 1:   : how to leverage and how not to leverage what we are my positions. To what the use of a 1:   and those bolts? 1:   : I mean that it's a very controversial issue. 1:   : I want to share with you my opinion, and I would be happy to open a conversation if you like. 1:   : So the first point is a we need to know a little bit more, or we need to be aware 1:   : How about how those things work. So those things that are 1:   : generally are called the large language models or llam such. Sometimes I call them just bots either ways. So the proper category name is Ln. M. So much language model that so those large language models 1:   : I don't like calling them generative systems because they are not really generating anything. They are just. I mean that compiling things. 1:   : So the the way those are, it's basically 1:   : They started with the in notion of data 1:   : we will go into how much in a moment, but just to stay in high level. So you have quite a lot of data, quite a lot of text that collected from a different sources. 1:   : This content 1:   : We are very high level, as being tagged 1:   : for recreating patterns. 1:   : meaning what are the topics for each one of the elements on this optional data? What is the relationship with between those 1:   : tags 1:   : basically similar to what we saw with the scroller with the search engine. So they had the an advanced version of the webcroller going into the page, so not just taking the 1:   : the URL and the tag, but the entire text 1:   : and thanks. 1:   : So 1:   : it's is indexing somehow. Then you have your query. 1:   : So the query as a elements that will be matched just like the so changing that will be matched with the content. 1:   : So at this point is matching, not the Urls meaning, not like Google, that is giving us only 1:   : the address of the URL meaning on the page, but the actual content, or the actual portion of the content that is being tagged in the same way as you're creating. 1:   : So it's a 1:   : matching patterns, so the that they have from the and the pattern from your query. 1:   : And then, once the match is done. They have another component. That is a 1:   : natural language generation. It's the only thing that is generating, adding a conversational layer 1:   : to this 1:   : match of patterns. and that's all they do 1:   : is basically like adding a layer to Google in terms of providing content instead of URL 1:   : and adding a conversational layer. 1:   : So that's all they do. 1:   : What does it mean means that the more details you provide in the the query. 1:   : the more rich will be the match that the Anlm will do for you if you send a straight question. 1:   : One sentence 5 words. 1:   : then the match is going to be kind of dry, so you will have a an answer. But the answer is not going to be much reach. 1:   : and 1:   : I'm. Heading a student, an undergraduate suit. I'm sorry in high school, the students, because, working on a comparing 1:   : the assignments, she is a a submitting in a history records with the the submission from a chat. G. GPS 1:   : and the teacher is analyzing both, and is giving a an equivalent of a grade for both of them. 1:   : So the results are okay, the main difference, so that the teacher noticed the in a. And who we are going to write a paper on that 1:   : that. I think that the teacher noticed. That was just that the facts we are there. 1:   : But there is no real connection between those elements, and there are no additional elements to explain the context where those facts happen, while the submission 1:   : from the student that was more reach somehow. 1:   : probably providing the more 1:   : details would make life. I mean. 1:   : I mean, answers more structure, the more complex. 1:   : How we can do that. 1:   : So let me share this screen now, and let me share with you the way i'm using a chat gpt. 1:   : So i'm using the plus version, because as a philosophy of your life. and I like to pay for what i'm using if I can. 1:   : because fortunately in this case I can. 1:   I'm paying for the plus version. 1:   : There are not many advantages in getting the plus version. Apart from this, he 1:   : that you have the session or sessions that are always open. 1:   : You don't have a services not available message. 1:   : but that's basically, it. So in terms of capabilities, it's pretty much the same. A. And they cost the for a 1:   I mean an online service. It's relatively high. It's $20 each month. 1:   : but 1:   : I mean for a while I plan to use it. We are planning to build something, by what service will never be as good as you would see in the reasons why. But that's what we have. 1:   : I'm writing a book. I probably already mentioned that that that is a 1:   : so societal applications 1:   : or a so size and implications for the user yeah machine learning. 1:   : There are several things that are so. Just to give you an idea 1:   : that's the Table of content. 1:   : So some of the things it it's a working progress. Some of the things requires sort of digging into sources. 1:   : So we are talking about 1:   : the AI revolution. 1:   : But is this a via revolution? 1:   : How you measure a revolution? What is the revolution? 1:   : So I basically went into a chat gpt, and I asked about the information about 2 of the largest one 1:   : technology-driven revolutions. 1:   : the industrial revolution and the 1:   : so. 1:   : and then I 1:   : I try to get as much information as possible. Nothing at the day couldn't have done using 1:   : Google. but the difference is that I have a compiled version. 1:   : The difference is positive, negative, because I don't have the sources meaning I can not just take what chat Gpt is generating and using in my book, because the chances are there will be an infringement of a 1:   : the rights of someone who wrote the the the the phrase 1:   : keeping in mind that that 1:   : Chat Gpt is not generating content, but it's taking pieces and 1:   patch them together. 1:   : So those phrases are from somewhere 1:   : at a certain point that someone can tell me this is 1:   : so. Once I have an idea from a chat, G. G Gpt. Then I need to process it, and if I want to use some of the content. Then I go to Google and get the actual source and then side the source. 1:   : So that's what you need to do. 1:   : and that's one of the reasons why we are talking about the chat gpt now instead of Chat gpt at the beginning of the course. So but let that. Let me stay for a moment with this creating context. 1:   : so you can create chats. 1:   : Each chat that you create. You provide information. So the system is not learning from what you are typing. 1:   : because 1:   : the learning is a batch process in a chat. G. G Gpt. Meaning that there is a training path that last about a week. That is very expensive. I will give you some details on that, and it's done, and then is the user of the model. 1:   : What you are giving is basically the context, as I was mentioning. 1:   : So 1:   : and where my new query. we'll be playing meaning that a 1:   : I don't need to re-explain things that are already in my chart. 1:   : so the more reach is the chat, and the more reach will be the answer us that the boss will provide. 1:   : So 1:   : if you want to have a good answers, you need to provide a lot of information. Now, sometimes this can be doable 1:   in a in an academic environment that sometimes not so much 1:   : keeping in mind that once you create the the context through the chat, the the answer that you will get that will be different from the answer that someone else can get to the same query. 1:   : because the result will be based on the on the context that the chat that you created. 1:   : All of this is creating a a new job that is called the Prompt Engineering. The way you prompt your query is 1:   : affecting big times the quality of the answer that you are getting 1:   : So that's the part on non-coding. 1:   : Now chat gpt and again I created the several chats, and each time I want to ask something in a particular domain. I can get 1:   : the answer based on the history of the conversation. 1:   : so could the it's a different. Sorry. So you probably read the that Microsoft invested the 10 billiondollar in the open. AI, that is the company who developed the chat Gpt. 1:   : They also provided quite a lot of assets. 1:   : One of the assets that Microsoft has is a github. So Gitab has a lot of code, and this code that's being used to train the 1:   : now I tested it. When I have a problem with some level of complexity, can give you a little bit of help 1:   : keeping in mind that that sometimes is doing things right, sometimes not so much meaning that you need to know how to code the to discriminate the the good answers and the bad answers. 1:   : Now. 1:   : if you will get a job in the next 1:   : 1, 2 years and coding will be part of your job. 1:   : You cannot get away with the using a chat gpt 1:   : at your work. 1:   : So you need to know how to code. If I don't teach you how to code, and that's what we did from a class one until today. If I don't teach you how to code the one that you will never know 1:   : when that child, gpt or equivalent, is doing right and is doing wrong. 1:   : Second, the 1:   : you will not be I on the 1:   : to use a chat, Gpt. You will be hired to write your own code. 1:   : so you can use chat, Gpt as a base. And also I mean, we are engineering management that we are not in computer science. We. 1:   : So we want to have a we want to be integrators. So we take all the inputs so that we can get from all the possible sources, including the pieces of code that we can take from a chat G. 1:   : And incorporating in our own code. Again. 1:   : keeping in mind that if you do not cite the the source, and someone else is using the same tool. 1:   : We will consider that as pleasure is Ma, and you will take. We will take points off. 1:   : So you would get 1:   : some point reduction because of that. 1:   : So feel free to use whatever tools you want, but keep using citing the the sources. 1:   : So let me 1:   : share a a few more slides, and then 1:   : we will work on that Any class assigned that. 1:   : So we mentioned those a large language models. So open AI started. 1:   : I mean that generating those large language models in 2,018, with the first Gpt gpt generative pre-trained the transfer. They are based on a An algorithm that is called the transformer 1:   : that was created a a few years before. 1:   Open AI: 1:   : Okay. A 1:   : based on a paper called the name. The attention is all you need that is based on using the attention mechanism 1:   : to victorize a text. So if you remember, from the class on tax mining. 1:   : you can transform a a text into sequences of numbers. This process is called vectorization, and you can create a a matrix in different ways. So one way 1:   : he's a 1:   : analyzing the call for us awards. You can say, okay, I will consider connected the words that are appearing together more than a certain amount number of times. 1:   : and then you have a number of the number of times, so they call core 1:   : somehow. Then you kind of normalize the madrics, and you will get each word that will have a a sequence of numbers. So that's a a basic form of vectorization. It's not the most basic, but it's a basic form of a vectorization. 1:   : The next stage is using what is called the war to back. We mentioned that that is a again, the next step. So, instead of having a the number of core coordinates, you have the the conditional probabilities, or one word, the upating because of another. 1:   : Then again, you normalize the matrix, and you will get the each word with a certain number of numbers defining it. 1:   : That would be pretty much like having a points in in a space. 1:   : That's what we 1:   : what to back. This calculation of the conditional probability is using a a a neural network with one hidden layer. 1:   : It's called the shallow neural network. So you have an input layer with your parameters. You have an output layer, and in the middle you have a one or more hidden layers. 1:   : What to back her head. One he, the layer. transform that so because they are not 1:   : only looking at the conditional pro probability, and not only forward, but they are by directional, and they are also taking into account the position of the word in the sentence. 1:   : But because you have more parameters, the neural network is more complex, and there are 1:   : much more hidden layers. 1:   : So to do the training, all the 1:   : a a system to calculate these madrics, using what to back in that when you have a an auto text that it could take 7, allow what's on one of our computer so it could take several days using their last 4 nights. 1:   : So because you have a a more complex fraction 1:   : based on the same principle, open AI created the Gpt initially. 1, 2, now we are 4. 1:   : So the complexity is measured the by the number of parameters. We are talking about a neural network neural network that are inputs in the neural network, and there are weights to the different inputs 1:   : for the parameters. So is the number of weights in the the network. So. 1:   : Bertha, that was a the one of the first 1:   : model based on science formats. They have a 340 millionparameters. 1:   : Gpt. 2, the first commercially available 1.5 billion 1:   : So we are a Gpt for that. That is a 1:   400 billion 1:   : of that. 1:   : So that's where we are. It's a 1:   the 1:   : really complex structure. 1:   : So the training of all of this is a kind of interesting, so the estimated cost to train a chat. Gpt is 4.6 million. 1:   : There are you months in the loop because they help the in the tagging process 1:   : and in a refining. The results generated by one of the components of this and the energy that was used. Keep in mind that the all of this is running on a gpus. And so Nvidia is doing a lot of money out of that. 1:   : The estimated, the energy used 1:   for a training chat. G 1:   : duration was about a week is a 1:   : pretty much the same amount of power that can run more than 30,000 households for a day. So that's the impact in terms of 1:   : ecological impact. 1:   : Keeping in mind that our brain is using a fraction of a fraction of a fraction or a fraction of that much for the entire 5. 1:   : So that means that 1:   : Chat Gpt is not an efficient way to represent knowledge. 1:   : but we already know that, but it's cool, and it's doing a quite a lot of things 1:   : eternal sources. 45 TB of text. So that's how much text is there. 1:   : Keep in mind. The text is not using a lot of memory. and 45 TB it. It's really a lot. 1:   : So those are some of the sources. So pretty much all the let's say open source sources that we have. 1:   : This is the distributional language. 1:   : Almost 93% is English. 1:   : and that's another indication that we should consider so. The message here is a a chat. Gpt is biased. It's a bias in terms of sources for sure, but it's biased in terms of a language. 1:   : Yes, the web. It's more English than any other language. 1:   : But 93% is a lot. So If there are 1:   : contents not in English, chances are they are under represented. So that's one of the things that we need to consider. 1:   : Another Things that we need to consider is that a chat gpt is a 1:   : based on data up to 2,021, 1:   : meaning that if you are asking a chat gpt things asked of that 1:   : it doesn't have any information. 1:   : So why Google lots is it somehow updated the pretty much in the of time. 1:   : considering how much the training costs. 1:   Chat, Gpt and Gpt. Whatever from 2 to 4 1:   : is definitely not a real time. So that's another limitation that we need to consider 1:   : how they perform so difficult to say in 2,016 and other. Introduce the data set to benchmark the ability of a language model to perform a task or oriented toward the language understanding. 1:   : So 1:   : if you look at the chart, we are getting better, probably with this particular metric in another couple of generations of Gpt. It it will 1:   : each somehow. I mean for that particular test that's human like performances. 1:   : You see, 0 shot at 0 shot means machine learning models performing a task without a specific training on that particular task. Those are not performing great, and I mean definitely 1:   : those with the so confused short are doing a better job 1:   : limitations. We already mentioned that 1:   : another thing that we need to consider is that 1:   : we have a different areas in our brain for a new ledger. So we have a 5 senses. 1:   : each sense as an area in our brain 1:   : getting the input but also storing 1:   : the information 1:   : and collected by the those 5 senses. And then we have another area. That is the language. But the language is the expression of knowledge. With the 1:   : hmm only the language component. So there is no representation of the knowledge. But the language is the knowledge somehow. So 1:   : again, we have a time limitation. 1:   : There is no real time. There is a limitation in terms of knowledge. There is an in in inefficiency. There is the fact that again. what all is doing, it's a matching patterns 1:   : that is greater, considering how much patterns it has to match, but it's still not the only way, we, as human being, a function. 1:   : Let me keep all of that, and let me go all the way down to this consideration here. 1:   : So I 1:   : was mentioning 1:   : while talking about how to use a chat gpt the fact that that is, that is a new job that is emerging. That is, is a prompt engineering. That is a a sort of super, user 1:   : and it's a combination, all but knowing how those can work, and not even more than that. 1:   : a knowledge of the domain. 1:   : So you create the context 1:   : to have a that you are a 1:   : giving your I mean more to match in terms of parts of. 1:   : So on one side this new job that is more less on the technical side, the more on the super user 1:   : on the other side. You have people 1:   : working on the next generation of a at an end. So we so the limitations and they have those limitations so 1:   : need to be addressed thinking out of the box. So we need the new types of engineering with skills that will go from that map 150 1:   : mit, c. 1:   : In a high demand in the near future. 1:   : It's not going to be easy, because no one as a model or cognitive science. So how the knowledge is representing. 1:   : But that's really something that that we love. We grow quite a lot. 1:   : So I hope you enjoyed the this going 1:   : into 1:   : chat, Gpt. And large language models. So again, at this point of the course, I I really think that that 1:   : is something that should be this cast. 1:   : Okay, so let me go now into the in class exercise and let me introduce a 1:   : this 1:   : assignment. 1:   : So the program will extract the data from a web page and perform some analysis. 1:   : so there is no input provided. But the website is going to be a New York , or whatever side you want. You want to print the headlines, and you want to generate a what cloud with the words in the headline. 1:   : So let me stop sharing. 1:   : I will open a breakout rooms, and I will a post. The example that I used. 1:   : I strongly encourage you not to leave the class because of the assignment that you will do for next week will be will benefit a lot from what you are going to do tonight. 1:   : So 1:   : let's create a 1:   : 8. Let's say 9 breakout rooms. 1:   : 8 8 breakout rooms. 1:   : and 1:   : i'm opening them. It's 8, 9. Let's say you have 20 min to work on it till that pretty much 8, 30, and then I will close the rooms, and I will not proceed with the 1:   : introducing the the next assignment, and spending a few words about the final that will be 1:   detailed the next class. 1:   : The rooms are open. Join the room, sir. and posing the recording. 1:   : not the 10 s. So 1:   : all right, so all the rooms are close, the you are all back. 1:   : Anyone want to share 1:   : the the what you did. 1:   : Come on. 1:   : Okay. 1:   : all right. So let me share the screen and let me go here. 1:   : So 1:   : in this case I use the 1:   : a different library. So I mentioned to you that there are many libraries 1:   : for doing web mining. 1:   : One is a newspaper. So if you plan to import the 1:   : newspaper, keep in mind that that is imported as a newspaper 3 K. So if I go here. 1:   : so you will see the the name of the library is not newspaper, but newspaper 3 K. 1:   : So if you try to install a newspaper, you will get nothing, because there is no newspaper. So the name of the library is newspaper 3, K. But when you call it from your 1:   : a script you will import it as newspaper, so I hope this is clear if you want to use it just I mean install it as a newspaper 3 K. 1:   : Anyway. So I imported this library. I imported the word cloud. I imported my plot lab because it's functional to 1:   work cloud. 1:   : I define the the URL that I want to mine. Then we 1:   : this library. There are a different parameters that you need to set. 1:   : So I, not going into too many details about it, just want to be aware that there are other options. 1:   : So then i'm reading, printing the number of articles. So so in this case is an attribute to whatever has been 1:   : downloaded. 1:   : Then i'm setting the the maximum number of optical. So just because I don't want 1:   : 100 of them for this part. 1:   : creating a at least 3 headlines 1:   : then generating. 1:   : I mean, that was a list generating the the what cloud. But before that I 1:   : in the into a string, and if I 1:   : run it 1:   : hopefully, it will come out. 1:   : You have the number of articles 1:   : you have a 5, 1:   : and then obviously Trump is a big thing. We knew that 1:   : all right, so that was for the in class. So exercise. 1:   : So let me go. Oh, here and 1:   let me talk up briefly about 1:   : the final. We we talk again about the final next week. I just want to be sure that it is clear the way we are going to do it so today. Considering it's a a 47. We do not want to 1:   : to go 1:   : too late, but I just want to give you some indications on how the final is going to be structured. 1:   : So next week I will give you more details about I mean the the, the the 1:   : outline. We'll give you some examples. We will be talking about that. But, generally speaking, the final up is going to be 1:   like one of them. 1:   : so that you wrote so far. But obviously the reports for a regular exercise 1:   : our short term, because you had a a week. In this case you have 2, 3 weeks for doing it. So it it's supposed to be, and it's a fine all, and there are quite a lot of more points 1:   : the for the final than a regular exercise, so it will be around 10 page all all. 1:   : it could be at the level or complexity that would be similar to the level of complexity that you have in the last, the exercises 1:   : minimum, the level of complexity of the last exercises. So how we measure the level of complexity, I mean, there is no way to be on measuring the level of complexity, but 1:   : but the length of the Python script is kind of an indication of the complexity. So line of Code I I is never a a to indication or complexity, but is a kind of a proxy. 1:   So 1:   : between a 100, 200, let's say 150 and 200, that that would be the range that would be acceptable. 1:   : Find out that individual in the past. I gave students at the opportunity to do their own 1:   : topic data set, but when I have classes so big as this one, I really cannot do that, because 1:   : there are issues with getting the right sources approving the the the the topic, being sure that you are not just copying and pasting something from. So it it. It's not going to work. 1:   : so I will give you 3 4 options, and you will pick one of those, let's say 2, 3 options, and you will pick 1 one of those, any one of those will be individual. You will have your problem, and you are a a data set, and you 1:   we'll work on those 1:   : so complexity. We already said that we said that the structure. We we talk next week about the structure samples. Next week I will give you samples. 1:   : presentations. So you don't need to do your presentation unless you would be selected. So there will be 1:   : 4 5 6 of you that will present during the the final class your final. because we will not have a lot of time to review what you are sending to us. You will have one day, before the presentation to 1:   : the notified that you will be a presenter. Once you will be selected as a presenter you have to present. 1:   : So, unless there is a a a serious issue, but issues that needs to be documented. I don't feel well doesn't work. 1:   : So you really need to have a certification if it's a help Already the issue or any other 1:   issue that you may have. 1:   : So again, some of you, few of you, let's say about 10% of you a little bit between 10 and 50% of you will do the presentation, then either myself or she you most likely myself. But we'll present some other. Find out, sir. 1:   : that may be of interest, all the class, because they are particularly good, but particularly not so good, the particularly unique. For any reason. We think that those are relevant, and we want to share them with the class. 1:   : You don't need to submit a Powerpoint. So if you will be selected for the presentation, you can go through your Pdf. Or 1:   : what file, and use that as a base for your presentation. Then, if you want to do a Powerpoint feel free to do it. But you don't have to. 1:   : all right. So next week I will give you more details for the time being up. That's it for 1:   : the final. Let me share the screen and let me talk about 1:   : next exercise. 1:   : So next exercise would be really similar to what you did today. 1:   : So the programmer that you will write will extract data from a web page, and we'll perform some analysis. 1:   : You will bring the headlines so generate what cloud, calculate the sentiment, and write an interpretation of the results. 1:   : So you will import the libraries. They find the URL that can be in your time. So whatever you want to use it, you will load the the results into your super. You will create a list of words with the headlines. You will clean that 1:   : the file. Then you will extract the diagrams just like you know how to do it. 1:   : You will merge the most common diagrams with the the list of words. 1:   create the work, cloud with the results, and then write the interpretation. 1:   : So 3 or more paid. 1:   The code must be welcomed, commented. 1:   : So keep in mind that that I mean we are not in a a computer science, meaning 1:   : the style. The quality of your coding is not essential up to a certain point. 1:   : meaning that if you send a a code that with 0 comments, so we will take points off. 1:   : I mean that it could be 1:   : 30%, so it could be less. But definitely. We will take points off if we have a no comments. 1:   : Then the interpretation document has to be original. Non-original documents will be considered. Now. documents. We count the 50% of the entire grade. 1:   : So obviously it it's a 1:   : it's a little. 1:   : But again at this point in in time for this courts I really want to be sure that you are using the code for doing something. 1:   : So that's basically it. 1:   : hey? Some Hmm. Students in the class had cases of a plagiarism 1:   : for more than once. 1:   : So 1:   : if you are in a debt category, and you are a close 1:   : 4 over 3. Be aware 1:   : the the next one will let Trigger. 1:   : the reporting to the Ethical committee of your case. 1:   : because you have a history of cheating. I'm not sure that you will pass 1:   : the the committee, meaning that you could be expelled. 1:   : So that's a seduce matter. So Don't, put yourself in that condition because 1:   : I will not do anything at that point, because I told you 1 million times, and I'm still selling you. So the next one we trigger the notification to the on our board. 1:   : Okay, so that's all I had to say. Most likely no one of you is in that situation. I hope the whomever is in that situation that we listen to to the recording, and we will act in the proper way. 1:
 

 

atts STEVENS

lw INSTITUTE of TECHNOLOGY

lll

TTT

Data Exploration - Template



clipizzi@stevens.edu

SSE

 

 

 

Contents

LJ Project Goals and Conditions

LJ} CRISP
L} Business Understanding
LJ Data Understanding
L} Data Preparation
L} Data Representation
L} Practical Results - Conclusions

LJ Attachments

 

 

a
STEVENS INSTITUTE of TECHNOLOGY | 2

Project Goals and Conditions je

What are the project goals¢ What is the key question you are
required to answer?

Are there any conditions limiting or somehow defining the project,
like limited access to data, data too old, time constrains

A brief description of the expected results may be added

STEVENS INSTITUTE of TECHNOLOGY | 3

 

 

Contents

 

  

L
 

 

 

CRISP-DE S

Background Info & Definition

An adjusted version of the CRISP-
DM: “CRISP-DE”, with DE being
Data Exploration
• Focused on extracting
information from data
• No modeling, only descriptive
statistics and visualization
• Consists of 4 phases, intended as
a cyclical process
• All phases are necessary in every
analysis

STEVENS INSTITUTE of TECHNOLOGY | 5

Business Understanding ui

S

° Definition*

— Define business requirements and objectives
— Translate objectives info data mining problem definition
— Prepare initial strategy to meet objectives

* You want to be sure to clearly describe the business
needs and the steps to address them

*: from D. Larose — Discovering Knowledge in Data

STEVENS INSTITUTE of TECHNOLOGY | ¢

 

Data Understanding iw

° Definition*
— Collect data
— Assess data quality
— Perform exploratory data analysis (EDA)
¢ Overall data description: sources, organization, key

characteristics (sensor/human generated, reliable/unreliable
source, ...)

¢ Here you run all the descriptive statistical tests that make sense for
the specific case, describing the different steps and their specific
meanings

*: from D. Larose — Discovering Knowledge in Data

ee
STEVENS INSTITUTE of TECHNOLOGY | 7

 

 

Data Preparation ie

° Definition*
— Cleanse, prepare, and transform data set
— Prepares for modeling in subsequent phases
— Select cases and variables appropriate for analysis

- First define the steps you are going to perform (e.g.: if you
normalize, why)

¢ Here you perform all the data transformation applicable to the
case: missing/miscalculated/misplaced values, outliers,
normalization

¢ Describe the final dataset (format, new records number, new
variables, ...)

*: from D. Larose — Discovering Knowledge in Data

ee
STEVENS INSTITUTE of TECHNOLOGY | g

Data Representation lw

Definition

— Select and apply one or more descriptive statistics to the
dataset

— Select and apply one or more visualization to the dataset

— If may be an iterative process: adjustments may be required

— |f necessary, additional data preparation may be required

Explain why you selected a representation to an other

Describe final results

Read the results with business sense and provide your comments

STEVENS INSTITUTE of TECHNOLOGY | 9

 

 

Contents

 

LJ Project Goals and Conditions

L} CRISP

L} Business Understanding
L} Data Understanding
L} Data Preparation

LJ Data Representation
LJ Practical Results — Conclusions

LJ Attachments

 

Eee
STEVENS INSTITUTE of TECHNOLOGY | 19

Conclusions iw

° This is the final recap: you briefly describe the whole
orocess, from the business need, to the data
collected, to the representations you choose, to the
results you obtained

° Describe possible limitations of your analysis and future
possible developments

STEVENS INSTITUTE of TECHNOLOGY | 11

 

 

Contents

 

LJ Project Goals and Conditions

L} CRISP

L} Business Understanding
LJ Data Understanding
L} Data Preparation
L} Data Representation
L} Practical Results - Conclusions

LJ Attachments

 

MU
STEVENS INSTITUTE of TECHNOLOGY | 15

Attachments ie

¢ All the additional/non essential tables and graphs will
go here

¢ Add only the outputs that can support the case you
described in previous slides

¢ Outputs have to be either readable (no 1M row table
in 1 page]

STEVENS INSTITUTE of TECHNOLOGY | 73

 

  
: Hello. 2:30   : Hi! There! Evening, Professor. 2:34   : How are you 2:37   : doing? Well? Happy to be out of work. We're doing our our release planning week this week. So it's 2:39   : hard. Course. 2:46   : What? What? What is the company you are working in? 2:49   : I work for? Lockheed Martin? I'm: a: software right now. 2:53   : Okay, okay. 2:57   : we have a quite a lot of business with Lo located Martin. We had more in the past, but we still have a quite some students from. 2:59   : We just happened to split my project into a dev, and then a set offs team, which 3:13   : kind of contrary to the like. I feel like that's set off. But you know. Oh, well, we'll see how it works out. I happen to be on the the set ups, too, so i'm excited to get 3:21   : it more into infrastructure and out of depth. 3:33   : Good! Good good is this call so helping you in your activities as some also did contribution. 3:36   : it's helping. So I happen to have applied to a program within the company that, and even if I don't get it, i'm going to try to move into more role that python plays more of a rolling 3:46   : currently. I'm. I only use really like bash scripting yaml 3:59   : like pipeline sustainment. 4:06   : I use this necessarily in my day to day, but I from my capstone and undergrad. I actually did a AI project that I let. And so this is kind of bring me back to that. But I haven't used Python. Really, since then. 4:08   : I mean that if you want to take the opportunity of the to do something that is more in line with what you are doing at work feel free to do so. 4:24   : I mean that I will be happy to help you as much as I can. 4:35   : Yeah, I think only halfway through the degree. Also, there are classes that, as far as electives go. 4:41   : kind of inner working, the 4:50   : you know, trajectory of the with. Even some like systems work would be like, really, you know. 4:52   : Help? My. 5:00   : Yeah. Yeah. Yeah. Yeah. 5:01   : Okay. Good. So anyone else want to share how the courts it's helping or not helping what you do at work. 5:05   : I will say one other thing, I think. 5:19   : At least it's helped me gain some more 5:22   : planning credibility like within my team since i'm one of the youngest contributors, so like that's kind of been a nice knowledge. Slash confidence to have the you know the classes in my back pocket. 5:25   : It's good. Good. 5:40   : All right. Okay. So 5:42   : let's move with the content for this class. 5:45   : So we we basically have 2 more classes, and then they they will be the presentations of your final. So again, if you have any question on the finals, that's the moment for you to ask if 5:51   : any. the again. The the final is going to be a a project. 6:11   : You can develop the project in a team, as some of you already communicated to me. 6:20   : You can pick one on the 6:29   : suggestions that they provided the on canvas. You don't need to do one of those, but it's just for your convenience, somehow. 6:32   : ere 6:44   : or a possibly data that even question that you have, and you want to take the opportunity or the final to do it, feel free to do so. 6:57   : Final will be pretty much similar to what you did in the last assignments. 7:06   : the 7:16   : keeping in mind that that is a final meaning. If for the assignments you do an interpretation of the results? That is a a few page for the final, it should be more. A substantial is not. 7:17   : and 800 project is, is not a TV, so it's not a dissertation, meaning it is not something of 20 page or or a 50 page. 7:34   : but it should be something around 1015 page, all included. 7:48   : I I share with you the 7:55   : structure that the final shouldn't have. So again, there are 4 phases defining the business defining the what the the data set and analyzing the data set. 7:59   : to be sure to bet it somehow, to be sure that the the data set is able to address the the questions so that you play the in the first step. 8:14   : then the the data preparation that is always a critical face. 8:28   : And then you have the data exploration in a broad sense. So we are. You do all the 8:34   : the visualization. So you calculate the metrics and things like that. 8:44   : and it's where you actually will write the the narrative that will put together all those pieces in a a way that people I can read, it can understand, and even if they may not be experts. 8:53   : all about how to do things. They may be expert or the domain, but they may not be expert in writing code or or extracting magics, 150, 9:12   : so 9:24   : a complexity. 9:26   : The last assignments are up a little bit longer, so they are around the 100 lines. So, as I was mentioning last time, the number of lines is not necessarily a 100% through indication of the complexity of the problem. 9:30   : But it's kind of difficult to have a a a problem 9:51   and analyze the with the proper level of complexity with the 20 lines of code. 9:57   : Yes, on not all the lines are created in well, but 20 lines, so it would be not on enough, probably not even 50, probably not not even 80.   : So be sure that that you have a a, a, a a script that you would use to extract the the the metrics creating the scripts, the visualization. So   : that is more or less, or at least   : not shorter than the last assignments. So something around, I mean from 100 50, and a move.   : So again, the length of of the script is not a a, a, a, through a complete indication of the complexity. But again, it is difficult to to do a complex analysis with just a few lines of code.   That's   : E. C.   : Generally speaking, when you send emails to me at the she you, because this way you double the possibilities to get an answer soon, we will give you an answer.   : But sometimes we are droning in emails, and in particular, when we are approaching at the end of the semester, and your email may get lost.   : So just to be sure, add always she you in the email that you sent to me.   : Okay. So this class and the following one: we'll be more on how to apply things. So there is nothing more that really we need to explore either in the informatics part or in the   pied on paths.   : The main reason is because obviously we could go into more details with Python. We didn't   : erez agmoni work with the quite a lot of potentially useful libraries. We didn't spend too much time in applying some techniques. 101.   : One of the things is what we we want to reduce loops using operations with Madrid sees, but that would be more on a spending time in a linear algebra, and this is not the goal of this course.   : At this point I I really think that you have all the elements in terms of pied on that, and in terms of how to develop projects, to use a   : coding, to create a story and to extract the meanings from a data set in the direction or questions that that you may have.   : I will spend a a little bit of time on using a chat. Gpt.   : A. Recently I mean Chat Gpt is a   : representative of the family of large language models   : ere   : so Chat gpt is one is not the only one it's for sure. The one who created the the   : the most of the hype that we have since the announcement, or the availability of this tool. That it was a you know November, but it's not the only one. There are some others that are coming.   : I I've been working on a those.   the   : relatively a lot.   : I've been interviewed on how to use those bots for education, and there is no answer, because no no one knows.   : But I will share with you my opinion, and I will also share with you a little bit of insights on how to get the most out of them.   : So it is a matter of fact, that is there a chat gpt like that, so that will come. It's definitely good to know what our   : the the potential uses on one side and the limitations that they have, and also how to better use the the tool to get something done.   : So we we will talk about that. It's not something that that was a scheduled, because I mean that we started the E. M. 6, 24 way before the so but I mean this area is evolving continuously   : and I don't want to give you the impression that we are not following relevant changes that are happening in the sector in the area, having a a potential high impact in what you do.   : So let's go down to what is more directly related to this courts, and let's go first on   : the proposed solution for the current assignment. So the assignment was basically on downloading elements from a a web page and do some processing.   and some I mean, some cleaning, some processing, and some visualization.   : So   : again, like always, that's one of the medium possibilities for addressing this problem I create. I imported all the libraries one of the libraries I didn't use it. I don't know why I didn't delete it, but that's the way it is.   : So. As I was mentioning last time it it it's a good way to proceed to create your own a cleaning function for a text. So in this case, again, may not be the most effective.   But   : look at it as a source of placeholder, so you can change it, you can make it yours. You can customize, but you want to have a your own, a text cleaning function.   : So in this case this function is taking a a string to a process to clean the minimum length of all the words that we want to consider.   : Assuming that that works, that they are shorter than a a given number of a characters, so are not semantically relevant, and then a list of so forth.   : So basically the function is a transforming the string awards into a list, and then it's looping into the list 250.   : It's a transforming in a lower case. It's checking, if it's Alpha medical or not, 250,   : and then it's checking. If the length of the the word the it's bigger than the threshold that we set, and then checking if it's in the so, what not? If all good will append that the word to the list of clean words.   : and then we loop up to the end of the list, and we'll return that, or of the initial string, and we'll return the the   release of cleaning up in clean words.   : So again, you have an explanation here on what is in and what is out.   Then   : it's about webpage. So I defined what did the what is the URL for the the web page? Do they want to get?   : I'm getting the the content?   : I'm moving the content into a 3 like structure, using a beautiful soup.   : Then I initialize a string that we hold the the words in the headline.   : i'm. Looping into the paragraphs Again: If New York Times will change the label for paragraphs from P to something else. My code will not work anymore, and   : those are the limitations, or this one, and simulates are the limitations for a scraping content from a website.   : and I appending the   then that   : initializing the listed will contain the so forth, reading the soap or file, extending the so so far, the the the so   : setting the minimum length.   : filtering the words, using the function that we saw before   : extracting the diagrams   : and getting the most, the 7 most common. I could be any number   : then   : creating a a a, at least of the most common   : filtering, I mean extending the the list of words, so that I had the after the cleaning with the those common diagrams   : transforming up the the list of awards into a string, because work cloud is taking a string solely.   : and then pretty much printing it   : and saving eventually as a Png file.   : Then I analyze in the sentiment, using butter. positive, negative and neutral printing it and the process running it.   : So you have here your work, Cloud.   : You have the headlines.   : I mean. Some more cleaning should be done.   : and obviously is, a lot about. Trump is real time, and today it's pretty much trump day.   I like it or not.   : and that's that's it.   : Sentiment   : so again, like in most of the cases, a new drama is the vast majority.   : It's more on the negative side now, so it it would be interesting to do it end of the day each day and see if and that is a trend somehow. It's negative.   : anyway. So that was the   : assignment that   : I mean if you look at the length. Yes, there are quite a lot of spaces. There are a lot of comments the length it's 132, probably the real number would be more 100, even a little bit less than that.   : So just to have an idea   : that's a kind of a basic, simple, relatively simple, but that that there is a process in it, that I mean the the script, the a as a pipeline. So the cleaning, the downloading, the cleaning, and then you have the different analysis.   : All right. So let me go here and let me   : go into   : the content.   so that that was a   : the the assignment that we just so the possible solution   : they close this we do not need any more, so the content will change a little bit. So we spoke about the machine learning a little bit in the past. I I I want to fault you more on   : how to extract metrics from text, and then we'll be talking about the the Ls.   : So let me start with the and I will definitely give quite some time for the the in class the assignment.   : So let me start with the   : a couple of how we extract   numbers from text.   : So one of the issues let me stop for a second sharing this. So one of the issues that we always have when we deal with text   : is that   : text is. I mean it's text, and there is   no number. I mean, not semantically relevant in the text, or may not be   : how we get insights from the text in a numerical way, in a way that we can   just take decisions based on that.   : So decisions it could be of any kind. So we will solve this on 2 examples, that they did the in the past few years. So one is on social media. So when I started my Phd.   The social media was kind of a beginning to be relevant.   : But there were not many metrics to measure the   : semantic, the meaning in the messenger that was in the social media.   : So my idea was, we need to find a way that is beyond the the the, the the statistical analysis, the number of weeds, the number of bits per time interval, or the number of tweets from someone at the number of retweets. So those are loosely toppled with the semantic meaning.   : Okay, obviously, you have more tweets from an individual that that individual is more relevant that someone I like myself with not many of those.   : but it it it's really not a great indication, I mean. It is not in in the content itself, but it's more from like like looking from a a outside.   : At that time they were not many solutions to go into the content, and extracting a meanings from the content.   : and that was one.   : After a few years I did a.   : I was asked to be the principal investigator that is, the project manager, the one doing the the, the the designing, the solution for the problem in a large project for the the Department of Defense.   : It was a the a 4 million dollar and change project over a 2 and a half years.   : with the 20 between 20 and 25 people in the in in my team.   and the question was   : the   : how we can do better the planning, the the the cycle of production or Webinars, so that   : customer was the the Piccadilly at all. So   : they plan a new weapon. So   what in advance   : and typically the way to do it is using the capabilities, meaning more powerful weapons and improving the effective mess all those weapons, so the usability and the effectiveness.   : But that was basically it. There was nothing working more on now.   : How they said now that   : events may have an impact on the cycle, the life cycle, and eventually the the planning of new weapons.   From now to I mean the duration of the life cycle.   : So we we we started thinking, okay, we need to work on a a. We need to analyze what our the technologies that are making an impact on the competitive scenario of the   development. And the so   : we fall used on papers, pardons, and use giving a 3 different stage somehow, or the maturity, all the technology.   : and we for use the on.   : I mean, obviously for the Department of Defense. The for use was on countries.   : and so the competitive scenario was with the competitors, being countries   : in universities, and in particular in a graduate program. So the vast majority, meaning probably 85% or something around that are international students.   : Those topics are are sensitive topics, and they need the at least   : people lot.   : I mean national. We have National is either a city zen, or a a green card holder.   : We didn't have many of them so. And then, when you move to the real content, the the real text that you want to analyze that you need the the security guidance. So I got my my security guidance. But I was the only one in the team with the   : because of that we switch the from the Via a scenario to and sort of an ultimate reality. So the alternate reality was about   : the security industry, and we for use on the   : operators in the security industry, publicly traded the companies working in the security industry. So at that point you have all the information you want, because they are a publicly rated.   : All the information are public, because again, they are publicly traded companies. So and you have, but because if they are publicly traded.   : chances are they are a relatively big. You have quite an other news, and there could be some pad and some papers that could affect the somehow. They've strategies in terms of technologies that they were using   : Erez agmoni. But the problem was still there. So how you guessed a a competitive scenario, meaning doing a risk analysis. Risk analysis is based on numbers 250.   : So you need to have numbers to work on a   : risk panel and work on a what if analysis with the different scenarios that you can have in this type of competitive environment   : decisions that could be what is going to happen in the competitive scenario If I invest more in one technology and less in another one.   : What if I totally dismiss one technology? What if I go all the way with another technology. So all of those are elements that really   : need to have a to evaluate.   : So there there was a missing link social media   : how you get so the metrics relevant I mean the semantic metrics relevant to to understand what's going on in the social media and on the other end how you get   : the metrics that will allow you to work on a   : management, risk management or a risk analysis in that the second case.   : So let me   : go into the 2 projects to   : explain a little bit how we did it.   : In both the cases I was a a force to create my own. I'll got it, because in both the cases there was a nothing that could help me at that point.   : So   : social media. Again, I I will go relatively fast, because I I really want to give you time for the in class exercise. So   : so most of the analysis at that time, but even now are based on either a statistical analysis or a sentiment analysis. That is what it is.   : So that   : idea was to extract meaning somehow from a a text. Why, this is   : social media at that time in particular. Now I don't know what it's going to be with a tweet or a   : but we tweeted a social media was a what we call the a back channel for a   : a real life event, meaning wise people. We are doing something in real life. They share the comments via the social media.   : I still through   : not sure that the Tweed, or is is still a the number one social media for those things. But still, when you are experiencing something that can be an event, or can be watching a TV show, or whatever   : or commenting a political event. Again, you share your toes on that   : this channel. So this channel is what is normally called back channel. So the front channel is real life that things that are happening. The back channel is is basically the social media giving you insights on what is happening.   : We consider a 2 types of events events that are not evolving in time meaning. You have one single event, the presentation of a new product, or or a a movie, or something like that.   : and you want to get some meaningful keeper 4 months indicators that that can give you a sense on what was going on, and then   : events evolving in time. And you want to monitor how how things are going, and eventually change what you are doing, based on the feedback that you got.   : So again   : we use, you know, Twitter with most of the social media. You don't have a real conversation, probably the only social media. I mean that that there are very few that can really be. I consider a conversational where conversation is a You say something.   : the other person is replying in a certain way, and there is a back and forth. So that's a compensation. There is no real conversation in most of the social media.   : But when you have a a common ground, so the event that that you are tweeting about the then there is a sort of a convergency around this   : common events, so this having a something in common, it's what is called the the common ground theory, and it's well documented in the the literature.   So   : the methodology that they use the was detecting the the event collecting the data. I use the a combination of the search Api   : from a twitter to python python for all the cleaning Mongodb for getting the data.   : I mean, when you download the the tweets, you get a a. Json file. A Json files are a really easy to be handled by a mongodb   : Mongod. It' be it's a a non SQL or a database.   : and the structure of the I mean, since to be kind of all the but the the the the engine of Mongodb is based on Jason structure, meaning. If you have a Jason, you dump the Json into Mongodb that's optimizing the tool You have.   : What some other things I extract from one? Would it be some particular characteristics? And then those will go to a a Mysql database   all the pre-processing   : when you have a events that are happening in a given period of time. I created it back. It's all tweets just to compare   : the semantics for each period of time.   : Then I I combined the metrics in   : sub events along this major again, so that i'm monitoring a and then create eventually classification model, and then visualize, evaluate the the capabilities of the model.   : So the methodology was in in multiple stage. So the first stage is obviously collecting, as in the previous chart, the data   : and then creating a 2 types of a networks, one that is the social network. That is the easy one.   Notes are of the messages.   : and 2 notes are connected. If one is citing another.   : so doesn't need to be retweets. But if you have the name of a another, user meaning another screen name, then you have an edge from the 2 notes.   and there's a social, and then I created what is called the bipartite network.   : That is a so bypassed networks we are the the elements that are connected the are are these joint? So the TV gala example is when you have a I don't know people activities.   : So you have a someone doing a a given activity. The same activity can be done by multiple things. So the same person I can do multiple activities.   : So, looking from the outside. That is a network like all the other. Inside, the the 2   : sets are semantically this joint, so you need to lay bowl up without the notes in one set, and what that the notes on the other set. But once you do that, then when you connect them.   the network that you have is what is is called the bypass.   : From the bipartite you can extract the it's called one mode where   : there is no 2 sets, but it is just one. You can have a a network, all the people only a network of words only what's or a backgrounds.   : who works are connected, if used by the same person.   : because we then go with the unique words, the same word that can be used by multiple people like creating networks that that can be somehow complex, like the example that you have on the top right and and is a the other example.   : Once you have this network, this network as a   : words or engrams only   : is a W. What is called a semantic network.   : Someone, in some cases they call it a knowledge graph   : I it   : because you have all what's works in a broad sense. Again, it could be a single words or multiple words like diagram and gram in general.   : Once you have that you can apply class setting. I'll go it so, and there are I'll go. It's for clustering in graphs, so the most popularly use the is the lobby and community, the detection method and basically you have a   : classes awards. Those classes awards are potentially topics, 2   : because those are worth, so that are highly interconnected. I had the student validating the topics created in in this way, and for using on the last topics the accuracy, I mean that   : he did it manually on that 5,000 tweets, and then compared the results with my script, and the accuracy was a close to 90%.   : So you have topics at that point.   : If you analyze in time those topics, you can see how the conversation is evolving in time.   : So we applied the to   : quite a lot of different events. I. I share the scripts with people   : in many places around the world, getting, I mean, and validation of the results.   : So this case, I mean, I consider that we are talking about. A few years ago I collect it. So this was a 2,015, but the concept is pretty much the same, was the announcement of what at the time was the new apple watch.   : I collected the 700,000 tweets. There was a quite wide, less coverage for   : everything that was presented in each particular moment of the event.   : So I did the what I presented before, so I created the the relational network, then the semantic network.   then I I They do the preparation.   : And then I created the visualizations. So, instead of a timeline with the time I use the the single events   : meaning the introduction on the apple TV, the iphone, the new functions for Macbook, and then finally the apple Watch.   So.   : and then I try to find a way to connect those. But this is not the the one that I want to spend more time. I want to spend time   : right here.   So   : I was mentioning that there are 2 networks, the relational network and the semantic network. One of the metrics did they extract it from the graphs is the clustering, coefficient.   : The clustering coefficient is a a way to measure the homophily of the network in a given time, meaning how connected the are the notes?   : So if you look at this spy. Here you have a a high value for the clustering coefficient for both the relational and and the semantic networks.   : That was the moment when they introduced the the new Macbook. So W. What is the interpretation? The interpretation is a. At that point. People where   : talking about the the the same things. So the new keyboard that that was a total of failure, but was a the new keyboard, the new, the new monitor, so few topics   : gathering, collecting the attention of the the Macbook enthusias. So it it was a people gathering together and talking about a few things.   : This is when the the apple watch was introduced. So you have a relatively high, clustering, coefficient for the relational network, meaning the   : people gathering together, talking about something. But the something it was all about the places, because they didn't know the product. So it was a.   : It's good. I like it. It's cool. Not only knows how much it's going to cost. So things like that it's more on the as more to.   : We use the same approach, to calculate or to evaluate the radicalization of a groups of people.   : So we measure the in time, the clustering, coefficient, and the semantic, I mean the costing coefficient for both the semantic network and the relational network, and we created the a a composite index   that is kind of a the summation. They normalize the summation of the 2.   : If you analyze in time, when you see that there is a growth on this composite index.   : There is a tendency to the radicalization, because you have a more people gathering together and talking about few things. So that's a radicalization. We use that to analyze   : violence related to political elections in Kenya was a few years ago.   : Okay, so this is an example. I don't want to spend too much time on that, but I just wanted to give you   very briefly. This was a. On   : predicting, so I collected data about 2 million tweets, 22 movies over a period of a few months   : a. And then I I wanted to predict the either one on those 3. So   : the box office revenues 3D score audience score. So we ended up not having much on both of the critics courts and the audience for a little bit on on the audience core, but definitely nothing with the critics score.   : So we collected the let me go very fast. This. So we collected the all the data on the   : then we created some prediction modeling. So in this case is a decision 3. But we also tried a different models.   : Then we put together all the models in one chat. So we created that for categories of metrics, metrics on the sentiment, medics on the traffic that was not the number of tweets, but number of tweets per time unit   : social meaning, the clustering coefficient for the social, and then some semantic medics.   : So and we I played with the combination of all of those.   : So the very end. One thing that is interesting, that that sentiment is is not a good predictor.   : meaning using sentiment that you cannot really, I mean, in this case, was predicting the the box office revenues after week. You cannot really predict the the box office revenues based on sentiment. What people say is not what people do   : so, but traffic, social and semantic. A combination of those is definitely what it it's more over an indicate or what people is going to do.   : and then we applied to other cases. I don't want to spend too much time on that.   : That's another application we try to predict.   : to calculate for the the emotion generated by artists. I mean   the   : target was a music. So artist's, albums, and songs, using the Bluchnik emotional wheels. So there are those major emotions.   : 3 shades, and we try to analyze the proximity of all the the the the   : I mean. We can see that the comments that people left. We had a huge data set of comments, the   : correlation somehow, between comments and emotions, because in this way you could eventually create a sort of a emotional DNA. All the the artists songs and albums, and then eventually   : see if there are correlation between the number of all booms, a number of songs sold, and the emotional partners.   : So there was another application. We use the   so we for use on   a fan of those 3. That's why we selected them.   : and those are the numbers of the a number of artists, lyrics, rooms, and members on this, on this website that is no more active.   : So again.   : in this case we use the natural language toolkit. That was not a a a great way to do things so natural language toolkit. We mentioned that has a   : whatnet where you have this taxonomy, and you measure the distance between what's and we measure the distance between the emotions. So the what to represent in the motions and the words in the comments like tools that you see here.   : But the problem is a word net is relatively old.   : and a comments are younger, and the match was not good. Do we revise it in that in a different way, and we didn't use a natural language toolkit. And I think we're definitely better.   : Okay. So let me jump up to the other project that I was mentioning.   : That is, this 1010. Not that was the internal lacking code for this project, and a again, the project   : was a about getting metrics out of a a text.   : So let me skip all of it, and let me go here. So the main issue, as I was mentioning before, was on the how to calculate the semantic matrix out of the   : a text.   : So that's why we I created the this   : room theory.   : So   : let me   : skip some of those and let me go here. So room theory. It's based on the framework theory that was created by Marvin Minsky in a mid or seventies.   : The example that that means he use the was a. When you enter   : into a room, you know, right away.   : If a is a a bedroom, a bathroom, a a kitchen, not because there is a label saying a path from kitchen or bedroom, but because there are things in the room that it's kind of a a resonate with a classification that that   : meaning a framework that you have in mind. There was a no computational component on it at that time.   A.   : But I like the idea of this framework approach. I added the a. A computational layer to it, and I   : named it the room theory, using the example that Marvin Minsky was using.   : So those rooms are a representation of the knowledge of the individual or the specific knowledge of the individual   : entering the room or doing something on a given domain. 1:   : I use the at the time what to back is a form of a victorization. It is one of the for not the first, but one of the first of the new generation of a victorization of text. So you basically have 1:   : an application of the Meta for a of the so to what so are related. 1:   : If they up here somehow together. 1:   : What to back is calculating the conditional probability of one word, the appealing because of the other is using a a shallow neural network for doing that 1:   : we are. Charlotte means 1:   : one hidden layer. So you have input. Layer output layer, and he delay it in this case is one in the layer. 1:   : So 1:   : you have this conditional probability, meaning that you start with the matrix. That is a and we're in the unique words in the text that you are using by N. And you have numbers. Then you apply a reduction in the dimension. 1:   : like principal component analysis. then you have. 1:   : that is, in by whatever it is, the number of 1:   : typically we use a 200 or 300, meaning that each word or anram will become a a, vector with the 200 or 300 components. 1:   : meaning the words in the text that will become a points in a n dimensional space. Now, if you go down from n dimensions to 2 dimensions. You have a a a Cartesian space, and you have a points on the Cartesian space, representing the words, the more the words are closer. 1:   : the more they are logically related. So that's basically what is behind the the vectorization, then I mean 1:   : with the attention it's all you need, the they use the potential mechanism to do better, that just the conditional probability awards. And 1:   : but the concept, I mean the result. It's creating a a matrix representing a the text in that, no matter of terms. 1:   : So you have a text transform into numbers. 1:   : And that's basically how the room theory would work. So you have a 1:   : your room. That is a a numerical representation of your knowledge base meaning. You collect as many documents as as possible. You victorize the documents, and this is basically a Madrid 1:   : representing the knowledge on the specific domain. 1:   : Then you have words or engram so defining your your 1:   : You are a elements all the attention, because I I mean you as an individual, I can do different things. 1:   : So with the same knowledge you can do multiple things if your fog is the I don't know, in the financial aspects your fog is will will be on. I think so. The that are on the financial side. If you are in project management you are more on a those words. 1:   : So the the the benchmarks are our collections of of keywords, and eventually wait the all the different keywords. Then you have a document that you want to analyze. You scan at the document what by word? 1:   : And you calculate the proximity, all the words with the the the words in the benchmark, doing a look up getting the vectors from the the madrics. That is the room, and then the the result will be the 1:   : the distance between the document and each one of the benchmarks. 1:   : So in a symbolic terms, you basically have your corpus 1:   : that many documents representing the knowledge that that will be a victorized. Then you have a 2 elements that you have the keywords in the benchmark, and you have the the document that you want to analyze. So 1:   : what you do is basically what? By what? That you take one word from the text to evaluate one word from the benchmark, and you calculate the distance 1:   : that most simple example of calculating the distance is the cosine a similarity. 1:   : So you calculate the proximity that is a number, and this will tell you how much each word is a far from each one of the benchmarks. 1:   : and then you add all of them. 1:   : You normalize the the results, and you have the similarity between the document and the 1:   : and and the in each benchmark and the cumulative value for all the benchmarks. 1:   : meaning that at this point I can tell that one document is more on a given technology or another technology. So meaning at this point, i'm really getting the values. 1:   : So that was a the way we use the for all of this. Then 1:   : we created the a. A. A process, a pipeline. So you have a gathering the data meaning You have a news patents, papers doing a little bit of a preparation, going into a Mongodb. 1:   : creating a the rooms or or just analyzing it. And then the results, meaning the the actual metrics will go in in a relational database we use. 1:   : but it could be Mysql or or any other, and those will go with the use, the by, the 2 systems, this panel, and then another system for monitoring technologies. 1:   : We will go back to the applications. But the goal for today is just on how to extract metrics from text. 1:   : So I I would stop here, and I will go back to examples. So 1:   : next week. 1:   : But I just want to jump now into a chat, Gpt. 1:   I'm. Using a slides that I presented. 1:   : Here we go. 1:   : that I presented the to a workshop that they gave 2, 3 weeks ago 1:   : so large language models. So we mentioned a a chat gpt that is, a Gpt based on a transform that's so the transform that again are sort of the next generation to what? To back 1:   : ere this is an essential step. You cannot really do much if you do not do the transformation. 1:   : But anyway, so erez agmoni, I mentioned also the fact that what to back was a shallow neural network, meaning you have one hidden layer one 1:   : when you work with a neural network. So, and you already know that you have parameters meaning weights that you give to each one of the inputs. So those weights are are normally called parameters. So when you see those numbers. 1:   : Gp: 3 as 175 billiona parameter, so those are the weights in the the network. That is an indication of how complex is the model. 1:   : So we are going into several 1 billionparameters. 1:   : So, Bertha, that was a one on the first using transform. That was a 340 millionNow we have more than the same number in billions. 1:   : So when you train those models like Chat 1:   : Gpt, the training? It's a really an intensive process and an intense I mean a a very time consuming a resource, consuming a process. 1:   : There are a few months in the loop for cleaning the data, meaning eliminating all the in appropriate content, but also to start the tagging. So we will go back eventually on that. How J. G Gt. Was trained 1:   : if you look at the number. So at the bottom, on the page 4.6 million. 1:   : It was the cost to train the the board. 1:   : the energy that was used because I I mean it's a lot of computing time. They use the 1:   : quite a lot of N. Media Gpu Graphical processing units a. A. And they use a lot of energy. So 1:   : the amount of energy is enough to power a more than 30,000 American households for a day. 1:   : So that's how 1:   : I mean that energy, intensive is the training keeping in mind. The human mind is using a fraction of a fraction of that 1:   : much energy for the entire life. meaning that those systems are highly inefficient. So we are applying brute force to an algorithm that 1:   most likely it's not really representing the way we reason. 1:   : So those are the sources. So there are 1:   an estimated 45 data bytes of text 1:   : That's the distribution. So if you look at the distribution, you have a quite a lot of English, and that's an intrinsic buyer, so that you have in Chat G. G Gpt. Meaning that other languages are are under represented. 1:   : then that Yes, the web is pretty much in English, but that doesn't mean that that you don't have a good representation of the other languages. 1:   : How we evaluate the how they it's very difficult to say so in the 2,016. 1:   : This lambda the data set. That was a 1:   : use the as a benchmark for a task oriented the 1:   : language understanding so based on that. 1:   : I mean, they are pretty good compared to humans. 1:   : It it's just one of the ways to measure it. When you see 0 short, few short, the 0 shot models with no training data for that particular task. 1:   : few short meaning. You have some examples. So 0, short and not very effective 1:   : limitations. We we really need to understand how they work. So they work as a a matching patterns. 1:   : So you have the training generating 1:   : with the some human intervention, and then you have your quidy. 1:   : and what the B is doing is matching the the pattern in your query with the pattern, so that 1:   he does. 1:   : and then, once it does, the match is adding a a presentation, a a a conversational layer. To present the results. 1:   : The more data you provide in terms of you are quitting. 1:   : the more reach would be the answer, because there will be more elements to match. So that's a a key point. 1:   : So first of all. 1:   : those so far 1:   : are pretty much the same as a Google with the difference that you have a compilation, or or the answers, and a a a, a, a, a a presentation of the results in a plane, English or a plane, whatever is the language that you are using. 1:   : and that's valuable. But you are losing the reference to the sorts. 1:   : meaning that if you are using a chat gpt, or another Llm. Or something that that we go public. 1:   : There are chances that someone can sue you for a. 1:   : because you don't know what the source is going to be. so I will show you in a moment the the way i'm using it. 1:   : So again. 1:   : It is not intelligence, but is a a a a nicer way to do what Google was doing, since more than a decade. 1:   : But it's great. I mean you have a a plain English interface. 1:   : i'm starting a project on having 2 projects. But before I go that let me go in another element that is relevant in in terms of limitation or or 1:   : and they are not domain specific 1:   : meaning, considering how they had been. Train the and those are the sources 1:   : you you cannot have a specific knowledge in one particular area. If you are in the defense industry. 1:   I mean that. 1:   : apart from the fact that some of the material is restricted, and for sure, it is not a public domain, but even in the in the public domain that are a several elements that are that could be potentially relevant if you want to do something in the defense industry. But in this case those elements there will be a drop in the ocean. 1:   : meaning the answer so that you will get that will be highly deluded, and you will not get the the knowledge from one specific domain. 1:   : So we we are working on 2 projects one to create an for the school of systems and enterprises, and I will start using 6, 24 as a an example. 1:   : the and we'll be sort of a Tudor out automatic tube, or for a 624, and I will use the the transcripts of my classes along with the articles, papers and textbooks. 1:   : and then eventually I will expand the to other courses. I will initially focus on 2 of the most popular courses that are 6, 24 and 6 12, but it is project management 1:   : the second thing that then doing a second project is to use the 1:   : the presentation. Uhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1:   : so it's like the next up blow instead of having visualization. So I have plain English. 1:   : Think about the the defense industry. You have people on the field, and they need to take decisions. They input the the values from the environment, and they have a scenarios. 1:   : But yes, it's a scenario. But at this point you have more either numbers or graphs. It would be great to have something in plain English. So once you have the conversational element that transform me, the conversational element from a written text to 1:   : oral text with the an automatic reader they can read. It is a no brain. And I mean that technology is there since 10 years. 1:   : So that's something I i'm working on. 1:   : I mentioned the mixed approach very briefly. 1:   So 1:   : I just want to go here for a moment. 1:   Yeah. 1:   : E: we are going a apparently to what the sort of so on one side the you have what is called the the prompt, the engineering again, a a a pattern matcher. 1:   : the better is your query. The more detailed, the more patents you are providing and the better output you are going to get. 1:   : There are jobs for prompt engineers. So people knowing how those lms are working, and on the other end the experts of the domain able to provide those input 1:   : it's basically like being a a super. User so that's a type of skill that is not 1:   : deep into the technology, but more on the use of the technology. And then you have on the opposite side what I call the the cognitive engineering, where you have those who will develop the next generation of of large language models. 1:   : I mean that the way they work now is is not efficient, and it's going to be changed. So that is no representation of the knowledge that is essential if you really want to do something that with an intelligent behavior. But we don't know how to do it. 1:   : So this discipline that will be at the crossroad between math, the cognitive science software, engineering, coding, the abstract thinking is what will generate jobs, not many, because it would be very challenging, but 1:   very critical jobs. 1:   : So briefly. 1:   : that's the way i'm using a chat Gpt: so for your information. 1:   : i'm writing a book that is called that I mean the name is still to be fully defined. Societal impacts of artificial intelligence and machine learning. 1:   : So th the idea is how artificial intelligence machine learning is is used right now, and what the impact could be to the society. 1:   : So the first question was, is this a revolution? So what is a revolution? How you measure a revolution. So 1:   : in that the the history of humankind that we had several social revolution, including the Industrial Revolution or the Dj. The the Revolution, are those potentially 1:   : the same impact that could the AI machine learning revolution be. 1:   : So I had to create somehow a a little bit of a 1:   : background. So I use the as a writing body Chat. 1:   Gpt. 1:   : I created the chats. So each chapter is a a 1:   : plenty of information. Each one is giving a pattern to match. So when I place a question 1:   : over here 1:   : over over here. I will get different answers 1:   : from another one. 1:   : So 1:   : that's 1:   the user that I encourage you to do 1:   : so. If you and place a query that is a a straightforward. 1:   : you will get an answer. That would be not much better that what you could get from Google. 1:   : But if you have something that is more complex, you will have answers that will be more complex. So keep in mind that. and 1:   : I get no problem. If you use a just gpt as a sort of a a in your development. 1:   : Don't, do copy and paste because you are doing your itself not a favor. 1:   : When you will get a job, I mean, down the road. The those boss will be so powerful that everybody and and so widespread that everybody will using it. 1:   : But if you use them now, let's say for coding. 1:   : keeping in mind that the chat G. G. Gpt was 3 in the on a github because Microsoft invested 10 billiondollars in open AI, and Microsoft is also your own of Github. 1:   : meaning all the code that is in Github, but somehow was used to train a chat. G. 1:   : If you use it for coding the you may or may not get a a a a good piece of code. 1:   : how you decide that if it's good, or if it's bad. you basically don't 1:   : unless you know how to code. So i'm introducing a a chat Gpt now, and not at the beginning of the course, because if you use it now, now, you can even using it for coding. But you know 1:   : what is right and what is wrong. So 1:   : also keep in mind that we will apply the same concept. The the assignments are individual. If any of you will do the same assignment in terms of coding. Each one will get the the to dollar divided by N. 1:   : Whatever is the tool that you are using. If you are just using someone else, or if you are using a chat G for it. 1:   : there is no way to detect, as today to detect if an output is generated by you, man, or from a boat. 1:   : So but again use it at this point. Use it to do better what you are doing. 1:   : Okay. So a again. There is not much time for it's a 7 min to 8, 1:   : and 1:   : erez agmoni there is not much time for the in-class exercise. But I want to introduce the the exercise and give you a little bit over 150 sense of what's going on, and then I will give you a few minutes just to 1:   : again familiar with it. 1:   So the 1:   : the data set 1:   : is one of the data sets, and I downloaded the from a tweed. So in particular, it's a 1:   : 20,000 rows, I mean. Tweets are more complex. But what I keep in the data set is a sender timestamp and tax. 1:   : and this is what you have here. So you have the sender. This is a 1:   a timestamp, but that is using what is called the epoch time. 1:   : That is a I mean some node that created a few years ago. That is, a number of seconds from the first introduction of unix. 1:   : But I mean there are functions, so to transform that number into a real time. 1:   : and then you have a in the text. So you have rt for retweet. You have the app for mentioning. 1:   : and so on. 1:   : So 1:   : you want to print the 5 most active sender, the 10 most retweeted, the tweets, the 5 most sided the screen name and the 10 most popular hashtags 1:   : so retweets Again, you cannot recognize them because there is Rt. 1:   : The 1:   : screen name. It's in the text is with the at sign. 1:   : and the hashtag as the 1:   : like over here and there are a few others 1:   here and there. 1:   : Okay. So I don't even create the the breakout rooms. So because it's a 7, 56. So 1:   : at this point I just want to be sure that you have everything published. So 1:   be sure 1:   : that you have what you need. 1:   : Yeah, you should have everything. 1:   : But I will post the solution for this in-class assignment in a moment. 1:   So we definitely don't have time for working on it 1:   : my apologies. but I hope it was useful. 1:   : So let me go into the next exercise. 1:   : That will be Exercise Number 11. So that's basically, what you have 1:   : when you click on it, and 1:   : instead of having a like that, that, you will have it 1:   : as a file. 1:   : So the file there is a spring 23 wn dot Docs 1:   : is a 1:   : is this one. So 1:   : I created that this assignment today. So if you see that there is something that it is not clear. Let me know, because I I 1:   : may have a a skip, something that that could be useful to you. 1:   : So one of the 1:   : considerations that some of the students in past semesters had on this course was, Why, Don't, we have more assignments that are more on how to use what what 1:   : we did 1:   : for managing situations 1:   : so. And that's why I created the this part. One of the things that I was managing in the past few years was a 1:   : the scheduler courses. So scheduling courts is in a university is not an easy task. Because you have a students. You have courses, you have instructors. 1:   : and you have classrooms. 1:   : So you need to match all those components in a way that you are serving the students and the best way possible. But you are using your faculty in the proper way 150, 1:   : meaning that each faculty as a teaching load, and you want to be sure that they fulfill that they load the they load. So in the proper way. 1:   : A. You need to be sure that the classrooms will be not too big, so there are many moving paths. 1:   : As you know it's even so. We have a a platform that is used for managing a the the community, somehow, that is called Work Day, with what they we can extract the the list of courses. 1:   : and the list would look pretty much similar to this one. 1:   : So you have a what is the academic units, the section, and a bunch of other information. distract or name or name, sir. did they 1:   : anonymized so instead of the name, you have numbers, so it's. Tractors are those numbers. 1:   : and then that the credit our what for each one and and and then you have a 1:   : some other information that they added. 1:   : actually just to be very open. 1:   : So the the actual file that we download the from what they is a 5, with all the courses from all the schools. 1:   : So I basically had to do a little bit of homework to cleaning it. To do this cleaning what I did that was creating a a a a a 1:   : configuration file. 1:   : That is this 1:   : where I have the name of the file. Then I have the 1:   : list of a names over a companies. We serve. What is the semester programs. 1:   : and then sort of a widely some black lease our courses that they want to keep or eliminate. And then the threshold for a 1:   : we consider as accountable courses with more than 8 students. 1:   : So, using those criteria, I 1:   : I created this script 1:   : to extract from everything, then what they is providing this script. That is a a reduced version. And it also added the level and program. 1:   : while i'm doing the calculation. So that's the data set that that you will use 1:   : on this data set. Those are it's 94 rose. This is related to to 1:   : last year spring 2,022. So those are the 1:   : the, the, the the rules, the the columns that you have meaning they attributes. And 1:   : you want to. The overall goal is to 1:   : get a sense of how things are going in that semester, eventually creating a dashboard to make sure that we are monitoring what's going on, and those are some of the elements that could that give an indication 1:   : printer the 5 courses with the highest number of students. So the number of students is in this column here named. The enrollment count. 1:   : Bring the 5 instructors from this column. Here instructors teaching assistant 1:   : with the highest number of students. Again, students are from enrollment count. Compare the total number of or students for a undergraduate graduate and a corporate, meaning that the corporate education 1:   : you will get that from 1:   : level. 1:   : So level is standing. Ug, undergraduate G Graduate Corp Corporation. You will skip the mixed ones. 1:   : Comparing that means, calculate the values so, and describe the results in a narrative in in the narrative path of the assignment. 1:   : Compare the number of courses that run at full capacity. So you have a this 1:   : that is a section status that will tell if it's open 1:   : or close the so if it's closed, meaning they are at capacity, open their top. 1:   : create a pie, chat with the distribution of students supper program a pie sh with the distribution of students per type of delivery. 1:   : You have a. 1:   : You have this. 1:   : this delivery mode in person or online. 1:   : and perform any other analysis that could make a sense to better monitor the the semester. 1:   : So you would read the file. You do the analysis. You will submit the the script and the interpretation of the So again, they need to be original, not describing the the process, but describing the the findings. 1:   : So what we are doing now is basically to use the script to get insights and eventually to take actions. So you have a 1:   : programs that are too small. You have a 1:   : classes that are too big. You have some instructors that are overloaded things like that. So all of those will give you somehow data points to take a decisions 1:   : all right. It's 805. That's pretty much the end of the class Questions. 1:   : Hey, Professor? It's me again. Yeah, Can you Did you see my email regarding the quiz for module 10, I think there are some issues or issues with question 5 and 6. It was marked wrong. But I think it's right 1:   : question. 10 1:   : Quiz: 10. 1:   : Yeah. 1:   : Okay, I will check it definitely. Check it. 1:   : Thank you for us, sir. 1:   : Yeah. When when when you send that email it's again add the she, you as well, just to be sure that you double your chances. 1:   : The the data set is quite, I I guess 1:   : you're reviewed and approved right, though the one I I sent you regarding the the airplane. Okay. 1:   : And for the 1:   : what do you call it? Analysis? Is it performed as a group? Or do we have to submit our in the visual analysis, like, what is your expectation of 1:   : of that portion? 1:   : Well, when you do a a a a group project. There is no need to to do things different. You want to do one single report, one single script that will contain the contribution of everybody 1:   : keeping in mind that that 1:   : if you have more people the an at least it should be more complex. because otherwise I I couldn't really compare 1:   : something that was generated by 3 people with something that was generated by one. and and the expectation for the graphs like, how many 1:   : is too little, or what's your expectation for that. 1:   : Well, graphs are just one indication of the complexity. So sometimes it could be metrics. So you you saw the presentation, the the the the for the tweets 1:   : A. I mean, if you consider a the the clustering coefficient is a number. But getting there, it took me several 100 of lines of code. 1:   : Okay, good. 1:   : And that's complex city. So 1:   : I for use Mmm: more on the complexity of discrete than the complexity of the All Research. In a broad sense. Then the number of either a tables or 1:   : okay. Thank you, Professor. 1:   : Sure 1:   : other questions. 1:   Scott Guetens: Alright, I just yeah real quick. I just wanted to reiterate what Kevin said. I also had an issue with 5 and 6. So I'm: yeah, I have it. Yeah, yeah, I I i'm sorry if I didn't do much again. We are. 1:   : But yeah, no worries. I didn't even email, but I figured I would just mention it because he had also 1:   Scott Guetens: same time. Thank you. 1:   : Sure other questions tissues. 1:   : Okay. So it's 8 or 9. That's the end of the class. I hope you enjoyed the at least the part on that chat, Gpt, and I hope 1:   : that could be useful again in the future. We will use those tools more and more. E-pola is thinking about replacing with the 1:   : so we will have a sort of a 1:   : personal assistance that would be hopefully more smart than studio or or Alexa so, or whatever my likes just started. 1:   : Professor, Why are your thoughts on on Chat Gbt, and the future of jobs? Do you think it will always be always remain as a supplement or a tool for developers, or 1:   : do you think it will replace 1:   : people in the future? 1:   : It's a great question, no one as a real answer. But there are some indications. So 1:   : I mean, if you look at the history A. 1:   When you have a new tool, you have some jobs that will go away 1:   : typically are the jobs that are more repetitive jobs that are more easy to Peter Blaze. 1:   and that's what is going to happen with the chat. 1:   : So 1:   : you need to add value. So companies will pay you for adding value If you have something that is already providing value, you cannot just 1:   : use it and 1:   : pass the through. So you need to do better. and that's why I, for example, presented you how i'm using it 1:   : so by productivity. In writing the book improved 1:   : for coding would be the same, so you can use a chat gpt just to save the time that you would use the on a stack overflow. 1:   : But we not replace you when you have a problem that is really complex. 1:   : I mean, right now, we also have the limitation that the chat gpt is not taking fines, meaning that if you want to do a processing of a fine, not like the one in the current assignment. You cannot pass the file and have it process. 1:   : but you can have some help. 1:   : So you want to use that to do things better 1:   : if you think for a 1:   : You probably saw the movie. I don't remember the name, but the movie on NASA with the human calculators. 1:   : So an army of people doing manually the calculations for the satellites. 1:   : Then they introduced the computers and their job was gone. But the job of a coding started. 1:   : So once we will have Lms that that that that could be more powerful. 1:   : that will have an impact, so we need to be ready 1:   : to move up to be more expert of the domain if you want to go in the prompt engineering side more expert on how to improve the the quality of those things. 1:   : One of the things that we are working on is on that, using those tools to provide the the call. More knowledge 1:   : representing common knowledge is a pain in the neck, because we don't really know what it is. So we know things, because just we know it. 1:   : how you represent it. 1:   : So one of the ideas is to use those tools based on a such a large data set to provide the common knowledge, and then use the common knowledge coupled with the domain that specific knowledge that we create. 1:   : So you have a layer that is more complex. 1:   : Another thing that that we are working on is to use those Lms as a sort of dispatcher for a specific knowledge, so it is said, over getting the results. 1:   : They will tell us what is the the island of knowledge that we want to activate? 1:   : Then they dial and could be another model based on something similar to an Lm. Or something similar to my room theory, and you will get the answer from that. 1:   : So again that 1:   those are the examples on how to build on top of it. 1:   : But 1:   there will be an impact so lower level jobs will go. 1:   : But that is what happened in the Industrial revolution. 1:   : So who is using the faxes anymore? So people manufacturing taxes, they are gone. 1:   : Who is using the the analog photo 1:   : with all the chain or products that are related to that. Think about digital music and streaming. So we have a less movies less here that we have a 1:   : less Cds or similar things. So that's the way it is. 1:   : That would be an impact for sure. And we need to be prepared. We need to know how to use them. Is this an answer, Kevin. 1:   : Yeah, Absolutely. Thank you for us, sir. And and I also read that it's it. It's really bad math. Is there a reason why 1:   : I I heard it can't do calculations correctly. 1:   : like simple. Well, I mean that is a domain chat. Gpt is not good in any domain, but it's good in all domains and like. 1:   : Okay. 1:   : And there's the reason, I mean, if you ask questions on the the defense industry, if you ask questions on something 1:   : that is not genetic. You will not get much, because I mean, if you remember the sources that is being used by a chat 1:   : gpt. They are very generic sources 1:   : that could be good to represent the common knowledge, the there are some paypal. So on a. I mean. 1:   : what is the common knowledge? So the common knowledge is what we have in our mind, because we are using more and more digit. 1:   : We could assume that then everything that is available line now 1:   : is the equivalent of the common knowledge. 1:   : There's an assumption. Now. There are papers saying that is what 1:   : is a available as open source, this formation, or everything. Assuming that this could be possible to do on the summation of everything that is available as an open source. Is there a presentation of the common knowledge of people 1:   : I 1:   : we don't know for sure, because we don't know we can really quantify what what is available, and we can not quantify the common knowledge. Common knowledge is very depending on the culture and and your specific culture meaning is really difficult. 1:   : But we can assume that what is available in terms of common knowledge is what is available as common knowledge to to an average. 1:   : So for those things. those tools are good and will be even better. So we have a. A. Gpt. For our chat. G. G Gpt. Is based on a Gtt. Too. 1:   : By the way, keep in mind that the chat gpt is based on a data that that stops at 2,021, meaning whatever it is, after 2,021 is not there 1:   : meaning If you are asking, I don't know something that is happening that happened after that. 1:   : It's not like Google. It is pretty much real time, but it's bad 1:   : because of the training it's. I don't know how many millions, but it's not something that you do on a regular basis 1:   : into one week. 1:   : so it's a batch process. That's another challenge. So we need to move. We need to have a way to have more real time. 1:   : Human mind. It's real time. We learn 1:   : every moment that every every sale on the from whatever we do, and what we learn now will increase our our our knowledge. This is not the way those tools are working. 1:   : Anyway it it it it's a very large topic, and 1:   : I would be happy to address other questions You may have down the road. 1:   : That that will be all. Thank you. 1:   : All right. Okay. So thank you. All. Sorry for keeping you till 8, 20, 1:   : and I hope I address your questions. Feel free to send us an email life to anything else.
fis STEVENS
lw INSTITUTE of TECHNOLOGY

Excel & Python for data understanding,
cleaning and transforming

At



clipizzi@stevens.edu

SSE

 

Introduction ©

Excel for Data Science

 


Excel is still one of the most
popular tools in Data Science
• It will not cover the variety of
needs a Data Scientist has, but
when the dataset has limited size,
it still plays an essential role
• Widely used tool for data analysis
and cleaning
• Easy to do work the data while
visualizing it
STEVENS INSTITUTE of TECHNOLOGY | 2

 

Pivot Table ¥

Excel for data organization

• Let’s say you want to know the average actual profit by
category and by calories:
Select all the data in the worksheet ‘Basketball Game Sales’ and click on
Pivot Table, and create it on a new worksheet
On Windows the Pivot Table is on the ‘Data’ menu, and on MacOSX it is
on the ‘Insert’ menu


STEVENS INSTITUTE of TECHNOLOGY | 3

Python for Data Preprocessing

There are several Python
libraries for Data
Preprocessing including
Pandas.profiling,
Dataprep.eda, Sweetviz




 

  

Python for Data Preprocessing


 

 

 

    
 

 

 

   

          

 

 

 

 

    

 

   

 

   

 

     

 

 

* Type inference: detect the types of columns in a dataframe
¢ Essentials: tyoe, unique values, missing values
¢ Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range

¢ Descriptive statistics, including mean, mode, standard deviation, sum, median absolute
deviation, coefficient of variation, kurtosis, skewness

¢ Most frequent values

¢« Histogram

¢ Correlations using Spearman, Pearson and Kendall matrices

¢« Missing values matrix, count, heatmap and dendrogram of missing values

¢ Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks
(ASCII) of text data

¢ File and Image analysis extract file sizes, creation dates and dimensions and scan for
truncated images or those containing EXIF information

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 2

 

ft STEVENS

lw INSTITUTE of TECHNOLOGY
iy

Test Driven
Development

meme



clipizzi@stevens.edu

SSE

 

 

Testing code e

Software testing is an investigation conducted to provide stakeholders with
information about the quality of the product or service under test. Software
testing can also provide an objective, independent view of the software to
allow the business to appreciate and understand the risks of software
implementation (Wikipedia)

Unit testing is a software testing method by which individual units of source
code, sets of one or More computer program modules together with
associated control data, usage procedures, and operating procedures, are
tested to determine whether they are fit for use (Wikipedia)

STEVENS INSTITUTE of TECHNOLOGY | 2

 

What is TDD? le

Don't write production code without first defining a unit test
Don't write more code than Is sufficient to pass the test
When writing code you need to address the following:

¢ What do! need to create to meet the requirements?

¢ How can | test that it does it¢

¢ What's the smallest thing | can code to satisfy this test?

STEVENS INSTITUTE of TECHNOLOGY | 3

 

Retrofitting e

Take existing code and change it so that it is more efficient
Don't actually change the output of the code, just how It works

May incorporate significant changes

STEVENS INSTITUTE of TECHNOLOGY | 4

 

Why use TDD? e

TDD makes you produce 100% testable code
You won't spend a long time with non-running code

Your code will better meet the design specifications

STEVENS INSTITUTE of TECHNOLOGY | 5

 

Using TDD Y

IN principle, it is just about writing the test before the program
But in consequence, it leads the developer to

first think about “how to use” the component (why do we need the component,
what's it for?)

and only then about “how to implement”
So, It’s a testing technique as well as a design technique

It results into components that are easy to test, easy fo enhance and adapt

STEVENS INSTITUTE of TECHNOLOGY | 6

 

Sh STEVENS
le INSTITUTE of FESHN OL bey

@
aoe

GREEN-C;NNECT

Financing a Sustainable Future

 

Siemens’ Challenge for Researchers
November 2021

Project Team:

Dr. George Korfiatis
Dr. 

Dr. Mohammad Ilbeigi
Ms. Azita Morteza

Mr. Hojat Behrooz

 

 

 

The GREEN-CONNECT:
Idea - Rational

 The transformation to a circular economy
The need for sustainable project financing
AI and ML science is leapfrogging
PROBLEM: SIEMENS FINANCIAL SERVICES PROCESS
FOR ACQUIRING NEW PROJECTS IS MANUAL

 

 

   

The GREEN-CONNECT
Platform Will:

 3
SOLUTION: A SMART AND FLEXIBLE DIGITAL
PLATFORM

 

 

BO The GREEN-CONNECT: >
Architecture

• We collect a corpus representing the
knowledge of the industry and Siemens
view
• We transform the “knowledge base”, into a
matrix of vectors representing the words,
called the “Room”
• We collect keywords with assigned weights
defining Siemens points of interest, the
“Benchmark”
• We collect projects and GSF opportunities from
crawling the web
=
We filter projects/GSF using the Benchmarks looking up the words from the Room
We match the filtered projects/GSF documents by looking up their words from the
Room and calculating the cumulative similarity
We perform analytics and visualize the results
    

 


 
TUTE of TECHNOLOGY | 5

W The GREEN-CONNECT:
T

he Proof of Concept - Architecture

  The Room is limited to a corpus we collected
from the web on “sustainability” (450
documents)

The set of Benchmarks is a first draft, shared
with Siemens (300 benchmark words/phrases)

We process only project related documents
that’s we manually collected from the web (11
documents)


We match the projects using our algorithms via
the available Room

We use basic graphic visualizations
Projects

  


 






The GREEN-CONNECT: Project Team

Professor George Korfiatis with expertise in environmental engineering,
sustainability and research project management

   

Professor  with expertise in advanced modeling techniques
of complex enterprise systems

Professor Mohammad Ilbeigi with expertise in analyzing sustainability
and resiliency in urban infrastructure systems

Graduate Research Assistant Azita Morteza with expertise in sustainable
development

Graduate Research Assistant Hojat Behrooz with expertise in Machine
learning and computer programming

 

 

STEVENS INSTITUTE of TECHNOLOGY | 10

Thank You!

 

STEVENS

INSTITUTE of TECHNOLOGY

We appreciate the opportunity to participate in this challenge, and we are looking forward to a long-
term relationship with Siemens!

Hello. Hello, everybody. It is 630 and a few seconds. 4:36 Just to give other people some more time is just a few of us. 4:47 So the Class B is not particularly dense, meaning that hopefully we will have more time for the in-class assignment and I think we can start that. 5:00 So the recording is already ongoing. Welcome, everybody. 5:18 It's a 631 up. 5:24 And let me start as usual with what is going to happen. 5:27 So this is the live session that we have. 5:38 We will discuss the homework. 5:42 We will review a little bit all the elements of machine learning. 5:45 It's some of the basic algorithms for machine learning between machine learning and data science and knowledge discovery. 5:54 And then we will do an exercise and classics of science. 6:08 I will introduce the midterm, and that will be pretty much the end of the day. 6:15 So the previous assignment was not the midterm. 6:25 The previous assignment was about to see if they have. 6:32 It's your. As far as well. 6:36 Okay. So the previous assignment that was on you, but as far back as two parts. 6:44 So the first part, that was a question, sir, we will not discuss that. 6:54 The second part was on coding and was about working on COVID comorbidities. 6:59 And the idea was basically to account. 7:09 I mean, the file had the cases in each case with the comorbidity and the class of the comorbidity, the age group, and the number of deaths. 7:13 And a few other information. So we focus on the comorbidity, the age group, and I mean the number of casualties. 7:30 So we counted the people per class age. 7:44 Then we have a bar chart with class age where the axis will be the class age and the Y, the number of deaths. 7:49 Then we have the we have a pie chart with the same data and this will show the distribution of deaths per our age group. 8:00 And then we will bring the comorbidity with the highest number of COVID 19 deaths for the population of less than over 35 years of age. 8:14 So let me go into the code. The code is relatively straightforward. 8:24 So you have the library, the CSP file to open, to open the file, and then you have my block LIBOR for plotting and the new PI for some calculation. 8:33 So the assignment was asking to create a function that will be named get index and 8:49 the input that will be the dataset or the rule under the all data set and the rule. 9:03 So this function will return the the dataset, the index meaning zero. 9:12 If it's a zero 24 one. 9:28 If is a 23, 34. 9:32 And so on. And if none of them meaning blank, I said the default value to 1000 could be zero, could not be zero, could be nine. 9:35 It could be in any other number, but just a number that is not between zero and seven. 9:49 So that's the function. Then the counter and I created a counter that will contain. I mean a little counter to each one is a counter for the specific age group. So where the first one would be zero 24 and the last one would be 85 and plus. Then. I created a blank dictionary that will contain the condition and the number and then account for the maximum number of deaths. The nephew's professor? Yeah. Did you please share your screen, if you don't mind, like you did for the last homework? Absolutely. I'm so sorry. I was supposed to do it. Yeah, no problem. Yep. Yeah. All right. So my apologies. So let's go back very quickly. So sorry about that. Okay. So the. The goal of the assignment was to create, again, two visualizations. One that is an is a bar chart the mighty be because I can believe that they didn't remember who shared the screen by my apologies. So bachata with the classes of age and the number of casualties and then the distribution. So going back again here. So we have the library that we want to import as of this year for reading the file to plot libor to visualizer and non pi that I will use for some calculation. Reading the file and placing the FILA into data, that is a list of elements. Then I created this function get index that basically based on the value in the the age category, the age group is adding it, returning zero up to seven where zero will be the category is a zero 24 seven. The category is more than 85. Then I initialize the list of counters where each counter will contain the counter for the specific age group. I initialize a dictionary that will contain a condition and number of that. Initialize the counter for the maximum number of deaths and the name will host the name of the co-morbidity with the highest number of deaths. So then I'm looping into the list and getting, let's say, the first one. I call the gate the index passing the road that I'm reading to. And then. I will get the index if the index is 1000. That means that is a blinker or is not one of the categories. And I will pass. Also I will extract the number of deaths. And then I accumulate the number of deaths in the proper index. So basically adding over year whatever is the. Appropriate to her age category. So let's say that from this line 61, I get zero a. The counter in position zero will be increased by the number over that's. So the first one that is the age category in zero 24 is what would be added. And what's. And as for this one, then I calculate the condition with more deaths. So I extracted the condition then that if the condition that is is in the is already in the counter the dictionary. Then I adding the number of that. That they could have used the probably that's that's fine it's the same value then that if the condition is greater equal than the max that's an initially we would not because we'll see it then I will replace it. Uh, otherwise, I'm not going to do anything. And then if it's not in the dictionary, then I will add that key. And body is pretty much the same that we did in a in class assignment and was also in one of the slides. So then I will print the accounts of conditions. Sales by age groups. Print in the condition with more COVID deaths. Creating the charts. And then, uh. I mean, the legend that took me a little bit more time because my clock LIBOR is making it a little bit more complicated. In particular, if you want to avoid overlaps. So but anyway, I citing where I took the this piece of code, then they strongly encourage you to do something similar when you use sources that are external. And then they plotted. So if I run it. So that's the account that by age the condition with more common deaths, injuries is respiratory disease with that much and those are the charts. So that's basically it for the. Exercise. Stop shedding for a second. Check if you have any questions. Before we move on. All right, Professor? Yes, sure. I just had a quick question. I must have misinterpreted some of the instructions on the line. So I ended up actually counting the number of deaths per age group, per co-morbidity. So I ended up making it harder on myself. Yeah, I then I then did come back and get, like, total deaths and then less than 35. There's not going to be a penalty for doing more work, is there? No. There is no reward, but no pain out there. Okay. That's right. I just want to make sure I must have been tired reading the instructions somewhere along the line and thought it said I needed it per co-morbidity. Yeah. I will make sure that she knew. We will just do that. That meaning not interpreting exactly what my what the what was asked the and matching the results and saying that is not a match and meaning it is not good, but in reality, you need more work. So don't worry about. All right. Thank you. Yep. All right. Okay. So let's move on. Let me share the screen again. Again, pretty much each assignment is a little bit more complex than the previous one. So that's basically the spirit, the know no is not going to be a major jump in the next one. That is the meter. But the meter is pretty straightforward. The only thing that you need to be aware of is making sure that you will allocate the proper time because there is nothing complicated. But there are several parts and you will need some time. I mean, I will go back to the midterms in a bit. Okay so machine learning we mentioned last time that be sure that we are. The right thing. Yeah. So. We mentioned that at the very end the machines do not really learn in the proper way. So I'm writing several things on the machine learning next week. That is a spring break, but there is unfortunately no spring break for you. Not for me, but the. And we will have a conference here in Hoboken organized by students, by the School of Systems and Enterprises and these on software engineering. And I will present a paper on natural language processing. It will be actually a workshop that they will give on natural language processing. And I will talk about machine learning in general following the big coverage that we have for Chad. That is a piece of machine learning in the real world by all natural language processing. Bottom line again, machines do not learn. There is no artificial intelligence because there is no intelligence in the systems that we create, or so it appears. And so including the fact that we don't know what intelligence is. So this is something that we already said that. One thing that is really essential in the way we do things in. Data science in a broad sense, including machine learning, is the type of learning that you can provide to assist them. So there are basically two types. One that is supervised and one that is unsupervised. So the supervised the that means you have previous cases are the same event that you want to predict or classify and you use the experience, meaning the patterns from the past, the occurrences of the event to either classify or forecast the future. So that's supervised learning. An example could be the weather. I know the weather in the past X number, number of years and I use that to predict the weather for tomorrow. That's not going to work well, but that's the overall idea. That's supervised learning. Unsupervised learning is when I do not have in the history of the occurrence of the event that I am having to predict or classify. That's the case when I want to just cluster her, my clients. And so I want to launch a new product and I want to be sure that or I want to launch a certain number of new products or a different flavor of the same product. And I want to sell the product or propose the product to the group that is kind of more similar to the potential target for the products that they have. In that case, all you do is to partition the data set in that subset. So that will be as much homogeneous as possible inside the each of the subsets and as much different as possible one from the other. So supervised learning, unsupervised learning is something that is sort of in between is a reinforcement learning. Reinforcement learning either when you do not have supervision, meaning information about the past, but you have at its core that you want to maximize and still think about the game. So you want to reach the highest number of points possible, so you start playing and you want to maximize that value. So in this case. You keep playing till you will get the high value of the score. The cases of failure, meaning the cases where you reach the very low score will be the supervision. The information about the past that you didn't have to begin with. So reinforcement learning at the very end is a self generated sort of supervised learning. So the two main categories are unsupervised or unsupervised, where reinforcement learning is a sort of self supervised learning, but is at the very end a form of unsupervised. When you have a supervised learning, you need to have samples that you want to learn from. So you have your initial dataset with all the cases. The weather for the past. And. Then you split the dataset into two portions. One that you will use for training and one that you will use for testing. So the largest of the two is the one for training. This model for testing typically is 70, 30, 80, 20, something like that. So you use the 70% to create the model. Once you have the model using the proper algorithms, you test the model using the remaining 30% or 20% of the dataset, and you measure the accuracy. Models are a combination of two elements that the data and the algorithm are good. Algorithms with no data means not great models and vice versa. Meaning if you have good data, either no algorithms or inappropriate algorithms, the model will not do much. As an example, Chad GP is using an unbelievable amount of data, so the algorithms are pretty much standard. They are open source. But the data is what is making a difference. So the quality of the result of the jeopardy in this case is high because the data is so big. Then you have the problem of the training model. That will take quite a long time. So we will talk later on about that. I mean, obviously, the largest, the larger the training dataset is, the more complex of the algorithm. So the more resources you will need for training. And then the resources that you need for training are pretty much just once in a while. You do not you don't retrain your model that frequently. So those models are pretty much in tend to be operating in a better way. So they are not operating in real time. So you have the dataset. My GP is working on data out to 2021. So if you ask Chad GP for something that is happening from 2022 to today, you will not get any answer because it has no data on that. But is a massive amount of data relatively complex how good it is? So the combination of the two making the training phase, we need resource intensive. When you use this training, set the you and then you have the mobile load, let's say with a good accuracy. So the accuracy is the number of correct classifications divided by the total number of cases. When you have your model that is performing well and then you apply the model to whatever is your target. So you want to predict the weather today. So you need to be sure that the training. I mean, the dataset that you used to create, the model meaning to train and tested the model is the same or is similar to the data set to the addiction that you want to do. So if you have the data for winter and you want to predict the weather in a day in summer, most likely is not going to work well because you don't have that in that conditions. So the way you select the dataset to create the model that it's really important, then the way you split between the training and the testing is also important. So if you have a dataset with the sales of apparel during the year and you have the training and you're split, I don't know, summer and spring. And then the testing is on winter. So obviously it's not going to work well because the Saints or some of them are different based on the season. So. We mention that the dataset has to be representative by even either even the way we do the split between training and testing. It's really important. So we want to make sure that when you do the split, there is a good representation of all the possible options so that you can have a for the dataset. So you want to be sure that you have the same distribution. And going back to the umbrella, all the seats for the four seasons in both of them. Because only in this case the measurement of the accuracy would be reasonable and as much accurate as possible. So again, when you do this type of learning, you need to to be sure that the distribution of values in the creation of the model, in a broad sense, meaning the combination of training and testing and this beta between training and testing is similar to that. This is part I mean, that the animal that you want to predict is part of that distribution, because otherwise it is not going to work. We mentioned reinforcement learning. So again, reinforcement learning is not, uh, anything completely different from. Supervised learning, but is basically self supervised, meaning the system is generating cases and is getting the results as a training dataset. And then we use it pretty much the same way as it would have done if it was a full supervised learning. So that's basically the bishop is representing that. Obviously, when you can apply that, you can apply that when you have a score. So if you are playing a game, if you are doing any task where you can measure the outcome. If you don't have that, then there is no way that you can do a reinforcement learning. Some of the examples. So you want to move into the board where each move will give you a reward and you want to reach the destination point with the maximum reward. So based on the moves that you do, you will get a certain value. So you can have a system that will generate randomly options. And then we use randomly generating options to create the amount to maximize. Really the same thing. If you are in a network, you want to reach the node aside from the energy or vice versa. Let's say have a supplemental side to the node. That's an example. And there are many options that you want to minimize the number of steps you do. So I will keep those. So you need to define your value and you need to maximize the value. Oh, pretty much see me later or an application. All of the reinforcement learning are a genetic algorithms. Genetic algorithms means that the system is actually generating scenarios randomly and then based on the result is applying. I mean, using that as a supervision, it's they, they named it the genetic because it's kind of mimicking the survival of the fitness. And that is a function of the fitness function that is measuring how far you are from the goal that you want to reach. Deep learning is something that is becoming relatively popular. So. You have machine learning. You have artificial neural networks. Artificial neural networks are algorithms that are based on linear algebra. You have like in this chart here, you have nodes that are equivalent of neurons. So they will get an input along with the wait for the input. Meaning that at the very end, each one of those layers will be a matter that extends and you go from one matrix to the other by multiplying the different Madrick seasons, using the weights and using the input stats. So you have an input layer and output layer and you have one or more hidden limits. So in what is called a shallow neural network, so you have only one key delay in the deep learning. You have a very large number of dominions. Now, if you consider that, you move from one layer to the next one with linear algebra, multiplying matrixes, any, I mean, they can be with a thousand neurons, meaning a thousand elements multiplied by a thousand elements. You'll have a quite a large combination. You cannot really know what's happening inside the when you have a million of you building. So it's called deep learning, but it's pretty much a neural network with a lot of hidden layers and is one of the problems in modern artificial intelligence. Because what is happening inside a deep learning artificial neural network. We really cannot be exact because there is no memory of the different states. It is just a continuous multiplication. So you only have the input and the output. You could have another scene in a network check in the first. But then you are not really doing much in terms of explaining what's going on. There are quite a lot of investments on explainable artificial intelligence realities that when you have deep learning, when you have a lot of those hidden layer, there is no way that you can really explain what's going on. That is a theoretical problem. You can learn what is happening if you have memory, if you don't have memory, and there is no memory at each stage of this. Large neural networks. You cannot really get it. So but anyway, they are effective in particular. I mean, obviously they are all nomadic, although they are they were originally created for recognizing images. Emojis are points are adults, meaning you have large mattresses with the peak cells that are the elements in a border. Somehow that is the picture that you want to imagine that you want to recognize. So initially they been created for that and they were a nomadic outlet. And still there is no way that you can do a neural network for nonmedical elements. So eventually you need to do a transformation from a. non-American to an American. Typical example is the use of machine learning to understand language. So languages, text so is not number. There is no number in a text. I mean, it could be, but it's words, it's alphabetical characters. That's what I'm out of it. So when you have that and if you want to create a representation that is in America, you need to use an algorithm to transform text into numbers. Those algorithms are called the vector is H. There are several of them that are some of them. And basically, it's transforming based on some assumptions. We will talk about that each word in a vector. And that point you can work on numbers and that's the point where you can use numerical only algorithms. So we spoke about deep learning. And again, that's what it is. Now, regardless, what is the algorithm that you are using? You will work with data that can be from all the possible venues. Some of the data can be cleaner because they are generated by machines that are collecting. Events can be messy because they are collected from human beings in different human beings. Eventually, meaning you can have some of the day that with the same meaning but different classification. You can have street as a street as to the other characters or as the daughter or as capital or s as Mola. So with the same meaning, that's an example. So there are quite a lot of not clean data around us, and if you want to play with it, creating your models, you need to transform, you need to clean it. So the data preparation, it's a not so fine step in data mining and machine learning. Keep in mind that. So I don't know how familiar you are if you ever knew the child GPT two train model. They use the algorithms but they also use humans. So humans did quite a lot in different states of this, of the process of creating all the training, the system. One of the steps is to remove inappropriate content so that something that a new man can do and is a terrible job but a human can do and a machine cannot. And then tagging it to make sure that you had the right classification for the different code, for the different elements that you are considering. So large systems like Chop GPT. What they do is basically they create, they detect the pattern in the data and then they match the request you have with the available patterns, compiling them with a layer of compensation. So it's pattern recognition, pattern matching and compensation element. So those three are the three components of something like a child that is part of the GPT class of machine learning systems. That is right now some of the most advanced I mean, for large distribution. Obviously there is always the issue of quality data. Quality. It's a relative term. So if you have a dataset that is several terabytes. If you have a few thousand elements that are not clean enough, don't really matter. But if you have a dataset that is several thousand and you have a few thousand there that are good enough, but then you have a problem. So quality is always in relative terms and is pretty much a function of the size of the dataset you have. The pre-processing is pretty much three phases of cleaning, integration and transformation. So the cleaning air is what we mentioned. You want to remove outlets, you want to remove missing data, things that are damp integration. You may want to have different sources for the target that that you have. You are in marketing, you are targeting potential customers and you want to be sure that you have as many points of view of your customers as possible. So you want to have the information about the the financials, the credit cards, what is doing in terms of easy passage GPUs. So all of those are all repeats visited on the Web. So all of those are with the layer of machine learning can really or activation intelligence can really or a data science can really recreate the profile of the individuals. But the problem with having multiple sources is that the integration is rarely an easy task. So the integration means you have elements that are that can be different but related to the same topic or individual or whatever it is. The difference it can be just formula is a different format or can be structural. So you have one data set with the entities that are covered in a certain way and the other that way are the same entities are called in a different way, or sometimes you don't even have the names of those entities, meaning you need to extract the, the name, the label from the context, the where the data is. And then you have that transformation. Transformation means you may want to combine variables if this may be appropriate. You want to eliminate variables that are not relevant for the analysis that you are doing. You want to normalize eventually. They need to have a more readable and usable dataset. So all of those. Are elements of the pre-processing. Now. If we go into the other algorithms, we mention the unsupervised learning. So the main algorithm in the unsupervised learning category is clustering. Clustering is a technique for finding similarity in data and putting together the elements that are similar. So those buckets are the class sets. So you want the goal of an algorithm. Doing the clustering is basically you wanna one up, you want to create groups that are it's one with two subtasks groups that are as much homogeneous as possible inside and it's much differentiated one from the other. So that's the goal of a clustering. I'll go to a typical level. Clustering algorithm is cleaning to where you define the number of class that you want. So that is key is and this would be decided with the combination of the inputs, the data scientists, but primarily the marketer or whoever is the client that you are serving. Because if you want to clustered, you are your clients, you need to define how many groups you want and you define how many groups you want based on, for example, the number of products you have. I have ten net new products, meaning I need to have ten groups. If I have three products. And in three groups I cannot have ten groups with three products because then it wouldn't make any sense. So but then there are ways to maximize or to suggest the ideal number of clusters. And so now there are measures of that, but the most commonly used is called the elbow or metal, but it is not part of what we are doing. So you define key and then you place randomly the elements into those key markets, and then you measure the distance between the elements and you rearrange the placement of the elements in the buckets to minimize the distance between the centroid of those packets. So we are centroid. So that, I mean that you can have two dimension or n dimension, but the average of the packet. So if is two elements that will be the element in the middle. If the number of dimensions it's 100, then is going to be three, go to see it in a hundred dimensional space, but that's pretty much it. So you move elements to minimize the distance between the centroid and then there is a moment where you cannot move any farther the elements because you are already at the maximum or the minimum or the distance in each group. So and at the end of the process. So this K means and that's what the algorithm is doing. Another very popular algorithm is a decision tree. A decision tree is kind of mimicking what humans are doing. So that's an example. Hey, you want to go to a restaurant, you step into the first one to check how many people inside. Then if the restaurant has no people, you may decide not to go because probably is not a great restaurant. If there are some available tables then you say if is full, then you ask what is the waiting time and based on your tolerance. So for the way that you decide to see or to go based on other conditions, like there are other options in the nearby how hungry I am. So those are. Things that you do consciously or unconsciously when you take a position. So you mentally create a sort of a decision tree, and that's pretty much what the algorithm is doing. You have all the possible options. So let's say that we are in a binary condition. Go, no, go. So you have a combination of options, meaning you have some conditions for going, some conditions for not going, and then you apply the different conditions and you reduce the uncertainty. So like in the previous case, you ask, is the restaurant cooler? What is the waiting time? So those are reducing your options. Somehow the representation that you have on your screen. So you have two options that are represented by the green circles. And the Red Cross is a reddish cross. This pink probably, I don't know. So that initial stage, you have pretty much a random combination of the two. Then you applied a force condition and you had more one type and less of the other type. And then you keep applying conditions. Still, you have no impurity, meaning all of the elements are on one type. So measuring the impurity is also measuring the entropy. So the entropy is higher when you have more chaos, like in the case on the left side, the entropy would be higher and the entropy on the right side would be zero. So if you measure the entropy, you will have a measurement of the entirety of the dataset and the goal of the. A decision tree algorithm is to minimize the impurity, meaning to minimize the entropy. Neural network. I mentioned before that artificial neural networks are a combination of single processing units that are kind of mimicking the neurons we have in our brain. So you have a bunch of input to the same number of weights. They will go into this processing unit that will do summation, and based on their value, there will be a function that will be on or off meaning of fighting or not fighting. So that's pretty much how a single neuron is working. So each neuron as a summation function and then transfer function function. And then you can have an input layer. And he then layered in an output leg. When we have a model, one of the key points is to measure the accuracy. So we generally use two main methods for evaluating the accuracy. One is called the error matrix or confusion matrix, where we measure the number of cases that the model is saying are positive. Ah, yes. And let's say in the cases that based on its testing, the subset are really positive. So measuring pretty much the true positive and the false positive as well as the false negative and the true negative. So in this example, the question was buying a computer, yes or no? So you have a total of 10,000 cases and the model is predicting yes for almost 7000 cases. And those are actual yes and is predicting yes. While the reality is no for 400, the meaning in this case, you have a very good accuracy in predicting yes for no computer. No. Buying a computer? No. Your model is say no. And it is yes. Meaning it's false. Negative. What is six know and is actual no for about 25 to 2600 cases. Now, most of the cases, you don't have the same balanced performance in predicting yes or no, because at the very end it really depends on the dataset you have again. But if you want to predict the cases of cancer in the population of the United States, you definitely have more cases of no cancer for centuries than cases of cancer, meaning you have data set up in terms of representative mass of the two categories. 1: It is it's really skewed toward the cancer. 1: It would be the same if you want to predict cases of terrorism, 1: if you want to predict the number of days of rain in the Sahara Desert, if you were and so on. 1: So in that case, it's like human beings. 1: If you don't have experience in something, you cannot talk much about that. 1: Then we know that there are people that are talking a lot anyway. 1: But generally speaking, you cannot probably talk if you don't know about the same metaphor, if you don't have enough cases. 1: So in predicting cases of cancer that you can predict better cases of no cancer than the cases of cancer, 1: the days of no rain in a day or rain in tomorrow. 1: So don't be discouraged if you see that there is a disparity between predicting one or the category compared to the other. 1: Go back to your dataset and probably you will see the reason why there. 1: Another way to to evaluate the accuracy is the receiver operating characteristics. 1: So it's comparing the true positive and the false positive. 1: So now you have the main diagonal that is measuring the irrelevance, the flipping the coin. 1: So you want to have a curve that is going a bulb where the flipping the coin line. 1: So you measure the line under the curve, meaning a desire to acquire the error. 1: And the larger the area, the more accurate that is going to be the TV channel the moment if you have cases that are all up to the top, 1: meaning an accuracy that is hundred percent most of the time there is something wrong. 1: So no model can predict the well hundred percent unless you are using a variable that already has a solution to the problem. 1: So if I want to measure, I don't know Ukraine or no rain. 1: And one of the variables is the millimeters of rain, obviously. 1: When the mm. The value of the variables. 1: Millimeters of rain is above zero. 1: That is great though. Meaning that variable is giving it up. 1: So then yes, you have an accuracy of hundred percent. 1: But you already knew that if there is a rain in the street, if there is water in the street, that is rain. 1: So that's an example. When you see 100% accuracy, this is a red flag. 1: Something is wrong in the dataset. When you have text, things are more complicated because while the data is numbered, its text is not. 1: So you need to find a way to create a metaphor and somehow to mine the text. 1: So there are techniques. Then that what is main text? 1: You can mine text to extract information, but it really depends which type of information you want. 1: You need. If you want just to measure the most frequent words like we did, then we would do in next assignments and in particular in the midterm. 1: That's an insight that you can get from the attacks, but statistical. 1: The main problem is when you want the semantics inside them. 1: How similar are two documents? 1: How can I extract a summary from a text? 1: So those are or what is the opinion of people on something? 1: What is the sentiment of people? What are the emotions generated by a text? 1: So those are semantic meaning is the meaning of the text. 1: So those are obviously more difficult to evaluate and you need to have a mode that you need to have a metaphor to. 1: I mean, evaluate that to calculate those volumes. 1: But what work is being created in 2013 then? 1: Now there are other approaches, but pretty much is a a good starting point to explain how those things are done. 1: So the process is called victory zation. 1: You basically transform each word into a sequence of numbers in vectors, and then you work with vectors. 1: So vectors are points in a m dimensional space. 1: So if you have a vector with two components, then this vector would be a point in a regular two dimensional space. 1: If you please mute your microphone malfunction. 1: So if you have like most of the time we use a vector is Asian, we did that 200 or 300 dimension. 1: Then those words will be points in a three dimensional. 1: So in 200 or 300 dimensional space, 1: we cannot really see that the but it's I mean logically similar to like the we're in a in a 2D space because they are two points in the space. 1: You can measure how fast they are and based on how far they are, you can determine how close semantically they are, meaning how similar they are. 1: Those two words, 1: the way we create the vectors is based on the calculation or the conditional probability of one word appearing in the text because of the other. 1: So the way those vectors are calculated that are using shallow neural networks, 1: meaning the neural networks with one layer and is already measuring what is after. 1: So there are different methods. 1: A few years after that, Google created another method that is called the transformer to the transformer. 1: Our victories in the bays not only on the conditional probability of one appealing because of the other, but also what was before and what was after. 1: So it's I mean, at that point, you need a neural network that is not more shallow but more complex, 1: meaning resources for training the or getting the model for a system that is based on the transformer is way much more than one for work to bank. 1: So let me skip that and let me go to this one. 1: So tools for data science, machine learning. 1: It really depends on who you are or what you do. 1: How long, how big is your experience? 1: So it's a matter of fact that the people who are in data analysis since decades, 1: they use less by doing R and similar a lot primarily Python today and they use more tools like SAS or similar that are. 1: SPSS did are more with the user with a graphical user interface like Excel on Steroids. 1: We do have faculty that are in the same condition. 1: So I graduated as a Ph.D. like five or six years ago and to me is kind of normal to write code for a faculty who graduated 15, 20, 30 years ago. 1: For them, coding is not that natural. 1: So they probably coded at that time in Fortran or C C++, but now they are not much into that. 1: So the professional age group, there is a correlation on the professional age and the I mean, 1: it is not discrimination and eventually I would discriminate myself and 65 are going 66 or so, but that's a matter of fact. 1: And then obviously it depends on what you are doing. 1: If you are a computer scientist, obviously you have no problem using coding. 1: If you are an economist, then you may be more reluctant in using those. 1: When you work with a python, you can do everything with basic python, 1: but you may want to use libraries or a framework that will make your life easier. 1: When I started working. With machine learning. 1: I mean, at the time there was not much of machine learning or artificial intelligence in general. 1: So it was a 1986. Pretty much we didn't have by done that. 1: So we had languages that were very basic. 1: Meaning if you wanted to do an algorithm for, I don't know, decision tree, you need to build it from scratch. 1: So now you run a python, you call a skill learner, 1: and you use one of the functions that is one function for generating a decision tree and you're good to go. 1: So life is easier now. So you have a. Languages that are easier to begin with. 1: The bite is relatively easy. You have a million libraries of medium, but you have a several thousand libraries that will make your life easier. 1: And then eventually you have a collection of libraries that are that can help you building the pipeline that you will need for your task. 1: So when we go into Python and the libraries in data science, believe it or not, the most frequent, 1: the most commonly used library is pandas, because at the very end, you have data and you need to represent the data. 1: So that's why you use PANDAS to create the somehow the data structure. 1: Some people are using them. 1: There are other options, but they are very niche. 1: So pandas are definitely the go to solution for data science. 1: It's basic, so it's not adding a lot of functionality. 1: There are no, I don't know, regression clustering, a decision tree algorithm in it. 1: But in order to do them, you need to have the data in a data structure. 1: And that's what's pandas doing. Elena is basically the next step. 1: So there are libraries for all the algorithms that we mentioned before and many other end is built on known by site, 1: by locally, meaning all of those are embedded in the library. 1: Just a warning. When you work with library says that it's so big. 1: Try to import the new code. Only the portions that you need then not the entire library. 1: Because the more you import in your code, the bigger your code will become. 1: If you then have also a large dataset that you may run out of memory. 1: So be mindful and you can import only the portions that you need. 1: But again, a skill learner is the way to go. 1: Being based in Mumbai, that means that is based on arrays as a data structure. 1: And so the combination of arrays and pandas is definitely the way to go. 1: But that's a comparison between some of the libraries and. 1: Now there are other options. One of the things that I'm not regretting, but right now we have a new Mac that is based on Apple silicon. 1: I mentioned that last class. 1: The New Macs, they have a graphical processing unit that is working along with the central processing unit, 1: meaning that they cannot be addressed individually. 1: Most of the libraries in machine learning are based on TensorFlow. 1: That is a great library, but is addressing a separately GPU in a CPU, meaning all the libraries are based on TensorFlow. 1: I'm not really working on my neck, so there are some way around. 1: You can build a virtual machine, but there are not many virtual machines that are running on those Macs or that are. 1: Sort of an adapter. But those are affecting the performances quite by the law. 1: So I mean by torture. 1: It's growing in popularity. Another big fan of Facebook. 1: Facebook created by torture. But the very end, I'm using it because I can not use TensorFlow anyway. 1: So that's pretty much it in terms of slides. 1: Let me go for a second here. Let me show you just a brief example of. 1: If I don't create that using some of those algorithms that I mentioned in class. 1: So that K means. Uh. 1: And then it's decision trees. So those are the two algorithms that I'm using. 1: Knew script. I use the quite a lot of comments. 1: Where I basically I copied from the Python library and the parameters that you can provide to the different modes. 1: So parameters are really important when you do this job because changing parameters would change quite a lot of results. 1: So I'm importing the different libraries, so I importing again and using Chrome because it's a killer and it's pretty big. 1: I want to import only K means I want to import the data sets and then I don't pay pandas closely. 1: But I was getting warnings and I imported the library to ignore the warnings is not elegant, so I didn't have time to go back. 1: And the reason primarily the warning was generated by one of the libraries and then was using that. 1: So I cannot go into the I could buy but it wouldn't be anything intensive so I'm loading the dataset is 1: I this is one of the most commonly used dataset for a class setting is basically how to create class. 1: That's all different types of hierarchies based on the length and the width of the sample and the length of the petal. 1: So the model itself is basically two lines. 1: So it's not that much, again. 1: And if you do it manually, it would be probably a good between 102 hundred lines that are in the library. 1: In this case means library from a skill learner, and it would be super easy in this case. 1: So then the representation decision tree similar think. 1: Importing the library is a. And I ordered imported. 1: The dataset. So same thing. Defining all the parameters. 1: Splitting the dataset into training and testing. 1: And then same thing, the representation. So you will have the file and you will play with it. 1: So I'm running it. So is generating a PDF with a decision tree. 1: So on the decision tree you have. This is the root node. 1: Her. And then you apply the different options and you will get that at the very end. 1: Am not saying that we are running out of time. So you will have the difference anyway. 1: Then. This is the cluster. 1: Oops. Three clusters for. 1: The dataset that I was using. 1: All right. Okay. So let me go back here and let me go to the in-class exercise. 1: So the reason why why I created this exercise is because back in time, I let students pick their own final intense problem. 1: And that does it for a strange reason. 1: A huge number of students in a relatively new age decided to go with analyzing us baby names at the very end. 1: I couldn't be sure that what they did was original. 1: So I decided to take this out and making it an in-class exercise. 1: So us baby names. You want to read the file into a pandas data structure? 1: Read the structure, delete the call on unnamed zero and IED. 1: Determine if there are more female or male being the top five in terms of number of occurrences. 1: But name meaning or frequency of the name, the number of names in the dataset, 1: the standard deviation of the name occurrence, and some basic descriptive statistics in the dataset. 1: That is a one line in pandas. 1: All right. So let me make sure that this assignment. 1: It's been posted. So I. Published the. 1: Q that size. And if I. 1: Okay. You have it. Let me stop shedding. 1: Let me start. So there are four breakout rooms with two or three participants per room. 1: So in creating it, let's say 10 minutes. 1: All right. See you in a bit. All right. 1: So it's 801, and we don't want to keep you too long. 1: So I will post the solution, but we will not discuss it. 1: So my apologies for talking too much during the lecture about I really hope that was useful or at least interesting for you. 1: Um. Let me, uh, just. 1: For 5 minutes about the meter mount. 1: So let me go. Right. 1: Everything. Yep. 1: Okay. So let me share the screen. And. 1: All right. So can we close? Let me go over your. 1: So the midterm the midterm a. One think that is essential. 1: For the regular on campus classes four and 624. 1: Uh, they do the meter, uh, in 2.5 hours. 1: So, I mean, it's both a regular on campus and regular online. 1: So you will have more time. So you will have until Sunday. 1: But that's less than the normal time. So if you think about asking for an extension, it is should be I mean, 1: a serious reason for that because you are already getting more than I should I normally give for the midterms. 1: All the other modalities we run AM 624. 1: So the. The assignment has two parts. 1: I mean, apart from liquids. One part that is a choking code. 1: So you have some scripts. 1: And I also copied those scripts in a note by FILA, so it would be easier for you instead of copying and pasting those, 1: and then there would be issues with the indentation. 1: So you have three different scripts that may work or may not work. 1: So you have the description and the actual script. 1: You will run it, you would check it, and eventually you would fix it. 1: So those are the three, uh, smaller pieces of code to check. 1: Then there are two more, uh, that are new to. 1: Right. They are smaller. So you read the, the file transfer fee into a list of words eliminate from the list the words that are in the stop word file. 1: So the so called a stop order is a thought is a word with no semantic meaning articles, pronouns, things like that. 1: So you want to read the modified and eliminate what is in the so-called the from the original text. 1: Then you will calculate the least frequent words. 1: The average occurrence of words where occurrence is the frequency is the number of times a word is appearing in the text. 1: The longest word. The average word length. 1: Again is based on unique words, so each word counts as one. 1: The last of the writing scripts is using the file cards. 1: They'll see as we you read the file into the structure and then he's about cards. 1: So they have several characteristics, including horsepower and average mileage. 1: So you want to print the three cards with the lowest average mileage, 1: the highest average mileage and the highest ratio horsepower divided by average mileage. 1: So. You have the list of files you have also this one with the files that you want to check and eventually fix. 1: And you should be good to go. So for. 1: Section one and two, meaning the part with the checking code and writing code, you will write one single dot by file. 1: So not too far. Not enough files for each of the state of the parts, but one single file with everything. 1: And that's basically it. So you should have her think. 1: So I'm sorry if I always staying longer than 8:00. 1: Questions? I have one. 1: I have to write one on the bequest. 1: That was for this module. I. 1: It was on. I was the first one. It was. Which one of, you know, A, B, C or D is not a software development model. 1: And I happen to like I chose the V model because that's traditionally a systems engineering model and 1: also was the one that wasn't in your software development PDF also it was the one it was the CNN. 1: But arguably like E model is a systems engineering principle, not a software engineering principle. 1: Okay. I will review that. And then I keeping in mind that that system engineering and software engineering, they share several principles. 1: So I generally call software engineering system engineering applied to computer science, but I would definitely check it. 1: Okay. And then my other one was. Let's do Sunday is that, you know, it's not measurable. 1: I just feel nervous about the midterm date being due Sunday being that I know for most of us. 1: Working full time, having less like Saturday and Sunday. 1: Only two full days we would have to. Yeah, you are going to have both the days. 1: So the due date is the end of Sunday. 1: Right. That's that's what I meant. 1: So having Monday evening for the homeworks is a nice cushion just in case working over the weekend isn't enough. 1: I know for me these homeworks take me at least like 8 hours to get fully correct, but I just figured it was worth the shot. 1: Well, I mean, I understand. 1: I, I really need to be, uh, consistent across the different modality. 1: So in this case, it was. It was a frustrating one time. 1: I understand that. But, I mean, I kind of did a sort of a compromise, 1: meaning it's kind of giving us wood and our fireworks and giving several days, including the weekend. 1: So you can work during the weekend because I mean, 1: you are professionals and you work during the week, but the weekend, so you have the entire weekend. 1: So I thought that was. Not saying genitals because that seems to be not appropriate. 1: You've got the sentence, I guess. Yeah. All right. Thank you for answering those. 1: Absolutely. Other questions. 1: Yeah. Gavin, I have a question regarding the grading on this. 1: Oh, what do you call that? You know, at the end of a typical homework, it asks us to interpret the results. 1: How does the grading work for that? Is there like a template? If we missed certain words, like, let's say, for the city situation, 1: if I didn't mention that people would often ride bikes during the spring because it's a warmer climate versus the versus winter because it's colder. 1: Is having points taken off? What was your thoughts on that point there? 1: Well, when you have questions that are more on the qualitative side, I mean, things that are always questionable. 1: So think about sport. If you have a gymnastics. 1: So how are you apart from things that that obvious the other kids fail but how you evaluate one 1: execution compared to another one so you have a human being the very end judging the results. 1: Human beings are fallible, as we know, and pretty much every one has a different opinion about things. 1: And in this case, when you when we evaluate the interpretation, 1: we want to be sure that you are using all the metrics that you extracted in the proper way, 1: meaning you use the metrics or the metrics to make the story. 1: So one of the most common way to do the interpretation and is the wrong one is to describe the process. 1: So I prepared the data, I loaded the data, prepare the data, I did this operation. 1: And then the other operation does the wrong way because we are asking for an interpretation, not an explanation of the results. 1: So. Uh, most of the points that we take out are for two reasons. 1: One reason is because there is no inside sense or elements of inside, so that can be derived from the matrix. 1: Most of the time, again, that is students describing the process instead of the results. 1: And the second is when you have, let's say, ten matrix, but you are actually using two. 1: So yes, it is insightful, but it is not as insightful as it could be or it should be. 1: So I don't know, in this case in particular, if you send me an email just to remind me, I will review it. 1: But typically when we take points off because of one of those two reasons. 1: Thank you, Professor Atmosphere. Thanks for the. Sure. 1: All right. Other questions. All right. 1: So that's the end of the class. 1:
 

ft STEVENS

lw INSTITUTE of TECHNOLOGY
iy
4
|

Data Analysis in Python

A process-based approach



clipizzi@stevens.edu

SSE

 

 

Data Mining Methodologies S

—Several non formal methodologies available. Two more
formally defined are:

*SEMMA. Ii is a list of sequential steos developed by SAS
Institute Inc

*CRISP-DM. Polls conducted in 2002, 2004, and 2007 show
that it is the leading methodology used by data miners
[Gregory Piatetsky-Shapiro — WDD Nuggets]

STEVENS INSTITUTE of TECHNOLOGY |>

9 A

Phases in CRISP-DM S

     

 A de facto industry standard for
data mining
• Created between 1997-1999 by
DaimlerChrysler, SPSS and NCR
• Acronym stands for Cross-
Industry Standard Process for
Data Mining
• Consists of 6 phases, intended as
a cyclical process
• Not all phases are necessary in
every analysis

   

STEVENS INSTITUTE of TECHNOLOGY | 3

 

 



Data Analysis using CRISP
An adjusted version of the CRISP-
DM: “CRISP-DE”, with DE being
Data Exploration
• Focused on extracting
information from data
• No modeling, only descriptive
statistics and visualization
• Consists of 4 phases, intended as
a cyclical process
• All phases are necessary in every
analysis

 


Data Exploration - Template



clipizzi@stevens.edu

SSE

 

 

Contents



Me
STEVENS INSTITUTE of TECHNOLOGY | 7

 

Project Goals and Conditions je

 What are the project goals¢ What is the key question you are
required to answer¢

¢ Are there any conditions limiting or somehow defining the project,
like limited access to data, data too old, time constrains

¢ A brief description of the expected results may be added

STEVENS INSTITUTE of TECHNOLOGY | g

 


STEVENS INSTITUTE of TECHNOLOGY | 9

 

CRISP-DE S

An adjusted version of the CRISP-
DM: “CRISP-DE”, with DE being
Data Exploration
• Focused on extracting
information from data
• No modeling, only descriptive
statistics and visualization
• Consists of 4 phases, intended as
a cyclical process
• All phases are necessary in every
analysis

¢ Focused on extracting

 

STEVENS INSTITUTE of TECHNOLOGY | 19

Business Understanding offs

¢ Definition*

— Define business requirements and objectives
— Translate objectives into data mining problem definition
— Prepare initial strategy to meet objectives

¢ You want to be sure to clearly describe the business
needs and the steps to address them

*: from D. Larose — Discovering Knowledge in Data

STEVENS INSTITUTE of TECHNOLOGY | 1 1

 

Data Understanding ie

° Definition*
— Collect data
— Assess data quality
— Perform exploratory data analysis (EDA)
¢ Overall data description: sources, organization, key

characteristics (sensor/human generated, reliable/unreliable
source, ...)
° Here you run all the descriptive statistical tests that make sense for

the specific case, describing the different steps and their specific
meanings

*: from D. Larose — Discovering Knowledge in Data

_
STEVENS INSTITUTE of TECHNOLOGY | 79

 

 

Data Preparation iw

° Definition*
— Cleanse, prepare, and transform data set
— Prepares for modeling in subsequent phases
— Select cases and variables appropriate for analysis

- First define the steps you are going to perform (e.g.: if you
normalize, why)

¢ Here you perform all the data transformation applicable to the
case: missing/miscalculated/misplaced values, outliers,
normalization

- Describe the final dataset (format, new records number, new
variables, ...)

*: from D. Larose — Discovering Knowledge in Data

_
STEVENS INSTITUTE of TECHNOLOGY | 73

Data Representation ie

Definition

— Select and apply one or more descriptive statistics to the
dataset

— Select and apply one or more visualization to the dataset

— It may be an Iterative process: adjustments may be required

— lf necessary, additional data preparation may be required

Explain why you selected a representation to an other

Describe final results

Read the results with business sense and provide your comments

STEVENS INSTITUTE of TECHNOLOGY | 14

 

 

Contents

 

L} Project Goals and Conditions

    
 

~ (LJ CRISP

L] Business Understanding
LJ Data Understanding
L} Data Preparation

L} Data Representation
LJ Practical Results — Conclusions

LJ Attachments

 

Me
STEVENS INSTITUTE of TECHNOLOGY | 15

 

Conclusions S

¢ This is the final recap: you briefly describe the whole
process, from the business need, to the data
collected, to the representations you choose, to the
resulfs you obtained

¢ Describe possible limitations of your analysis and future
possible develooments

STEVENS INSTITUTE of TECHNOLOGY | 7

 

Contents

 

L} Project Goals and Conditions

    
 

~ (LJ CRISP

L] Business Understanding
LJ Data Understanding
L} Data Preparation

 

L} Data Representation
LJ Practical Results — Conclusions

LJ Attachments

Me
STEVENS INSTITUTE of TECHNOLOGY | 17

Attachments lw

° All the additional/non essential tables and graphs will
go here

¢ Add only the outputs that can support the case you
described in previous slides

° Outputs have to be either readable (no 1M row table
in 1 page)

STEVENS INSTITUTE of TECHNOLOGY | 7g

 

: It. I'm resuming the recording. Welcome everybody! Today. It's it's by day. It's a march, the fourteenth, so 3, 14, 0:16 : I mean being a mathematician. I cannot escape from that. So we have she, you our ta for this. 0:29 : and she will share with us some of the criteria that she is following on a grading and give you some 0:40 : comments indication directions 0:51 : to you. Go ahead. 0:55 : Thank you, Professor. May I share my screen? And 0:57 : i'm sorry because I use the screen, and the screen is on top of the laptop. So sometimes you may wonder why my I is brought in this area because my square is over there just 1:01 : functions. 1:14 : Okay. So can you see my 1:20 : Okay? Oh. 1:24 : I don't know why. There's some problem when I open the 1:27 : files. 1:31 : So I need to reopen it. Now. 1:33 : Probably. Yeah, this one, though. 1:37 : Okay. So can you guys see the midterm description of the word file? 1:44 : Hello. 1:51 : I just see the pie charm. 1:52 : Read again about it. So let's go through the 2:03 : problem. Other questions, one by one. So for the first one, the requirement is the question. One has string. But 2:07 : typically this 2:16 : head of scripts return us a list, because, no matter what you append. 2:18 : they are, is a list. This is definitely what the this other requirement to want, so we can first around the oh, i'm sorry, Professor, because you have very dense agenda. I don't want to run the 2:26 : this file. but we want to fix it 2:39 : so because they requirement to once a string we will give it history. So they they normal way I will do is just to 2:44 : use John 2:56 : to put the elements in the list together, and according to the requirements 2:57 : they are, they should be 3 characters like this one. 3:04 : So we will add the 3 letters 3:09 : in that list before we join the list before we put all the events in the list into a string. 3:13 : So this is the this is my solution for this 3:22 : questions. So let's run it as required. We can put the input 3:28 : the 3:35 and 3:37 : like this is. 3:38 : Does anyone have a problem with this question you can ask me now. 3:42 : No, okay. Let's move forward. So for the second one, I 3:49 : to be honest that I did not follow this wrong 3:55 : syntax. Oh. 3:59 : very well. But according to the requirements 4:01 : I can share with you guys, my logic on my solution to figure out this problem. So calculate to. The longer is the word of a text, and printed them with a long it to the second longest word. So first 4:06 : we need to open the file. 4:23 : I read the file. If you open the file as this format and the the file, the rate, it will come up 4:25 : with a stream format, but we can also use red lines. I don't know if you can distinguish what is read, and reliance that rate will result with us. I I can share with you right now. 4:34 : So see, the word list with data is a string. 4:53 : This is the format. It's a string, and if we 4:57 : with reliance it will be 5:02 : list 5:06 : like this, but it's still with the slash. That 5:10 : okay, that's for the open the file. 5:19 : because I open the file with the stream format, and then I' it. 5:24 : and the 5:30 : what it in the pure 5:33 : where list. 5:38 : and because we want the unique 5:40 : one. So we take the site and convert the site into the list. 5:43 : So the site here is to select the unique word in the word list, and the list here is to convert to the site into the list they have. 5:52 : Yeah, did I have? And we use Lambda to sort the list. and we can come up with a 6:04 : Well. 6:18 : yeah, this one. because this is 6:30 : what happened. 6:37 : Sometimes when you run a part of the code, the reading files is not working. Okay. 6:59 : but I mean you gave us an idea. You can just move to the next exercise. 7:06 : Okay, okay, so I will. 7:13 : I will share with you guys with this solution on the through the announcement. Sorry for that. 7:16 : and for the third one. 7:22 : I think 7:26 : the Professor provide provide us a very good solution to come up with 7:28 : this question. So first we want to check if the input is a a digit, or if in the lower case or uppercase, and then we'll put all the V in the word list. 7:35 : and 7:48 : when you type a a character, the file will tell you it. 7:50 : That is a consonant, or it is about 7:57 : so like this one. If I put it. J. J. Is a constant. 8:05 : So 8:09 : does Annual have problem with this 3 questions. 8:11 : Okay. So let's move forward to the next one, the next one. I don't think I input. Oh, I input the 8:18 : numpy. 8:27 : And the the next question is to actually, this is the second section. 8:28 : the 8:36 : fourth question to calculate the tenth list of frequent words, and the average account of the worst and the longest word and average 8:37 : average. What length? So first. this is the way. I open this to file because we want to remove staffwards before we calculate we do the statistics. 8:49 : So 9:04 : first I will open these 2 files. 9:06 : One is our target file, and the second is the stop horse file, because, of course, I use read, so I need to split it 9:10 : into the wordless version. 9:19 : So this is the stop horse, stock, or list, and the air a I trend. 9:25 : This is the strength version. 9:31 : and we want to remove stuff for us. Industry and taxes. My way is to put the string into what let's first, and then remove any item 9:34 : in the stop Force List that up here in the target pocket for 9:48 : So 9:54 : this this is the way to split the stream format into word list. 9:56 : so we can see : this is the string format. This is the list format. and then we can remove : the stop course from this where list the stop was like to the : Okay. So up to that, Up to this step : we have a sort of clean list. We can see the difference. : The number difference between these 2 were list. : and then this is to count the frequency, and this is to count the length. : And so, first I I do not use the collection counter. I initiate a counter dictionary to count the occurrence of a word. So first, if the word ha appear in the : dictionary keys, the if they should the first time up here in the dictionary keys. The occurrence of this word : equal to one, and if the word ever happened, appeared in the dictionary keys. : the apparent the frequency plus one. : So this is the counter : we calculated from the data : I have to remove. Stop, stop. but according to the requirement, we want the 10 min to frequent word. So we need to search the dictionary : and : select the top 10 worse. You don't need to worry if you have different order of the to list the frequent work. : Once you follow the procedure, the procedure, or the similar procedure. : they all that doesn't matter. : And next, we want to count the average account of the world. So we : took the dictionary value which is their current. Yeah, which is the currents of the unique words. And we technique of that : values. So we can see the outreach : frequency is 1.2 5 5 around this, and then we use the similar way to calculate a less. : So first they initiate the : we put the word : in the list as the dictionary keys and the length of the word as the dictionary values. And then we saw the dictionary and use the similar pro the same procedure as the frequency we got the average length. : Oh. they should be less. It doesn't matter. : So the average length should be 7 point to yeah. Does anyone have problem with this question? : Hello. Oh. : because the good job. : So in the next one is to use Pandas, it's basically a data analysis of how to use pandas. And I use the function. : Goodbye here, which is a very concise and : yeah. : concise way, a simple way. So first we use Pandas to read : the data file. : and my habit is to job other. I use the columns. So first we need to : print out the column name. : So this is the column name. I recommend you guys to copy and paste like this instead of just a typing for yourself, because Sometimes there will be a small space between the names, and you may not : notice that. And if that happens, you can now to drop, or you can not manipulate to the data frame animal. That's my experience. That's my license. So we first print out all the columns, and according to the requirement, we only want to the cars. which means the company name and the average knowledge, and they house power to average knowledge. So we drop all the other : I used call ups which make our data Freemo : concise. So : no, I would do. Have a our data frame only have Well, it has 3 variables, the company, the house power and the average manage. And : I don't think this is : okay. So based on we want to the 3 cards with lowest average Malays. : So we grew by the company and take the me of the average knowledge, and the one day 3 largest one. : See, this is the 3 largest 3 car with : largest average knowledge. This is the company name, and this : this is the me of the average knowledge for each company. : and the same : to that smallest. : These 3 are the smallest. : and the the last question is 3 costs with the highest ratio has power average. So first, I need to calculate the ratio of house power to average knowledge. This is the : one to calculate : the ratio of them. But I want to put the put that results to the original : data frame. So I need to convert. Actually, this is a right. I need to convert them into list. And : at this result to the original cars data : they are free. So we can see now our data frame has fall : variables. This is the power to average knowledge, and the value is the ratio of each company : and for their house power to average marriage. : It's out, and we follow the same function to get the 3 cars with highest ratio. : Okay, so : that's all : for the for the mid term : great to you. So stay on life for a second that just want to show the class and you a different way to do the same last exercise. So just a although curiosity. The reason why i'm going to do it is just to explain the fact that : pretty much everybody is different. One of the things that we consider when we grade the the assignments is is to notice the differences. So the assignments are individual : meaning. Each one is doing a in a different way. If you stop sharing for a second that I will share mine just 1 min. : So this, for example is what I did the for the last assignment. So : i'm in that : I did that probably in a less a rigorous way. But it's very concise. So. So I read the file into a pandas data structure. I pre I I I didn't delete anything I I I printed the the first rows, then I I print the ratio, and then the the cards with the highest ratio. : So once you have the the head that's of the of the Pandas data structure. : You may not really need to to delete those, but just use the rules that that that you need. So I mean in this case it's pretty straightforward, and and that that's what you get. So : I mean that the first rows the last rows, the Cs with the highest ratio, and you can do more intense over what is the highest value or or the lowest value. : So I mean that keep in mind that that Pandas so has a noon pie in it, meaning you can do all the operations you want with the directly with Pandas. One thing that I want to stress with you is this operation here. : So in this case I created the a. A new column in my data frame without going through creating a list. So it basically had a the column. Now the new column ratio : by calculating the ratio between horsepower and average my larger. Then I sorted it, and then I printed the the the head, the meaning, the the ones with the highest value. : So I mean, again, there is no right and wrong apart from the fact that they generate the right results or not. But this is again just to : be sure that you got the fact that that everyone is individual, everyone is different, and and if you do in a different way. Doesn't mean that that way is wrong, or it's right, and the other by by it's just a different way. : Yes. : okay to you. We really thank you. : Other people in the class. Any question, an issue, any studio city? Thomas Poklikuha: I have one question about the homemarks. Thomas Poklikuha: So for one of my homeworks. I accidentally didn't submit the pi file. : and I got points taken off, but I have the screenshots. If I resubmit that pie, I sent you a message on that in canvas. Yeah, I I mean that unfortunately, canvas is not. That is Mark, meaning. Each time you resubmit that you will cancel whatever was before. Yeah. So just send us an email with the dot pie. Thomas Poklikuha: Okay, sounds good. Thank you. : I apologize. But unfortunately this canvas. Thomas Poklikuha: It's my fault first. So it's all right. : all right. Okay? Other questions for she, you : Okay. So thanks again to you, feel free to stay if you like. Are all things that you know quite well so, and feel free. : No, I I love your class. I pretty enjoy that. : Thank you. Thank you. : Erez agmoni this coming Friday I will give a a workshop on a natural language processing. I will post on a linkedin the recording, if it will be available, and the slides 101. : It took me 3 full days to Redo. It completely is probably the sixth of the seventh time i'm. Giving workshops on a natural language, processing inclusive for the same conference that I mean in the past it was in a different location was in to you. But what what was the same now I had to redo everything, because so much is changing in natural language processing : that I I couldn't reuse what I had in the in the math. : and Gbt for is released today. I know, I know, I know. : I mean that i'm kind of skeptical I mean I I I see the good and the bad, but that that that's a a story for another moment. : Thank you. Bye, bye. : All right. Okay. So let's move on and let me share the screen again and let me go into. : So that was the midterm, as we we all well know. So for today we will talk about a little bit of data mining a little bit of a methodology. What is data exploration? And that's the methodology. And then we'll be talking about the visualization. : So I will go relatively fast on that, because it is already almost 70'clock, and I do not want to skip the the in class assignment. : So let me go on the slides, and let me introduce the concept of a extracting knowledge from data. : It's pretty much in line with what we already : said a few times about storytelling. : So you have data. You have a problem, and you want to use the the data to work on the problem, meaning. You want to get a facts that then you can explain. But the facts you will extract them from the data. : So sometimes things are easy, sometimes are more complicated, sometimes some some basic statistics. I can do the job for you some other times. You really need to find the more complex elements that can : create a narrative for this sort you want to tell : if you consider for a second that when we talk about the correlation, so the correlation, what we use it generate the linear correlation, meaning one value is growing, the other is growing, or one is growing. The one is decreasing. That's a correlation. : and how much at what degree they degree they they increase or decrease, is how close the the variables are to be identical in that case will be one, and the correlation 150 : erez agmoni, but not always. You have a a linear coronation. So there are many other ways. So I just mentioned one example. So if you go to the 150 : that sometimes we use models that that are more complex than that we use a a decision. Trees we mentioned that last time in the the class when we talked about : machine learning. So those are the methods. Those are a matrix that we extract from the data, and we use as a data points to tell the story. : So let's explore a little bit more very fast. I mean it's a lot of slides, but i'm not going to spend much time on each one. : So what is : data, mining data, mining knowledge? Discovery is all about data, but it's not the only thing that is a about data. So in this case is that somehow a process used by companies to to gardener role data into useful information. That is very basic : definition, but it makes sense. So you I mean it's it's it's basic, and it's very general. But that's what we do. So why we do data mining. Now we do now because we have data, and that's the first point. The the second point. We have a a computational power, and we have tools like a python that can really help us going through the data in a : in a timely matter and get the results. So the combination of those things again, and the availability of data. : the the computational power, and the power of all the software tools like Python is making a data mining possible. But there is also a a a market reason. : So the market environment is becoming more and more competitive, and you need to find the niches where you can leverage your offer and get more satisfied customers. So what customers, or what whatever, is the goal you have? : So now there is more need, the competition. It's more. : I'm not saying that it's hard right now, but but it's definitely more specific, more targeted than it was in the past. And then the technical reasons that I was explaining before. : So what we do, generally speaking, in the data mining is to discover patterns so common behavior. So in the data that we have. : That's pretty much what gpt for that she you was mentioning, or any of those large language models are doing. : discovering patterns and a matching patterns. : They, the mining. It's kind of a multi disciplines discipline with the several components from obviously computer science information science, because at the very end that we need to write programs, but we also need. I'll do it. So that for machine learning bid up most of the time are in data basis. You need to know how to deal with the other basis : Statistics, because even if sometimes they may not be so so sophisticated, but will be useful to clean the data, to understand the data and to start initially working with the data. And then visualization. Will we talk about that a little bit later on today. : And there might be a other discipline based on the on what you are exploring, what you want to discover. If you are in finance, you may want to be an expert in finance. If you are in a health care, you want to be an expert in that care, and so on. : So sometimes, when you are talking about sociology. You know, when we are all social media, you are talking about people like you may have a sociologist, anthropologist, psychologist on board. : Obviously, when you work with few data. Life is easy. When you work with 7 on terabytes of data things will become more complicated. : I was working few years ago on a a large database, all the data collected from an insurance company using the black boxes in the cards. : and that those black boxes generated input for each car every few seconds. So was a large insurance company, meaning a lot of cards, and you can do the map and imagine how big the data set was. : So my model was working fine in in theory, but when I had to scale it up to the size of the data set, then issues happened. : I ended up doing a little bit more of a a parallelization : at the very end that I ended up working on cloud computing, so moving, creating a virtual machine, moving the virtual machine in the cloud, running the model and then getting the data. : But again, scalability. the size of the data is important. : So one of the most common questions is, what is the difference between data, mining and and. : generally speaking, statistical analysis? It's it's top down. So it's deduction, data, mining is induction, meaning that you have a few examples, and you extrapolate what what you have. : You have a a bug over points, and you say, okay, it's full of coins, so I extracted the : 20 of them, and they are 5 cents, all 5 cents tribulation. They are all 5 cents can be good, can be not good. But that's an example, that pretty basic example of the induction. So you are : assuming that because you had the a certain number of a certain type. You are established the type to the entire population you have : in statistical analysis you count how many coins you have. And then I say, okay, 30% of my coins are of this time. Sometimes you cannot really count because you have a data set that is huge. I I was working few times with social media. You have several 1 million : data points, and and I mean it is difficult to do in : so. Again, the main difference study signal analysis is is deductive data. Mining is a. In that. : We mentioned that there is a huge growth of of data. So the data we have in the last few years, so it's pretty much comparable with the all the data has ever been collected in the previous history of humankind. : So and it's growing so because we are generating content. We are generating content through social media to communication. We have a sense of Internet. Of things. It's, Matt watches another whereabouts. So all of those are continuously generating data. : Everything we have pretty much is digital now, with very few exceptions, and they are going to be 0 sooner : if it is not digitally. If it is not the analog, that means that it's the that means that that we can use it somehow. : Now, because of most of the data are in the Internet Protocol and IP for a convenience. That means that somehow they are collectable with the all the degrees of privacy. Obviously. But they are collectable for those with the right right to do so. : So we are leaving a a a process that someone is calling the thatification a meaning. Everything is becoming a based on data. So the latest chat Gpt is probably the on this trend : Chat Gpt is based on data. It's based on that whatever, or any AI could collect the from open source, and someone is saying even something that is not fully open source. : And you don't want to argue on that. So : it's a process. You go from a a row data to element, so that we give you the opportunity to take a decisions. : There are 3 main types. All the the : I think. So the data science is is doing the analytics. So the data science is doing so. You have a the descriptive analytics meaning you are applying the method to to get what's going on from the data. : and that's basically : metrics that you extract some of the metrics. Again I mentioned the correlation analysis may be very deterministic, very statistic. I'll rather not so much. And I mentioned that the decision trees and the metrics never associated to that. : Once you have a a descriptive analytics, you may want to : go predictions. So saying, okay, considering what is happening up to this point. So this is what is going to happen. : But you don't do anything that the next step is the prescriptive meaning. Okay, considering that this is what happened. And this is what is going to happen. I'm going to do that. : Think about the autonomous vehicles. So you have the autonomous vehicle assessing the situation. So what is on the road, and is the descriptive. : then the predictive? There are a pedestrians that are crossing the street. I will see them in front of me in a that much seconds. So that's predictive. So they are not actually crossing in that particular MoD. They are not in front of the car in that particular moment, and then prescriptive. I will stop. : So those are the 3 faces. Not necessarily all the analytics. So we'll have all the 3 faces, but those are the T. V. Gala, 3 faces for analytics and the 3 TV gala usages for a data science. : Considering how much, again, that data we have this notification transform the and it's keep transforming pretty much every industry. : So some industries are more reluctant. So we still have a quite a lot of projects in the data engineering with the defense industry, because some industries are more structured and they require more time to do the transformation. : Some other industry are already transformed, and they already 100% a digital, but not all of them : examples of user pretty much all the the cases where you have a a a big use of all the data you will have the possibility to in use : data science in a in a middle sense. We already mentioned that the type of data that you can have. So most of the time you have data in a structure form that can be tables. So that can be pandas. It can be data basis, so they : pretty much resemble the table with rows and columns. We are at the rows are the data points, so that you have the observations that you have for your data set, and the columns are the attributes of of those observations. So if you have the weather in a given period of time you may have the temperature, the pressure, the humidity, those are the attributes, and then you, or variables : or columns, and then you have the the rules that are the samples. So the instances, or whatever you want to call them. : That's another example. You have a the Irs checking. If someone is cheating on taxes. : and he's collecting data like. If the subject the taxpayer asked for a refund, or what is the marital status? And what is the taxable income? And then the result was cheating on taxes was not : so based on that. You you can do a prediction. Obviously no one would do a prediction or something with so many vulnerabilities, and so much at stake, with only 10 observations. : But that's pretty much a the sense. So now, if you want to create a model to predict the who is achieving who is not. You need to analyze the the variable you have, and then create the model based on the variables may not be. Linear most of the time is now, but that's what you have. : Obviously, if you had the same poll that you have. : or better, if you are using the sample that you have on your screen you. : I mean, if you look, the number of No is the vast majority. You are only 3. Yes, meaning the 70 is No. If you create a model saying that always No, you will be always right, or you will be 70% of the times right? : Oh, that means that that the all that it wouldn't : do a great job. But this is : something that I would use to tell you. What is the the accuracy of a model? So. : and most of the models, they drew the line at 50%, meaning that if is less than 50% doesn't really bother. : I mean, you flip the coin pretty much, if is a above 50% may be useful. : but it really depends on what is the case. : So if you have a social science predicting what human beings can do is really difficult, and getting a high accuracy, it's really difficult. If you have a an industrial production. We are all the processes are very : a well-defined and consistent in time that an accuracy over 85 95% it's reasonable : when you have something that is not on the side, and you have an accuracy that is, 99, 95 there is something wrong most of the time you have a high correlation linear correlation between the the variable that you want to predict and the variable, so that you are using to create a model. : So think about. You want to predict the weather if it's raining or not. And one of the variables in the is the number of inches of rain. : Obviously, if that number is different from 0, there is rain : erez agmoni. So those 2 variables, rain or rain, and the accumulation of water, they are highly correlated. So if you include that that variable in the model, 150, : the model will tell you things that you already knew. meaning that the model is part of less. : So when we do this type of analysis. So you want to remove all the variables that are so obvious that wouldn't add anything to your knowledge. : So let me skip that we already know that they are tribute. So we know from 5. Don't know the categorical or numerical, pretty much the to make categories : some examples of all the documents. So you have a on data. You have different documents and different words. This is a a document term occurrence table one. : So you have a document one with the with the term a team happening at 3 times a play 5 times, and so on. So this is something that is used as sometimes as a first stage to the terminology, to see how much 2 documents may be similar if they have the same words. : What's use the the same number of times? That's a red flag for a possible. Then you would do the second set, but that will be analyzing the position of all those words. : So if you have the words, so that record in December now the same number of times in the same positions. Then there could be a platform, and obviously it's never a 100. You can have a degree on the first stage and a degree on the second stage. : The other example is transaction data. So when you buy something in a store, either online or not, you have item in your : basket, and you have a a transaction, Id that are meaning what you are buying. : So you have a transaction. Id and list of items. So that's an example of transaction data. : You can have graphs. So some graphs are pretty obvious. Social media is generating graphs. The electric greed is a graph. : The the telecommunication network is a graph. But you can create graphs from text with the proper algorithm : all right : with the problem metaphor, and then the algorithm, so that's another example. Like each one of those will have a intrinsic characteristics that can give you the possibility to that, to do some types of analysis, and not that : when you do data mining, you actually do a process. So the most commonly use the methodology is what is called Chris the end. Chris DM. Stands for a cross industry, standard process for data mine. : It is composed by 6 faces. You start with the defining: what is your business? So business understanding? Why, i'm doing what i'm going to do what is the goal that they want to achieve. So that's the first step. : If you don't know what you are going to do, chances are you will fail, or you will never be satisfied. So you define the problem. They say on the it's data mining. You need to have data. : and you need to have the data that will allow you to solve the problem that you define in the in the business understanding phase. : So this face is a sort of an assessment of the data you have. So you analyze the data. You see, if the data can provide answers. : may not, because you may not have enough data, or the day that may be consistent, or there could be so many missing values that you are not going to get much a out of it. : Things like that. So : the outcome of the data understanding that can be okay. I have what I need to do. The modeling in a broad sense, or I don't have enough data, and I need the either to : tune up somehow, or the business question, or just to table it and say I cannot do it Once let's say you are okay, and that's why you have the double : are also here. Once you are okay, you go in the data preparation. So you want to be sure that the data has been cleaned enough as being the most enough as being normalized enough. So all the or you do the correlation analysis : to the move a variable, so that are, if relevant. If you have a one variable, that is the double or another one. There is no need to keep them both. So you want to remove one of the 2. That's an extreme example, but is in in that line. : So that's the data separation. Once you have the the right question. : the right data to address the question and the data in great shape. Then you start the modeling modeling means you apply. I'll go it to the data. : So the combination of all the algorithm or algorithms and data will give you the model. : Once you have the model, meaning that the algorithm for your specific case, you : then you can start the evaluation of the model. How good the the model is I mentioned before the 85, 95, or the 50% that those are the accuracy of on the model. : Once you are okay, with the valuation. But you can also be not okay. Say, okay, I was doing a bunch of things. I tried several different algorithms, but I didn't get much. : So at that point, the the evaluation, the accuracy is so low that they need to go back to the business question as saying with what you have, I can never provide you with an answer you would be happy with. : If this is not the case, meaning that if the is good enough, then you start with the point. : So that's the : I would skip the the following slides, and I would go, but you will have them, and I would go with a specific case for what we are doing. : So we are not doing modeling, because this courses on data. Exploration is is not in data mining, but they want it to give you a little bit of context in terms of : broadening the goal and going more on the mining. Besides the explanation. : So. : starting from the Chris DM: I created the or I adopted the the methodology to something that is more for our cases. So data exploration, instead of having a : 3, is per : DM. Is crisp de we are instead of data. Mining is data exploration. : So you and you will use this. this. : this sort of table of content as a guidance for a a future data exploration assignments that you will have, and you can use in other circumstances of that data exploration that you may have in the future. : So you start defining what are the overall project goals? What are the questions you want to answer? : And then you start. Okay, let me introduce a methodology. The methodology is called the crease. The we are. There are 4 faces, business understanding. They'd understanding and data preparation. There are the same that we mentioned before, and they the representation. So that was a : not in the the Chris DM. That was modeling an evaluation. We don't have modeling, but we have a representation. Representation can be. Visualization can be tabled so can be not added : to consider all the sort of revolution that we are experiencing with the Chat Gpt. One of the representations is basically : conversational. : So the data are a represented in terms of phrases generated by by the board. : So that's another data presentation that is still a quite a lot of room to for improvements. : But that's a possibility. So think for a second that you are an alternative on the field. So you don't have time to go that much into a charts tables. So if you have a your system telling you in plain English what you do? What is the result of the current variable, so that you are considering you would save time. : So that's why can be so relevant that this new approach based on a conversational : data to. : So : again I will go fast into those faces. So we already mentioned that when you we do your own, instead of having a general definition that you will say, okay in a business understanding, I want to determine how to reduce the charm rate in my client base. : I want to predict the weather. I want to do something. So that's what you do in business understanding. and you will define criteria to define a success or failure. : Then they'd understanding. You will collect the data, and you will critically evaluate what's going on. So if the data, it's good enough for what you have. : So you will perform an an exploratory data analysis on the data that will be for the the for mana validity of the data. So how many missing values. How many outliers, how much correlation things like that! : Then they the preparation. At this point you will do the cleaning. You would do all the preparation that you would do in a in your data. : and then you will do the representation. So again, most of the time would be the tables, or a visuals. Will we talk about visuals in a moment? : But that's basically in a broader sense, data representation. Then conclusions. You want to have your conclusions here, saying, based on what I have. That's the results. And then you may have a attachments. : So you don't want to create a report with the me on tables, a 1 million visualizations. You'd want to create something that is the right combination between visuals. So tables and narrative. So the narrative : plays the to explain tables, visualizations, and things like that, so you can have the narrative first, and the visualization then, or vice versa, depending on your assign. But those will go side by side. : You don't want in your rep on to have code, because that's not the point. So we are a more like a consultant, so like business consultants, but based on on data based on facts that what we do : so attachments again, they need to be read Ebola, and then to be is useful, but not essential. So what is essential will be in the the main part of the document what is not essential, but may be relevant. That will be in the attachments. : So that's basically it for this portion. : If you don't have questions. : I will jump into the next portion that will be : on a visualization. So we mentioned the visualization is an important part of presenting the data. So let's talk a little bit more about about what visualization can do for us. : Okay, I will share the screen again, and I will go, and the the last part of the on the lecture : before in class assignment. They will be on a visualization, and how visualization can be done with private. : This this class is pretty much the first one after is the first one after the met or my and it's the first is the first one : that is less basic than it was in the first house. : So those of you, with the experience encoding experience encoding in Python, that from this point on that they maybe : more engaged in the assignments, because it would be more on the application than just on that. I mean that learn how to use the tool. : So we will like keep talking about how to use the tool, obviously, but with the a specific application. The application for the day is visualization. The application for next class will be the text processing. : So the history of visualization is somehow related to the history of all of the computers we had. So in the seventies, in the eighties there was not much computer graphics that we can leverage on. : I. I started working in the eighties, and there there was no windows, so the the first one was a I mean a a larger distribution was the Mac. : and before the Mac it was a it was a with the subsystem that is by, and the steep jobs with the Mac. : If you don't have a graphical user interface. There is not much that that you can do. So. In the 90. In mid eighties we start in heading a graphic and user interfaces, but late eighties, not : the the computer. We are not particularly powerful meaning. You cannot do much of the rendering that you may need. So rendering is essential to have a good representation of images on your screen. If you don't have that, then graphics and visualizations will not be good. : so : it's pretty much a 2 sides approach on one side that you develop a more powerful hardware. On the other side you develop a better techniques, but you cannot develop the good techniques without the hardware, and that can support you. : So if you look at what is happening now, I i'm using a Mac that is Apple City on so in the past that we had the the gpu, the the the CPU, the central processing unit, the doing all the calculation. : And then at the certain point we move the we introduce the Gpu, the graphical processing unit to handle up the the video. : Now, what most of the companies, including apple with this apple ceiling are are blading the line dividing the to. So when you do : calculations that are more complex automatically, the operating system is using one of the 2. So the gpu that is intrinsically parallel, because it's a it's addressing the pixels on the screen, and the so intrinsically parallel. Then you use that 1: : i'm not saying that these are all good, though, is good because it's more powerful. It's faster, and that's for sure on the other end. I i'm having a really bad time using some of the traditional machine learning libraries like tens of flow that is not running on my new Mac. 1: : But I mean that's a a mine of things 1: : again, that 1: : the history of a visualization is highly correlated with the with the history of the hardware. So now we talk about visual analytics, meaning visual, so that can really tell you a story right away without much of interpretation. That's 1: : kind of a a a media approach will never really happen, a a reality. But some visuals are more self explanatory than others. So you want to be sure that you you go in that direction. I remember 1: : many years ago, a visualization of the traffic in Singapore. 1: : So they came out with this visualization. We had the map of Singaporea changed the by the hour based on the traffic, meaning that the more traffic and the distance between Point a and Point B will be longer, less traffic a. B. We're closer. 1: : So that's kind of a meaning stretching the the the actual map based on the the the traffic in in a particular moment. 1: : That was particularly interesting, is an example very self-explanatory 1: : visual, but with the embedded analytics. So there was the calculation of the traffic and mapping and the those metric in the the app 1: : some examples. I will go super fast, whole part relationship. That's something that you can do. Discovery relationships, doing. 1: : combined, explored, or in confirmation, analytics. 1: merging together on multiple data types. 1: : time, view of events, and analyzing the evolution in time. So all of those are a good examples of different types, all 1: : visualization. So before we go into Python. I want to go for a moment to this book 202. 1: : That is a a a quite interesting, a book, and I posted that 1: : I will pause an extract. It's called the Out of Knowledge. 1: : One of the things that I like on this book, a part of the visual that are standing is a 1: : the attempt of creating a a classification for different visualization. 1: : So there are quite a lot of different visualization. 1: : But the certain point you need to define what are the user needs which kind of data is going to visualize? What are the the interactions that you may want to have. 1: : So the order came out with this chat. There is one of the options. It's not saying that is the only one of the best, but it's definitely a good one 1: : where you have a types and levels. So you have a statistical analysis, temporal analysis, and so on, and then the level. If it's a a micro mes or macro, based on that, that you have a different visualization that can serve you better. 1: : I will add also. 1: : there's a I mean I was intend to be for a a lines, but you will see different visualizations based on the different. It's kind of a a. 1: : a, a 3, a flow chat that you can use to pick somehow the visualization, and it seems to be most appropriate for your particular case. 1: : So those are some of the options again that the book is giving way much more than that. So 1: : it's a it's a beautiful book, and then there are, I mean I for each one of those. Obviously there are a explanations that will give you all the details. 1: : so visualization that somehow is an 1: : So we have a a 1: : a court. So that is, I am 622. That is a visualization and risk analysis 1: : there. 1: : Good! 1: : It's a good course. It's been created by one of our engineering management professors. and I mean I'm. Mentioning that just to say that there is an entire a semester just on that visualization. 1: : So going back to Python. So there are different packages that you can use. So Matt Plot Li, but is what we already know is a very basic visualization. Package is still the bread and butter for any visualization that you may want to have, and some of the other, including Cibona and in part bouquet, are based on Matt Plot lab. 1: : It's not great. You cannot do very sophisticated things. You can now generate right away HTML page, but it's doing it's job. 1: : See, Borne is more he is is a layer on top of that, but he's also using Pandas, and that means he's using the noon by meaning that you have an environment that there is more structure 1: : Gg: plot that was originally developed for Oura. There is a python version no one is really using match but it it. It is there. 1: : bookie. It's more recent, and the main advantage of book is that it's generating automatically the HTML. 1: : If you want to do things that are more complex, like a dashboard blockly is probably the way to go. So let me go now in the next few minutes 1: : on. 1: : and you will have, by the way the solutions for the midterm in canvas in a moment. 1: : So let me go into the visualization and let me start with the bouquet. 1: : So that's how a bouquet. 1: plain, basic code that I can be. So you import the the like, the libraries of sub libraries that you want to. 1: : You create a file that will be generated. 1: : You pass the characteristics of on the graph on the visualization you want to create. You pass the data. You specify what is the type of 1: : visualization you want to create in this case is a line with those X and y value that's the with of the lane. 1: : the line, and then you you I mean with this statement that you presented 1: : on the screen. So if I run it 1: : erez agmoni. So that's a good point, I mean is generating an HTML. So if you look at here, it's really an HTML page that means that you can export it. You can use it 150, 1: : you you you can send to clients. You can embed it in into a website. Things like that 1: : so it's kind of interesting. 1: : So I have a bunch of samples, so so let me skip some of them. I will give you. Then you know the the the the script. So that's another one. Now let me run it. 1: : So that's a little bit more complex. So you have multiple charts. So keep in mind that you can 1: : zoom as you like you can reset it. You can download it if you want, but it is already in the HTML format. 1: : So, just to make it more complex, so let me jump to the most complex. So that is this one. So you are giving a a more complex that is, the low as carve a more complex 1: : points to dot on your screen that a. And you are using the bouquet for the visualization. 1: : So what you will get will be something more complex like this one. So you can define again the caller. So the shape and obviously the the values in your visualization. 1: : So those are all examples of. Let me now go to examples of seaboard. So see born a 1: : a again that it 1: : it's not as a sophisticated as a for something I mean in terms of visualization, not as sophisticated as as bouquet, because it's not generating any HTML, 1: : but it's more sophisticated in terms of the underlying. Layer. It is because it is a Pandas. 1: : It is known by. So in this case is basically working on a a data set that is relatively large. So let me run it up. 1: : Thanks to 1: : my powerful Mac is going fast. I'm kidding, so it's 1: : pretty much what you would get with Matt Plotley, but but it's kind of nicer. 1: : So finally. 1: : let me talk for a second on the about plot, Lee. So there are 2 versions of plotley. One is open source; one is commercial, the commercial as a more feature. 1: : but the open source is working just fine. 1: : I mean, when you use it for commercial reasons, you may want to pay the commercial fees. 1: : So in this case I'm using a data set from let me just run it just to see the differences. 1: : So in this case, i'm analyzing schools. So if you think for a second. 1: : I mean it's a relatively complex script. You you will have that, and is a analyzing universities by different accounts. 1: : So, and it's generating the HTML so, and it's generating a in a kind of a cool way. So you have a multiple. So this is a a 1: : pie chart. You have the buyer chart with the hoovering that will tell you 1: : where they are from. 1: : and then you have a line chart that is pretty much the same as the other. So there's what rank on the top 100. The universities, in terms of sanitation in terms of teaching. 1: : so we are not there. By the way. 1: : we would go there. So another example of Plotley 1: : is a type of visualization that I like a lot. Because how about how much stories you can extract out of. That is a San key diagram 1: : that was originally created to analyze the the the the flows, all that. 1: : So it's easier if I run it, and you will say it. 1: : It's what you have. So you have the starting point and the endpoint. So think about the traffic. 1: : Think about the pipes. Think about concepts evolution. 1: : So you can see how powerful this can be. 1: : So we created something similar to understand that now how people generated that their knowledge in a given field. So from the very beginning to the user of that knowledge. 1: : so it it's very cool. 1: : You will have that in you are combust. So let me stop sharing now and let me go 1: : to the in class assignment for 10 min, 1015 min, and then we we go back 1: : they have me close, all of this. 1: : I go here and in class assignment up. 1: : Okay, and let me share this screen again. 1: : All right. So then, in class assignment, is about working with the a. Csv file. That is, a collection of all the 1: : tweets. I collected it 1: : 2 years ago from the whole book, and you open the file. So the tweets 1: : and the files being the file with the tweets and the file with the supports, You know, when that means with the so called the file into a list that that tweets into a Pandas structure remove the software. So using the software list 1: : perform additional cleaning as needed, and then calculate and print the the top 10 words and the top 10 standards. 1: : So let me 1: : stop sharing. Let me post it. 1: : Thank you. 1: : Okay. I'm publishing everything. 1: : all right. So i'm creating a 1: : breakout rooms. 1: : We have a 3 rooms, 2, 3 participants each. You have about 10 min 15 min to work on it. 1: : The rooms are open now. See you in a bit. 1: : Okay. So we are coming back. Sorry I didn't pose the recording, so if you will listen to the recording, you would see quite a lot of mapping. 1: : So this about 1015 min of nothing, my apologies so another 1: : 25 s, so all the rooms will be closed. So give us another few seconds, and then we will set 1: : 5 s. Now. 1: : Okay, so you are all back. You want to come back. Is there anyone who want to say something, present 1: : the results, or just 1: : sharing the experience? 1: : All right, so let me share my screen. And this 1: : Okay, so that's basically 1: : what we have. 1: : I imported pandas and a counter. I'm in the Probably this is a script that they did the 1: : No. Totally recently. Probably, if I would do now, I would have done in a different way 1: : erez agmoni. But that's the way it is. So reading into a Pandas data structure, the Tweets reading the soapore file into a lists 150, 1: : then creating a 1: : another data structure with the the send that the screen name from this one, creating a list 1: : getting the top 10. 1: : I mean that there are other ways that again, I would have done it in a different way. But that's the totally legitimate that is, is working fine. 1: : So the top words I initialize a at least of a an empty list. Then I started 1: : the iteration within the data structure. 1: : a little bit of transformation or lower or alphabetical. 1: : Then I created a a string out of the list, because a counter is taking a list, not a a string. And on the list. 1: : Then i'm getting the the 50 most call mona, and that's going to be it 1: : all right. So you have a the top 10 centers. So the top 50 words, and that's basically it. So questions 1: : All right. So let me go into 1: : in the assignment. 1: : So the assignment is going to be pretty much on working on Pandas. So this assignment 1: : is one of the not so many assignments, so that I didn't change in very recent times, because I really want to be sure, the students will know how to work with Pandas. 1: : So you basically have a 3 types 1: : to 3 sources of data. So movies users a rating. So it is all about movies. So the website is a a group L. So Dot or G, with all the information about the movies. 1: : And this is not the most reason, the one that you have a a. As a file. So. but you have it. So there are some additional files to get some information about it. 1: : But you basically have a movies. 1: : users rating. 1: : You have a occupation 1: : for the different users. You want to 1: : replace the numbers 1: : from 0 to 20 with the what they are. 1: : You want to pre into the last 5 rows. You want to find the 3 occupations 1: : giving the highest rating for movies in the data. and that's basically if you want to merge the data 1: : using this form of managing. There are several others. 1: : You will use the txt fine that you have this moving Answer with me, Don't Txt, that will really drive you into how to do it. 1: : Oh, again, that is, it is not 1: : one of our most complex assignment, but is a good way to practice with pandas that are so essential for pretty much everything we do. 1: : And I was not sharing. Oh, that's better. 1: : Okay. help talking about that. So that's the homework. 1: : So again, you have a 1: : the Pdf. The Txc. That will give you additional information. You eventually have this website. If you want to know more. If you want to update the device and using the files that are more recent than those feel free to do that, so they are available in their website. 1: : the structure. It's pretty much the same. 1: : And that's basically what you what what you want to do. So you want to pre in to print the the 5 rows for each one of the other frames merge it. 1: : print the number of records for each of the 4, the data frames. 1: : and then you want to replace the occupation, and that it's originally in 0 to 20, with those 1: : in the last 5 rows of the data frame. 1: : and then up in the 3 occupation, giving the the higher the highest ratings for the movies on the the, the, the, the the the the 1: : so that's basically it. 1: : If you don't have questions. This the end of the class a little bit over time. My my apologies. It's 808, and 1: : I will make sure that you have everything 1: : again. We are still having a 1: : here, and there are some issues with the population of the canvas shell that we are using for this course. So if you have any issue like the previous one, and the 2 answers in the quiz that we are pretty much identical. Send me an email, and I will act as as soon as possible, and definitely will have no impact on your grading. 1: : If you see that it's something missing and let us know, and I will definitely address it right away. 1: : Okay. So if you have any complaints, send me an email just to be sure. Send also an email to she you. So you have a a double possibilities to have your issue result. 1: : Okay. So thank you all. 1: : I really appreciate you being here, and we talk next week. 1: Leona Chia: Thanks. 1: : Thank you. 1:
SYSTEMS
ENGINEERING

RESEARCH CENTER

 

Extracting decision-making metrics from text

 

A Semantic Approach

by

Dr. 
clipizzi@stevens.edu

This material is based upon work supported, in whole or in part, by the U.S. Department of Defense through the Systems Engineering Research
Center (SERC) under Contract HQ0034-19-D-0003, TO#0150. The SERC is a federally funded University Affiliated Research Center (UARC)
managed by Stevens Institute of Technology consisting of a collaborative network of over 20 universities. More information is available at

www.SERCuarc.org

March 2021

sera tanee The Context: Scenario

ENGINEERING

RESEARCH CENTER

e “Inter-state strategic competition, not terrorism, is now the primary
concern in U.S. national security” (National Defense Strategy, 2018)

Technology/Innovation is a key factor in this competition: “We cannot
expect success fighting tomorrow’s conflicts with yesterday’s weapons
or equipment” (National Defense Strategy, 2018)

"Know the enemy and know yourself; in a hundred battles you will
never be in peril. When you are ignorant of the enemy but know
yourself, your chances of winning or losing are equal. If ignorant both of
your enemy and of yourself, you are certain in every battle to be in
peril" (Sun Tzu, The Art of War)

e Among the key capabilities: “Command, control, communications,

computers and intelligence, surveillance, and reconnaissance” (National
Defense Strategy, 2018)

March 2021 2

 Key Issues Shaping the Systems we developed

ENGINEERING

RESEARCH CENTER

The reality we target to monitor first and evaluate then is
unstructured and unpredictable

 A top-down, model-based approach wouldn’t work

 Majority of the potential sources are textual, while we need
measurable semantic insights

 Text/Natural Language may provide different meanings for different
people/context

Most of the insights we could get are from the evolution in time of
specific elements

 The evolution in time may contain indications to predict future
scenarios

March 2021

nqeeteea Extracting Semantic Metrics from Text

RESEARCH CENTER

 To reduce the risk of wrong/subjective interpretations when making
decision based on text, we need to extract metrics out of it

 How do we get numbers from text? Statistical methods provides a limited
view, because of their lack of semantic analyses

 If we use semantics, how can we deal with subjectivity/contextuality of
the interpretation in text that can or cannot be in given semantic
structures (such as ontologies)? Plain use of a generalized reference
corpus does not provide any subjectivity

 The task of analyzing a text has a bias, that is related to who is
reading/analyzing it. For example, if we want to detect emotions in a text,
“ecstasy” may have different meanings for a narcotics officer, a Vatican
scholar or a psychologist

March 2021

avenue The “Room Theory”

ENGINEERING

RESEARCH CENTER

 The “room theory” is a way to address the relativity of the point of view by providing a
computational representation of the context we want to use to evaluate the text

 The non computational theory was first released as “schema theory” by Sir Frederic Bartlett
(1886-1969) and revised for Al applications as “framework theory” by Marvin Minsky (mid
‘70)

 For instance, when we enter a physical room, we instantly know if it is a bedroom, a
bathroom, or a living room

 Rooms/schemata/frameworks ...
 Are mental frameworks that an individual possesses
 A mental framework is what humans use to organize remembered information

 Represent an individuals view of reality and are representative of prior knowledge and
experiences

 We create computational “rooms” by processing large corpora from the specific
domain/community generating numerical dataset (“embeddings table”). We consider a
table as a knowledge base for the context/point of view

 The “room” method makes the whole approach easy to be moved to different domains



 The key components are:

How the “Room Theory” Works

“Room theory” enables the use of
context-subjectivity in the analysis of the
incoming documents

Context-subjectivity can be the point of
of view of a subject matter expert

The context-subjectivity in the analysis is
represented by a domain specific
numerical knowledge base, created from
a large domain specific & representative
corpus that is then transformed into a
numerical dataset (“embeddings table”)

1. A point of view for the comparison (the “room”). This is represented by the embeddings table
extracted from a large/representative corpus from the specific domain

2. A criteria for the analysis (the ”benchmark’”). This is a list of keywords defining the “what we
are looking for”. Different benchmarks would provide different analyses

March 2021

<e,
08 @e°¢

SYSTEMS The “components”

ENGINEERING

RESEARCH CENTER

 The “Lego” approach

 Systems are developed as growing prototypes based on
components developed offline

 Components cover the tasks the systems have in common
 Components team developed 61 tasks, ~4,900 loc

 Sample of tasks are pdf -> txt; text cleaning; n-gramming;

 

text vectorization; “room theory”

March 2021 7
 Case 1: WRT-1010



 Title: Meshing Capability and Threat-based Science & Technology
Resource Allocation (contract [HQ0034-19-D-0003, TO#0150] )

 This research was focused on providing a computational model to
support the planning cycle injecting relevant threat-based
intelligence and operational scenarios into the more traditional
capabilities-based planning

 This approach will better inform the technical communities
charged with future systems developments and has been piloted
in late 2016 at the U.S. Army Combat Capabilities Development
Command Armaments Center (CCDCAC)

March 2021 8

a Logical View of the
 Threat Based Decision System (TBDS)




 

 The system (Threat Based Decision System - TBDS) has 3 components:
— Risk Decision Support System based on the competitive scenario and the “threats” detected from the incoming text
— Technology Monitoring System analyzing streams of domain-specific documents and detecting emerging technologies

— Technology Forecasting System analyzing streams of domain-specific documents and forecasting coming/probable future technologies

 The TBDS has a user interface to input documents to evaluate, was developed over 2 years, by a team of 20+
people. The minimum viable product has been released at the end of August 2020





 

TBDS: The Technology Monitoring System (Demo)

ENGINEERING

RESEARCH CENTER

 

 The Technology Monitoring System (TMS) scans the horizon using streams of domain-
specific documents, detecting emerging technologies, and forecasting coming/probable

 


  

 TMS is a semantic radar screen for upcoming and “future” technologies, along with
a technology taxonomy generator

 The system can monitor any other user defined topic using any given point of view
for the analysis

March 2021 12

 “Predicting” Technologies
ENGINEERING

RESEARCH CENTER

 

The future cannot be predicted as such, but in areas such as technology and science,
most of the new is based on an evolution of the old

Leveraging on the “room theory” to provide the point of view, we represent in time
the technologies as either points in space or as network of technologies/application


Using predictive/ML algorithms, we can predict the new interactions and relevance of

the technologies/nodes, meaning the new applications for technologies or upcoming
technologies


 

+ Support Vector Machine | 2012

March 2021 13

 TBDS Use Cases: The Questions We Answered

ENGINEERING

RESEARCH CENTER

Horizon scanning:
A Radar Screen for emerging and upcoming technologies

 What are the technologies that are emerging that | may have
been missing?

 How are the new technologies related to the older ones?

 How is a given technology in terms of life cycle: is it growing or
fading out?

 How has a given technology been applied?

 Considering the past and present technologies, what are the
possible technologies in the future?

 Considering the use of the past and present technologies, what
are the possible uses of the same technologies in the future?

March 2021 14

ee TBDS vs. Commercial Solutions

ENGINEERING

RESEARCH CENTER

 TBDS is the result of an applied research and is optimized for the
specific need

 It is based on the latest academic-level researches

 The focus for commercial solutions is on text summarization,
keywords/”concepts” monitoring, and traditional topic
detection

 There are no available commercial solutions that can analyze
text by extracting semantic insights according to a predefined
point of view

March 2021 15

 Case 2: WRT-1023

ENGINEERING

RESEARCH CENTER

 Title: Analyzing and Assessing Contracts for Embedded Risk

 The goal of this research effort is to apply data analytics to
understand the assessment processes undertaken by a
contracting officer. The intent is to bring significant efficiencies to
these assessment processes and develop a prototype tool
covering relevant parts of the DoD contracting process from
beginning to end

March 2021 16

everems WRT-1023: Key features

ENGINEERING

RESEARCH CENTER

 Leveraging on current MO and literature, create a logical
framework to classify requests based on given Contract Types

 Create a computational model for the logical framework
 Create a visualization systems to present the results

 Deliver the results with an agile approach, developing
prototypes/proofs of concepts with increasing capabilities

 As per our contract, during the first year we developed an early
prototype to prove validity of the approach. The prototype covers
the basic functionalities highlighted above but with limited
robustness, interactivity, proactivity and reusability

March 2021 17
 The context: scenario

ENGINEERING

RESEARCH CENTER

 We have been informed of the existence of 10 different contract types,
some of them with relevant degrees of similarity

 Working with the Sponsor, we
identify keywords that are
characteristic of each contract type.
No plain combination of those
keywords leads to an exact
classification

 

 The classification is rarely a black and
white decision and is mostly based
on the knowledge and experience of
the contracting officer

 

 

March 2021 18

eee
eee

o%,
°ee@ ee

 The value of the knowledge base

Coverage of the room
• We started with ~200 documents corpus that generated a
knowledge base not able to identify the keywords either
properly or at all
• The updated knowledge base is based on 537 documents
and is performing much better



Differentiation capability
• With the new keywords and weights,
the system can better differentiate
between contract types

 

 Classification results



The ability of the system to differentiate between contract types is calculated using PCA and performing
T- tests for “within group distances” vs. “between groups distances” and then running Monte-Carlo
Simulations to generate a validity report



RESEARCH CENTER

afs) STEVENS

INSTITUTE of TECHNOLOGY
THE INNOVATION UNIVERSITY®

  

Thank you!

sy

Dr.  NLP
clipizzi@stevens.edu https://niplab.sercuarc.org

Assignment, live session should be this one. 1: Okay. So there's assignment is the in-class access is 7, 39. 1: So you have about 15 min to work on it. 1: So you have a you want to go into any newspaper online, I mean, New York Times can be anything. 1: And you copy and paste. So you select a given text. 1: You copy, and then paste it into a, so you create a text file. 1: You can use word or any other, and then, once you have the file, you already have the soap war file. 1: You open the file, the files, you read them into. 1: Later you remove the So ports you eventually remove other words that are very frequent, like, I remove the Facebook in the previous, and then you calculate and pre into the top 10 words, and you will generate a word cloud using a cloud that should be in right here. 1: Okay, so let me stop sharing and let me create a breakout rooms. 1: So we have a for breakout rooms. We, too, 3 participants, each room. 1: I will pause the recording. Once I create the rooms, create an opener, and you have about 15 min need to work on it. 1: Okay, all the room, Sarah Opener. I will make sure that everything is available in terms of files. We're talking 15 min. 1: Alright! Welcome back! Let me resume a the recording. 1: So we have another 30 s before the rooms will be officially closed. 1: So let's wait there. So what we are going to do now would be. 1: I will ask you if someone what to book about what you did. 1: If not, I will present a possible solution. Then I will introduce the assignment for next week. 1: Alright, so all the rooms are closed. Any volunteer anyone want to talk about what you did? 1: Hey! You don't have to, but just to share the experience. 1: Session issues that you may have had. 1: Alright. Okay. So let me share the screen. 1: Let me go here and. 1: So we are recording. And that's fine. So I mean, I skip the step where I take the text and create the txt file that because otherwise we wouldn't have time. 1: So I use the same Russian agent that they use that in the other example. 1: So what they did there was basically importing the libraries that they needed. 1: Open the 2 files. So Russian agents, and stop words initializing the 2 empty lists with the the soap that we'll contain. 1: The stock ports and the text, reading the stop words into the. 1: So what list? Adding some of the work, so that I know that are very frequent. 1: Than doing, importing the file and doing a little bit of cleaning, so meaning and removing the Hmm. 1: And the special characters, and the spaces, left or right, with strip, splitting a meaning and generating for each line that I'm reading a a list, and then I'm looping up into the list transferring the word into lowercase and then i'm looping into the list transforming the 1: word into lower case and removing the stopwards. So I mean, if it is not in a store port. 1: So then I will update the the word into the empty list. 1: So at the very end I have in in my list this txt words I would have the list of elements. 1: Of words. Then I, removing the non alphabetical with a counter function that they imported right here. 1: I calculate the the most common. 10. 1: Then generate in the work, cloud the same way I did in the other example. 1: So, if? 1: That's me! 1: If I run it. 1: So you have the what cloud, and you have the top, the the 10 words, so that are more frequent. 1: I mean, that obviously, it's the same information in this sort of table, and in the workplace. 1: So you have poster. That is the biggest you have rush on a video and account. 1: So that are the the segments, even if accounts doesn't seem to be so big. 1: But that's the way it is. 1: Alright. Okay, so let me let me close this and let me go. 1: Oh, possible questions! 1: Okay. So if no questions. 1: I will share again the screen and. 1: I want to show you this website. So this website I mentioned is before, is . 1: That is a an interesting website, analyzing controversial issues in societies. 1: So every something they update with the issues that are more controversial at that particular time in the. 1: So you have some issues that are kind of here, since quite a while, and some that the seems to be more new topics. 1: You can pick. I mean, there are several categories. 1: So let's say, artificial intelligence. So you have a description of the concept, and then you have a pro corona, for the question is artificial intelligence good? 1: For society. So you have pro corner, and you'll have a several. 1: All those opinions. So that's basically where we are going to get the data that we will need for. 1: So you can pick anyone. All the issues. 1: So. So I was showing aificial intelligence. But as you saw it, there are many other you can pick whatever is resonating the most with your interest. 1: The goal of the assignment is to compare the 2 texts. 1: What comparing means that means that you want to extract some matrix that will give you somehow a way to make the compiler. 1: So you want to clean the text removing software, what's a shorter than a 3 characters, words and characters that are not relevant with like, we removed the Facebook or Russian in the previous example? 1: Because we know that it was very, very popular. You want to remove the punctuation you want. 1: You want to remove that end of file and a black line. 1: So using strip, then using the library. Rather, you want to calculate the sentiment for the 2 texts you want to extract backgrounds. 1: So you have the example. Using an SDK, you want to create the work clouds for the 2 texts. 1: So at this point you have the sentiment. The backgrounds, and the word cloud, meaning. 1: You have elements that you can use to do a comparison. What you are going to do will be to write a brief report. 1: Couple of. 1: With the URL visuals, meaning the backgrounds, the word cloud, and the results of the sentiment. 1: Analysis, and your interpretation. It is not a description of the process, so don't write down. 1: I clean the text. I'm imported the libraries I applied by those are the results. 1: So is an interpretation. You need to tell me the story. 1: You don't need to tell me the process, because the process is already here. 1: I mean, I know that. So I want your take. I want your interpretation of the sentiment. 1: The biogram, sir, the word cloud. Why, there are those diagrams that are more relevant. 1: Why, there are those. What's that? They're more relevant. 1: So it's a brief report. A couple of page would work. 1: You will submit 2 separate files. The Pydor scriptor and the record. 1: And that's basically it. Questions on that. 1: Alright. Okay. So that's basically it. It's 807, slightly later. 1: But not too much so if you have any question, if you have, if you feel that there are some open issues, send me and see you an email just reminding us that there's still something that we didn't do, I mean, we are receiving quite a lot of emails. 1: So overall. I have. I don't even remember if 75, 80 students. 1: Okay, on top of other things. So we may just hmm, have left your email in the mailbox without processing it. 1: We apologize for that. So send us a reminder. 1: Please. Otherwise we will talk next week. I wish you a great evening.
  
: Oh, do we have class today? 4:41   Yes, I believe so. 4:46   : Hello! Hello, anybody. 5:04   : It's 5:08   : 6, 30. 5:09   : So let's start the class. 5:11   : First of all. 5:15   : Is there any open issue, any something that you want to bring up and 5:16   : review discuss? 5:25   Scott Guetens: Yeah, Professor, I had a couple of questions about the quiz for this week. 5:27   : Yeah. Question 5. 5:31   Scott Guetens: I I was a little confused. What specifically was asking. Like it? It seemed like, so it says the answer was none of the above, but it it seemed like from like when I was researching. 5:34   Scott Guetens: It was asking about the like 5:44   Scott Guetens: classification models. Svm: naive, base rainforest, and none. 5:48   Scott Guetens: And it said the answer was none, but 5:54   Scott Guetens: from what I was reading it seemed like any of them could have been applicable. 5:57   : Okay, what was the question just reminded me 6:00   Scott Guetens: it was. Assume you've collected 5,000 textual social media posts, and your objective is to develop a classification model that it classifies into 3 categories, positive, negative and neutral. 6:06   Scott Guetens: and then which model can be employed for the task. 6:19   Scott Guetens: And the options were none of the above Svm. Naive bays and random forest. And I said, Svm. 6:23   : Yeah, I mean, strictly speaking, that when you want to do a classification. 6:32   : I mean either classification or class setting it, because in reality the the question was more on clustering than in classification. If you want to do class setting. 6:43   : you may not want to apply models requiring a supervised approach. 6:56   : So if you have your social media data set. 7:05   : and 7:11   : I mean that all those methods, the the the 3 options, are on the supervised, the approach. 7:13   : But you don't have a supervision that in a. In that case, if you had a a supervision, and then you can apply pretty much any one of them. 7:25   : because you don't the approach has to be unsupervised. and none of them. It's streetly unsupervised. 7:36   : Then I I I mean you can stretch things so a a little bit. 7:48   but the straight answer is none of the 7:54   Scott Guetens: Okay. All right. 7:58   Scott Guetens: I also had a question on Number 7. I'm: sorry I don't want to take up too much time. 8:00   Scott Guetens: and the options are sentiment, analysis, speech, recognition, machine learning, translation, sorry machine translation and advertisement matching. 8:10   Scott Guetens: So it says the answer is speech recognition. But I don't see how speech recognition is not an application of national language processing. 8:23   Scott Guetens: Well, again, it's one of those cases, so that there is no real black and white, because it really depends which kind of speech recognition you do. 8:31   : meaning that in strict terms it is not really processing text. When you do the process in text. 8:46   : You do some of the things that we mentioned last time 8:58   in the speech recognition. You basically are mentioning words 9:02   : that you you here with words that are in in a sort of a cross reference table. 9:10   : Then things are never black and white, meaning. There is no one to one 9:20   : replacement for the world, for the for the sounds as the world 9:29   : meaning. I would advise the number 7, and I will get more answers because 9:35   : you are right. It's it's board the line. It really depends how you do the speech re recognition. If in a the speech recognition, you apply semantic rules. 9:45   : then at the very end, meaning that that you do one step. That is a kind of a mechanical. But then the th that is a set that is more semantic, and then is more on the Nmp side.   : So that that's a great points call. Yeah, I I appreciate that. Yeah, all right. Thank you very much. I appreciate you taking a look. Sure, Thank you.   : All right. Other questions issues.   : Okay. So let's move on. Now let me start sharing the screen   : and let me go here, minimize this windows here and   : okay. So   what we have a   : in the agenda is a reviewing the homeworker. We already spoke about the quizzes.   : Then we will talk about what mining primarily, and   : we will. We will talk a little bit about the final project. 250   : and we will do an in-class exercise. We will talk about web mining, webcrolling, whatever you want to call it.   : I mean the the topic is not as wide the for sure as a natural language processing.   : but it can be pretty wide. It really depends on a specific cases. 200,   : not all the websites are created. Well, I mean you may have a a a crawling that in some cases it's kind of a a straightforward, but in other cases we really require some sort of strategy   : erez agmoni. We are not going to do that that way. We will mention it, but we will not do it. 250.   : We Don't offer courses on web mining the school of business does.   : I'm. Not totally sure that I mean that the topic would require an entire course. But this is just to say that there is a a wide topic while natural language processing it, it. It's really wide, and i'm creating a a courts on an Mp.   But on web mining we will   : use some of the approaches. We will do any class exercise, and you will do an assignment at home   : on the same topic, and that could be pretty much it, and then you can use it for a a I don't know for the final or something that you may want to do   : but anyway. So the previous assignment was about   : analyzing the the website Proconn Org, and compare the 2 sides. the pros and the on a given topic.   : So i'm using the same assignment since a while, because it it's pretty generic, and it really depends on what are the topics that you are picking.   : So the example that you will see in my solution is not the most recent one, because I I don't even sure that the same topic is still available about the concept that will be exactly the same.   : So you basically copy and paste the the 2 sides of the opinion. Then you do some cleaning. Then you you calculate the sentiment.   : diagrams and the word cloud, and then you will write a brief record. That was the previous assignment.   : So let me go to by charm.   : and   : that's the code. So i'm importing the different libraries. So the counter for counting elements. We will use a couple of times in this script.   : but that's sentiment for the sentimental analysis work, cloud.   : and not clock, but because it's required by water cloud.   : I   use the function we already mentioned the opportunity of doing so. But   : for a a. When you do   : any form of a natural language processing, you may want to have a a a you. You want to create your own function for a cleaning the text, because it's something that you will do over and over, and probably have it once you can reuse it.   : This function, in particular is not the most sophisticated way.   : I wouldn't use it alpha that much, because we know that it's not the most reliable or the detecting. If the content is no medical or not, but is an example of a function for a cleaning text.   : then you have a little bit of explanation. The way is a   : I mean the the comments sort of shaped in this way, because there there is a library in Python. They can go into the code and create the documentation out of the code, using the comments in the code   : posted with this   : specific format.   : But I mean you Don't need to do that, but it's kind of a useful lot to have a a description of what what are the the parameters in, input and what is going to be the parameter for the output.   : because down the road that you may not remember what you did. But you have a clearly as stated here.   : So   : that's function for a cleaning text   : opening the files.   : but I mean at the pro and corner that I previously created. Then, opening the stop or file.   : initializing the the lease so that we like contain those elements. So the list of stopwords to the list of Pro Ancona   : I   : created 2 different variables for the entire line or for the single words   : setting the parameter for the cleaning. One of the parameters is the minimum length for the word.   : Assuming that words that they are shorter than 3 characters. They they   : have no semantic meaning. So you send a minimum length.   : Then, loading the stop word   : the I added the some of the words, so that they know that they are going to be there because this was a pro corona marijuana.   : and obviously cannabis. So marijuana drag. Legal legalization is going, are going to be popular, and   : I mean they are. They may hide so that they want to stress more.   : Then Obed, in the files   : i'm cutting.   : so that are not on the proper length. So in this case.   : if it's not an empty one, and the len is more than 30, because it   : i'm not considering a comments that are too short to get a a real opinion.   : But I mean that   I could have   : parameter to make it more flexible, but the same concept. So this is pro and con.   : then i'm doing the cleaning for both of them   : using the function. Then I calculating the the common, the most common 10 for each one printing them.   : Then, calculating the diagrams I created, the   : that will   hold the the values   : a   : well. Then doing i'm creating. I mean, this is definitely not the most optimized and the most rigorous way to calculate the diagrams. So I don't know if I mentioned it to you.   : I brought a paper on how to Create diagrams. So it is. It's not exactly an easy task if you do it the right way. So in this case I'm. Just taking all the awards that are one next to the other, and then this this would be   : largely, and then taking those that are most common.   : So that's basically what I'm doing. And I'm adding, I mean, i'm joining the 2 words with an underscore.   : And then taking the 5 most common, I mean   : on those candidate playgrounds. and i'm printing them.   : Then. Sentimentalities. sentiment, analysis.   : The the library is taking a   : strings, and not least I have a list and transforming the list into a string, calculating the the 3 components of the sentiment, positive, negative, and nutral.   : Then for both.   : I could have   : use the function for that, but I mean just to repeat it. It's not elegant, but I mean it's just too, but   could have been better   : then printing it   : then   : generating the world cloud so same thing. I I joined the pro cones. I could use the the same that I did before, but that's fine.   : And then   : setting the param, the parameters, for the what cloud generally in the world cloud and printing it.   : So if I run it.   : I have a   : the top words with the quotants. if I most common biograms.   : that kind of make sense so   : sentiment, analysis.   : and then the word cloud here.   : So now, in terms of the interpretation that I mean. Obviously everybody has a their own interpretation. So when you see something like this, you take out the new parallel because it's not giving you much of the the insights.   : So   : you have a the pro that is more on the negative, I mean, both are more on the negative   : sentiment analysis. It's kind of a   : a very row.   : not very reliable measurement.   : When I need to do something that is more, I mean a sensitive. I use different methods. I I use a more than the sentiment, the the emotions.   : I I have a classification of a motion, and I calculate the the similarity, the words so, or the the phrases or the documents may have with those emotions.   : Emotions are are better defined. So we know what an anger is. We know what joy is a sentiment is kind of a   : it's combination of things. So a positive sentiment makes by a joy. But it really depends who you are.   : So, and that's the point. So you have both with more negative   : meaning. The people supporting the legal use of marijuana are stressing more the negative aspects that doesn't mean that the pro   : is actually corner. But is, they are using more negative arguments to support the the pro marijuana   : that could be. If we do not legalize marijuana, there will be more illegal news or more criminality.   : So those are a negative concepts   : on the corner.   : Yeah, and   : meaning against marijuana. They may have used the   : arguments like it would be a sort of endemic user Everybody will use it. There will be no control things like that.   : So what is interesting is the fact that the against marijuana they use more of positive arguments than the the pro marijuana.   : Then that the what the cloud, the I mean it's probably   : I mean I don't know how to interpret the State. That is a so big. But probably the argument was, the legislation should be at the State level.   : The being so big in Washington could be Washington should take a format position in one way or in the other.   : Then alcohol. It's probably saying   : the user of marijuana is not going to be that much different from the user alcohol alcohol. It's legalized. We don't see why we should use we shouldn't we the marijuana, or vice versa, someone could say the number of   : diseases or casualties related to alcohol is pretty high. We do not want to do the same with marijuana, so this could be a reason   : legalizing should go. Then you have Colorado. That was one of the first seats legalizing a marijuana crime could be related to the fact that on the pro and the call the the pro legalization could be. It will reduce the crime.   : The code now could be. People would be less in control of themselves, and there will be there could be more crime, whatever you read them, I mean in this case, i'm. Using just one of the 2   : I don't remember which one it was. It was a and the pro. Okay.   : So that's basically the all assignment that you saw the a python. We verbally analyze the results.   : So if you go in analyzing the diagrams, I mean the   : by, Grams are not telling a lot. Probably you may have wanted to remove the United States   : because it is so popular. So   : the pro the fact that it is economic activity could be related to the fact that then, having a legalized use of marijuana, would be good for the economy creating Jobs   : emergency room. I could be in the call, and I could be. There will be more people in our emergency rooms. Heavy users, so it could be, could point to our individual. So.   : having and heavy use of the   : Madijuana Black Market. I don't know how interpret it, but I would expect more on the pro saying, this will reduce the Black Market.   : but probably could be related to the fact that some people I can think that the   : people may become a a more prone to the use of or recreational drugs, and may get a additional drugs from a black market for drugs that are not very well.   : So I mean that those are all examples of how to interpret the data   : on at the Topwards states Washington I mean, those are   : the same that you saw in the the what cloud the world cloud the the sizes of the words are based on the frequency on the awards. So in the document.   : so. And we have the same thing that we had in the what cloud? So States. Washington alcohol. Those are united is probably part of the United States that, should it be removed.   : meaning   : we should have removed the   : If you go all the way here   : we should have a standard that the so forth. So with the United that because probably   : I mean not States, because United States, we want to remove it. We do not want to remove   : so when they are by itself.   : anyway. So that's all about the the assignment questions issues   : All right. So let's move on   : and let's go to   : the   : web mining there.   : So the reason why you saw   : this presentation for the engineering management program is because we are introducing the program in the Chinese market with the format, that it is a one, a semester in China to some assets in the Us. And and   : Erez Agmoni, and we were reviewing the material. One of the things that was kind of interesting in the conversation we had today was the fact that the school of systems and enterprises as the highest one   : employment rate across Stevens is 98, so the average on stevens is 95. After graduation from a master degree   : so the average is 95. We have 98 in in terms of salary. We have a one or what program so that is a software engineering with $105,000 as a for salary. That is the highest in Stevens   we are. The average is around 95 97.   : So I mean just   : give you some   : background information.   all right. So what mining using Python?   : We we talk a little bit about the what web binding is, and then we will go in a python to see how to do it   : in practical cases. So what is web binding web? Mining is basically going into into webpage and download the   scrape content   : if you think for a second, this is what.   : in a sense, the different web such sites are doing, what Google is doing, what being is doing. So they go into the page. So what they actually do is to read the content.   : tag the content, and then match the tags with the requests that you may have.   : we may want to do something different, because I mean the goal of a so changing is just to let people   : go to the page. This the specific page that is matching the the specific request.   : We may want to do something more. We want to extract the the content to do some semantic analysis. We want to extract some metrics, so to do, some combined the calculation.   : Erez agmoni. There are sites that are collecting information from different sites, aggregating and adding a a layer of integration and some additional analysis.   : You may want to do web mining for a   : optimizing the performances of your own website for seeing what is the distribution of content to analyze if there is any problem going. So for   : several of those examples, web mining is useful that   : even if mining mining, what mining data mining that those things are really different. So data mining is is more on that working on the   : even if sometimes they'd be not so much the being text or or a similar, but it's more confined. So you know pretty much what you are going to get   : with web mining. You don't know what you get, because you don't have control of the content. You may not even know what the structure is, because the owner or the website to change the the the structure overnight.   And your beautiful script that was running yesterday is not running today anymore.   : So when you do web mining, you really have quite a lot of issues, and obviously   : the owners of the websites, and in particular, if they offer the content as a as a service for money they want to avoid that. You just go there and scrape the content.   : This is the case for news. This is the case for a sites like a linkedin, so meaning that they are really doing their best to to make the life on the web scrab, or as much difficult as as possible.   : Nevertheless, there are cases where this could be useful anyway. So there are sites that are open. I think about all the sides that are from the government.   : You want to download the all the facts or the official filing of companies. Those are those. Those are information that are publicly available. You want to have the list of patents.   : Things like that that they are, I mean, not saying it is easy to get them, but those are public, and you are not infringing any   : or a limitation just downloading it. You're just doing it in an an automatic way.   : What is the web that we want to? Mine is a a a a a huge number of page, so so no one can really count it. So 63, Leona, it could be 100 threed on, and we wouldn't notice the difference   : that there is a lot of duplication, and some of the page are not indexed. If they are not indexed, they they are part of the so called dark Web.   : Not necessarily. The dark web has to do with the illegal activities and Doesn't do.   : It only means that you cannot. Google them. So Sometimes people do not want to be googled because you   : is, is a pager that is restricted to the people. I know people. I will ask the the URL at the address by myself. And that's it.   : So that's a possibility, meaning that those are not indexed by by Google. So again, not all the page.   : Someone was saying that.   : using the metaphor of the people, the iceberg. So we see the tip of the iceberg. But there is the rest of the iceberg. That is the rest of the web.   : So when we want to do a web mining.   : A few years ago I was teaching this topic to an undergraduate class   : and a certain point. The students were asking me 2 questions. Why do you want to do it? First question and say on that? Is that legal?   : So on the first one. Why you want to do it when there is Google, Google is not giving you the content, but is giving you the point to the content.   : So if you want the actual content, because you want to do something with the content, then you need to go into the specific page. If there is more than one page, then more than one site, then I mean, if you do it manually, we'll take a   : quite some time.   : and then, if the content is changing in time, you want to create a system that will automatically go there and get the content for you instead of doing it each time manually.   : So   : that's why we want to do it. The E. Is it legal? Well, I mean companies like Google made a business out of it.   : So if they are doing it, the the planetary level.   : why shouldn't be able to do it? The at a Mini school level? Why, we can do with the our own web mining.   : So it is legal. Up to a certain point there was a case a a few years ago between a linkedin and a small company.   : So this is a small company.   : was a scraping data from a linkedin, analyzing people up with the sort of   : abnormal user of job search in a linkedin. Assuming that if someone is a changing the path of looking for jobs most likely is looking for a job.   : and the employer may be interested to know, just to either make a counter offer or just let them have the employee, or, if   : so.   a   : This small company was collecting the data, adding value every selling the the result. Linkedin.   send   : a legal note, saying.   : Erez agmoni, what you are doing is an infringement of the terms of use, because you are supposed just to look at the results, not to use 150   to create a business.   : So it the the company was teeny, tiny, and compared to linkedin it   : to take the case for the small company pro bono, just because he or she didn't like the fact of the big giant linkedin suing the teeny, tiny company.   : So the litigation that went on for a quite long time.   : couple of years. So in the meantime a and and there's more company one.   : but in the meantime they lost clients. They lost key employees. They couldn't do Dave job in the proper way, and they went out of business. So the model of the story is, you really need to check. What is the thermal use of the   : web mining that you are doing? May not be illegal, but maybe some restrictions.   : Some companies created the their own business out of a web mining. So, indeed, that now is one of the 2 giants in a job search along with the Linkedin.   : Initially.   : the only thing that we are doing was to collect job postings the   : from a different websites and integrate them in the website, adding a layer of all the analysis I mean filtering and analysis.   : So that was the the initial web model or a business model that they have in the business.   : Then, at the certain point they became pretty big. They started the accepting the accepting a sponsorship for a specific job. So a and some people play their jobs in a   : a sort of an exclusive way into indeed, meaning that they didn't have to scrape the the content. But they have. They had their own, and they still have.   : But the initial business model was a no data that we are, they own, just integrated the content   : it's great from   and other websites.   : So when we talk about web mining, there are a few things that we want to consider.   So   : first of all, how a web search is is working. So we mentioned Google, we mentioned being so, you basically had the web with the all the paid. So, and they usual links   : that we haven't on on the web. You have a   : this piece of software, the web that is going into the different page and indexing them indexing means tagging by given keywords.   : So each page will have a   : several keywords that are defining, based on the taxonomy that was created by the Google people or the people, whoever they are, to classify somehow the   : So once you have the the page index in the, then you have a a a data set with the all the page and all the the indexes. I mean that the keywords, the tags.   : Then you have the user placing up the equity.   : and then what the search engine is doing is basically matching the keywords. So, and he's a pretty much an exact match between the the keywords. So in the queue and the the tags from the page.   : and then the result will be presented.   : Then things can be more sophisticated, so I can   : automatically correct misspelling, just to be sure, rather than getting what the user really needs, I can have a synonym.   : So   : that's an additional step. So when the user is placing a query, then there is a an intermediate piece, analyzing that kind of natural language processing I mean   : low level, but sort of analyzing the towards the keywords, eventually a little bit of context.   : So that's the basic functionality.   : Google being another. They also have a the placement meaning.   : We mentioned that you have the tags for the page is tagging the query from the user and basically what the add the index   : is doing is okay. I have one tag from or in tans from the greedy. I will match those stunts with the the list of advertisers with the same tang.   : and then, based on who is paying the most. I will place a that piece of add on top of the list.   : So the basic search engine, as no AD management component, but pretty much all of the search engines do have one.   : So   : search engine as a a a crawler that will create the the corpus of those tagging. Obviously, again, you need to define what are the tags.   : so meaning that you need to create a sort of taxonomy of the page before classifying by the specific tags.   : Then you have something that is the indexing. So the spider is just collecting. But then the indexer is creating the cross reference. So you have a tags and page, and you need to be able to go back and forth from one another.   : and then you have a query processor that is getting the   : quidies from the user and matching with what is in the indexer   : again. I I already mentioned that   : you may have a a query process, or that is a little bit of a intelligence in it, being able to work on like a visualization   : on using a bul. So operator. So so in Google you can do, and or you can have a   : must include the with the quotation. So all of these are additional functionalities to the query process. So, but not   the the core ones.   : So the data on the Internet, as we know, are based on the concept of a hyperlink. So page with   : plain, basic content that dumb content. And then you have content to where you can click on it, and you will go to a different page. So those are the hyperlinks, and the hybrid links are linking a page one to the other.   : That's an example of a how our we is going back and forth.   : So the   : they are based on a language that is called the HTML hypertext markup language that is a 3 like structure with the   : I mean header and a food, or that will be the same HTML. And then you have the different components, like chapters like paragraphs of a document, and each one as a tanks that are on a in brackets with either the tag   or the content, and all the content.   : When you send the the a request that could be just open a page, or or doing something a more elaborated, that you use a a a protocol for a   : requesting the page. So when you go to I don't know Cnn  you write http   : call on   : So this Http is basically telling the browser, and then the the server to send your request in a format. That is Http.   so and that   : in the Hdp. There will be a Ww. There will be a name Dot, and a   domain family.   : and that's the standard for Http, and it's pretty much the same if you are sending a different types of request.   : The request it's that will go from your browser to the server as more information that you may expect. So the server will see   : in this sort of back at the request for for the page. But we'll also see information about the   : who you are. So what is the the browser you you are using? What is the type of encoding that you are using? If there are cougis, what are the cookies that can be used.   : What is the type of connection you have eventually? What is the the IP address that you are using   : the response? The   : you see, as a a page meaning an HTML file, that again, as a more features in it. So you also have a what is a   the opening system on the server?   : What is the the type of software that this is in? It is being used. What is the type of the release of the version of a Http. So those are additional information that 99% of the time, so we do not use.   : But if we want to have more information about either the sender or the receiver. Then those are are available in the back and forth on the request   Oops!   : So that's another information. I will skip that.   : So the spiders, the crawlers. Again. We can bite them. We need to be sure that we are using the the right protocol to go to the server, and we have the right level, so of the authorization to do it.   : Keeping in mind that that when we work on a on a web page I can change meaning. We are not the owner on the page, and the owner on the page can change it the way they want.   : So if we go into the HTML, and we are looking for one particular tag and take the content. After that tag.   : Yeah, well, the owner can change it if they change it. Our roller is not going to work anymore. Some other times information on on multiple page meaning you need to click on something   : to go to the next page, and then you need to stop when you reach again on the page for that particular information.   : So as you may not, we can do that because we see, we understand, and we act   : a   : for playing dumb cooler. So that's not possible. You cannot go to the page and get it the content if the part, the content is on multiple page.   : so there are ways of doing it. So one of the ways is to have a   : a system that can sort of simulate the the user so there are   : the   : Erez Agmoni, I mean, there are libraries in Python where you can download the an executable version of a browser, and your python 3   : will run this digital browser, and it will act as it was a human.   : So those are a little bit more complex so. But it's what we do when we have a a file with the higher level or complex it   : erez agmoni. When you do a search, it really depends. What is the strategy on the search? So you can do, Bradford, meaning you go 150   : from the root node to the first node. Then you move up horizontally until you reach the end.   or you can go   : all the way down and then go up so bathes. If we use a a fi f, or a alli, a full meaning   : per scene for south or or last team for south approach there are pros and cones. So if you use this approach, for example, you can get lost, meaning there could be quite a lot of all the levels.   : and you will never jump to the next one if you use this one while you are completing one level.   : things can change in the meantime, and kind of will lose the initial relationship they had with the the higher level node.   : So again, it really depends on the type of search that you are doing.   : When you get the data from a server you use a protocol that again one side is a HTML in terms of a page that that exchange. But then you use a a protocol to get the page. So   : in Python we use a there are several options, I mean in Python there are always a several options. You can just   : ask Python to go to a certain page and get the HTML. You can use a libraries that are specific for one category of page. So there is a library or a newspaper that is called the newspaper.   : and if you want to download it.   : you need to install as a newspaper 3 K. I don't know why they're using this notation. Probably there is a newspaper that is a different one, and they do not want to be confused. Whatever is there is.   So   : one of the approaches is this one? So let me stop here for a second on.   : and   : the   let me go   : your.   And then, when we go to   : I, Sharma, with an example so beautiful Super, that is, from Ellis in the Wonderland   : is a the Python library that is taking a a 3 like structure like a HTML like Jason.   : ex Amanda.   : and parts of the structure in a, that it's easier to navigate. So, generally speaking, you download the the HTML. You pass the the HTML to beautiful super, and then you start processing it.   : So that's basically what we normally do.   : So before going to the actual code, let me spend 3 min on the Apis.   : I think we already mentioned what an Api is. The Api application program interface is a a protocol for exchanging information between the service.   : So when you 1:   : want to download the tweets. 1:   : and you use a a a protocol created by Twitter to get or do something on the 1:   : the tweets that are in date service. 1:   : Either they are to the website, the the the web server, or a store in the in the server. 1:   : The Api is a better option compared to 1:   : web coding, because 1:   : it's a like a contract that you have with the owner of the website. So you are saying, okay, I will give you my credentials. So, using the credentials, so I can do several things. So some of the Apis are free, meaning 1:   you just need to 1:   : create the the the the credentials for the authentication process. But there is no cost associated with that. In some other cases you pay for 1:   : you getting the data using the Api. If you have a subscription to a news service, you can download the a certain number of 1:   : news based on the contract you have through Apis, so you will specify what are the 1:   : I mean, the the the page you want to kind of 3, 6 based on the contract you have, and you will get 1:   : everything that you requested. subject to the contract you have. 1:   : So if you have a an Api, either a with free or paid access is definitely a better option that just scraping the content. Sometimes you don't have the Api, and 1:   : you need to do 1:   the the scraping 1:   : we twitter. You have a a limitation in terms of a number of quidies you can do, because the Api it's free in that case, but with the limitations 1:   : they don't offer directly a 1:   : paid Api. Probably, you know, mass will change something. 1:   Yeah. But 1:   : in the past it was not possible, and I think it is still not possible. 1:   : My dissertation was on analyzing Tweets meaning. I spend quite a lot of time on the 1:   : and in the past it was free with limitations in terms of a number of tweets that you can download in an interval of time 1:   : unless you do things in different ways. 1:   : then that I mean, the way out is to create a time based script, meaning you download the the maximum number of tweets for the time you have. Then you place 1:   : your system in a holding mode for 1:   : a few seconds, and then you start again. 1:   In 1:   : In that case you will never reach the the maximum number that is allowed a, and you will continue. I mean. We take longer, obviously, but you will get the tweets you want. 1:   : So again, that's an example. 1:   : There is an Api. You have a a. 1:   : So those for our the that you are asking. So the the credentials. So there are 4 credentials in a Tweed. 1:   : You can do it directly, using I mean creating the script. I mean a a bomb with the query, asking all the elements what you can use. A library in this case is that Twitter 1:   : That will do in this case the authorization for you passing the the the credentials that you created. 1:   : and 1:   : and then eventually you can use the same library to get the tweets you want. 1:   : So let me go now on 1:   : the example. So in this example 1:   : i'm working on the the New York Times. So if we go here, and we do 1:   : so. That's what we have right now on the the New York Times. 1:   : So if I go 1:   : here, so i'm importing a beautiful super and this library that is the one that we use to go to the website and get the the HTML. 1:   : So i'm 1:   : getting in body the HTML page. Then i'm using beautiful soup to parts it in a tree like structure. 1:   : and then I think, printing the the content. 1:   : So if I run it. 1:   : that's what I have, so I have 1:   : everything that is, the the ruling was a and so on, that this one 1:   : the shooter, and this this one. 1:   : and so on. 1:   : So 1:   : I mean, we also have things that we may not need that this 5 min rate 6 min speed that is, those 1:   : pieces of information is not something that we really want. 1:   : So, anyway. 1:   : he is a 7, 32 1:   : before we go to the in class exercise 1:   : I want to spend a few minutes on 1:   on the final project. 1:   : So 1:   : I will publish shortly 1:   : the That's it. There we go. 1:   : The ideas for the final project, so the ideas for the final project 1:   : will be 1:   : on this document that you will have online. 1:   : The final project is basically defining a problem 1:   : having a a data set to address the problem and then do all the data preparation for, and then using scripts, getting insights from the data set 1:   : mit Ctl. And that can address the questions you had in 1:   : the problem definition. 1:   : and then 1:   they can explain your findings. 1:   : That's basically it. 1:   : Now the level of complexity of the script has to be at the same level all the 1:   : I mean not less, let's say, than the level of complexity or the last assignments. 1:   : So if you go and see, there are 1:   : that right here we have a 1:   : about the 1:   200 lines. You want to have something that is at least 1:   : Erez agmoni, 200 lines, 100, 5,200, then obviously not all the lines are created equal. So there are a lot of comments, some blank lines, 152 1:   : additional comments. There is a sort of a redundancy here, the dimension before. So those 2 could be combined in one single creating a 1:   : a function 1:   : ere I mean that 1:   : yeah, If you optimize this code from under the 90, you can go easily to 1:   : a little bit more than 100. Let's say 120 something around that 1:   : so? You want to have something around the 200? Why, I'm measuring the 1:   : complexity by lines of code. 1:   : I mean, i'm assuming that there is a a direct correlation between the number of lines and the complexity. That's not always true. 1:   : but it's true most of the time. So you want to be sure that you have a code that is, with the I mean complexity enough to go into the data creating a enough insights. 1:   So 1:   : that's the goal that you have a problem. You have a data set. You clean the data. Either. W. Whatever is the way you want to do it, and then you create your your script to analyzing it. Then you use the outcome to write the paper that could be around 10 page 15 page, or included. 1:   : It's like some of the last assignments that we did think about the pro con, but expanded. 1:   : I'm. Suggesting 3 topics. 1:   : You don't need to to use those if you have a a better option if you have something in your mind that you want to analyze, keeping in mind that you need the problem. But you need also the data 1:   : and getting the data sometimes is not so easy because either they are not available. They are in a format that is not usable or is not enough data, so there may be quite a lot of issues. 1:   : but I leave it in open. So for this class I leave it open for the other classes. I'm teaching 1:   : on Wednesdays. I don't give 1:   : that students the option, because this class is more, and I can have a I can manage a a multiple 1:   : searches problems that you may have in a class with 55 people. It wouldn't be possible. 1:   : So we did in the past, but it was a complicated because you have a data set, then you didn't attach the data set, and I need to chase you. What is the data set? But it is this one. But this is not what you use. 1:   : I mean that if you multiply by 55 it will become a really difficult to handle 1:   : giving in mind that 1:   : that we need to 5 the final grade within 72 h after the end of the final 1:   : Erez agmoni, meaning we don't have time to all the back and forth on I mean, then there is flexibility. But the rule is 72 h, 2. 1:   : So 1:   : to shorten up on the time i'm giving you 3 options. So so analyze a people. Migration data is one that you would read it. There is no need for me to go through that analyze research projects. 1:   : analyzing COVID-19 data. So each one as a challenges. they are 1:   pretty much equivalent. 1:   : So you will have all the 1:   : the data set here. 1:   : So that's basically on the final, I think, already published some of the examples. If not, I would do before next week. 1:   : Okay. So let me now go into 1:   : what is it? Yeah. 1:   : So the in-class assignment 1:   : you will write a program that will extract data from a web page and perform some analysis. So you can go to New York Times or anyone that you want to do it. You want to print the headlines, generate the word cloud, the for the what's in the headline. 1:   : so I will post. Why you will reaggregate in. 1:   I mean. 1:   : in the different rooms this 1:   : small script, to be sure that you can use it to. I mean write the code for the in-class assignment. 1:   and you will have a 1:   : let's say 15 min to work on it. So let me open that 1:   the breakout rooms. 1:   : so there are 3 breakout rooms with 2, 3 participants, but each room 1:   : I created it. I'm open it. I will pose the the recording, and I will make sure that all the content that you need that will be 1:   : available through a canvas. See you in 15 min 1:   : resuming the recording up. 1:   : So it's a 7, 58. All the rooms are closed. 1:   and people is coming back. So 1:   : questions, issues. 1:   Okay. So let me 1:   : share the screen and let me go through a possible solution. 1:   So for 1:   : this case I use the a library that I was mentioning before, that is newspaper. So when you import the newspaper, you need to make sure that that you import the the library as newspaper 3. K. 1:   : So even if, when you use it in a pydon, you will just write in port newspaper. If you try to install a newspaper, you' not going to get it. 1:   : but I mean once you install it up. It's it's a little bit more as smart than the 1:   : in a way, or doing it, using a request and a beautiful super one. 1:   : So, anyway, I imported this library. 1:   : I imported. What Cloud and Matt Plot, lib. 1:   : I specified what the the the 1:   is. 1:   : Then i'm extracting, using this library. Those are the parameters that that are 1:   required. 1:   : and 1:   I mean that 1:   : the library is giving a a little bit more feature so than the playing request. But you you are with size. You have the number of articles. 1:   : Then you have a 1:   : in that with 1:   : with paper you basically have the content. And then what you are doing here is a downloading the the Rt. Also, and passing it. 1:   and then 1:   : printing the 1:   : I mean, I created the some. 1:   : So so if the article is now in August, then 1:   some, I mean i'm reducing the the 1:   : I mean that 1:   title so that i'm getting you not me to do that. 1:   : So running it 1:   : 131 articles. Those are 1:   : the titles, and as the word cloud 1:   : and I will pass this, I I would do it right away. 1:   : Yeah. So 1:   : okay, I need to post it 7. Okay. 1:   : all right. So for next week exercise Number 10 mining webpage surprise. So you you will get 1:   : any news website. You will bring the headlines so generate the word cloud for the words and diagrams. Calculate the sentiment to write an interpretation of the results. Again, write an interpretation. 1:   : so you will import the libraries. You will remove what is not essentially using the the software file. 1:   : you will clean using a soap for fights and other the headlines. 1:   : You will extract the diagrams, just as in the previous assignment 1:   : you will merge the list of single words and diagrams. You will create what cloud 1:   : you can or cannot calculate the sentiment, keeping in mind 1:   : that when you have a diagram with the underscore, it will not match anything in the sentiment, because it's not a world that that is known by the 1:   : and then you will write a. What Pdf. Document 3 plus page with the interpretation. 1:   : So be sure that the interpretation is the original, that there are comments that 1:   I mean that 1:   : the document is important because of the very end. 1:   : We are not in computer science. So we do not write code. But we use code for doing something. 1:   : Okay, so that's basically it. It's 803. Again, my apologies to be a little bit late instead of 8. So let me stop sharing. 1:   : So if you don't have any other question, I will make sure that you will have a the in class except size. So the solution on the in class exercise on your canvas, just to use it eventually for a next assignment. 1:   : And as usual, if you have any question, send me and she you just to be sure that you have more chances to get an answer. We will get back to you 1:   : all right. Yes, Good question for the final. Is it a group assignment or individual? 1:   : Well, it's typically an individual assignment that, considering the type of class, we are, I would be okay with doing a a a group assignment. 1:   : a small groups meaning not all of you 1:   as a a single group. 1:   : So in the specs I wrote, 2 people could be 3 people. And that's absolutely fine. 1:   : Okay. thank you, Professor. 1:   : Do do we have to let me know through email if we form in my group. Yeah, I mean, we want to avoid the the the certain point. We will grade it. And then we would say, you did the an identical, the final and the other, and you cheated. So that's obviously not the case. Yeah.
  
: Now we go, so it's Wednesday, April the nineteenth, and it's a 6, 33, 0:01   : and let me start sharing as usual the roadmap. 0:08   : So we are. 0:19   : So that's where we are. So we are 0:22   : April nineteenth. So we will do sort of a review of projects. So this visualizing a social webc. That not necessarily is what we will focus on. But we would talk about the things that are related to that 0:28   : I would introduce except Size Number 9. It's a size. Number 9 is going to be, let's say, the most complex of the assignments that we did so far 0:46   : erez agmoni. So that's what we will we will talk about. We will do an in-class exercise 3, and I would be happy to address questions you may have on the final or the other. 1:00   : Again, I will introduce the that size 9 that is, on the management side that let's say somehow similar to the one that you did the on the faculty courses. 1:15   : Exercise that you did last time or a 2 times ago, actually. 1:35   : and and that's would be it so, the in class except size. I would be the usual 20 min or so. We may finish a little bit earlier. I would see how things will go. 1:41   : Okay, before I I start. Is there any question 1:57   : on? I don't know the final, the timing of the final, the topics of the final 2:05   priyanka marwaha: Hello. 2:16   : Yeah, go ahead. 2:17   priyanka marwaha: Yeah, this is I had a question like I had mailed 2:19   priyanka marwaha: a proposal for the 2:23   priyanka marwaha: I I wanted to, because of the project. 2:31   Okay. on Monday some kind of like waiting on your feedback on the same. 2:35   : Okay. So your voice was chopped, but I guess that you send me a a a proposal for a final, and 2:42   : I will get back to you. Keeping in mind the the generally speaking, there should be a good reason for doing a a a project that is not in the list of projects that that I sent you. 2:56   : meaning If the project is somehow relevant to you, because it's something you are working on already is something from your job by something that you want to do for a specific reason. It will be approved 3:10   : if is a genetic project that eventually has been already 3:26   : analyzed on cargo, and there are several submissions already on cargo that that would be denied. So I don't remember quite honestly the project that the proposal that you sent to me. I I will review it. 3:32   : But keep in mind that the general approach is, unless you have a compelling reason for doing something different. The projects you have are the projects I mean in the proposed projects are the projects that you are supposed to choose from. 3:50   : So okay. 4:10   01FB15EEC157: that's the 4:14   all that thing does make them 4:15   : all right. So let me 4:21   : move on. And 4:24   : again, the class today is going to be sort of a a an overview of some of the research projects that they did so far. Just to give you a a little bit of an idea of what you can do 4:30   with a an extended version of what you learned in this course. so sharing the screen again 4:46   : and let me go here 4:56   : and let let me start with the this project up. That was a somehow 4:59   : in Mystone in my research. So in my research there were 2 mystones. One was during my Phd. When I addressed the social media. 5:09   : and the second was 40 years ago, a little bit less when I started working in a Dod sponsored project that was a pretty large one, was a over 5:26   : about 2 years, about 4 million dollar, about 2025 people working on it with the combination of all the 5:45   : students 5:56   : faculty and external contributor, that they high as the as a full time or part time employees. 5:58   : So what's up 6:11   a large budget? 6:13   : And was a relatively large project. 6:15   : So the the project was with the within circ within the software engineering research system, engineering Research Center, and was a sponsored by 6:19   : the departmental defense and in particular by the Picadini apps now. So the official title was a meshing capabilities and threat based the science and technology resources, allocation. 6:37   : So what does it mean? 6:52   The Picadinia? It's doing 6:57   : the national. So they are developing new weapons. 7:02   : so they do a hembel, the weapons meaning no tanks and all submarines, things like that 7:07   : and a a they develop a somehow the new generation of weapons. 7:15   : So the the way they are doing it normally is based on my capabilities, meaning having weapons, so they are more and more effective. 7:25   : but they don't have a real sense of the competition, and they don't have the a way to evaluate the how the current threats can change the the of the of the weapons. 7:38   : So that was the project. The for use was on technology, meaning how technology. 7:58   : all right. 8:12   : science and technology. But let's say how technology is used by the the opponents, and and how the different use of our technology can impact the the 8:14   : balance of forces that we have in the world. 8:29   : So a as you can imagine. The the project was a quite an issue, so I mean that the the budget for some issues, and 8:35   : we face the several challenges. 8:46   : So 8:50   : the Picadini Arsenal or Us. Army combat capabilities, Development to man, Arm and center. Ccdc. A/C. 8:52   : It sounds like a a a rock band. 9:05   : They developed a framework, and the idea was to consider that framework and expanded the creating a more capabilities. 9:09   : So we 9:24   : again, in the first phase in the the the first part of the project for Sierra. We kind of replicated the the process that they highlight was very high level. 9:26   : I mean a sketch of the project and creating the model to do in a a different way. So that's as napshot in a certain moment all the size and the type of people within the the project. So 9:43   : a lot of students faculty one post, Doc, some employees, the good portion with the Ph. D. Some Phd. Candidates some, and the rest master or a master candidates.   : We wanted to use a an approach that was a bottom up. meaning using text and extracting from text based on a given profile, the the information that we needed   that   : challenges where many one of the challenges was how to extract the numbers from text, because at the very end what we needed was to create a sort of this panel that was able to to   : I mean, based on parameters. So to determine how the balance of forces can change, based on the different conditions or the different uses of the technology.   : And this was the first challenge, the second challenge. Let me stop for a moment here. The second challenge was, we will go back to that was on selecting   : the right   : domain. So we were in the condition that that and   : most of the people in the team we are not in. We're in international students, meaning they could not have access to the   : actual hold on just 1 s. Sorry. There is some background noise that I We want to stop for a moment.   : Okay, my apologies so, and the the project had some levels of of security related issues.   : A.   : I wanted to have a a graduate students working with me. Stevens, like the majority of a STEM universities or colleges, so   has a 85% roughly, of international students   : at the good outlet level. That means that   : they are not us persons. Us. Person is defined as a as someone, either a citizen or a green card holder.   : and obviously they cannot get any security clearance.   : So in the project I was the only one with the security clearance. But obviously I cannot develop the the project by myself. So   : I follow the an approach that was a a on a to   : assumptions or to   : basic choices that they made one to change the domain. Instead of talking about countries and a kind of a   : opponents of potential opponents. I changed the domain and the new domain was security companies.   : public security companies, meaning companies that are working in an area that somehow is a similar, or it's related the strictly related to the defense industry.   : but with information that are publicly available, because we are talking about public companies and meaning the companies that are publicly traded, meaning that they have   : all the information   available online and all the news eventually related to them and that are available.   : So that was the the first step, and obviously I discussed with the sponsor what the best scenario could have been, and that we decided that that this security industry was a a way to go. And again, the security industry.   : the private security industry publicly traded companies. Anyway, it is an industry that is big. It's technology driven as some semantic proximity, and is measurable.   : so those are some of the numbers that are pretty big. So   : those are.   : or where some of the trends, so that somehow we are similar to the trends that the Dod was seeing in a domain   : that the other approach. So this proxy domain again. Obviously.   : I, it is not the perfect choice meaning that there are differences.   : So when we talk about creating a competitive scenario, creating a a a risk panel in the countries. What you want to do is to check for a so to change   : the world wide balance. So I mean that   the Department of Defense is not the Department of War.   : So it's defense, meaning Their goal is to keep the status quo, avoiding that there are reasons to start the war, but be prepared. The based on what the others are doing   : to re-establish the equilibrium between the forces.   : So   : the the equivalent in our proxy domain was the market balance. So you have those companies with the the market share. So market shares that can change. And and   : if there is a new technology.   : the new technology can change the balance all the forces. Or you have one company taking over one key technology. That was the the coordinates or not another company changing the all that balance.   : So I mean that we had to do some sort of adjustment, and that was a a critical   : a   : we for use the on the 3 layers. So companies, market segments and technologies. So we basically analyze the those 3 components. So data about companies, data and texts about companies, market segments. So   : Erez Agmoni, I mean, those companies are are big, some of them. They are several 1,000 employees, and they have market segments, so that can be 150.   : Strictly security can be.   : and a technologies so that are enabling, or this enabling some of those offers that those company may have.   : Oh, the analysis is dynamic, because things are changing over time.   : A another component that we use the in the approach was to create a sort of a separation of duties.   : I basically identified the 2 main projects, one that is, this risk panel and the other one that is just analyzing the technologies like monitoring the the evolution of the technologies all over time.   : I assigned those 2 projects to to people with a green card.   : meaning they are a Us. Person, so meaning they have access to information that are restricted to international   : people. but not to information that are recovered by other restrictions.   : All the other, the the large portion of the team was developing components.   : so I   : called it the the the the legal approach.   : So this largest part of the the team was working on a specific tasks components   : that are independent from the user that they will do, but they can be integrated just as a a a a legal   : Erez agmoni. Then the 2 people in charge for the development of the main projects. They will take those components 150,   : and integrate creating a a specific system. So those 2 approaches the proxy domain, the alternate reality, the and the the   : Erez Agmoni development, based on components where the 2 ways to kind of overcome the issues that we had with the   : not having a U. S. Persons.   : Each team member went through a required Dod training, anyway.   : But I mean again, the majority had a restrictions.   : The the technical problem still state. So the the first 2 approaches are more on a team management and a project management more than on the technical side.   : On the technique outside the we we still had the the problem how to go transform a   : a text that we can collect from different sources. We use the news, but and some papers for that. So how you collect the how you transform the information that you can get   : from one of those 3 sources into number, so that you can use in a a dashboard for a risk analysis. So that was the the real issue   : to do that. I created that I already mentioned it, but I would go relatively fast on that, just as a recap   : what I call the room to. So the room theory is a knowledge based approach   : to develop a a system for   : evaluating the documents based on a given characteristic   : analyzing text. As we know from other classes, it's   : a complex task.   : It's becoming even more complex now, because the communication that we use is a shorter.   not much structure is chopped the   : meaning we cannot use any strag, any formal structure to extract the knowledge. Now we are developing methods to get the formal structure from a unstructured text. But that that's another story.   : So   : then, you have the issue with the language that is evolving meaning. You cannot really rely on a taxonomy or a   : existing to   : represent the language in a broad sense.   : And then there is the mother of a subjectivity. So the tibi gala example that they use 1 million times. So we did the a a classification of the emotions.   : So, using that classification, you have a certain number of different emotions, one of them being joy. In this classification. Each a motion has 3 grades joy, the   : the extreme.   a shade of a joy is a   : but if you name the word ex to see if you are into emotion, then you say, is a shade. That joy.   : if you are into narcotics, then is a drag. If you are into religion, then it's a a stage to one, the   : so it's the same word with 3 completely different meetings   : erez agmoni. So the subjectivity, along with the the dynamic evolution of the language and the lack of structure 150   : make the all the traditional approach not usable so. And that's why we   : I think I mean that   : I mentioned medium time. I'm working in a AI since many years. So back in time Marvin Minsky was a one on the points of reference   in a I   : A and a might be Minsky use the a schema theory that was developed a century ago more than a century ago, and renamed the I mean, elaborated the concept, renamed it the the framework theory.   : the basic idea of the framework to, or is that   whatever is a   : the element that that that we want to either analyze   : and understand, express an opinion on on is based on mental frameworks. So things that are how we see things.   : and you use the as an example when you enter a in a physical room, you know, if it's a bedroom, a bathroom, a living room, but not because there is a label on it saying bedroom, but because of. There are things in the bedroom that resonate with your ideal bathroom.   : so I use the the example on the room, so to rename the approach room theory, and I added the a layer that was computational, because I mean the the original version by my   : There was a a   : no computation outside. That was just it theoretical approach.   : I wanted to have a a computational representation of a knowledge base of a point of view.   : So to do that I use the vectorization. So now the type of authorization that I use that may not be the best way to go. But is it still a a a good base for   : this kind of thing. So it's what to back create in 2,013 by some Google people. Interesting that pretty much no one of them is still on Google and   : what to back is creating a. But yeah, it's creating back to us out of each one of the words or engram. So in the text.   : The way it is done is based on the conditional probability of one word, the upating because of the other.   : So that's the and that assumption is that if they appear together they are related.   : or a close one to the other, they are related   : from the conditional probability. You can reduce the the dimension on the matrix. On a a given number of components. Generally, we use 200 or 300,   : and at the very end, at the end of the process. So you have a each   : water or engram that will become a a sequence of numbers, meaning a vector and that's the name.   : So at the very end that you do for all the words. So and you basically have a a a matrix that is a representation of the text.   : So If you have a large data set, then you can.   : You can. I mean a data set of texts. Then you can victorize the the text.   : And this data set, this large matrix will be the computational representation of the knowledge base.   : So that would be the point of view. Then you have the document that you want to analyze, and what that your points of interest, because I mean with the same knowledge, you can do multiple things. So you want to have a keywords defining what you are looking for.   : Keywords can be. A single words can be and grams. but you may also want to have a a little bit more. You want to have a a scenario. You want to have misspellings, and you may want to have a wait for the different keywords.   : So what you do is basically you start the victorizing this corpus. So this is a set of   : text that you collected creating the matrix. Then you have the text that you want to evaluate, and you have the list of keywords.   : You basically take one   : one word at the time from the text you want to evaluate, and you compare the text. I mean the the word in the text with the each one of the benchmark.   : And you do that looking up in the madrics. What is the vector representing those 2 words? And you calculate the the singularity   : between those 2 words By measuring the distance between the 2 back to us.   : We use the initially, the cosine similarity other for so of a similarities it could be analyzed, can can can be used.   : So you do for each one all the words here with the each one on the benchmark here, and you will have for each benchmark the similarity.   : or for each document, the similarity for each one of the keywords.   : Then you normalize. You would use the weights for each one on the keywords. I mentioned that the keywords have   : a waiter, because not all the keywords are at the same level, all relevance. And then you have a overall, a number that is going to be all   : I mean. The similarity between the document and let me go back for a second. The similarity between the document. the benchmarks, based on the point of view that is represented by the computational version of the knowledge base.   : You change the knowledge base. Yeah.   : you will have a different backwards, and the result can be different.   : So this approach works well, because it's a a knowledge dependent that, replacing the knowledge, you have something different.   : So there are 2   : main variables, the benchmark, meaning what you are looking for, and the knowledge base, the core post that is, Who are you?   : So if you change one of the 2 is the same. You looking for something else is another new looking for the same thing on the previous one. The same documents will have different results.   : and that's why we applied this approach to different cases, so that they will   and just go through with you.   : So then, obviously, there are a a bunch of things that are making a deal a process more complex.   : So you want to   : do they programming in the proper way.   : and use the same way of doing the programming for the 3 components from the for the room, meaning the knowledge base for the benchmarks and for the documents that you are analyzing.   : We wrote a paper on how to do this chunking this programming in the proper way.   Generally speaking, you are   : required the to remove the the software so. But if you remove the software so when you are analyzing a a keywords at Stevens, you remove all of that. Then you don't have this call of business.   : so you may want to delay the removal of so forth. So after you do the first round, the all the the and grand generation, and then eventually   : remove the so forth, so that they are either at the beginning or at the end of each one of the engram, so that you created. then the generation of the room was create was a requiring quite a lot of time, because it it's. It's it's a long process. When you have a a a large data set.   : it's pretty much the same problem that Chop G G Gpt is having, I mean creating the model.   : What to back as a form of a   : victorization that is a easier and less accurate than Gpt. Gpt. Is based on transformers, and the number of he delay as a goes from one in a world to back   : to many. We don't know how many and the different GPS for those GPS each time you have a more on those either layers. So in the neural network, analyzing the text. The more resources the more time is required   : in our case on our computers was still a matter of a day, a day and a half so was still not substantial.   We use that for different approaches. We will go back to that.   : So that's basically the how we process the the information. So we had. We. We collected the data from news, but in some papers   : for patents and papers was relatively easy for news. We initially told that was a, a, a, a, a, a convenient way to do it with the we failed because the roller I couldn't really get match after a certain number   : of downloading, because that the sites are blocking after   : a certain number of queries, and we ended up paying for a service.   : So then we we added the level of security before entering the actual system. We 2 could then all the those news but and some papers that we are.   : Jason files into a Mongodb database, and then we use the those entries into ways, one to   : in the longer term creating, updating the room, meaning getting more knowledge of how things are going, and it's more like a smaller match that would be analyzed.   : Then you have the pre-processing in both cases if   : erez agmoni, I mean every few weeks we regenerate the room, the embeddings, and for the analysis we I mean, in both the cases we placed everything in international, a database one.   : the 2 systems. So the technology monitoring that is, panela are seeing the only what is a in the rational database. So meaning they don't know what is happening before that.   : And   : I mean that each one, the the technology monitor and the use the room theory   : for extracting the the metrics.   : So that was a basically, what was there? The the system is not running any more. I will not check if I still have a some snapshots or the dashboard   : based on that. We created the several other systems in the different projects that we did   : Later on   : this project was again for the the Department of Defense, and was for analyzing   : contracts. So   : one department of the Department of Defense is a Dau, and that is a defense acquisition at University. They have 2 missions, one that is, the University teaching Dod and people   : within the deal. The universe elements is is more training than education, but they train people on a given topics. So we are currently working with them to redefine the the   : training in a AI machine learning and data analytics.   : So   the other portion is the acquisition.   : the opposition. It's huge. So everything that is a quiet. By the Dod we go through them.   : It could be a   so.   : a new computer. It can be a so based on the on the type   : of request that they have the different types of contract that will protect them more against issues that they may have down the road with the provider.   : There are, I don't remember, a 1015 different types of contracts, but the the people in charge for assigning a a project to contract our contracting offices. They are   : that there is a high number of contracting offices that. and they tend to use the same types or contracts because they are more familiar with them.   : So the idea was to use to create a system, replicating somehow the knowledge of the contracting officers, and use that to evaluate the what in theory it could be the most appropriate form of contract.   : So that was the overall idea. So let me keep most of that.   : So we created the a a room for that particular domain. Initially, we had about the 100 key words defining   the benchmark. So this portion here.   : and   : let me go in presentation mode.   : and initially we had a about 200 documents creating the knowledge base.   : Again, what's our   : single words and   : good?   : We didn't have a weights initially, and it said that the   way to calculate the similarity was between the vector so what's the cosine? A similarity?   : The system didn't work well, so was not really able to   : to create a a a separation between the different documents.   : When you have a a knowledge base, that is mole, then you have a a several of the words, so that may not be a present.   : The   : in the knowledge, base works from the documents of the benchmark that may not be in the knowledge base. And at that point you don't have any input in terms of similarity.   : And then the lack of keyword, all that weight, so on the keyword. So it was another element.   : So we expanded the the number of key words working with the the the sponsor. So from 100 to 400 and and 64.   : Then I ask them, when you hire someone for the position or contracting officer, what are the requirements you have? Do they need to have a an equivalent on an Mba. Do they need to have a   : a low degree?   : So we we ended up with the some of the academic background or educational background they should have. And we use the some of the textbooks from those areas. So   : we went from 200 documents to 537 documents, and and from about 30,000 unique words, or and grams to almost   : 120,000 unique words. And then we added the the the weights, and then we changed the the way we calculate the distance between worse with the man approach. It is called the what mover up.   : That   is not   : an original one there. There is a final library for that.   : So   : that's basically a comparison between the the previous approach and the new approach. Where the difference is, is the number of documents and the number of key words.   : This is to say that   : this approach is data driven. So you need to have enough data to make it work. So that's another a a representation. So is a a principal component analysis. So the   : I mean, we have 300, the components. This is a down to 2 to 2, meaning is it to the vision of a a a 300? The the dimensional world?   : So it's. It's not exactly a good representation, but it gives you give you a an idea. So you have the different form of contracts for the the   : documents   that the   : projects that we are analyzing with the pre-use version that there was not much of a separation with the second version, with the larger data set, both in terms of knowledge base, and in terms of   : of a a benchmark, you have definitely a better. So the output is something like that. You have the name of the document, the different types of contract.   : and how close they are, how close each document is to each one   : of the types of contractor, and then that you have as a last column. Now the value I mean the the formal contract with the highest value   : contracting officer can eventually consider the first and the second, I mean in the first line there is   : not much to say, but in the other line like in this one you have a 2529 and 25, and change meaning. Yes, the the largest value is the 29.   : But you may want to consider the other 2.   : So that was a the project on analyzing contracts, one more example. That is a more recent.   : so that the project is for Siemens financial services.   : and we work the initially within a an international competition that they created. The international competition was with the 1,400 participants   : across the world on a 7 tracks. One of the tracks is a funding, a sustainability, and the idea was to develop projects that could help Siemens financial services   : to fund the projects within a sustainability. We won the track with the project that we named Green Connector.   : and the idea of the project is to create a a tool that can help Siemens financial services   : Extract from the web   : projects potentially in need to be a funded to send to Siemens for 200.   I mean proposing to be funded by that.   : So that's basically a is a matchmaker is in theory similar to any search engine, with the main difference that in this case it's a based on what Siemens financial services people would do   : so. The architecture is pretty much like this one. So you have a benchmarks defining the points of interest. You have a a   : collecting documents, then you have the   : projects that are evaluated, based Same room theory approach, and you have a at the certain point, at least, of projects. And you will have some visualization.   : So the idea it's basically   : being based on knowledge. So the a knowledge is really the the the core of the system, and the knowledge is a represented in 2 ways. One is in the room. There is a knowledge of the   : operators in the Zeeman's financial services, and the second on the benchmark, representing the elements that they are for you.   : One of the issues we had was that when you have a large document, how you evaluate, what are the elements that are more interesting, because the same concept can be scattered in different places. And then you have a a a large document that can be   : interesting in part. What are the parts of that of interest.   : So we I developed a way to chunk a large document   : in a parts so that are semantically   : and independent, or less related, and then reassemble the document into digital paragraphs that are aggregated, based on being semantically similar.   : and we use 2 different and and then you evaluate each paragraph using the same, a room theory approach   : So in that the first stage there is a smart paragraphing. The victorization is at the document level. Then once you move to the evaluation of interest, then you use the the other traditional approach on the wrong theory.   : Those are the paragraphs, a representation of the paragraphs. This is a a a representation of all the benchmarks. So you have a a of benchmarks. You have a   : again, misspellings things like that.   : and then you have a wait. We didn't use the time, but   : result is basically the documents will be placed in in in a space representing the similarity.   : So you have a. At the paragraph level you have a an index of similarity.   : You have, I mean, a a a point of reference to go back to to the both our actual position of the paragraph in the document.   : and then and now that   : result.   : we can highlight in yellow the original Pdf: so this is giving us a way to send back to the user and highlighted version of the representation.   : Then we developed a a webc. That so far is a a. A. D one, so it's not doing much, it working on keywords. We provide the keywords, and it's getting all the results.   : The next stage would be to add the a little bit more of a intelligence in that. The   : So those are 2 examples, all the applications.   : I want to go now into something that is completely different. Few few years ago we developed the we. We wrote a paper with the   : in collaboration with Unicef on the   : impact, the of the lockdown to domestic violence and children abuse. We wanted to create a sort of   : awareness on the problem through the paper   : having a unicef a as a partner was great, because we had the   : expert in the domain, and we also had the the possibility to reach a a wider audience.   : So we use the social media to detect the   : to measure somehow the impact.   : and   : we use the red. It and twitter for different cases, and I will show you how. So those are 2 are the 2 units of   : great working with them and the for us. so myself.   : that you may know from other courses and Fernanda that we are 2 of our Phd students. Now both are   : senior data scientists in their companies, and Polya is working in one of the lab, just the financial companies in us. But now that is working in a larger consulting company.   : so let me keep all of that.   : So again, the goal was to for use on a children exposure to fire, to family violence, cyber bowling.   : and we focus on us, but also other countries. So South Africa, Indonesia, we have data for Indonesia and Brazil. Fernanda is from Brazil so whole, or was kind of easier.   : There are many issues, as you can imagine.   : So we collected the tweets from 15 different countries over a 2 different timeframes. It's a completely different anymore. You have a more domain, specific   : messages, meaning it. It. It's kind of a easier to work with ready data compared to Twitter, and that can be all over the place.   : The the first application was analyzing Twitter. so Twitter has a some J. Location, so it's not 100% reliable, but as some   : your location capabilities.   : So we collected the the the data, filtering the data for a abusive or a hateful language map by community it can be geographical or not, and then analyzing and interpret the results.   : So that's us. So you have a   : a   yeah.   : What is the before and after we created a software in index before and after in terms of hate speech.   : So some of the   : either our States or districts had a really large increase.   : So it visually. That's what we have. So 1:   it's a tweets per 100,000 population before 1:   : and after the lockdown 1:   : number of tweets for a different countries, abusive, non abusive. 1:   : 8 speech. So so some countries really standing, then the the the Scandinavian people are always good, as we know, but the rest of us. Not so much. 1:   : Then we 1:   : we classified somehow those speeches. So, using either a 1:   : a more strict index for hate, speech, and a less a 3 to index, and then you see those that are above the the are below. 1:   : So that was a on analyzing tweets. All right it again that it. It's kind of be easier in a sense. So, because we went into a subreddit that was a more for use on on the on the topic 1:   : 18 months in interval. 1:   : Majority of the red. It's we're from the Us. So that's a limitation. 1:   : So that's an example on the the subreddit the abuser 1:   : and collected the data, doing a a a temporal analysis, extracting topics and try to classify them. So number of active users in the the subreddit related sub that it's a related to our views, and you see what is the growth. 1:   : then? The the number, the ratio of of number of daily posts before and after 1:   : A. 1:   : So you see that there is a a growth of some of the elements that that seems to drive toward 1:   abusive conditions. 1:   : We created using a lda. That is what it is. Clusters somehow, and we try to make sense out of it and try to label them in a way or or in the other. 1:   : we identified the 5 different 1:   : intimate, partnered abuse, a physical abuse, a sexual child, the practitioner support that is, the the good one. 1:   : So, and each one based on that was a with a certain percentage. And those are the keywords that are more related to each one on those topics. 1:   : And that's basically what we had. So if you look at the the, the, the 1:   : the time, evolution of the elements, I mean overall, that is a growth, each one of them 1:   : so. Unfortunately, one that grew. The list was the practitioner, support all the rest. 1:   : So 33% more exposure to abusive language. 37 us states so a large and increase in tweets containing a abusive language that's an overall key results 1:   : 94 more child abuse what I read it during the lockdown. 88 more intimate part I am used by work on red it during the lockdown. 1:   : So that's basically a another way to use data to analyze data. 1:   : the the paper at the this the last one with unicef and quite a lot of citations. 1:   Quite a lot of views. 1:   : I mean. It's 1:   : an important topic, and it is definitely a driving quite a lot of attention. 1:   : All right. I can go on and on, but I not totally sure that you would 1:   benefit the 1:   : Erez Agmoni. That much giving you other examples, and just to give you an idea on what I'm working on right now 150 1:   : in kind of a following the craziness on a chat, Gpt. And then doing the 2 projects: one that is, on creating a an Ssc. Gpt that could be based. 1:   most likely not on Gpt itself, but will be based on on the same, the transform of the 1:   : and I. I will use the transcripts from our classes, and in particular for a 6, 24 as a the knowledge base. And then I will use the the question answers that you 1:   and the 1:   : through emails to myself or the ta to fine tune the system 1:   : initially. I will be sort of with you, Dora, for a 6, 24, and then, if the project will go well, then we will expand the to other courses. 1:   : The second would be to leverage on the language, generation, capabilities of those large language models 1:   : to have a sort of visualization so. But instead of a visual, you will have a a narrative. But you will have a a language base reading of the results, so it will be a sort of a visualization, but not 1:   : and visual, but compensation. So those are 2 of the projects i'm working on. 1:   : I'm. Continuing, working with zoom and the system that I show to you. 1:   : Up to about a month ago we were working on a creating a company a out of that. The idea was the Siemens idea was okay. We developed the proof of concept in the hack upon 1:   : Then we are now developing a prototype. The prototype will be ready by the next by the end of the year. But 1:   : we would like to use the system at that point 1:   : as a a university. We cannot provide services, so we can develop prototypes, proof of concepts, but not systems. As a service. 1:   : I created companies in my in my previous life. and I want to use that experience to create a company that would serve as a first client to cens up down the road. The other companies, I mean no company can survive with only one client, so we will need to diversify. 1:   : Siemens wanted to invest in the company, but then our 1:   : our point of contact left the company. So now we are remapping the entire involvement that they have, and I 1:   for the time being, I I put on all the the idea, all creating the company. 1:   : but the 1:   : 2 levels up the person who left the Siemens there was an agreement. So we just need to recreate your chain. 1:   : My idea with the company is basically to give students the opportunity to work in the company develop skills 1:   eventually, if the company would be up to that, and being hired by by the company. 1:   : So it it's like adding a a level to the education that we are providing at Stevens. 1:   : I presented the idea to the pros. The browser was happy. But now we are on all the because of the changes at Ciem. But i'm still a pretty confident that this will happen. 1:   : So that's basically what i'm working on now. 1:   : All right. So let's move on now, and let's go now to 1:   : the in class assignment. So is a 7, 43. Let me share this screen. Again. 1:   : Let me go here. So that's the in class assignment. So you will read the some pre-processed tweets, so they will look like 2 1:   : like this. so you will have a a list of send us a time stamp. So the time sample is on epoch time. 1:   : That is the number of seconds from the moment the unix was created. So I don't know why they are using this way. 1:   : but 1:   : in most of the geek it think so. That's the way to do the time sample. 1:   : and then you have the actual text. So there are 3 columns, center time, sample and text. 1:   : So there are a 1:   : 20,000 Rosa standard times 10 texts. 1:   : which, being collected during a Presidential debate, using Trump as a keyword. 1:   : That means most likely, the the comments are related to the to trial. 1:   : The output that is required is, you want to print the the 5 most active Send us that. Then most we we did the tweets, the 5, most side of the screen names. 1:   : and the 10 most popular stark words so retweets can be determined by checking if they start with the Rt. 1:   : And that our 1:   : a few of them, you can probably spot it somewhere. Then here we go like. In this case you have a rt. 1:   : then 1:   : the the screen name, we'd be determined by looking at the at the symbol. like in this case they are. This subject here is citing 2 people with those screen names 1:   : can be determined. If they start with the the 1:   : there should be somewhere or something. 1:   : Okay, some we have that there should be, an 1:   : if you use Pandas setting, coding parameter to encoding. You will, like unicode escape. When you read the the file. Keep in mind that the when you work with weeds things are always massive. 1:   : because because people is sending tweets from all over the places. 1:   : each place is come e each country as a a given character set. So we already mentioned that when you have a I don't know from Germany you have a so with the double dots on it, the 1:   : in Italian you have the apostrophe that is using a different way 1:   : in Spanish. You have this idea under the sea sometimes, and so on. 1:   : So when you read the those messages with an encoder that is based on 1:   : English American language. Those characters will not be recognized as such 1:   : meaning. 1:   : There is the risk that those characters will be considered in a different way. The different way it can be 1:   : end of file and of line, end of file, that meaning it will stop at the point and the entire processing, and the line that means that we'll skip to to the next line. So that's why is important that you set the the encoding in the proper way. 1:   : All right, so let me publish 1:   : the material. 1:   : So my the material is published, and let me. I create the breakout rooms. 1:   : So I created the 7 breakout rooms, 3, 4 participants per room. 1:   : creating a opening. You have a oh, 20 min, 20, and change minutes to work on it. Then we will come back, and 1:   : I will talk about the the solution. You will present your results if you want, and then I will introduce the the next assignment, and that's going to be the end of the class. 1:   : So the rooms are open now. See you in about 20 min. 1:   : So i'm, resuming the recording. 1:   : How was it 1:   : anyone want to share the experience? 1:   : All right. Okay. So let me 1:   : share the screen and let me 1:   : good to. 1:   : Okay. So 1:   : E. C. 1:   : You read the file. the the the so forth. 1:   : You load the the words into a list. then you find and print the top 5 standards. so you do not really need to create another data structure, but that's the easiest way to to do it. 1:   : And then most common 1:   : retweets 1:   : create a another data structure 1:   : most call mona. 1:   : and 1:   : this case it's a little bit more complex, because i'm i'm looking for a specific character. There could be different ways without the loop 1:   just working on 1:   : the capabilities of palm do but I mean that that's the the easiest way to do it. 1:   : So in looping into the text. And then, if I find the and 1:   at 1:   : then 1:   : hi. 1:   : and not checking if there is a 1:   : a column. 1:   : if there is a i'm taking the character that is next to it 1:   : Same thing for the app. 1:   Yet 1:   : i'm not the phone sign. 1:   : And then and in the words 1:   : calculate the the top 1:   : screen names, hashtag words printing it. 1:   : I mean it was relatively straightforward 1:   : to send that top 3 weeds 1:   : most cited names. 1:   There was a glitch here. 1:   : most popular hashtags. 1:   : most common words. So 1:   : again. 1:   : There is a glitch in the text, so I will not 1:   : check it. And 1:   : the updated version. 1:   : All right. Okay. So 1:   : the assignment for next week. So that's a 1:   : the last assignment. Apart from the final 1:   : meaning it does to be the most complex. So 1:   : in the past I use the as a last assignment, an assignment that was a on a a combination of 1:   : Oh, the webcalling and text analysis on a a multiple page website. 1:   : It was a a complex, I realized. do something with the different type of complexity. 1:   : So we are engineering management. There is a management in the name. Whatever we do has to be somehow in a tweets that is a related to management. 1:   : So one of the things that we do in the universities is managing courses. So in particular, at events, we use a work day. 1:   : what they can generate 1:   : ere 1:   : I mean, extract only those that are related to a specific school, and we need to add some other, you know, capabilities. So I wrote a a a python script to cleaning and to add the features like level up program and and suffix. 1:   : So that's basically the the the output of of my script. So you have this pretty large data set from work day. Then there is my script doing the cleaning, and then you will end up with something like that. 1:   : I sanitize the the data set with the removing the names. So on the tractors and replacing it with numbers 1:   : just to protect the privacies. amend that 1:   the privacy of the instructors. 1:   : But anyway, you have a academic unit like engineering management program 1:   : systems and enterprises in a broad sense, and so on software engineering. Then you have the sections of those so open or close close. The means that there are no more seats of all Ebola open, there are still a seats of Alabama. 1:   : Then this is sort of a duplication. You have a the 1:   : full name of the courts, and only the the first part of the name, the title. 1:   : the meeting. the building. the campus can be online. 1:   : the name of instructor credit ours number of students abroad 1:   : the capacity of the section, so the enrollment is the actual number of students in the class. The cap is the 1:   the size of the classroom. 1:   : so in this case there are 1:   : 2 more seats available if we go like. In this case this section is closed. So there are a 130 students. We did. They already got capacity on the classroom or 1 24, and that's why it is closed. 1:   : Then you have what is the academic period that they are all for on spring 2,022 one year ago. 1:   : It's update and data the type of it's fraction of format. Most of them are like shirts. 1:   : the delivery, if in person or online. 1:   : then 1:   : pretty like we see it somehow. 1:   : Then you have 1:   : academic level. 1:   : graded them not. 1:   : and then you have a additional value, so that I calculated they are 1:   labela program and suffix. 1:   : So that's basically what you have based on that, what you want to do. He is a reading the data set with all those Rosa. 1:   : You want to print 1:   : the 5 courses with the highest number of students from the enrollment counter. The 5 instructors with the highest number of students. Again, it's the role, my count. 1:   : Compare the total number of students for undergraduate graduate and a corp from level. We are comparing. That means to calculate the values and describe the results in the narrative part of the assignment. 1:   : compared. The number of courses that run at full capacity marked as close with those that not 1:   : comparing same thing. 1:   : then, that create a pie chart with the distribution of students per program. 1:   : create a pie chart with the distribution of students. So per type of delivery. So the first one is program, meaning engineering, management, system, engineering, software engineering, and so on. 1:   : The here is for delivery mode, meaning in person or online. Do other analysis if needed, you do not need to do it. But if you think about other analysis that would be great. 1:   : You read the file, perform the analysis, and you will submit a  with interpretation. 1:   : All the usual rules would say, meaning cheating, I would be 1:   : considered again after 4. 1:   : Let's say a ha! After the fourth cheating, and some of you are on 3, 1:   : very few, but some the fourth will trigger the reporting to the owner board. 1:   : Those who are not in that condition will be penalized in terms of a a point reduction. 1:   : So that's Basically, it. I will 1:   : posts the description and the assignment. And that's basically it. 1:   : Just to remind you, Don't, forget that on the final you have 1:   : let me share again the screen 1:   : on the final. You have 4 options. Analyze people percept. I'm sorry they go from the beginning, Analyze people, migration data. 1:   : analyze the research projects 1:   : the COVID-19 case, and then the fourth one. I analyze people perception of a AI. So you can pick any one of those follow this fraction and do your final in a very special cases I would be okay. 1:   : giving you the opportunity to do the final project or something different. 1:   : But you need to convince me, as I will say in the very beginning of the class, that there are a good reasons for that. 1:   It's 1:   : something that is part of your work. It's something that you want to do to write in your resume, because you are going to apply for certain positions, and you want to have something that can resonate better. I mean, I need to have a good story for that. 1:   : And please Don't use data sets like those on cargo that is being used a 1 million times, and I will never be sure that the the the code and the analysis that you are doing it's really original. 1:   : So I will check if what you are asking is on cargo. If he's on cargo, it's quite likely that they will deny the project. 1:   : So that's Basically, it. 1:   : Next week we'd be 1:   : office time. So the only thing that I would do would be to present the the solution for the current assignment that there will be no 1:   : extension for that. no extension for exercise 9. So unless something that 1:   very relevant is happening, and I hope that that it would be not the case for any one of you. 1:   : but apart from that, the no extension will be granted. So take the time you need. The the the assignment is not particularly easy. 1:   : and for a reason, so it's the last assignment as to be, I mean, we said medium time, so the the course is very gradual. So but now we are at the top. Meaning is, it has to be the most complex of them all. 1:   : anyway. So does the end. I'm stopping the recording.
 

ft, STEVENS

lw INSTITUTE of TECHNOLOGY
iy
4

Machine Learning
using Python



clipizzi@stevens.edu

SSE

 

a

What is ML lw

Most of the methods and techniques used in Data Mining are used in Machine
Learning

Some of the "definitions” associated to Machine learning include
Automating automation
Getting computers to program themselves

From TechTarget - Special Report: Artificial intelligence apps come of age:

Machine learning provides computers with the ability to learn without being
explicitly programmed. Machine learning focuses on the development of
computer programs that can change when exposed fo new data. The
process of machine learning is similar to that of data mining

Data mining Is extracting knowledge from data, Machine learning is a broader
discipline, focused on creating systems able to show a data driven “intelligent”
behavior

STEVENS INSTITUTE of TECHNOLOGY | 2

Al & ML and “learning” we

° AI/ML systems need data to perform behaviors/take actions

°- Based on the type of data available, they can perform a
category of task or an other

¢ Predictive/Classification systems, like humans, need
information about past behaviors on the same problem

° Quality and quantity of the information about past
behaviors determine the quality of the
oprediction/classification, where “quality” is the ability to
oredict/classify situations in a comparable or better way as
a target sample of humans

- In all the cases when no data about the past is available,
the “learning” is either based on reaching a target or on
creating criteria for clustering data

STEVENS INSTITUTE of TECHNOLOGY | 3

 

 

Ss
Supervised, Unsupervised and Reinforcement Learning S

¢ Supervised learning
— Supervision: The training data (observations, measurements, etc.) are
accompanied by labels indicating the class of the observations

— New data is classified based on the training set

¢ Unsupervised learning
— The class labels of training data is unknown
— Given a set of data, the task is to establish the existence of classes or
clusters in the data

° Reinforcement learning
— It is based on the goal of maximizing a cumulative reward
— Inasense, it is a a form of supervised learning, with the “supervision”
that is created by “randomly” generated behaviors with results
measured by the values of the reward. The values of the reward and
the values of the variables leading to that result create the
“supervision”

_
STEVENS INSTITUTE of TECHNOLOGY | 4

Supervised learning process: two steps S

° We split the dataset into 2

 subsets: one to train the
model, one to test it

Step 1: We apply the

selected algorithms to the

training data to get the model

 Step 2: We test the model on
the testing subset and
measure the accuracy



STEVENS INSTITUTE of TECHNOLOGY | 5

 

Fundamental assumption of learning le

Assumption: The distribution of training examples is identical to the
distribution of test examples (including future unseen examples)

° In practice, this assumption is often violated to certain degree
¢ Strong violations will clearly result in poor classification accuracy

° To achieve good accuracy on the test data, training examples
must be sufficiently reoresentative of the test data

STEVENS INSTITUTE of TECHNOLOGY | 4

[I
Reinforcement Learning

 

¢ Learning from interaction
¢ Goal-oriented learning

¢ Learning about, from, and while interacting with an external
environment

¢ Learning what to do - how to map situations to actions - such as
to maximize a nuMerical reward signal

STEVENS INSTITUTE of TECHNOLOGY | 7

[~~ Sl

Reinforcement vs Supervised Learning S



 

STEVENS INSTITUTE of TECHNOLOGY |

Reinforcement Learning - Key Features im

 

e Learner is not told which actions to take
° Trial-and-Error search

° Possibility of delayed reward (sacrifice short-term gains for
greater long-term gains}

¢ The need to explore and exploit

¢ Considers the whole problem of a goal-directed agent
interacting with an uncertain environment

STEVENS INSTITUTE of TECHNOLOGY | 9

Reinforcement Learning — Example S



 

Reinforcement Learning -— Example jw

¢ Consider ag complex graph, and we want to
find the shortest path from a node §; to a goal
node G S,

° Traversing an edge will cost you “length edge”
dollars

° The value function encodes the total
remaining distance to the goal node trom any G
node s, .e. V(s) = “1 / distance” to goal from s

° If you know V(s), the problem Is trivial. You
simply choose the node that has highest V(s)

STEVENS INSTITUTE of TECHNOLOGY | 11



Reinforcement Learning — The task S

¢ To learn an optimal policy that maps states of the world fo actions
of the agent. Examples: if this patch of room Is dirty, | clean It. If my
battery is empty, | recharge It



° The agent tries fo optimize the total future discounted reward:


oO

/

STEVENS INSTITUTE of TECHNOLOGY |

.
Reinforcement Learning - Value Functions \&

e The Value Function estimates how good It is for the agent to be in
a given state

e The "how good' is defined in terms of future rewards that can be
expected, in terms of expected return

e The rewards the agent can expect to receive in the future depend
on what actions It will take. Accordingly, value functions are
defined with respect to particular policies

e A Value Function V Is the state-value function for policy p

e We also define a function Q, that is the action-value function for
policy p

e The function Q is the value of taking action in state s under a
policy p

STEVENS INSTITUTE of TECHNOLOGY | 13

.
Reinforcement Learning - Value Functions \&

¢ Two basic approaches to compute the optimal action-value
function are value iteration and policy iteration. Both algorithms
compute a sequence of functions Qk that converges to optimal
action-value function Q*

e Computing these functions involves computing expectations over
the whole state-space. In reinforcement learning methods,
expectations are approximated by averaging over samples and
using function approximation techniques

STEVENS INSTITUTE of TECHNOLOGY | 14

Genetic Algorithms S

¢ Genetic algorithms are inspired by natural evolution. In the natural
world, organisms that are poorly suited for an environment die off,
while those well-suited for If prosper

¢ Each individual is a bit-string that encodes its characteristics. Each
element of the string is called a gene

° Genetic algorithms search the space of individuals for good
candidates

¢ The "goodness" of an individual is measured by some fitness
function. Search takes place in parallel, with many individuals in
each generation

STEVENS INSTITUTE of TECHNOLOGY | 15

 

 

Genetic Algorithms lw

° The algorithm consists of looping through generations. In each
generation, a subset of the population is selected to reproduce;
usually this is dg random selection in which the probability of choice
Is Proportional to fitness

¢ Reproduction occurs by randomly pairing all of the individuals in
the selection pool, and then generating two new individuals by
performing crossover, in which the initial n bits (where n is random)
of the parents are exchanged. There Is a small chance that one of
the genes in the resulting individuals will mutate to a new value

STEVENS INSTITUTE of TECHNOLOGY | 14

 

Deep Learning lw

¢ Deep Learning Is a machine learning subfield of learning representations
of data. Exceptional effective at learning patterns

¢ Deep learning algorithms attempt to learn (multiple levels of)
representation by using a hierarchy of multiple layers

« When the system Is provided with a very large amount of information, It
begins to “understand” it and respond in useful ways

Machine Learning


Input Feature extraction + Classification Output

 

 

 

STEVENS INSTITUTE of TECHNOLOGY | 17

Deep Learning lw

- ‘Deep Learning’ means using ad neural network with several layers
of nodes between input and output

¢ The series of layers between input & output do feature
identification and processing in a series of stages, just as our brains
seem to

¢ Multilayer ANN have been used for while. The “new” ones use
algorithms for training many-later networks that were not in place
before, primarily due to HW limitations

¢ Those algorithms are based on weights adjustments using clustering
methods

STEVENS INSTITUTE of TECHNOLOGY | jg

 

 

Training a Deep Network lw

- Weights are learned layer by
layer via unsupervised learning

° Final layer is learned as a
supervised neural network

° All weights are fine-tuned using
supervised back propagation

 

Hinton and Salakhutdinov, Science, 2006

 

STEVENS INSTITUTE of TECHNOLOGY | 19

Data Preparation we

- Data in the real world is dirty

— incomplete: lacking attribute values, lacking certain
attributes of interest, or containing only aggregate
data

— noisy: containing errors or outliers

— inconsistent: containing discrepancies in codes or
names

¢ No quality data => no quality mining results

— Quality decisions must be based on quality data

— Data warehouse needs consistent integration of
quality data

— Assessment of quality reflects on confidence in results

_
STEVENS INSTITUTE of TECHNOLOGY | 99

 

 

 

Forms of data preprocessing w@



 

STEVENS INSTITUTE of TECHNOLOGY | 94

Clustering &

¢ Clustering is a technique for finding similarity groups in data,
called clusters
— |f groups data instances that are similar to (near) each other
in one cluster and data instances that are very different (far
away) from each other into different clusters
° Clustering is an unsupervised learning task as no class values
denoting an a priori grouping of the data instances are given

¢ Due to historical reasons, clustering is offen considered
synonymous with unsupervised learning

STEVENS INSTITUTE of TECHNOLOGY | 55

 

 

k-Means Algorithm Y

    

STEVENS INSTITUTE of TECHNOLOGY | 53

 

_ cfs
Decision Tree we

° It represents a human like thinking pattern. We take different
attributes info consideration one by one and arrive at a
conclusion for many problems

- A decision tree reaches a conclusion by performing a series of
tests

¢ Each internal node in the tree corresponds to a test of the value
of an attribute

¢ The branches from the nodes represent possible values of the
attributes

¢ Each leat node represents the final value to be returned by the
function

¢ Decision trees are popular for pattern recognition because the
models they produce are easier to understand

_
STEVENS INSTITUTE of TECHNOLOGY | 54

af
Decision Tree - Example eS


   

STEVENS INSTITUTE of TECHNOLOGY | 95

 


Impurity

Very impure group 
Less impure 
Minimum impurity
STEVENS INSTITUTE of TECHNOLOGY | 36

 

lw

Neural Networks

° An Artificial Neural Network (ANN) consists of a pool
of simple processing units which communicate by
sending signals to each other over a large number of
weighted connections

 

STEVENS INSTITUTE of TECHNOLOGY | 57

 

Elements of ANN x

 Processing element (PE)
• Network architecture
– Hidden layers
– Parallel processing
• Network information
processing
– Inputs
– Outputs
– Connection weights
– Summation function

STEVENS INSTITUTE of TECHNOLOGY | 59

 

Model Evaluation

¢ Evaluation metrics: How can we measure accuracy?

¢ Use validation test set of class-labeled tuples instead of training
set when assessing accuracy

¢ Methods for estimating a classifier’s accuracy:
— Holdout method, random subsampling
— Cross-validation
— Bootstrap

* Comparing classifiers:
— Confidence intervals
— Cost-benefit analysis and ROC Curves

STEVENS INSTITUTE of TECHNOLOGY | 99

Y

Classifier Evaluation Metrics: Confusion Matrix

Actual class\Predicted class yes no

True Positives (TP) | False Negatives (FN)
False Positives (FP) | True Negatives (TN)

 

Example of Confusion Matrix:

Actual class\Predicted | buy_computer = | buy_computer = Total
class yes no

buy_computer = yes | 6954 7000

buy_computer = no

 

STEVENS INSTITUTE of TECHNOLOGY | 39

 

Model Selection: ROC Curves ‘

ROC (Receiver Operating Characteristics)
curves: for visual comparison of
classification models
• Originated from signal detection theory
• Shows the trade-off between the true
positive rate and the false positive rate
• The area under the ROC curve is a
measure of the accuracy of the model
• Diagonal line: for every TP, equally likely to
encounter FP
• The closer to the diagonal line (i.e., the
closer the area is to 0.5), the less accurate
is the model

_
STEVENS INSTITUTE of TECHNOLOGY | 31

~)Sh—l3slCUri

a ft
Mining texts lw

¢ Mining text is one of the applications of “Machine Learning”/Al, being focused on
creating systems mimicking one of the most human characteristics: communicate via
“natural language”

¢« Python is one of the best tools to mine text. There are many Python libraries focused on
this topic, some of them will be addressed later on during this course

¢ One of the non traditional but increasingly popular methods to extract topics from text
is word2vec, that is a group of models that are used to produce word embeddings
(Mikolov, Chen, Corrado, & Dean, 2013a)

¢ Word embedding Is the collective name for a set of language modeling and feature
learning techniques in natural language processing where words or phrases from the
source are mapped to vectors of real numbers (Mikolov, Sutskever, Chen, Corrado, &
Dean, 2013b)

¢ tis based on Artificial Neural Network and it represent an application

STEVENS INSTITUTE of TECHNOLOGY | 35

Word2Vec lw

Published by Google in 2013
Python implementation in 2014 (gensim library)
Generates distributed vector representations of
words (“word to vec”) using a neural net
In those distributed vector representations of words
o each word is encoded as a vector of floats

© VE@Cgueen= (0.2, -0.3, .7, 0, ..., .3)

0 VE@Cwoman= (0.1, -0.2, .6, 0.1, ..., .2)

o length of the vectors = dimension of the word
reoresentation

key concept of word2vec: words with similar vectors
have a similar meaning (context)

STEVENS INSTITUTE of TECHNOLOGY | 33

 

——_ A
i
i in

Why this approach is relevant & how is structured lw

¢ Zellig Harris (1954):
— “oculist and eye-doctor ... occur in almost the same
environments”

— “If Aand B have almost identical environments we say that
they are synonyms.”

¢ Firth (1957):

— "You shall know a word by the company It keeps!”
¢ Intuition for algorithm:

— Two words are similar if they have similar word contexts
¢ The meaning of a word is a vector of numbers

— Vector models are also called “embeddings”

STEVENS INSTITUTE of TECHNOLOGY | 34

 

 

[Sls
nie

From words co-occurrence to embeddings lw

¢« Word co-occurrence measure how often a word occurs with
another, within a given number of words of separation

¢ Using co-occurrence we can create a word-word co-
occurrence matrix, where rows and columns are the unique
words in the source text

¢ Each word Is represented this way by a vector, that Is soarse and
with most of the values being zero

¢ The whole approach is based only on the actual words proximity

STEVENS INSTITUTE of TECHNOLOGY | 35


Tools in Data Science/ML


STEVENS INSTITUTE of TECHNOLOGY | 36

Using a framework we

How do we express machine learning models?


 

General purpose computation ———* Machine learning ——> Deep learning


STEVENS INSTITUTE of TECHNOLOGY | 37

 

 

Python Libraries for Data Science

Pandas: pandas

= adds data structures and tools designed to work
with table-like data

provides tools for data manipulation: reshaping,
merging, sorting, slicing, aggregation etc.
allows handling missing data

Scikit-Learn: lean

= provides machine learning algorithms: classification,
regression, clustering, model validation etc.

= built on NumPy, SciPy and maftplotlio

STEVENS INSTITUTE of TECHNOLOGY | 39

 


STEVENS INSTITUTE of TECHNOLOGY | 39

Hey, did anyone else get a message from the professor saying the class was ongoing? Please join. Yeah, I just got that. But I think he's in the wrong one. Where am I? Where are we? Joining me, I also just got that. But we all like the one for like today's date, I assume, right? Yeah, I tried to. And those aren't working either. Oh, fabulous. Great. So where is he? I don't know. He must have known. The code is the same every time. Right? Yeah, it should be the same. Meaning he must be in the wrong class. Is somebody going to message him back? Yeah, I just don't know. Okay, cool. Because I kind of hope he'd figure that all of us didn't just totally not show up. Yeah. Maybe we should copy and paste the meeting idea and just send it to him. Yeah. That's not a bad idea. Let's wait a third zoom. No, he said go to the left hand side and. Yeah. I've been joined in meetings the same way for the past three other classes. Yeah, just. Oh, he does have a cell phone. If anybody wants to call. I tried to tab the tab in canvas, and it just takes me right back. You. Yeah. And I tried to join one for like next week or whatever. It's all the same meeting, so. In the calendar link is the same. I just tried it. Yeah, I tried it all to. He must joined like the wrong class or something. It didn't respond to my message. I sent them to actually. Do we know how many people are supposed to be in this class? I would assume, like high teens. I think there's 11 if you go into canvas and check the people who are trying to join. Oh, right. So 34. But the trouble is, we go. Okay. So. I'm really sorry for the trouble. So I was explaining to Dean before. I'm in Washington, D.C., for a meeting with the D.O.D. I realized yesterday late evening that most of the content on campus didn't work well, so I had to replace the entire content and some of the links. Uh, it's kind of evident the not working fine, so my apologies. So apart from zoom, that is the link to the check your knowledge. That is not working. So again, my apologies. My train was 725 this morning and I couldn't stay up the night to fix it. So my apologies. Next week, uh, everything will be fixed, I promise, but. Again, apologies. So we were just starting on the other room and, uh, we were surprised that no one was there. So I send he left saying, Where are you? So it's nice to see your faces. Okay. So brief introduction to myself. My name is currently pizza, the full time faculty and students at the School of Systems and Enterprises. I'm also the program director for Engineering Management, Graduate Programmer and Systems Analytics, Director for the Center for Complex Systems and Enterprises. Uh, I teach data science, machine learning. An LP to about between 150 and 200 students each academic year. I manage research projects for primarily for the D.O.D., but not all for D.O.D. the same field. So. So a machine learning, natural language processing. Uh, before that, I joined Academy, uh, six years ago, pretty much before that. My academic background is in math. I am a master in math from the University of La Sapienza in Rome, Italy, then an executive MBA from Amba in Switzerland, and then age of 50 and change. I went back to Academy, got my Ph.D. in system engineering at Stevens, and then I started this sort of a second career. So that's basically my story. I was mentioning to Dean on the other room that when I joined Academy and this second career began, the goal was to work on social media and find ways to analyze the social media. I mean, it was at this point more than ten years ago and that social media was becoming as popular as it is today, but was not there yet. My idea was not to use not to code because I was coding medium years ago, coding with the quarter and COBOL language and that now I know that non-active even if several places that a C user I didn't want to code again and then I realized that if you want to work with data you really need to code. There is no other way in that. Working with data, it's really essential for pretty much everything we do. One of the issues that I have in several classes in that in 624, so 624, it's pretty much hundred and 20, roughly between hundred and 20 under the 50 students each year when I teach in particular to professionals. So one of the strongest reaction that they have, the negative strongest reaction that they have is I don't want to code. The code is not essential for me. I'm so sorry that coding is a required I mean, it's a core code. Sam 624 for the engineering management program and I wish I wouldn't have to. I mean that. I mean, on 100 and 2050, it's probably five or six people doing that. And I don't know. What is your opinion on that? And I will open the floor for your opinion. But quoting again, that is an essential part of working with data. Whatever we do today, somehow we need to deal with data or we should deal with data to have more leverage on what we are doing. Also, as an engineering manager, we are exposed to people coding in our company, in our division, in our team and that point knowing how to code, not just because you read the table of content or the book or coding, but because you really send the code that is really essential to better understand the problems that the coding people may have and eventually even the potential that coding can have for you and push eventually the coding team to do something more. So very briefly, let me ask you, is there anyone strongly against the coding? I mean, there is no judgment there. I'm not against it. I just don't know anything about it. Okay. Any other comment on that? I do it for a living. It's not my bread and butter, but it's, you know, it's okay. Okay. All right. So and then another question or a concern that they have from several students. So we're up five seats in the first group in the segment. The second group, probably it's a good 20%, even 30% of my students that I have zero experience, including I'm scared of the I think I will fail. I mean, even 624 is one of the most popular courses we offer at the School of Systems and Enterprises. So this semester I'm teaching two classes over 620 for this class. In this modality with the core approach is a small one. The other class that is online is about 60 people, and that's a normal size on my glasses. Most of the people have no experience, including some of them we are from. Completely different background. The example that they generally use a couple of years ago. A student of mine, she was working at CDO Montclair as a clerk at the city of Montclair, and she said, I have a bachelor in Arts. I know nothing about coding, but I'm afraid that I will fail. So we started the again very low with just installing, installing Python, copying code, adding little bits. By the end of the course, you will be able to write scripts that are a few hundred slides, so with a certain level of complexity. So that student ended up getting a job as data scientist in a technology company. So she left the city of Montclair to get a job initially as a developer in Python and then became a data scientist. So I mean, obviously everyone is different, but this is just to say that even if you have zero knowledge, it doesn't mean that you are at risk of failure in the codes. One thing that is very essential is don't fall behind. So the course is very gradual. Initially, assignments may be easy if you kind of skip overlook the first two or three assignments. Catching up, it's really complicated. So another issue that we have in this course is cheating. Speeding means using external sources or working with other students so that the most popular form of cheating that I experienced so far. Now we have a chat group and cheating will be completely different from now on. But sharing, doing the same assignment. It's something that. Happen? Probably. Each course large classes. I have a four or five that are cheating at least once when I have an students doing the same assignment. But let's assume that the assignment that what the hundred each one that we get a hundred divided by eight and there is no second chance. I mean that you did it, that you will get a fraction of the points. When you do more than once, you are losing quite a lot of points and there are chances that you will train the courts. So you definitely want to avoid that. If you cheat the cheat. Well, cheating well is complicated. So the most common form of cheating, it's kind of maybe arranging the statements, changing the names or the variables. I mean, that's a low key cheating and it's very easy to be detected. So you are paying for an education. You are not paying only for a degree. That's a chance to get an education or something that you may not know but may be useful in the future. So cheating, you're not doing a good job to yourself. Obviously, it's your choice and you can do whatever you want to do. But keep in mind that those are the rules. In theory, we should report the cheating to the ethical committee once the issue is at that level. There are chances that the student will be expelled by the university. If this is happening, then you will have issues getting another admission. You may have student loan that could be more difficult to repay. So we don't want to go there. But again, because that is something that is happening, not say frequently, but is happening to a degree, I mean, and to some extent on a significant portion of students in class. I want just to tell you to be sure that you will avoid it. All right. So let me start sharing this screen and let me go. Here. So again, we will start on Tuesday 630. My office is a Bible of his number of five or seven on the fifth floor. That is the floor when we are most of the SCC faculties are. We do have a DEA integrator. They are working on both the classes. I wouldn't have either a DEA integrator or a class that is as small as ours. But the combination of the two classes is more. It's about 70 students. We will use Python. So there are several versions of Python. One of the advantages of Python is the library is that you can integrate in your code. Not all the libraries are compatible with all the versions. So the most recent version may not be compatible with some of the packages that we will use. So up to 3.7, 3.8, eventually 3.9 is working. If you do, three point then may not work. We do have a virtual office out. It is better if you check with me sending me a message, an email, if you want to meet me during the office hours or at any other time. Um. Assignments do they eat? Generally speaking, I present the solution the following week. So if I give you an assignment for this week, I will present the solution. Next Tuesday, 630. Meaning, at that point, I cannot be sure that the submission. I mean, your solution will not use what I presented. So the deadline is 6 p.m. the Tuesday following the assignment. So by next Tuesday at 6 p.m., that will be the deadline for the assignment that you will start this week. You have a video with the introduction. You have a syllabus. So the syllabus is pretty straightforward with not saying that much. There is a formal structure with the outcomes. And I mean, those are just details. No deadlines on. So again that are labor assignments deadlines I mean 6 p.m. on the following week deadline. If you go. I mean, if you submit after that time, there would be some penalties. Then obviously there are exceptions. Life is life and the things can happen. If you know that, you will be late. There's not a big deal. But you need to let me know. And me and. And she knew that you would be late. And y you are going to have a late submission. I mention the. The issues with the cheating death, the distribution of content. Let's say that pretty much up to the midterm. You have a basic python and some general information on data exploration and a kind of introduction to software engineering, because, again, it's something that may be useful down the road. After that is more on the application. So if on the other side you already have experience in coding, then at that point, for the first part of the course, you will not be that much engaged because you already knew that. But after that you definitely would be more engaged with natural language processing or web mining and some other machine learning a little bit. And then I will present some of that projects there to give you an idea on how to use it, how to use Python to do things that may be relevant for what we do or what you could do. Course material. Back in time, I had the textbooks and then I realized that students do not really read books anymore. So the slides that they will present, the material that we are providing should be in. There are sort of textbooks that are available on canvas as PDF. Use them as a sort of a reference. There are quizzes. I'm never a big fan of quizzes, but I'm in there. Useful to fix some points, let's say. And. That's a general distribution of points. We do have I mean, we have students to participate to be in the synchronous classes. Uh. Midterm is an open book so you can use all the sources you want. The final is a project that you will do. You will have a few days and I will give you some options on I mean, that it will be centered on a data exploration. I will give you a dataset and the problem more than one and you will pick the one that is resonating the most with you on into this. Apart from that, there is not much to say. But we don't use Latin bloodaxe and I really don't like it. So word is doing a good job. There's the distribution of crates and that's again the cheating part. And that's basically. So we are in the live session. So does the pre work. So I hope you had the opportunity to go through the material again. I'm sorry if this link is broken. I will fix it. This one on a complete check your knowledge quiz. We are the two most relevant questions are are you familiar with the coding in general in a scale at 0 to 10 zero nothing then professional level. And the second question is, are you familiar with coding in Python saying that 0 to 10. Um. And that's it. So what we are going to do is basically just go through some of the rules of the game and then I will use some slides just as a base for the compensation and then I will post this lights online. Let me go back here for a moment. The philosophy of this type, of course, is again that at the School of Systems and Enterprises, we offer courses in different modalities. The different modalities are on campus. Obviously online we offer all the courses we have at the school, all systems and enterprises online. We also offer courses to corporate. So the corporate education is a good portion of our business in in India in a broader sense. And then we offer up the format of online that you are taking now. So this format, the online in theory will become the only formal online that we have. We partnered with a consulting company working in education and we devised the entire program. So within the School of Systems and Enterprises, we have only one programmer that is available in this modality and is engineering management. This modality has a pretty much the same content as the older version of online that we call the webcam. So it's not. But there are some differences. There are more videos, quizzes. Most of the material is posted upfront. Um. Why? I like the idea of being a fully transparent, but the transparency sometimes is working against the gradual approach to learning that when you learn a language, it's really essential. So jumping from class one to Class ten would not serve you that much. Well, so I'm posting one class in advance, meaning you have module one and you also have access to module two. So there is no need to go to a module to. Before next week. But if you want that, you can go through that and you can read the material, eventually do the quizzes and so on. If for any reason you need to go more down the road, meaning more than one class in advance, let me know and I will make it available. Okay. So let me stop shedding. And let me check with you. There are questions. Issues. Nope. All right? Yeah. Great. Sorry. I have a question. Hey, I. I'm not sure if everyone else is having issues. I'm having issues finding, like, 3.7, 3.8 version of Python. It looks like it was decommissioned or not available. Well, I think 3.8 should be still available if not go with the 3.9. Okay. I mean, the main difference in Python was between the two point something and three point something that was a major change. Uh, after that, that the differences were more on the performance side than on the syntactic side. So don't go with 3.10. But apart from that, whatever is available, that's absolutely fine. The one I'm using is I don't remember if I use in 3.9 or 3.3 you in the moment. I'm using 3.9. All right. Okay. So let me go now. Let me share the screen again. And let me go to this presentation. It's not many lights, but I really want to go through. Here we go. Some of the points so builders use. We are sometimes users, sometimes builders of technology. So we use the television, but we may not be able to build our own television or repair the television. We are using computers, but we, most of us cannot building computer. So but then we use it. We build some we may build some of the components of those technologies. So sometimes we are moving somehow, somewhere in this continuum between being a user and being a builder. When you are on the code inside the build that are programmers or coders, developers or whatever you want to call it, so you basically write code to do something and. You are building a tool that somehow will help you doing things in a more efficient way. In a faster way. Whatever is the reason. Writing code. That means writing instructions for your computer. Then you can use different languages because different languages will be interpreted somehow by or a filter by that language. That is a piece of software. Taking whatever you write and transforming it into action for your computer. Computer. It's I told you, we started from the very beginning. You have input, output devices, whatever they are. You have the processing part of the computer that is a central processing unit and the memory that is used to store the information and to. I mean, they handle the information while they are processed. And then eventually you have memory that is outside of the I mean, the core computer. You have devices that can be hard drive, can be thumb drives and things like that. So that's basically the basic schema of a computer. Now nowadays for processing most of the computers we have, they are processing through two different units. One is called the Central Processing Unit, and the other one is called a graphics processing unit. And so the GPU, the graphics processing unit that was originally developed to work on on the pixels that are on display because the peak sets up in a mattress. The way the computer is dealing with the Mavericks is in a parallel way, meaning the GPU is giving the computer the possibility to work in a parallel processing that is way faster. So the development or GPUs started with computer games with Nvidia being the largest provider of those GPU, but then they've been used in machine learning because most of the machine learning algorithms are based on neural networks that are based on operations between matrixes, meaning a lot of the potential parallel processing. Some of the most recent computer architectures, like the most recent Macs with the Apple, Silicon M1 and two. Uh, they have no. I mean, the architecture of the operating system is done in a way that there is no separation between CPU and GPU, meaning the computer is actually allocating the resources. So the best way possible that makes the computer faster. But then if you have a particular software that is addressing either the GPU you or the CPU, then that particular software would not work. That seems to be a trivial problem, but one of the libraries that is working this way is TensorFlow that is the most commonly used library for machine learning. And TensorFlow, developed by Google, is not working on Macs unless you have a sort of intermediate piece of software that is unfortunately slowing down the process. So it's kind of defeating the bugs. So we were talking about machine learning. So machine learning is one of the buzzword so that it's going on since a few years. Machines do not learn. There is no artificial intelligence because we don't know what national intelligence is. That seems to be I mean, raining on the parade, but that's the way it is. So I'm writing a book on the society, the application of a and machine learning. Uh, I started working in AEI in 1986, so not saying that I'm against either machine learning or artificial intelligence, but we should I mean, give the words the proper value. So what is normally called the machine learning is basically. A way to use systems that can leverage on data and have different behaviors based on different data. Pretty much discovering patterns on that large amount of data and matching those partners with your request. So what is the pattern now that is the closest to the one that the machine has and then that will be the answer that you will get. Seems to be like not a big deal. But when you have I mean, right now, one of the largest models based on this approach that we have is called the GPT three. GPT three is a based on deep learning that is a pretty large neural network, meaning the algorithm itself is complex, but then the data that is using is pretty much all the data that is available on open source there is out there. So machine learning has two components. One is the data and one is the algorithm, a complex machine learning system like GPT three that is based on that GPT too. So an earlier version can really run only on systems that are huge system computers that are huge, but they are so big that for training that GPT they needed as much energy, that the energy to run a few thousand C medium size CDs for a week. So that's how much energy is required for running those things. GPT two is based on roughly half a billion, a quarter of a billion parameters, meaning those things that are inside of the neural network in GPT three four that will be announced shortly. It's several billion parameters. So those are huge models that based the on machine. Again, machine learning is basically a system based on data and algorithms. The algorithms, what they do is to find patterns in the data and match the pattern with your request. You have different data or you have better data. Then there are more partners that will be discovered, meaning the system will be smarter, whereas this madness is just the ability to match those patterns. So when we talk about machine learning or artificial intelligence in general, we talk a bunch of different competencies that you need to have at work on machine learning, or it can be involved in machine learning. You have a little bit of cognitive science, unique skills, more on that side. We will go back to those things you need to know how to do all the work with those algorithms. So I mentioned neural networks in one and there are adults and then there are a few other disciplines. Getting some context. So A is a in a broader sense a part of automation. So not all the organization as an intelligent behavior, it should consider a machine doing only one job typing. Uh, I don't know. Bolsa is not exactly intelligent but is an automation because instead of doing manually you have a machine doing it. The. There are robots, some that, like the one who was mentioning before that are an example of organization, but they may not be intelligent then that artificial intelligence is in the real world of automation and may or may not be related to autonomy. So a self-driving vehicle is an example of a robot of organization and of artificial intelligence. Artificial intelligence is a broader discipline containing machine learning. So there are two ways of dealing with artificial intelligence. One is called symbolic, meaning you have symbols representing the knowledge that can be. If you do this, then you do better. 1: Those are statements that can represent a knowledge. 1: You can have what is called taxonomies, like the classification of the animal kingdom. 1: So you want to see how close the two animals are. 1: You have mammals, not mammals. And then you have, I don't know, birds. 1: You have cows on the outside and so on. 1: And then you expand those trees and then you may want to know how similar to animals are. 1: And you basically measure how far they are up from a common parent in this sort of thing. 1: So that's an example of a symbol. 1: So if the males are one example, other example does taxonomy. 1: So those are not part of machine learning, but they are part of artificial intelligence. 1: Machine learning is the one based on data in the statistics on steroids, both the user, some data science. 1: So in particular, machine learning. For preparing the data. 1: Getting the algorithms. All of those are in the area of data science. 1: We use Python. So why we use by them? We use Python because it's the most popular programing language. 1: So. Obviously not all the programing languages have the same scope. 1: So there are languages like Java, C++ that may have different rules. 1: So C++ is faster, is using less memory, meaning if performance is essential for whatever you do, 1: probably Python may not be the best solution, even if right now. 1: I mean, our computers are fast enough to run even. 1: Critical, but not super critical systems in Python. 1: On the top writer. I will briefly talk in a moment about Stack Overflow. 1: Stack Overflow. You have the link on canvas is a website with quite a lot of aura. 1: That's a huge amount of questions announced. 1: So it's related to coding. Coding in general, meaning all the languages. 1: The number of items that are related to Python is the highest among all the languages. 1: Then on the bottom right, the number of jobs mentioning Python. 1: So again, you may not be interested in a job site in Python, 1: not because you do not want to be a programmer, but that's an indication of how popular a language can be. 1: There are several reasons why Python is so popular. 1: Probably the main reason is the amount of libraries that you can use to add functionalities to the basic language. 1: And then, I mean, it's relatively easy to understand. 1: The syntax is relatively simple. 1: Even the basic python lab is pretty powerful. 1: When I started working in visual intelligence. 1: So again, we are talking 1986 or around that. 1: There was no python. So we wrote our assistants that we are based on what we now call artificial intelligence 1: or the symbolic artificial intelligence of writing their names statements. 1: So conditional statements in a broad sense and we use the languages that we are, I mean, 1: just developed for that, but there was no library, meaning if you want to do an algorithm. 1: So we will talk in a few classes about some of the algorithms in the data science. 1: In a broad sense of one of those I, I just pick one is a decision trees. 1: So now you call a library and you pass the parameter to the function within the library and you have your decision tree calculated. 1: In 1986 we had to write the algorithm for the decision tree. 1: So you basically start from the raw ingredients and you don't do the thing now. 1: So meaning you are moving up the level of abstraction or complexity of what you doing. 1: So in Python is the best example of up to a few years ago, 1: Python and R we are kind of for second, second, fourth, but then I mean for several reasons. 1: Some I cannot really explain. Python became the winner of this competition. 1: So right now, definitely Python is the most popular language that we are using. 1: There are two ways of dealing with coding or languages. 1: One type of language is interpreted like Python, 1: meaning you have the code and the language Python that is the software and most of the time is written in by the story you can. 1: Analyze that each line starting from top to bottom, each line from left to right in real time. 1: So. On the opposite side that you'll have a program set with the compiler, meaning you have your code up in whatever is the language. 1: You have the compiler translating your source code into machine code, and then the machine code, the order, I mean, 1: the code that will be executed that will be in detail, 1: one in a machine language that will run on your computer and it will be faster because it's a detail one. 1: The drawback is that if you have to debug it, then things will become complicated because you need to go back. 1: You don't have a real point of failure. 1: So debugging is more complicated. In the past, it was more important to have the compilation step up because computers were lower. 1: Right now, the computers we have are pretty fast and there is really no need to do the compilation. 1: Python is not from the snake, but is from Monty Python. 1: That is a movie, a Broadway show from the late eighties. 1: And Theodora was most likely a fan of those movies and named the language Alphabet. 1: So that's the example of a how to download 3.7. 11 app. 1: Obviously there are different. What else? 1: For different versions. And let me go now to. 1: To Pi. So we use the. 1: What is called the Integrated Development Environment. 1: So there are several of them. 1: The idea that we are using is called by Sharma. 1: You are the link on your canvas. 1: You can download the either the free version or the academic version or the commercial version that is also available for academic use. 1: And that's the one that I'm using now, by the way. It has a few more functionalities. 1: So if you do not need to use a pie chart. 1: Uh. In the past, I didn't use any I.D. I just used an editor, a text editor for grading the code, and then I ran it on on a terminal window. 1: So that's kind of a coding, like a caveman has a lot of Alexa, 1: so that IDs are giving you way more amendments and more options to do things in a faster way. 1: But there are other options in the realm of what is called notebooks and in particular call out. 1: But that is the version from Google. 1: Uh, we will talk about that. Um, I encourage you not to use those solutions. 1: I mean, if you don't know what I'm talking about, don't waste your time. 1: We will. We will talk about that. If you know what I'm talking about. 1: The meaning that you use the OR are using notebooks. 1: Keep in mind that when you use a notebook, notebooks cannot be integrated in a process. 1: So if you have some software doing some things, generating the file so that your software, 1: your python code will do, and then you will generate other results that will be passed to the next module. 1: So if you have this pipeline and your code is running on a on a notebook, you cannot really do the integration. 1: So you really want to have a separate file until the python scripts have a suffix. 1: Don't be why you want to have adobe WIP file to be integrated in your pipeline. 1: Again, if you don't know what is a notebook. 1: Just disregard what I said. 1: That's fine. In Pi Sharma, like in that many other I.D., you have an area with your files up. 1: So you will create a directory or a folder or whatever you call up the equivalent of a folder in your operating system, 1: and you will point your python to your pi sharma to that folder. 1: In this case, I created on these directories the M 624 and I have all those files and then you have right here and inibitori so is a text editor. 1: You can write whatever you want to write on your program, on your script, and then you will save it. 1: With the proper name, making sure that we go in the direction that you want then. 1: And this one is the python console. 1: So the python console is basically what you can use to do basic operations like 33, multiply by four, 333, whatever, and you have the result. 1: You can assign variables a equal to oops. 1: Meanwhile, to. And you have to. 1: The six. And then you do A multiply by B. 1: And you had the result, keeping in mind that. 1: Python is a case sensitive meaning if instead of using more a is more be a user. 1: Let's say you do a capital plus b. 1: So. But be. I will go than ever. 1: So because the COVID delay has not been defined. 1: Only the small one. So I could continue doing. 1: I don't know, whatever you want. 1: Like six. And divided by a. 1: 2.3. Then you will have the results. 1: So you can do all the operations in the in the Python console. 1: But obviously, you can write an entire program here. 1: But then when you close this section, it will be lost. 1: So that's why we want to do. We want to write scripts like this one. 1: Few other things. On by Sharma. 1: So, Impi Sharma, you have said things. 1: And you have your project. I mean, this is a directorate. 1: It's a this M6 24. Those are the libraries that are within this project. 1: One of the nice things about biopharma is that you can have multiple interpretations, 1: so you can have 3.9 for one projector at 3.8 and another one at 3.10 for another one. 1: Then you can associate the. Different projects, two different versions of Python, even different packages, different alignments. 1: So in this case, and a million libraries that are associated with this project using 3.9. 1: Mm hmm. When you have these smaller row is telling you that I currently have, in this case, 3.1.1, but 3.1.2 is available. 1: So if I want to update and just select whatever you want to update and then click on the arrow and I will upgrade. 1: The reason why you can do something like that having multiple versions is because of Pi. 1: Sharma is working on what is called a virtual environment, meaning each project as the directory. 1: With all the packages associated to that particular project, 1: you change project that you have another virtual environment and that in the sense is good 1: because if you do something wrong it will stay in that particular virtual environment. 1: So that's something that you may want to do. 1: You can add packages. But you can also manage packages using Python packages here. 1: So if I click on one, you will have here eventually a little bit of. 1: Explanation should be right here. I don't know why it's not showing up, but that's basically the way it works. 1: So you can install the new packages, you write the name of the package, and it will do in that you are a line that you set. 1: That is the default anyway. That is the paid on or website. 1: And you will then download the package that you want to download. 1: You also have. The terminal window. 1: So in the terminal window, if you're familiar, 1: if you know where the doors are in the operational part of Windows, that's pretty much the same in MacOs. 1: It's called just terminal. So you can interact with your computer directly already. 1: This is not by bypassing by. So. 1: Let me leave things here and let me jump back. 1: On the presentation. So. 1: We saw some of those. You can also do a printer and let me show you that before we blog. 1: So we saw that you can do. Again, you can do like operations like that, but you can also do print. 1: A Class B. And you will get the same result. 1: So in Biden on a three point something, you need to write the parentheses on what you want to print. 1: So you can print variables like in this case, you can print values, you can print nothing. 1: Like if you do print. Nothing you will get just a blank line sometimes may be useful. 1: If you want to bring the even stronger. 1: You need to use either a single. Or. 1: Double quotation. And for Biden is the same thing. 1: All right. So. Again, we saw several of those examples. 1: You can assign again. We saw that by the user to body of both variables. 1: You can use any name, any combination of letters. 1: Numbers underscore names are a case sensitive. 1: Keep that in mind. You cannot use some of the what are called reserved words so you can not call the variable print because print is a result of what? 1: When you name body of bolts, you may want to use names that are representing the content somehow. 1: So if you want to calculate the body mass index, the body mass index has a weight and height as variables. 1: So you can call those variables A and B and then do the operation, and the result would be the same. 1: But next time you will read the script, assuming the script is larger, because otherwise you will remember it. 1: You may not remember what A, B and C means. 1: So calling them height, weight and BMI would be really helpful for maintaining the software down the road. 1: So those are what is called the mnemonic naming of variables, meaning helping. 1: Remember the role of the variable in your pronoun. 1: Um, we worked a little bit with the interactive approach to meeting right. 1: In the statements that actually in the pilot console and then. 1: We mention that writing and using the text editor a sequence of statements is the best way if you want to keep up. 1: The statements for later news. Okay. 1: Let me stop here for a second. Questions. 1: All good. Okay. 1: So let's continue. Let me go back here and let me introduce the assignment. 1: So the assignment is on installing a python and pie shop to do some of those operations on the python console. 1: And then we added the except side zero that you will find the on your canvas. 1: So for the time being, don't worry too much on the syntax of the program. 1: I will go through the syntax, but again, we will go back to all all of those. 1: So when you have this number sign up, that means that everything following the character will not be interpreted by Python. 1: Meaning those are comments. There are other ways to add the comments. 1: But that's a pretty straightforward way. So order is a comment that accepts a zero and so on. 1: Those are all comments. 1: So this wild group is what is called a loop. 1: So one of the tricky part of Python is the use of indentation, meaning all of those statements. 1: Are all part of the same loop. So they are indented the kind of creating a separation from the rest of the code, 1: meaning everything on this indented portion will be executed the one through. 1: So while true, it is a statement that will mean it's a loop that would be broken when there would be the break condition. 1: So in this case, you are asking the user to input a number or a done to stop. 1: If the user type done, you break meaning you go out of this loop, you go here and you will print that. 1: Thanks for using this tool. If not meaning if else you are else. 1: Now you are asking for a second number and then you do a calculation that is adding two 1: numbers and you will continue indefinitely until you will type down and you will exit. 1: So when you do loops like that, you want to be sure that there would be an exit condition because otherwise they would be infinite loops. 1: So let me run it. To run it. There are several ways to do it. 1: One of the ways it's right click on the tab. 1: Run. But now he's running for center chair 20 to thank the juror for. 1: And you have the sum of 22 plus 44 is equal. 1: 66. Let's assume that they do then. 1: I'm exiting the loop and I have. 1: Thanks for using this tool and that's it. You would see those signs. 1: So those are what are called special signs. 1: So those special signs. Ah, with the backslash and then a character. 1: And that means new line. So when Biden is reading it, that is keeping one line. 1: Writing whatever is here and then is keeping another line. 1: And that's why you have all this space here. So let me go here and. 1: Let me recap what you are going to do for homework one. 1: So you want to set up am6 24 folder. 1: You want to obviously download the homework one and that will be the instructions. 1: That's the link that is working at this. 1: And then you will prepare the environment, meaning you will download the python you will solve by following the instructions. 1: Then you do pretty much the same things that you did on the by the console here. 1: So those are a sample of operations. 1: You will take a screenshot and you will posted the. 1: And then. So that's basically the detailed version of it that you will have the print run by your name that is missing here. 1: I'm sorry. Then you will bring the. 1: I mean, you would run. The code again. 1: To run the code, the right click the prime. 1: Or you can just the name of the script is here. 1: You just click this run here or you can go here and run or just run and specify what you want to run. 1: So there are many options. Them or just do this and we run again. 1: But I mean. Again doing. 1: Right click around. It will keep in mind that there is also a shortcut. 1: So control. Shift control? Yeah. 1: Shape control are our control option. 1: Are we running? That's pretty much it. 1: So let me stop sharing. And that's pretty much it. 1: Oh, it's about 8:00. Some of the classes down the road may last a little bit longer. 1: So, uh, normally it's 630, 8:00. 1: I will do my best to say in the 8:00. 1: But if there are would be questions, we can say a little bit longer. 1: Generally speaking, the structure of the class would be we will start with comments on the material that was posted. 1: And in particular, I would like to have your comments and then that start the discussion. 1: After that, I will introduce the content of the week and I will use some. 1: A PowerPoint to kind of highlight some of the key points that those power points that will be posted and 1: then there will be some in-class exercises that can or cannot be four points but is for practicing. 1: Then I will introduce the assignment for the following week. 1: Comments. And that's the end of the class. So those are the normal components of a normal class. 1: So again, comment on the material. 1: New material presented in class. Exercise introduction of the new assignment. 1: End of the class. There is not much that you can do today as an in-class exercise because we really just that. 1: But from next week we would do it. So that's basically the end of the class. 1: Uh, if you don't have questions. 1: If you have questions, feel free to ask. I am happy to address it.
 

fis STEVENS

lw INSTITUTE of TECHNOLOGY
is
fi

Visualization & Python





clipizzi@stevens.edu

SSE

 


History of Graphics and Visualization 

 

¢ 70s to 80s 
— CAD/CAM Manufacturing, cars, planes,
and chips

 

Sr
STEVENS INSTITUTE of TECHNOLOGY | 2

 

Visual Analytics Definition S

Visual analytics is the science of analytical
reasoning facilitated by interactive visual interfaces

People use visual analytics tools and techniques to

# Synthesize information and derive insight from massive,
dynamic, ambiguous, and often conflicting data

= Detect the expected and discover the unexpected

= Provide timely, defensible, and understandable
assessments

= Communicate assessment effectively for action

STEVENS INSTITUTE of TECHNOLOGY | 3

What is not visual analytics? ie

¢ Large graph structure with no labels

¢ Heat map with no labels

¢ Search and retrieval systems

¢ Chart with no interaction

¢ Image with no semantic interpretation

¢ Stand alone image that does not tell a story

STEVENS INSTITUTE of TECHNOLOGY | 4

 

 

Whole - Part Relationship ws

¢ Scale independent representations, whole and parts
atsame Time at multiple levels of abstraction, offen
| al ke qd meer 5 selected, 8 more from groups eles

    

 

        
 
 
 

    

 

 

 


 

 


 

 

 

 

 

 

 

 




  
 
  

   
 

 

 

STEVENS INSTITUTE of TECHNOLOGY | 9

Relationship Discovery te

¢ Explore high dimensional relationships, theme
groupings, outlier detection, searching by proximity
at multiple scales 

T
STEVENS INSTITUTE of TECHNOLOGY | 4

 

 

Combined Exploratory and S

Confirmatory Analytics

¢ Develop and refine hypothesis

¢ Evidence collection, management, and matching to
hypothesis

¢ Tailor views/displays for thematic/hypothesis focus of interest

¢ Often suggestive of predictions enabling proactive thinking



 

 

 

 

 

 

 

 


 

Select a document title to view the contents

 

STEVENS INSTITUTE of TECHNOLOGY | 7

 

Multiple Data Types S

¢ Supports multiole data types:
structured/unstructured text

¢ Imagery/video, cyber
¢ Systems of either data type or application specific


STEVENS INSTITUTE of TECHNOLOGY | 8

ji =.

Temporal Views and Interaction S

 

¢ Most analytics situations involve time, pace, velocity
¢ Group segments of thoughts by time

¢ Compare time segments

¢ Often combined with geospatial



 

 

STEVENS INSTITUTE of TECHNOLOGY | 9

Neen i

Reasoning Workspace eS

 

¢ Workspace to construct logic and illustrate reasoning
¢ Flexible spatial view of reasoning: stories


 
 

STEVENS INSTITUTE of TECHNOLOGY | 10

    
   
   
 
 
 
 
 
 
 
 
    

 

 





   
Grouping and Outlier Detection
   
   Grouping and Outlier Detection
• Form groups of thought/data
• Labels and annotation
• Compare groupings
• Find small groups or outliers
   
     
     
 
Labeling
 
 • Critically important
• Dynamic in scope, number labels, size, color
• Positioning
• Almost everything has labels
• Labels tell semantic meaning


Multiple Linked Views
• Temporal, geospatial, theme, cluster, list views with
association linkages between views
 
13

Reporting S

¢ Capture display segments in graoh modes for
putting in reports, PPT etc

¢ Capture reasoning segments of analytic results
¢ Capture animations

 

 

STEVENS INSTITUTE of TECHNOLOGY | 14

 

 

Major Python visualization packages ie

¢ matplotlio: htto://matplotlib.org/
— Gallery: htto://matplotlib.org/gallery.ntm|
— Frequently used commands:
hitp://matplotlib.org/api/pyplot_summary.html

e Seaborn:
http://stantord.edu/~mwaskom/software/seaborn/

e ggplot:
— R version: http://docs.ggplot2.org
— Python port: http://ggplot.yhathg.com/

e Bokeh

— htto://bokeh.pydata.org/

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 15

Seaborn

¢ Seaborn is a library “for making attractive and informative
statistical graphics in Python”

¢ Itis built on top of maftplotlib and tightly integrated with
the PyData stack, including support for numpy and pandas data
structures and statistical routines from scipy and statsmodels


https://stanford.edu/~mwaskom/software/seaborn/


STEVENS INSTITUTE of TECHNOLOGY | 16

 

 

Bokeh _—CONTINUUM le


¢ Interactive Visualization
i ¢ Novel graphics

 * Streaming, dynamic, large data
¢ For the browser, with or without a
server
‘ No need to write Javascript



7

hito://bookeh.oydata.or

Sr
STEVENS INSTITUTE of TECHNOLOGY | 17

 

 

Versatile Plots 

   

 


 

STEVENS INSTITUTE of TECHNOLOGY | 18

 

Novel Graphics



STEVENS INSTITUTE of TECHNOLOGY | 19

Linked Plots (Notebook 2)

 


¢ Easy to show multiple plots and link them

¢ Easy to link data selections between plots

¢ Can easily customize the kind of linkage straight from
¢ Python, without needing to fiddle around with JS

STEVENS INSTITUTE of TECHNOLOGY | 20

 

 

és
Flexible Tools (Notebook 3) ‘

 

 

 

¢ Many useful tools with built-in functionality
¢ Easy to extend with Javascript, if so inclined

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 21

Traditional Web Viz - Interaction 



¢ Simple dashboard: Server language generating HTML, JS, CSS styling, subset of
data

Handling user interaction: Custom Javascript, calling Server endpoint, which
generates updated JSON or JS that gets pushed back to client via websocket

STEVENS INSTITUTE of TECHNOLOGY | 22


Bokeh Conceptual Architecture

Simple dashboard: Single language, no need to write HTML, JS, CSS

Handling user interaction: Single language that you already know; interactive
data updates feel seamless to the user

STEVENS INSTITUTE of TECHNOLOGY | 23

 

What IS Plot.ly? $

Plot.ly is an open-source data visualization tool lth

.. was built using Python (Django) & JavaScript
.. Offers a web application for visualization & analysis
.. provides plotting APIs for many popular languages

.. plots are fully interactive, and rendered with D3.js or WebGL (for 3D)
.. free, paid, and on-site offerings



Compose a Plot 

Figure() - The Plotly “Plot” Object
Composed of two parts: ‘Data’ and
‘Layout’
Data()
• Contains the information to be
plotted.
• Composed of ‘trace’s
Layout()
• Contains information about the plot
• i.e. title, labels, fonts, annotations,
etc.
Figure()
Combines Data() and Layout()

STEVENS INSTITUTE of TECHNOLOGY | 25

 

oft STEVENS

w INSTITUTE of TECHNOLOGY

Mining Social Media


2016



clipizzi@stevens.edu

SSE

 

Why Mining Social Media Y

 

° How can we discover, interpret and use
what Is relevant in the volumes of
available information generated by

 social media users using streams of
messages?


Semantic Analyses provide
only part of the answer



° Potentially relevant role for text mining,
BY with specific issues:

Social network metrics * Time Sensitivity

° Short Length

° Unstructured Phrases

 

STEVENS INSTITUTE of TECHNOLOGY | 2

of,
Aspect #1: Time Sensitivity °

« Social media’s real-time nature

— Example: some bloggers may update their blog once a week,
while others may update several times a day

¢ Large number of real-time updates from Facebook and Twitter
contain abundant information

— Information > detection and monitoring of an event
— Use data to track a user’s interest in an event
¢ A user is connected and influenced by his/her friends

— Example: People will not be interested in a movie after several
months, while they may be interested in another movie released
several years ago

STEVENS INSTITUTE of TECHNOLOGY | 3

 

 

Aspect #2: Short Length Y

Certain social media websites have restrictions on the length of
user’s content, like Twitter's 140 characters rule

Short Messages > people become more efficient with their
participation in social media applications

Short Messages also bring new challenges to text mining

STEVENS INSTITUTE of TECHNOLOGY | 4

 

fs
Aspect #3: Unstructured Phrases Y

¢ Variance in quality of content makes the tasks of filtering and
ranking more complex

¢ Computer software have difficulties to accurately identity
semantic meaning of new abbreviations or acronyms

STEVENS INSTITUTE of TECHNOLOGY | 5

‘ in

Extracting “meanings” Y

° Using a combination of semantic and topological
a et analyses we can extract dynamic concept maps

 © Conversations are characterized by structural
patterns whose properties can be assessed
through quantitative metrics

© A tool has based on this approach can be used
as a “backchannel’” for real life events

° Conversational patterns can support sentiment analysis and provide dynamic
insights on what people say about the event

° Conversational metrics potentially contain early signals to predict users’
preferences and choices

STEVENS INSTITUTE of TECHNOLOGY | 6

 



Classes of application we

 

° Extract concept maps from message streams to detect elements to screen
conversations for two classes of events:


 
   Classes of application
• Events non evolving in time,
such as launch of a new
product or a new movie. Final
goal is to ascertain whether
differences in streams patterns
are correlated to relevant KPIs
• Events evolving in time, such as
political/marketing campaigns.
Final goal is to detect people
reactions to evolving stimula
   
 
 

STEVENS INSTITUTE of TECHNOLOGY | 8

unit of analysis
Social streams originated by Twitter micro-blogs
because
• Twitter popularity
• Micro-blog is the main social medium
to share & broadcast opinions
• Reduced semantic complexity due to
the 140 character format

 

cf.
Perspective: Conversational analysis 

 

° The research uses a conversational
metaphor and assume that in
backchanneling applications micro-blogs
stream exhibit some properties of
conversations

° According to the Common Ground theory a
conversation is a form of collective action
requiring participants to coordinate on
content and on process (Brennan & Clark,
1991)

 

STEVENS INSTITUTE of TECHNOLOGY | 9

Perspective: Conversational analysis 

 

° Twitter streams as loosely-coupled

conversations
° Common ground accumulation

generating shared knowledge

° Exploit adjacency pairs (Clark &
Schaefer, 1989) fo connect tweets
into a collective knowledge map
evolving in time

    

STEVENS INSTITUTE of TECHNOLOGY | 10

 

 

ts _
Methodology Ye



 

STEVENS INSTITUTE of TECHNOLOGY | 11

fs
Methodology in action - 1

Step 0: Extract messages, create the dataset

 

Step 1: Extracting the social network - to 
get “social metrics” 
 Step 2: Create a bipartite network keywords-
senders - G = (U, W, A)


STEVENS INSTITUTE of TECHNOLOGY | 12


Methodology in action — semantic basics 

  
 

Step 3: Extracting a 1-mode with words only -


GW=G × GT=(W,AW) where AW={(p,q), ∃ x∈U:
(p,x)∈A and (q,x)∈A}

Step 4: Detecting clusters (“topics”) — using a
A Sn en Sm = combination of k-core decomposition and
cae a, Louvain community detection method


Step 5: Interpreting the results — Using a
combination data mining and visualization

ME
STEVENS INSTITUTE of TECHNOLOGY | 13

eee A ||

Test & refine through Case Studies 

° Obama —- Romney Presidential debate in October 2012, with about 30,000 tweets collected from a
“swing county”, the Tampa region, in Florida

 

° Kenya, general elections in March 2013. To analyze occurrences of violence-related
conversations. About 50,000 tweets collected

° Boston marathon bombing, in April 2013, fo analyze people reactions. About 30,000 tweets
collected

° Five non eventful days in Hoboken in November 2013, fo analyze streams not related to specific
events. About 100,000 tweets collected

Several Apple and Samsung events, in September and October 2013; June, July, August,
September and October 2014. Tweets collected in each event in 2013 were between 40,000 and
60,000. At the latest event in October 2014 about 600,000 (recent change both in Twitter policies
and in the tool) — sample of outputs in the following slide

° Super Bowl in February 2014, to analyze people comments. About 40,000 tweets collected

Oscar night in March 2014. About 60,000 tweets. From this stream extracted the companies people
talked more about. Monitored one of them, Samsung, and collected tweets over the following 2
months — for a total of about 1 million tweets — to compare the “average” and “peak” people
opinion on the brand

° Movie Openings from 10/2014 to 2/2015. A total of 21 movie opening has been monitored so far,
collecting movies performance/KPIs to validate the model

° AppleWatch event in March 2015, to face validate the representativeness of the method. About
700,000 tweets collected

° A total of more than 4 million messages related fo 35 different events

ME
STEVENS INSTITUTE of TECHNOLOGY | 14

ft.
Applying the Methodology Y

Case 1: monitoring on-going events

° Those are events whose evolution in time can provide insights that we
want to monitor, such as in ongoing marketing or political campaigns,
where messages sent by the campaign managers can be adapted,
based on people reactions

° Subject of the search can be a combination of geographical location
and keyword, such as in monitoring the needs of citizens in a region (no
keyword required) or regional political campaign (combination)

° Methodology in this case will have
° Dynamic visualization
° No data mining steps

STEVENS INSTITUTE of TECHNOLOGY | 15

 

Empirical study 1



One of the key events In
consumer electronic In
2015

Over 700,000 tweets
collected

Full press coverage, with by
the minute event logs
available

Possibility fo overlap event
log with data from our
system

STEVENS INSTITUTE of TECHNOLOGY | 16

Case study on Monitoring
Networks Extraction



STEVENS INSTITUTE of TECHNOLOGY | 17

 

Case study on Monitoring Y


Case study on Monitoring
Data Preparation

 

Data have been pre analyzed to increase quality, applying
exploratory statistics and preliminary data mining

El
STEVENS INSTITUTE of TECHNOLOGY | 19

 

cfs
Case study on Monitoring 

Visual Interpretation — 1
Reconstructing the event narrative through Twitter users reaction

 Size is based on
number of tweets in a
given time frame
• Color/blurring is
based on the focus of
the conversation in a
given time frame. The
less blurred a circle is,
the more focused the
conversation is
• Focus is calculated
by number of
clusters*number of
word in the cluster


 Words on the “globes” are the most relevant for the time frame
• “Globes” are connected based on semantic similarity between clusters
STEVENS INSTITUTE of TECHNOLOGY | 20

 

cfs
Case study on Monitoring 

Visual Interpretation — 1
Reconstructing the event narrative through Twitter users reaction



Positive and Negative sentiment polarities are by the sides of the “globes”

STEVENS INSTITUTE of TECHNOLOGY | 21

Case study on Monitoring

Visual Interpretation — 2
Identifying homophily

[| Relational Network

Words Network


Homopbhily is the
extent to which
actors form ties with
similar others

It is calculated using
clustering coefficient

The top peak shows
highly related
people talking
about focused
subjects

Isolated relational
peaks shows highly
related people
talking about
unfocused subjects

STEVENS INSTITUTE of TECHNOLOGY | 22

 
  

Case study on Monitoring 

Interpretation

The different visualizations display
a consistency in their pattern

The different relative peaks are in-
sync with topical moments in the
events

There is a noticeable overlap
between the changes in the
metrics and the actual events

Even if the key announcement
was the Apple Watch, the most
intense reaction has been for the
new MacBook

STEVENS INSTITUTE of TECHNOLOGY | 23

 

ft.
Applying the Methodology Y

Case 2: Predictor detection

Those are events whose evolution in time is not as relevant as gathering
collective opinions that can be analyzed to determine factors with
predictive capabilities for given KPIs, as in finding patterns for successful
product launches

Subject of the search are several with homogenous characteristics, such
as in the launch of a new movie, where subjects are the different movies
opening during the period of observation

Final goal is to ascertain whether differences in streams patterns are
correlated to relevant KPIs

This is a data mining task, where the dataset to be mined is composed
by the values of the metrics for the different subjects - such as the
different movies — and selected KPIs as target variable(s), such as box
office revenues

STEVENS INSTITUTE of TECHNOLOGY | 24

 

oo A

Case Study on Predictors Detection ©&
New Movie openings in 2014-2015

 

The data

° We collected data during the
weekends of opening of new movies

° A total of more that 2 million tweets,
related to 22 movies, that are 22
different datasets

° Each dataset has been split into 1h
long time windows. Each partition has
| ! been analyzed using the methodology
Base a) Pe in the previous slides, and then
consolidated in a matrix with 1
datapoint/record per movie, with 50
variables each



STEVENS INSTITUTE of TECHNOLOGY | 25

Case Study on Predictors Detection 
KPIs

° We are using 3 indicators/KPIs

° First week box office revenues,
weighted by number of theaters

° Critics score from Rotten Tomatoes
(“Tomatometer”)

 Audience score from Rotten
Moto

 

STEVENS INSTITUTE of TECHNOLOGY | 26

 

Case study on Predicting 
Independent variables

° Sentiment polarity

° Traffic metrics (tweets per unit
of time)

° Social network metrics
(centrality, density and
 clustering)

 ° Semantic metrics (lexical
re) A CUHULGS diversity and topological

metrics based on the
concept maps)

STEVENS INSTITUTE of TECHNOLOGY | 27

 

 

Case study on Predicting
Networks Extraction

The relation/social
network

STEVENS INSTITUTE of TECHNOLOGY | 28

 

Case study on Predicting Y
Networks Extraction

A semantic clusters
network, for Kingsman
for the 1st time window



STEVENS INSTITUTE of TECHNOLOGY | 29

 

Case Study on Predictors Detection

Preliminary Processing

 We apply all the


 We apply all the
topological and statistical
analysis we use in the
Monitoring ongoing events
case
• Instead of considering the
single partitions for the
next steps, we collect in
one dataset aggregated
values from each of them

Case Study on Predictors Detection
We used Decision Tree –
CART, Random Forest, Linear
regression and Artificial
Neural Networks for the
modeling
• We targeted the first week
box office revenues as KPI,
because the results with the
other KPIs are poor
• We tried several combination
of variables to determine the
one with the highest
prediction value


Case Study on Predictors Detection
• To evaluate the performance
of the model, we applied a
Predicted vs. Observed
analysis, using a subset of the
original dataset as testing
dataset
• “Sentiment variables”
performed poorly, with the
best result being 0.47, using
Linear Regression (bottom
left)
The Model - Evaluation



 

 

Case study on Predicting we
Data Mining - Evaluation


 

STEVENS INSTITUTE of TECHNOLOGY | 34

 

 

cfs
Case Study on Predictors Detection Y

The Model - Interpretation

° Each group of variables, when used in
isolation, does not make a good job in
predicting early sales for new movies

© This is particularly true for the sentiment
metrics, which achieve by far the worst
oredictive performance

° In general the topology of the keywords
network and the traffic values seem to
play a key role

° The model does not work well in
oredicting what people think (critics and
public reviews) but works to predict what
people do (sales)

 

 

Tyt gly

 

STEVENS INSTITUTE of TECHNOLOGY | 35

ft.
Applying the Methodology Y

Case 3: Emotion recognition in crowdsourced text

This application is about classifying text from public sources on given
subjects based on detected emotions

It can be applied to cases like
° Product and service reviews
° Forecasting: election pools, movie theaters

It could increase the human-machine interaction for example in
consumer electronic

STEVENS INSTITUTE of TECHNOLOGY | 36

 

 

EMOTION CATHEGORIZATION THEORY lw

Plutnik’s Emotion Wheel

Eight Basic Emotions (inside dark red
ring)
- 4 pairs of opposite emotions:

Joy x Sadness

Trust x Disgust

Fear x Anger
Surprise x Anticipation

- 3dimensions of intensity
Annoyance < Anger < Rage

 

STEVENS INSTITUTE of TECHNOLOGY | 37

 

DATA SELECTION

SongMeanings encourages users to discuss on the meanings of songs.
110,000 artists
1,000,000 lyrics
14,000 albums
530,000 members

Artists: Pearl Jam, Madonna and Muse

STEVENS INSTITUTE of TECHNOLOGY | 38

 

 

DATA GATHERING 

API documentation (JSON files)
Convert to CSV
Filter fields of interest:
Comments content
Comment rating (weight)
Replies (weight)


 

 

STEVENS INSTITUTE of TECHNOLOGY | 39

 

DATA PREPARATION 

565 CSV Files
One file per song
One comment per row
Script (Python)
Look-up table: replace internet slang
Clean non-English words (non-stop word list)
Delete context-specific words (stop word list)
Tokenization list of words

STEVENS INSTITUTE of TECHNOLOGY | 40

~)=h—l33slUui

aft:
NATURAL LANGUAGE PROCESSING 

 
Natural Language Processing Tools
Natural Language Toolkit for Python
Accessing Corpora (Information Content)
String processing (Tokenization)
Part-of-Speech Tagging
WordNet
Semantically-oriented dictionary
Organized in collections of synonyms (synsets)
Based on hierarchies (word tree)
nodes correspond to synsets;
edges indicate the hypernym/hyponym relation

STEVENS INSTITUTE of TECHNOLOGY | 41

SEMANTIC SIMILARITY lw

Similarity = Common Information + Differences
Senses and Information Content:
“Bank” (financial sense) is more similar to “fund”
“Bank” (river sense) is more similar to “slope”
 

 

SIMILARITY WITH EMOTIONS



STEVENS INSTITUTE of TECHNOLOGY | 43

FINAL DATASETS

 

[ &

Pearl Jam: 11 albums; 158 songs

Madonna: 16 albums; 160 songs

Muse: 8 albums; 102 songs


STEVENS INSTITUTE of TECHNOLOGY | 44

VISUALIZATION — Scatter Plot

 

Pearl Jam’s Backspacer Album Emotion Map
Each song has eight points (one for each emotion)
Songs that stand out are identified and labeled.


STEVENS INSTITUTE of TECHNOLOGY | 45

 

 

VISUALIZATION — Emotion Wheel

Pearl Jam's No Code Album
Emotion Wheel

Overlay Radar Chart and
Plutchik’s Emotion Wheel

 

“No Code” by Pearl Jam

 

 

STEVENS INSTITUTE of TECHNOLOGY | 46

cfs
Case Study on Emotion Detection Y

The Model - Interpretation

e In this case study, emotion detection can
be used for example to determine what
makes an album/song from a given artist
more or less successful, using the number
of units sold as KPI

° Some of the emotions -— like
Surprise/Anticipation — do not play a
relevant role in songs

° Artists generate moderately distinctive
emotions

 

  

Tat aly

STEVENS INSTITUTE of TECHNOLOGY | 47

 

 

Beyond Twitter Y

 

° Other potentially relevant Social Media are Facebook and LinkedIn
° Linkedin is more complex to mine, due to their strict privacy policies

° All the media can be mined via html-mining, but this process can
be very time consuming, with APIs being always the best way to go

° Image/video based media are still by the technology frontier in
terms of mining, due to the complex task of extracting semantic
from pictures, when the subject is not human

El
STEVENS INSTITUTE of TECHNOLOGY | 48

 

 

Mining Facebook Y

° Private pages in Facebook are protected by
Facebook privacy policies and cannot be mined
via their API

° Html web mining is always possible, but quite time
consuming: you need to go to the page you
want to download, save the html and mine It. If
info are in multiple pages, this can take a while

° Public pages are accessible via Facebook API

 

 

 

 

STEVENS INSTITUTE of TECHNOLOGY | 49

Mining Facebook Y

Case 4: Comparing Democrats and Republicans in ‘16

° We compared Facebook comments on

© Hillary Clinton: 07/24/2016 — 07/30/2016; week of the Democratic
National Convention

° Donald Trump: 07/17/2016 — 07/23/2016; week of the Republican
National Convention

° The quantity of comments mined per day varied from 10,000 to
50,000

STEVENS INSTITUTE of TECHNOLOGY | 50

 

‘ “a

Mining Facebook Y

Case 4: Comparing Democrats and Republicans in ‘16

Trump relational

network (right) is more
Trump-centered than
the Clinton one being
Clinton-centered (left)

    


STEVENS INSTITUTE of TECHNOLOGY | 51

 

Mining Facebook Y

Case 4: Comparing Democrats and Republicans in ‘16



Those are the semantic
networks extracted,
with Clinton’s one on
the left, Trump's on the
right

 

 

STEVENS INSTITUTE of TECHNOLOGY | 52

 

oo A

Comparing Facebook and Twitter lee

Case 4: Comparing Democrats and Republicans in ‘16

 

  

° We compared results extracted from Twitter and Facebook on the same
subject/period

° The two media appear to be used in very different ways

° Despite possessing features and capabilities allowing for prolonged
conversations between users, Facebook is instead used as a soapbox and

a microphone for each user to proclaim his or her message, with no regard
to if anyone Is listening

° In contrast, Twitter relies heavily on the relaying of information about a
topic via highly influential users who are retweeted over and over

El
STEVENS INSTITUTE of TECHNOLOGY | 53

Hello. Hello, everybody. It's 629. 1:13 Hey. Let's wait. 1:21 Two more seconds. So I hope the assignment was not too bad. 1:26 And I hope that you have time to review the material for this class. 1:35 But we will have plenty of time for going through all of it anyway. 1:42 And. I think we can start. 1:48 It's 630 now. Oh, right. 1:53 Okay. So it's February the 14th and I the Happy Valentine. 2:00 If you celebrate, I am not particularly in to death, but that's fine. 2:08 Too commercial. But that's me. Happy Valentine, anyway. 2:15 631 Now. So we are starting. We will cover the usual topics. 2:23 So we will talk about a little bit more by Donna. 2:32 We will add some elements that are more related on the management side, 2:38 but we will talk about the themes in software development and I will share some experience and then we will do an in-class exercise. 2:44 So we will review that. 2:57 We will discuss the new assignment that is going to be a little bit more complex, but that's basically the spirit of the course. 3:00 So the idea is to start the very low profile, easy assignments, 3:12 and then build up a class by class up to the midterm when after that it will be a little bit more challenging 3:19 also for people with some background in coding because we'll be more on applying coding to cases, 3:34 situations, problems, topics, and that would be more the real goal of the entire course. 3:46 All right. So let me start sharing the screen. 3:56 And let me go first, as usual. 4:02 Cause this wouldn't go here. Okay, so we will. 4:09 We are. In Module four Software Development Lifecycle. 4:17 A dictionary is another structure. In Python, you had some material to read. 4:28 We will we will talk about those up in with the support of some slides. 4:39 I hope you are not having additional issues with the material. 4:48 Again, it was a little bit of messy. 4:53 The canvas shell that we use and in part was my responsibility. 4:58 I mean, it was my responsibilities, my course. But I physically messed up a little bit thinking that you're replacing, I mean, 5:07 the new material that was kind of replacing what was there and what not. 5:20 Campbell said that he did it. He did not. But anyway, so I really hope that now is working well. 5:28 If it's not working, send an email to me to show you and that we will address that. 5:34 All right. So let's start with. 5:42 With the previous assignment. 5:49 So the assignment was pretty straightforward. That was actually there was two components of the assignment. 5:54 So one was the coder and the other one was the. 6:02 So that was those things was on testing. 6:13 So on the testing, that's pretty much always a little bit of an issue with some of the students because they don't really get that what I was asking. 6:20 So I try to be more clear up in the way I introduce the exercise, but sometimes students still have a little bit of issues. 6:34 So just to be very clear, the testing is for testing. 6:54 The script that you would write. So he's not a genetic theoretical testing, but he's testing the program that you are going to write for you, Europe. 7:02 Excuse me. Oh, right. Okay. 7:18 So. So using the template that I gave you. 7:22 You have a program goal that is I mean, that description that you already have, 7:34 the program will check that user input if it is a number that is multiple of five. 7:39 The strategy, meaning what are the logical steps that you would write again? 7:48 It's defining the logical step that will be translated into the language that you will use. 8:00 Meaning it could be a python, it could be R, a could be any language. 8:08 So the program will ask the user for input. 8:14 The user input will be checked first for the willingness to proceed. 8:17 If the user will type done, will exit the program, will then test if the input is an integer. 8:24 If not, the will go back and then it will check if the input is a multiple of five. 8:34 If not, the will go back. 8:40 Otherwise we print a positive message. 8:46 This is a flowchart. Is the representation in the blocks, the logical blocks of the logic of the program. 8:51 So generally speaking, you have the square blocks that are assignments, things to do. 9:02 So ask user input. Those robots are the question that you will ask. 9:12 So the conditional statements. So as a user input is done. 9:20 Yes. And goodbye. And is the end of the program done? 9:28 No. I am asking if is integer. If not the you and the message will be you enter a non integer value and you go back to EPS. 9:32 You ask is this a multiple of five? 9:41 If no you the message will be you enter a number that is now the number of five, a multiple of five and you go back if yes, you print. 9:44 Okay. Uh. In reality, there should be a going back here that is not in this chart. 9:55 So for each one of the alternatives, you want to have a value that will test that particular branch of the shot. So if the input is done in testing this conditional here, if the input is done, is done, then the output will be good by. If the input is x, y, z, then the output and testing this, the output will be under non integer value if it is a 61. And testing this condition here you enter a number that is another multiple of five. 95 will be okay. So with that, I tested every single conditional branch of my chart that it's basically in plain English this testing strategy. So that was the testing portion. Questions on this. All right. So let me go back to sharing it and let me go to the code. So does the code. The usual, while true. Lupo. So I'm asking the user to enter a number or a done to quit the. I'm checking if the input is done. It doesn't break. If it breaks, it will go here and it will print goodbye. If not, the will try to transform the input that is stringer into number an integer. If we get an error, we go into acceptor. Will print that your input was not a valid number of cents. Please try again and then continue. Meaning we go back to the beginning of the loop. If there is no except no error, I'm asking if. The value is less or equal to zero. I mean, that's something that is not in the flowchart that is an additional check was not required. But if it is negative, please enter a value greater than zero. Then I'm asking if is a multiple of five. If a is not a multiple of five, then you will get the message and go back. I'm checking also if it's more than 1000 again, you're not being asked to do that. If it is a print, please enter a value that's more than 1000. Continue. If I mean that all the ifs are negative, then you will print under that amount of sense. If you run it. So if you have a 22. You have not enter multiple of five. If you do. A certain number of hives. And if you do, minus two. And then if you do 55, you will get the message. And if you do, quit the. Oops. It's done. It will be goodbye. Sorry about that. Okay. So that's basically what was in the in the code, the questions on that. All right, so let's, uh. Anyone has any question on the assignment? I have a question, Professor. Go ahead. For the homework. Are we. Are we required to test of a floating number like a decimal? Well, I try to be as much clear as possible in the requirement. If it is not required by the requirements, you don't need to test. So in this case, the two tests less than zero, greater than 1000, that we are not required and you are not requested to do it for the floating or the integer? Well, it's pretty much the same in this case. So that being said again. So in this case I use the integer, but I could do the same with floating. So you don't. I mean that either one is fine. I mean, there is no difference. In this case, I know that is. I mean, I want to have an integer because if it has to be a multiple of five, it should be an integer. And that's why I used integer. But I mean that the problem would work as well if instead of Integer, I would have a float. Oh, I guess my question was do if I used the intention. Well, but I didn't, I wasn't explicit in the I guess that the testing portion in the bottom, I didn't put like 10.5 or anything like that. Is that is that fine or would we have points off taken for that? No, no, that's okay. Is absolutely. Because I got points taken off for, for not testing like 10.5. I will check that with you. I mean, again, if you're not required to do a test, you don't you don't have to do it. Then if you do it and it's working, that's good. If you do and it's not working and it is not good, then you will get some points off. But I mean, in this case, no one was asking for shaking her. If her assertion not meaning. I will take your. Great. And actually, that's not the best. All right. Still, I'm Professor. Yes. I just want to make sure I had the same thing. I had a note that said Float not tested and I had to take it off as well. Okay. I'm not sure if anybody else had that same issue myself also. Yes. Okay. I will review all of you. Okay. Thank you for telling me. Yep. All right. Okay. So that will be fixed. That again. If is not in the requirements, you don't need to do it. So I really tried to be as much clear as possible in the requirements just to avoid the misunderstanding of any kind. But things can happen. All right, so let's move on and let's go now. To the lights. It's 647. We are definitely on time. So we will cover two different topics, two very different topics. So one will be on working in teams. So all of you, most of you are professionals. You most likely work in teams. You know what the dynamics are. But I just want to revise to review with you some of the key points of working in teams in particular when you are developing software is engineering management. You may be in a position of managing people and in particular managing people who are developing software. So knowing how those environment may work and what are some of the key elements could be useful to you or to some of you, or for most of you. When you develop any organization, you have pretty much the same problems. So you need to hire people. You need to keep people on track. You need to motivate people. You need to eventually replace people. You need to solve the issues. I mean, the most recent software team that I managed, it was few years. I mean, apart from research in a company, so was a few years ago I was managing as a co-CEO and CTO. Not huge, but a sizable team of developers in a technology company in Central Florida. But the mission of the company was managing the tickets. So we manage the tickets for a bunch of clients, including Disney. So Florida. Orlando. Disney. That's why we were there. And some of the issues are. So I use that example because I had situations that are very common and they are kind of a good example on how those things can happen. So I will use that example a few times in these presentations. So when you have a. An organization on a project you may want to have a project manager or a team are doing the the management of the project. That can be called a million different ways, but pretty much in the one setting, the time being the drummer of the rock band, I wrote for Up with McKinsey for about two years. The company was very, very focused on organizations. Typically, what they recommend the most is a mavericks organization, meaning you have people, teams committing certain functions within the company and serving across the organization as a subject matter expert on that particular field. That's the Mavericks organization opposed to the hierarchical organization where you have each team with all the components. So let's say you have a team developing user interfaces, each team meaning each group working on one particular project within the company, they have their own sub team working on the user interface. The two organizations. They have pros and cons like everything in life. So the pros of a hierarchical organization is that you have more control of the resources you can use because you have your entire structure. The cons on the company side is that you have duplications because the same user interface expertise is in that the multiple teams. So. Hierarchy is okay in some cases, but you lose in terms of the optimization of the resources. The Matrix organization on the other end. It's great for optimizing resources, but the very end that each unit will become, it will serve multiple teams, meaning each team does not have a full control of all the components of the project, but they need to rely on an external to the team, internal or the company part for a function. One of the advantages of the Mavericks organization is that at the certain point you can outsource some of the components, meaning the you have a more, let's say, cluster of competencies than then in a second round out of a reorganization, I'd say you were a hierarchical. You move to mavericks. But people look. We'd say in competencies that we stay within the companies. And the certain point, you may decide that some of the functions are not core functions for your organization, and at that point you take them out. So I was working in a telecommunication company and in a certain point they outsource a good portion of the information technology because they decided that was not strategic. So they pretty much kept the the network and the marketing not so sure that they would be a good choice because I.T. is developing software that is developing the real differentiation elements for your company and outsourcing, that is, cannot be easy. And now that issue is also. I mean, when you outsource something, you want to reduce the costs, meaning the vendor providing the service will have a very strict policy of what they can do for you. If you are for anything cut. It is outside of the scope of work or they will charge you more. So at the very end you may lose control on the budget. So I mean, you don't need to go all the way to outsource everything that is outsourced by the Mavericks organization. That could be a preliminary step toward not the outsourcing. That's another example. When you develop software, you may want to have teams with high specialization in one particular area. So you always have a project manager setting the time, but then you have the different competencies. A little bit different is a. The support team or customer care. So the structure of a customer care is pretty much the same in many organizations, so not all and is in that honor on three levels. So the first level of the customer level one is basically the customer is calling and you have someone or something or you can have a both doing that will provide answers based on a script or a flowchart. So that's basically what you have at level one. Level one, no, the three levels is the one with the lowest cost because you can have an automatic system, you can have a people with no particular training. But the answer is that this level one can provide that may not be enough. So if they are not enough, then at that point that you go to the next level, the level two, where you have a people in the team that are with a better training and they are trained to functionally address the problems. So they don't know the code, but they know of functionally how the code is working. So when you go to level two, you have more course meaning. If you measure her the course by the time spent on each case, multiply that by the cost of the resort itself. Then that level to add will have more time and resources that will cost more. So. But they can see the meaning. They may not be able to address the problems that the user may have at the point that you go to level three, where you have a people trained on the software knowing the software and they are able to check if there is a bug, if there is something that was supposed to work in a way but is not working the way supposed to be. So obviously at that point, it will require more time and resources that are more expensive. So obviously you want to have up in level one as much powerful as possible, but that's basically the way it is generally organized. When you set up. Um, software development team. Uh, you go through the recruiting phase that can be intense sometimes. So you want to check, obviously the technical skills, but you want to check also the compatibility of the candidate with the rest of the team and with the culture of the company. I always use an example of the same people that it was mentioning the experience in central Florida. I will. I'm hired. I mean, I set up the hiring process looking for hiring. The team leader of the developers and I went through several candidates. At a certain point, the one candidate was my favorite. We had other people doing interviews with the candidate and then we hired him. From the technical standpoint, he checked all the boxes, but then at a certain point, his starting asking questions like all the people in the team are wearing shorts. I don't feel comfortable wearing shorts. I wear khakis. Is this a problem? I said, you know, a lot at Central Florida, people tend to do that. Where would you feel more comfortable with? Then the second time was a. I came to the office very early, 630, 7:00, but then I want to leave. 330 4:00. But the rest of the team is coming late. The 930, even , leaving late in the evening. Can we fix that? I said, you know what? Again, you are the team leader. You are supposed to be the first to come and the last to leave. So you want to be as much as possible. You want to spend as much as possible time with your people. So all of this is to say that you have the technical skills and what we call the soft skills. So in this case, this individual was not really compatible with the way we were working. So fortunately, we had a probation period. We used that period to replace this candidate with another one. So again, we had, I don't know, probably a total of five or six interviews, but we were not able to spot it. So right now it is a see it sequence. We are hiding faculty and we are having the same issue, I think, by the way, researchers and are having the same issues. So the technical aspects are important, but not the only important factor. So when you set up a theme. It's like a life cycle. So you have the formation of the team and the development of the team and the maintenance of the team. It's basically, again, like a life cycle in software development or in any product development. Each one of those faces has issues as a critical element. So when you create the team, you need to be sure that you again check not only the technical skills, but also the compatibility, the social skills that the individual needs to have. And when you work in a consulting company, I worked in consulting for quite a while. You may have a lot of projects coming and going and you have teams that you need to create each time. Sometimes, Sir is a small team. Some other times, depending on the size of the contractor, there can be a large team. So at that point, the time can be from few months to several years. I mean, if he's serious, you will have the same problem over and over because again, people can leave the company for any reason and replacements. They contract with the client can change. And you need to adapt somehow. When you create the team in particular, if you're working for a consulting company, you want to be sure that you maximize your revenues or your networks and you maximize the quality that you are providing. So you want to have a people with the right technical skills at the lowest cost possible with the maximum compatibility within the team. It can be a difficult problem. So some of the company, KPMG, is one that developed a software sort of a creating the teams. I mean, keep in mind that if you are a small consulting company, that's a minor problem. But if you are a large consulting company, international, sometimes you have team members that are coming from different countries because there are skills that are very specific and you don't have many of those in all the places across your what we call manage. So developing a software for that information may be a good investment that. We mentioned soft skills. That is something really important to keep in mind, that there are other elements that that on the soft skills side, but not probably a skill. So you need to read the candidate. You need to understand what are their personal ambitions and let's say longer term goals. And you want to be sure that those goals are aligned with what you can offer in that particular case. We have case it can be a general higher in the company or hiring someone parttime on one particular project. So let me keep all of that. So I really hope that this gave you a little bit of insight on working in teams. Again, the assignments that we are doing in this course are all individuals. The reason why I use individual assignments instead of team assignments, that seems to be more in line with what I'm talking. Today's is because I want to be sure that each one of you as exactly a good knowledge of what we are using in this course, meaning Python and the use of Python for some verticals or applications. But generally speaking, when you develop software, you don't develop software by yourself unless it is something as more or less specific and. And amendments apart from special cases doesn't really happen. Okay, so let's move on and let's go now. It's seven on seven and let's go to. The second part of the lecture that is a on Python that we will talk about the input output dictionaries and tuples. So two different topics. One, handling files and the other other types of variables that we are going to use in our coding. So, uh, unless you do something very specific, unless you ask everything to be type the by the user that you need to read. Five. So you need to take that data from the outside world, bring it to your script, process it, and then generate some output. So that's a normal way of doing things. Defiance that we will use the most. That would be text files or a comma separated value of the files. They are both, in essence, text files with the different skills we find so it can be handled by Excel or a similar province. So they are intrinsically matter that exceeds while the text files. I mean, that proper text file is like what you have in your screen that could be in a file and that would be at the five. So where each line is a record. So when you start working with the file with Python, you start with a statement that is open. So the open function in Python is the one creating and handle a pointer to the file that is in your computer. That's an example of a text file. So you have each line that is a recorder in the file. And each time you read one line, you will read, for example, the first line is this from Steven, whatever, that 2008. And the last one will be this early. So each one is a line when you read it. When you open, if I let the statement that open, that is required. So you create a variable that will contain the pointer to it to define. So we call it handle. So is actually a pointer and the statement that Auburn has a two parameter. One is the name of the file. The name of the file can be a variable or can be a string. And then the mode, meaning how you want to use the file can be for reading, for writing. So if it's a reading, it's our writing is w surprise. And if you are reading the file you don't need to specify. Ah, because the default for the reading for open is reading. But if you open for writing, then you need to specify that you. So that's an example you have open. This is the FILA. That was something similar to what you saw in the previous slide. You printed that what you get is really this obscure value. That is a pointer to the file. Other example, you have a text file with the three lions, so the quick brown and so on, a blank and goodbye. So you open the file with R, but again, this is redundant. You can remove it. Then I start a counter. I initialize the counter to zero and then insert the loop. So loop on the file on the content of the file. So the variable that I use in the loop is called the text line. Anything that handles in the pointer that I created in the first line. So the first iteration, it will have the first line and I'm increasing the counter by one and then I print the value of the counter and the line go, meaning the first round the line account that would be one, anybody would be the first line, then the second one will be a blank and third would be goodbye. Again, power is optional. You may or may not use it, and the handle is essential for the process. That's another example. We used in the previous example, the counter. That's a repetition. You can read the whole file. So you open the file again. In this case, I didn't specify the R, I mean I mean the mode, the meaning of the mode is R. Then I read the with the statement, the file, read the entire file into a single variable. And then if I bring Lang asking for the length of the content of the variable, it will be the number of lines. And then if I want to have the fourth part of the fourth, I mean the entire variable, meaning the first line I can use the partitioning method that we saw in the previous class. Another example. You have a bunch of lines. So this is a. Collection of tweets that I downloaded a few years ago. And that's an example of the of the user. So in this case, I opened the file that saw the file again, this one. So the goal was to counter or to have a list of all of the elements, starting with an aspect. So I initialize the list of elements with Ashtanga to zero. Then I loop into the file. I split the file to get a list. So again, just as a reminder, I am reading the entire string, entire line as a string. And then when split, I split the string into a list where the elements are the words. Because I didn't specify anything here. Meaning the criteria for splitting the variable. The string is this piece. If there is a space will be two different elements of the string. So at this point I have a list. So the list will contain all the words that you have in each line will line a good time. Then I set a loop within the list of words and I'm asking if the word starts with Ashtanga. If is starting with the hashtag, I will append the word to the list of stocks that they created as empty. When I finish this loop, I will analyze the entire line. When I finish the outer loop, I will complete analyzing the entire file. And then I will print the first ten. So in a loop, I could just printed the way it is, but it wouldn't look nice. And that's what you got. That's an example of application opening and analyzing content. We know. Continue. That is a way to do nothing and go back to the top of the loop. We can use if F in or not in. Again for selecting portion of the file or I'm in favor of the line within the file. Splitter. We mentioned that if you have a bad Ebola like this one. So that's what entities are missing. It's just to the code. So you split that by exclamation mark, meaning the string will be transformed into a list where each element is extracted from the original string based on the character, exclamation mark. So you have the first one, the first element, this would be Hello, no exclamation mark, because this is the element that is defining the separation. It's this one. Then the second will be this piece up to the op called this. And then the last one. If you don't specify anything, it will be split by space, meaning each element will be a word, keeping in mind that you are keeping all the punctuation that you can have in it. Because I mean, unless you remove it before doing it, the python will not know what the punctuation can be. Combining reading skills wi fi and a split the. So you open. You have this file with name and nationality. Then you open the file. You print nationalities. Then you start the looper into the file and you print the. I mean, you take the string, you split the string creating a list, and you take the second element. So because the elements are separated by comma in split, I specify comma. So karma is the character that I'm using to split the original string. And then I take the second element that is not number one. So printing nationalities at the first round is a joint American splitting, taking America. And then I have a space because even if we do not see it after each character, after each line, you have a special character that is new line trailing and she is the is a text file with the special character new line. I'm telling whatever is the area code that I'm using that you need to go to the next line. So you are just like eating right up. So but that's a character meaning when you go that next hydration that you will have a space. So if you want to remove it, then you need to use another functional strip. Strip is basically eliminating blanks and special characters left and right. If you do strip are it will be from the right. The strip l will be from the left. If you don't say anything. Just strip will take those characters out, both left and right. So if you do that, then you will remove the spaces that you have between the lines. Writing, If I look, is kind of similar. So you have the open with the W telling a side that in this case you are writing a file, not reading your file. I mean, one could be the default. You cannot not have two defaults. The default is R, meaning if you want to write, you need to write to add the W as a parameter. You read the first line. You want to add the new line because you want to keep it. Then if you want a separation, you would write a new line party. And then the third line, and that's what you have. And then you close it with Python, a two point X clause that was essentially after white writing. If you didn't have the the closer the actual right statement will never happen with python three is not essential. Other genera use it. Um. Let me skip that and then you have some information on these 3.6. If you replace 3.6 with whatever is the version that you are using, you will go there. If you water on other types of variables. We will talk about dictionaries and tuples. So dictionaries are a sort of data structure is a sort of archaic data structure in Python. We saw Lisa. So Lisa, again, defined by the square brackets. And inside you have elements that are separated by a comma. Elements can be strings can be other variables of any kind, including list. So you can have a list of these. I think we saw an example in the previous class. You start from zero to the end, meaning if you want to get to the third element, you will address it as a two. So two will be C. So the elements are ordered, meaning they will keep the order you use to create them unless you change it in a sort of explicit way. Again inside, you can have all possible types of elements. That's another example. These are mutable. Meaning you can change that. And in this case is a string. You have a banana. You cannot replace values. I think you final three. You could. And we mentioned that that you can change from upper to lower. We saw some of the functions last time in the list. You can change elements. In this case, I replaced the third element that was 26 to 28. You can get the length of the length of the list using land. You can create a list giving of numbers and giving the range, but you can specify the starting, the number of steps, how you want the numbers to be separated and what is the end. Or you can just do like in this case, four and you will have four elements starting from zero. There are quite a lot of additional functions that are built in in Python. So we may learn. You have Max. I mean some that they have the meaning that you can get. You can split again. You can split the variables pretty much any time, including strings. We mention that. We mention a little bit more of the split function. And now we have two bills, that the two bills are pretty much similar to this with two main differences. One, they have regular brackets instead of square brackets. And the second is they are immutable, meaning you cannot change the value of one of the elements. So once you create them, they will stay the way they are. So in this case, I'm addressing the second element. That would be if I tried to replace that B with an X, with an x, I will get an error. So Y and when you want to use tuples, so you want to use tuples. If you want to be sure that a value will stay the same across the entire program. So you do not want accidental change of the content. That's why you use a tuples. How many times you are going to use tuples? Not many, probably. But just keep in mind that that's an option. Dictionaries. As I was mentioning before, they are sort of a data structure, so they have a key and a valuable. So in this case you have the variable, the dictionary population, the so dictionaries are in brackets and you have a key in this case is USA, 1: Italy, Japan, and you have the number and a column separating the key from the value and the number is the value for that key. 1: So in this case is the population of a I mean is not exactly updated U.S.A. 380,000,380. 1: Now Italy 59, Japan 127. 1: So if you want to know one element, for example, Italy, you just do population Italy and you will get 59. 1: So where we go here, like in this case do print populations in Japan and you will get you will get 127. 1: You can get the keys into a list. 1: So population keys should not specify anything. 1: You have all of them. You can do the same with the values. 1: So you can transform a dictionary into two lists. 1: One list will be the list of keys. 1: One is that list of values. The dictionaries are immutable and they are not ordered. 1: Meaning if you have a. If you have the same dictionary you printed, not necessarily. 1: You have the same order that you used in the infinite wisdom. 1: The creators of Python, they thought that you don't need to keep the order because at the very end that you have a key and whatever the element is, 1: you will pick it using the key. 1: So if you do, if you create the list and you print in the most likely you will not get the same order that you used to create it. 1: So if you want to add an element, you can do it. 1: So you do population's name, other dictionary in the square brackets, the key and the value. 1: And then when you printed the I mean, if we go anywhere, not necessarily at the end, if it was a list and you use append that it will go at the end. 1: In dictionaries you don't have an upper end and there is nothing like a pen. 1: Loops so you can loop into dictionaries. 1: So. You can loop by element, by keys. 1: You can retrieve elements using the key. 1: That's another exemplar. I was that. 1: So let's say you have a text file like this one. 1: So we will read that one line at a time, split each line into words and we want to add one to the value. 1: I mean that it's counting the number of times each word is appearing in the text. 1: So you have one word. If the word is already in your dictionary, you will add one to the counter. 1: If the word is not in the dictionary, you will create the entry for that word. 1: So. Keep this in mind because you will use it. 1: In the in-class assignment. So you have a file, you are opening the file. 1: So again, is related to this file. 1: In re again, you don't need to specify r you initialize the to empty a dictionary and you start reading the file. 1: So one line at a time. 1: Then what you do is basically strip the line left and right by special characters and blank spaces, and then you split it by space. 1: So you will get. This line in a list where the first element will be in the last element would be the and with all the others. 1: Then once you have the list, you start looping into the list. 1: So you try if what counter there is a dictionary of that particular word, meaning you are using that particular word as a key in the dictionary. 1: The first round up. The dictionary is empty, meaning when you try to address that particular element, you will get an error because there is no. 1: Let's say you are in. In. There is no in. 1: In the dictionary. So you will get an error. 1: You will go here and you will create the first entry. 1: So in the first round in that particular file, the first line that will be in fill, whatever is the end, and then the first word will be in. 1: You will get an error. When you try to execute this statement, you will go into except the new will create a new entry. 1: That will be an element where the key is that particular word in in this case and the value will be one. 1: When you go to the second word, then the second word may be there, maybe not. 1: The certain point that would be awarded there was already there. 1: And then at that point that you will add one to the counter. 1: So when you finish board the loops you will have. 1: A dictionary with all the unique words and the number of times that they appear in the text. 1: And then you create a list of the first ten, and then you loop it and printed. 1: And that's what you got. But in this case, we have key and value. 1: And that's what you have. Um, counting objects. 1: Again, there's something that. 1: I don't want to spend too much time because it's kind of similar to the previous one. 1: Just briefly. So. You know, is that replacing it with that one? 1: Okay. So that's basically it. 1: Let me see if you have questions. 1: Just have one quick question on the homework. Sure, yeah. 1: For the procedure, it says to skip a line. 1: I assume that we. Are we counting the first line in the files as the total line count, or are we ignoring it? 1: Well, there are two ways that you can do that. 1: One is to use a counter that is starting, let's say, mean that you normally initialize counter to zero. 1: In this case, if you initialize the counter to minus one, then at that point the first line will not be counted. 1: And that's one way, the other way. You can read the out of the loop, the first line, and then in the loop you will start reading after the first line. 1: So those are two options that you may have down the road. 1: We will use a different way to read the fine into variables. 1: So we will talk next week about the. Data structures and in particular, pandas with those structures. 1: You can just read the keeping the header, but in this case you don't have that meaning. 1: You need to do either one. So either you use an account that you initialize the account that in a different way or you'll be out of the loop. 1: The first line. Okay. 1: So if we read or exclude the first line, it's still correct then, right? 1: Yeah. Okay. Just making sure. I was trying to understand it fully, but to avoid any point loss. 1: Thank you. Sure. Sure. Absolutely. Okay. 1: So it's, uh, 730. 1: 830. Um. It led us to 15 minutes of in-class exercise. 1: So let me go. Let me share the screen. 1: And let me go here. So the in-class exercise is a two portion one. 1: Read the file. That is, name the names. 1: The C containing a list of names. Count how many unique names that are in the file and print the results on the screen. 1: Part two. You want to print the name that is use the the most in the input file. 1: Keep in mind what we did. 1: What we did here. We'd be pretty much the same. 1: I mean, just consider that those prints are supposed to have parentheses and they have not in this example, 1: but all the rest would be pretty much the same. Okay. 1: So let me. Publish. 1: This one, right? Yep. Let me publish that in class an exercise. 1: With the file. Let me stop sharing. 1: Create breakout rooms. So we have three breakout rooms. 1: I'm opening it is a 740. 1: You have about 10 minutes just to go into it. 1: So I don't expect you to finish in 10 minutes, but you will have a sense of it. 1: Okay. All the rooms are open. 1: I'm posing the recording. Let's resume recording. 1: So welcome back. Anyone want to share what you did? 1: Again. You don't have to, but there is no judgment and there is no grading or nothing. 1: Okay. If not, let me. 1: Go here and let me go to. 1: Resolutions. It's pretty much in line with what you saw in one of the slides. 1: So I initialize the counter as I'm in dictionaries to zero. 1: I initialize to a mean to empty dictionary to zero number or max frequency and the name for the word with the maximum frequency to a blank, please. 1: Blank. Valuable. 1: I think so. So and then there's the loop again. 1: There are several different ways to go into a fight, 1: and so that's another one that's so opening the file and the loop will continue till we reach the end of the file. 1: So reading a line and then in the line that other loop stripping left and right. 1: Then if the line meaning that particular name is in the dictionary, 1: then I will add the one to the value and the dictionary meaning into the key, so that I will add the one to the value of that particular key. 1: And then I'm checking if the value is greater or equal to maximal frequency. 1: If yes, I will replace that value with the new value and I will add the name as the name with maximum frequency. 1: Otherwise, I will initialize the value for that particular key to one. 1: And then I will keep reading. But when I finish, I will get the entire fine process. 1: And in those variables, the values that you will see in a moment. 1: So there's the value of the dictionary. 1: So you have the names and the occurrence. 1: And then I used those two variables to get those two numbers. 1: So those there's 54 is coming out of the number max three and this LE is coming from the name most frequencies. 1: And it's basically. Okay. 1: So for it's 757. 1: I don't want to keep you for too long. So. 1: Next assignment is a little bit more complex. 1: So there are a few parts to the assignment. 1: The whole assignment would be about those two files. 1: So you have both the sides are related to the combined program and they are from different periods. 1: So one in January, February 2016. 1: And the other is April May 2016. So the structure is the same year, the duration, the day time, sort of time and a bunch of other information. 1: So those are the files are relatively big, but you don't care much about the length because it will be either ten or 10,000. 1: That million will be different, but the number is not going to affect much. 1: So the first one, you will read the first file that is seeded by New York City City by January, February 2016. 1: And you will do some processing. So before we move on, I want to drive your attention to one point. 1: So if you look at in this case, that's the this is in classic set, say three meaning, is this guy here? 1: And the file has to be in the same directory. 1: So when the file is in the same directory, I don't need to specify the path for the file. 1: Specifying the path for the file is not what you want. 1: Because if you change a computer, meaning I don't know, 1: you go to from a mac to a window so different structure that you move the file the 1: program for one rectory to another one and then you need to do the same for the contents. 1: I mean, generally speaking, I strongly encourage you to have files, data files and program files in the same directory. 1: So I'm not going to take points off for the time being, but down the road I may do it. 1: You don't want to use data structure management libraries like pandas. 1: Not for now. You will use in in two classes, but not now. 1: So you will basically go through the file and you will keep the count or number of lines to the count or number of lines with the customer. 1: But just to be sure, you have a user type that can be a subscriber or customer. 1: So you want to count the number of customers and the number of subscribers and each one will have a separate count. 1: So N0 and zero or the number of lines and one or the number of customers and two for the number of subscribers. 1: And you will print the last five lines. Then after processing, you want to calculate the percentage of customers on the totals. 1: So you have the number of customers who have the total. You do the you divide the two and you will get by somehow. 1: Then you will get the percentage. And then you will print the file as the end zero lines of those and one as a user 1: type as customer end to end user type subscriber customer are the 1% of the total. 1: But two is pretty much the same for the other five. So you will calculate, I mean, we already use from end zero two and two, 1: two and three will be the counter for the number of lines, the second one and four for the customer and five for the subscribers. 1: Same thing. You calculate the percentage. 1: So this one was Z one. 1: This is Z two. Part three. 1: You want to compare those results? So you want to check what is the five that is bigger? 1: What is the file with more customers or non subscribers? 1: And then you want to write one page or interpretation. 1: So what is an interpretation? Its narrative is plain English. 1: You have one file that is from winter, one that is for spring. 1: There must be differences. So are there more customers during the winter than in the spring? 1: So look at the results that you extract, that you get, the mean you get from the analysis, 1: meaning at those end zero two and five and use those to create the scoring. 1: I mean, to write those few lines. 1: These one page of interpretation again is a narrative describing, explaining in plain English the results of your scripts. 1: Keep in mind that this sort of storytelling will be something that we will use a lot in most of the remaining assignments. 1: We are not into writing code for writing code. 1: We are on writing code to get insights from the data on the domain that we are analyzing. 1: We are an engineer in management. So we want to use the tools to take a more informed decision so we can decide, okay. 1: During the winter you have more subscribers and let's do a marketing campaign to. 1: Incentivize non subscribers to user bicycle. 1: I'm assuming that will have an effect in New York with the freezing temperatures anyway. 1: But I mean it just to use the numbers to take decisions. 1: But you want to have a narrative describing what are the findings. 1: So you want to submit the three parts in I mean, the initial parts with the analysis of the first one, 1: the second with the analysis of the second phyla, 1: and then the comparison in one single piece, we file with the interpretation in a separate doc for PDF files. 1: And that's basically it. So let me make sure that everything has been published. 1: So I published that in class. 1: Solution. And you have the assignment. 1: So you should be good to go. Okay. 1: Questions? Quick question, Vanessa. 1: Yes, yes. That's a four part one. N0, if N0 equals N1 plus N2, then there's a good probability that we did it correctly. 1: Is that right? Yes. I mean, keep in mind that it is possible that there are some blanks. 1: Note some errors in the input or the file. 1: But assuming there's no blanks, assuming there's no blanks, then means that there is no blank. 1: Yes. Okay. Got it. Thank you. Sure. Other questions. 1: All right. So thank you for staying with me. 1: Still ahead on six. See you next week. 1: If you have questions again, send me or you an email and we press that.
SYSTEMS
ENGINEERING

RESEARCH CENTER

Technology Forecast

 

Sponsor: DASD(SE)

by
Dario Borelli, Denisse Martinez, 

clipizzi@stevens.edu

February 2020

aveTE Me Introduction

ENGINEERING

Technology Forecasting System

Future cannot be predicted, but reasonable evolutions can be.
We “predict” the next state of a technology, given:

e historical evolution

e environment (industries, applications)

Methods explored:

1) Based on Linear Algebra - dynamic variation of n-dimensional distributed
representations

2) Based on Graph Theory - neural networks for dynamic graphs

Dynamic variation of n-dimensional
ENGINEELING distributed representations

RESEARCH CENTER

 Dynamic variation of meaning is often referred to diachronic variation, that is how a
something — like language — evolve in time

 Technologies and market segments are represented by words in papers, news and
patents over the years, and they can leverage on the concept of diachronic variation

 We use a computational and visual analysis of changes in technology related
semantic elements over the years

 

Dynamic variation of n-dimensional
ENGINEEHING distributed representations


The current stage of the prototype, can predict the “neighborhoods
where a technology can be in the future


 

SYSTEMS Graph neural networks for dynamic graphs

ENGINEERING

RESEARCH CENTER

Capture system’s state through time

Time | Time 2 Time 3

Using the same numerical representation of text we use for the room theory, we
create graphs where the nodes are the technologies and the edges are their
relativeness/proximity. We have a numerical representation/graph per each time
interval. All together, they are a dynamic graph representing the evolution of
technologies up to the current time

Using predictive/ML algorithms, we can predict the new interactions and
relevance of the nodes, meaning the new patterns for technologies

ee @e-e
 Graph neural networks for dynamic graphs



      
   
 

   

   
   

 
Graph evolution 2009 - 2018



SYSTEMS What iS next?

ENGINEERING

RESEARCH CENTER

Dynamic variation of n-dimensional distributed representations
1. Increase training dataset — Increase historic data points for prediction
2. Increase training of word2vecs

3. Determine "predicted spaces” for technologies

Graph neural networks for dynamic graphs

1. Increase training dataset — Better representation of the system
Network prediction
Prediction validation

Visualization design for dense graphs

of. & Wf

Test different networks configurations

cee @ee

a“eTEa References

ENGINEERING

RESEARCH CENTER

e Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim
Kaler, Tao B. Schardl, and Charles E. Leiserson. EvolveGCN: Evolving Graph Convolutional Networks
for Dynamic Graphs. AAAI 2020.

e EvoNet: A Neural Network for predicting the Evolution of Dynamic Graphs. ICLR 2020.

e Hamilton, William L., Jure Leskovec, and Dan Jurafsky. "Diachronic word embeddings reveal statistical
laws of semantic change." arXiv preprint arXiv:1605.09096 (2016).

e Klingenberg, C. P. "Analyzing fluctuating asymmetry with geometric morphometrics: concepts, methods,
and applications. Symmetry 7: 843-934." (2015).

SYSTEMS
ENGINEERING

RESEARCH CENTER

   

as] STEVENS

INSTITUTE of TECHNOLOGY
THE INNOVATION UNIVERSITY®

Thank you!

 

fis STEVENS

lw INSTITUTE of TECHNOLOGY
is
i
i

Extracting Knowledge
from data

Repel



clipizzi@stevens.edu

SSE

 

What is “Data Mining”?


What is Data Mining? we

"... The process of discovering meaningful correlations, patterns and trends
by sifting through large amounts of data stored in repositories. Data mining
employs pattern recognition technologies, as well as statistical and
mathematical techniques...” (Gartner Group)

A process used by companies to turn raw data into useful information
(Investopedia)

",..the practice of searching through large amounts of computerized data
to find useful patterns or trends...” (Miriam-Webster dictionary)

"...(the analysis step of the "Knowledge Discovery and Data Mining"
process, or KDD)}, an interdisciplinary subfield of computer science, is the
computational process of discovering patterns in large data sets involving
methods aft the intersection of artificial intelligence, machine learning,
statistics, and database systems. The overall goal of the data mining
orocess Is to extract information from a data set and transform if into an
Understandable structure for further use...” (Wikipedia)

STEVENS INSTITUTE of TECHNOLOGY | 3

 

Why Data Mining now? &

¢ More intense competition
¢ Recognition of the value in data sources
¢ Availability of quality data

¢ The exponential increase in data processing and storage
capabilities and decrease in cost

STEVENS INSTITUTE of TECHNOLOGY | 4

 

 

How Data Mining Works le

¢ DM extract patterns trom data

— Pattern: a mathematical relationshio among data
items

¢ Types of patterns
— Association
— Prediction
— Cluster (segmentation)
— Sequential (or time series) relationships

STEVENS INSTITUTE of TECHNOLOGY | 5

 

 

of
Data Mining: Confluence of Multiple ©&
Disciplines


STEVENS INSTITUTE of TECHNOLOGY | 6

Challenges of Data Mining &

Scalability

Dimensionality

Complex and Heterogeneous Data
Data Quality

Data Ownership and Distribution
Privacy Preservation

Streaming Data

STEVENS INSTITUTE of TECHNOLOGY | 7

 

Data Mining vs. Statistical Analysis

Statistical Analysis:

ll-suited for Nominal and Structured Data Types
Completely data driven - incorporation of domain knowledge not possible
Interoretation of results is difficult and daunting
Requires expert user guidance

Data Mining:

Large Data sets

Efficiency of Algorithms Is important

Scalability of Algorithms is important

Real World Data

Lots of Missing Values

Pre-existing data - not user generated

Data not static - prone to updates

Efficient methods for data retrieval available for use

 

STEVENS INSTITUTE of TECHNOLOGY | 8

Data growth

  
    
 

The digital tools we are using every day are
creating data from everything we do at an
unprecedented rate: every day, 2.5 quintillion
(1018) bytes of data are created and 90% of the
data in the world today was created within the
past two years.

Data piles up quickly in business applications, and
compound annual data growth threatens to bury
today’s application infrastructure. A senior
executive at a major bank remarked, “There are
only 3 things certain in life: death, taxes, and data
growth” [from Wired]

Because so much of the population is generating
it, Big Data can provide potentially useful
information for our lives and businesses

Mining the Big Data requires a combination of
tools, ability to represent knowledge and
domain-specific expertise

STEVENS INSTITUTE of TECHNOLOGY | 9

 

The “datafication” 

 It is happening as result of the digital transformation
process that is creating a new kind of economy based
on the “datafication” of virtually any aspect of human
social, political and economic activity as a result of
the information generated by the digitally connected
individuals, companies, institutions and machines

STEVENS INSTITUTE of TECHNOLOGY | 10

 

 

A

Why data is relevant 

 Data is the core of any ML/AI
algorithm
It must be supplied in the form that
mens aie ng

The main function of ML/AI
algorithms is to unlock the
concealed information/knowledge



 

 AI/ML is an actuator of data-driven strategies, rooted on data and on the whole process
to make it usable, from data collection to exploration and preprocessing

ee
STEVENS INSTITUTE of TECHNOLOGY | 11

 

 

What to do with Data Science $.


 
STEVENS INSTITUTE of TECHNOLOGY | 12

 

 

Business Environment S

 

STEVENS INSTITUTE of TECHNOLOGY | 13

 



Using Big Data and Social Media - we
Examples

* Human Resources management. Organizations and teams are continuously evolving, because of
changing market needs, merge/acquisition, intense competition. Human capital has to be
rearranged based on many factors, including internal and external data/information on the single
individuals. Data such as demographics, emails, credit card usage, web surfing pattern, past
experiences, “social” behavior: these and more all together to create the right mix

* Energy saving/green solutions. We all know we should need more energy and more efficient
devices but in reality we need to deal with what we have, at least for a while. There are many data
we could leverage on the better use the devices and the networks/grids we have. From regular
data we can easily collect - such as usage, energy/hours, location - to external data - such as
demographic population profile and user behavior analysis - to data collected by ad-hoc sensors

* Marketing & Sales. Nothing unheard in this area: some of the best examples today in predictive
analytics and behavioral analysis are from this vertical. What we do different is the way we deliver
solutions: on-demand or in-a-box, as well as the usual friendly traditional ways

° Transportation. Think it as the next step from popular logistics solutions. We focus on all the more
data intensive components: from soeed optimization for railway transportation to on-board
personnel optimization

° Security. This is an other relatively common vertical for Business Intelligence solutions, from
combating terrorism solutions. We target small and medium business, delivering same approach the
larger companies have. Working with a B2B2C approach, we focus on security as a service for
consumers

¢* JTelecommunication. Putting together traffic information, network Usage, Current campaigns,
location based information, social behavior to pinpoint one-to-one offers

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 14

 

cf
Data Mining: On What Kind of Data? ©&

Relational databases

Data warehouses

Transactional databases

Advanced DB and information repositories
eObject-oriented and object-relational databases
«Spatial databases
*Time-series data and temporal data
eText databases and multimedia databases
*Heterogeneous and legacy databases

“Web

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 15

 

A Dataset 

Rows: Datd/points/instances/examples/samples/records
Columns: Features/attributes/dimensions/independent
variables/covariates/predictors

Variables: Target/outcome/response/label/dependent-
independent

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 16

 

a on

What is Data? 

Collection of data objects and
their attributes
• An attribute is a property or
characteristic of an object
•Examples: eye color of a person,
temperature, etc.
•Attribute is also known as variable,
field, characteristic, or feature
• A collection of attributes
describe an object
•Object is also known as record,
point, case, sample, entity, or
instance
STEVENS INSTITUTE of TECHNOLOGY | 17

 

Attribute Values 

Attribute values are numbers or symbols assigned to an
attribute

Distinction between attributes and attribute values

‘Same attribute can be mapped to different attribute values
Example: weight can be measured in pounds or Kg

Different attributes can be mapped fo the same set
of values
Example: Attribute values for ID and age are integers
But properties of attribute values can be different
«ID has no limit but age has a maximum and minimum value

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 18

 

‘ ry

Types of Attributes &

There are different types of attributes

Categorical (also called the qualitative type}

Examples: gender, eye color, genre
Numerical (can also be qualitative, allows for rank order by which data can be
sorted)

Examples: rankings (e.g., taste of potato chips on a scale from 1-10), grades, height in
{tall, medium, short}

Interval (measures the degree of difference between items, but not the ratio
between them. A interval scale doesn’t possesses a meaningful, unique and non-
arbitrary zero value)

Examples: calendar dates, latitude/longitude
Ratio (measurement is the estimation of the ratio between a magnitude of a
continuous quantity and a unit magnitude of the same kind. A ratio scale
possesses a meaningful, unique and non-arbitrary zero value)

Examples: length, time, mass

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 19

‘ on

Document Data we

Each document becomes a term' vector,

each term is a component (attribute) of the vector,
the value of each component is the number of times the corresponding term occurs in the

document

 
      

 
 

 

 

STEVENS INSTITUTE of TECHNOLOGY | 20

 

Transaction Data we

A special type of record data, where

each record (transaction) involves a set of items.

For example, consider a grocery store. The set of products purchased by a customer
during one shopping trip constitute a transaction, while the individual products that
were purchased are the items

TID Items

Bread, Coke, Milk

Beer, Coke, Diaper, Milk

4 Beer, Bread, Diaper, Milk
Coke, Diaper, Milk

 

STEVENS INSTITUTE of TECHNOLOGY | 21

 

 

Graph Data e

This is graph for the 2" presidential debate (Oct. 16-2012), showing who the most influencers are in the
conversation. Nodes/Vertices are users

It reflects the “popularity” of the users. For example, the big aggregation is around a user (“politifact’”),
with 910 re-tweets, with the second largest being “Eat_A_Brick” with 115 (south-east of politifact)

  



STEVENS INSTITUTE of TECHNOLOGY | 22

Data Mining Methodologies we

Several non formal methodologies available. Two more formally defined are:
*SEMMA. It is a list of sequential steps developed by SAS Institute Inc

*CRISP-DM. Polls conducted in 2002, 2004, and 2007 show that it is the leading
methodology used by data miners [Gregory Piatetsky-Shapiro — WDD Nuggets]



STEVENS INSTITUTE of TECHNOLOGY | 23

 

Phases in CRISP-DM we

 
 

¢ A de facto industry standard for
op pets ea data mining
¢ Created between 1997-1999 by

DaimlerChrysler, SPSS and NCR

¢« Acronym stands for Cross-

   
   

Industry Standard Process for
Data Mining

¢ Consists of 6 phases, intended as
a cyclical process

¢ Not all phases are necessary in
every analysis

 
 


STEVENS INSTITUTE of TECHNOLOGY | 24

 

 

Focus on Data: Data Processing Flow ©&


¢ Types of Data Quality Problems:
Ambiguity
Uncertainty
Erroneous data values
Missing Values

Duplication
etc

 

 

 

 

 

 

STEVENS INSTITUTE of TECHNOLOGY | 25

 

Forms of data preprocessing



STEVENS INSTITUTE of TECHNOLOGY | 26


Sourcing Data: copyright-relevant Y


STEVENS INSTITUTE of TECHNOLOGY | 28


What is open data
• Data freely available to everyone to use
• Can be republish without restrictions from copyright, patents or
other mechanisms of control

What is open access (OA) to
scientific information

OA = online access at no charge to the user (& further distribution
and proper archiving)
to peer-reviewed scientific publications
to research data
Two main OA publishing business models
Gold OA: costs covered (e.g. by 'authors') immediate OA:
provided by publisher
Green OA: deposit of manuscripts immediate/delayed OA:
provided by author



STEVENS INSTITUTE of TECHNOLOGY | 29


Open Data - Federal Government,

USA

&
STEVENS INSTITUTE of TECHNOLOGY | 31

i

Mashups: Getting and Analyze Data lw
from the Web

¢ What is a “Mashup”?

— lItisa web page or web application that uses content from
more than one source to create a single new service
displayed in a single graphical interface. For example, a
user could combine the addresses and photographs of
their library branches with a Google map to create a map
mashup

— Commonly associated with web applications that facilitate
interactive information sharing, interoperability, user-
centered design and collaboration on the WWW

¢ Typical characteristics
— Web is used as a participation platform

— Users can run software applications entirely through a Web
browser

— Data and services can be easily combined to create
mashups

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 33

 

Mashing up content via API ie

© API stands for
" A pplications P rogramming I nterface”

° An API interface is used for the exchange
and further processing of content and
data. This allows various software and
hardware components to connect

   

 

 

 

 
• Housingmaps.com is a mashup created of
Craigslist and Google Maps
• The properties described in Craigslist are
placed on a map
 

 

 

 

STEVENS INSTITUTE of TECHNOLOGY | 34

 

APlIs to get Web Services iw

A Web Service Is the “service” provided by a server via
the API. Definitions:
- “a method of communication between two
electronic devices over the Web”
— From Wikipedia entry on “Web service”
- “a software system designed to support interoperable
machine-to-machine interaction over a network”
— From W3C definition

STEVENS INSTITUTE of TECHNOLOGY | 35

What is a Web Service we

¢ “a method of communication between two electronic devices over
the Web”

From Wikipedia entry on “Web service”

- “a software system designed to support interoperable machine-to-
machine interaction over a network”

From W3C definition

STEVENS INSTITUTE of TECHNOLOGY | 36

 

of
Technologies &

° Web Services are possible using Application Program Interfaces
(APIs), a way to exchange data between systems

¢ APIs are defined by the system providing the service and need to
be used to interact with the system

° APIs use:
HTTP as the basis
XML and JSON for data exchange

STEVENS INSTITUTE of TECHNOLOGY | 37

 

tts
Twitter Web Services - API we

° Twitter is social networking and microblogging service that
enables its users fo send and read messages known as tweets

- Tweets are text-based posts of up to 140 characters
displayed on the author's profile page and delivered fo the
author's subscribers who are known as followers

- Twitter has offered a comprehensive set of APIs fo access
core Twitter data: update timelines, status data and user
Information

e User sensitive data Is protected by the HTTP Basic
authentication mechanism

STEVENS INSTITUTE of TECHNOLOGY | 38

 

Twitter REST API - GET example -User &
Timeline

Method:

Description:

URL:

Formats:

HTTP Method:

Parameters:

ER )S—l3S lure
i

 

statuses user_timeline

Returns the 20 most recent statuses posted from the authenticating
user. It's also possible to request another user's timeline via the id
parameter.

http://api.twitter.com/1/statuses/user_timeline.format

xml, json
GET
id optional Specifies the ID or screen name of the user for whom to
return the user timeline.
since id optional Returns only statuses with an ID greater than (that is, more
_ recent than) the specified ID.
max id optional Returns only statuses with an ID less than (that is, older

than) or equal to the specified ID.

STEVENS INSTITUTE of TECHNOLOGY | 39

  
Hello. I mean, I know that there are several issues. 1:28   We are we are fixing it. Sorry. 1:36   Thank you, sir. I can appreciate it. I'm sure. I don't know why. 1:43   It shows me the homework that is. Like the one that's there, but not the one that's due. 1:47   Okay. Okay. So there must be something in setting up the old courts that is not working fine. 1:55   But we will fix it. Thank you. Sure. 2:05   Thank you for your patience. And I'm sorry about the inconvenience. 2:08   Okay. The 629. Let's give people another minute or so and then we will start. 2:19   So in the meantime, any particular issue? 2:36   I have a question about the previous homework. 2:46   Yeah. So I had difficulty coming up with that to end the loop. 2:50   So I was trying different things, but I couldn't get it to work. 2:58   Maybe you could explain. Yeah. Do you want to share it? 3:03   Because, I mean, I don't have it in front of me. 3:07   If you have it. I have it on a different computer. 3:12   Unfortunately so. All right. 3:16   Uh, okay. Y you will do the class assignment. 3:20   I will retrieve it, and we will discuss by the end of the class. 3:26   Okay. All right. Okay. 3:30   So it's 631. Today is February the seventh and this is 624. 3:34   So. Let me start with sharing the screen. 3:44   We are already recording. Let me share the screen. And let me goo here. 3:51   So minimize this. All right. 4:00   So. That was the previous week. 4:04   So module two was a software engineering and some coding in Python with the in-class assignment and the exit site zero one. 4:11   And let me go right away on. 4:26   The code. So. So the assignment was relatively easy. 4:33   So it was on. On converting a temperature in Celsius to a temperature up in Fahrenheit and. 4:43   I use the same loop that we used in previous exercises. 5:03   So the loop is a y looper, just as we did the previously input with the commenter. 5:08   What is the temperature in Celsius that you want to convert the type done to exit? 5:19   And then the name of the variable towards a celsius temperature. 5:25   Just to have a mnemonic. Variable. 5:31   Name of variable. If this value is done, then thanks for using the tool and that breaker would go out of the loop and the program will end. 5:34   It wouldn't be the same not to have the print statement here, but outside of the loop. 5:46   But again, it would be exactly the same. 5:54   Then I'm checking if the number if what the user typed is a number, if it is a false meaning, is no the number. 5:57   Then you go back to the beginning again. 6:11   That's not the only way to do it. We know that there is also the possibility to do in different ways. 6:15   We will go back and say again, just as a reminder, if is not numeric, continue meaning going back. 6:24   If it is numerical. Then I calculate the temperature in Fahrenheit using the formula and print it. 6:32   So let me go here for a second. 6:41   So just as a reminder there, there are two ways to get to test. 6:46   If a number if a variable is numeric or not, there are many ways, but that's one of the ways. 6:53   Two of the ways so one is using is digital and one is using try. 7:01   Except let me run this. 7:07   So if you type a number, an integer is digit, that is working fine. 7:13   If you type a floating point number like 11.22, then you would get the is now the number meaning that is digit. 7:20   That is not recognizing the number with decimal values. 7:33   The. Right except the is working in a different way. 7:42   Same number 11.22 and then is number because in this case what it's doing is basically trying to do the conversion into floating. 7:47   If we get an error, then it will print the number. 8:00   Let me run it again. So if I do a Q. 8:07   There's no number. If I do Q In the second case, it will be the same. 8:14   So again, with a try, except that I'm able to test the also the floating point numbers. 8:20   So numbers with decimals with the is number is digit. 8:30   I can not. So in this case, I use that is digit, but is clearly not the best way to do it. 8:36   But it works in particular if the number is integer otherwise without. 8:43   So if I run it, if the number is 37. 8:52   Then I have 98.6. 8:59   Pretty much periodically. It is an approximation and that obviously done. 9:03   What? Okay. So that's basically the. 9:12   Uh, exercise number one. 9:21   And let me share again. 9:25   And let me go back here. So for today, what we are planning will be again revising, reviewing the assignment, in particular the coding part. 9:31   Again, that was a relatively easy code, just I mean, that is the second assignment. 9:49   So the complexity will grow, but we will grow gradually now with big steps. 9:59   So we will talk about software testing.   We will give some examples.   We will talk about the errors, exceptions in Python.   Then we will go back to Python and we will talk about some data structures.   I will stop after that for a Q&A.   Then we will start the in-class exercise.   We will discuss the solution, and I will present the next assignment.   So that's basically the menu for the day. So.   At this point. Let me go to the power point and let me start with this one.   I'm not going to spend much time on the test driven development.   The main reason is because a true test driven development is something that   you would do when you have a quite complex program so that you would write.   But you need to be aware that generally speaking, the problem of testing a software is essential.   So there is no software that comes out of the developer right away with no error.   So you can assume that in a way you would in either there would be an error.   The point is how to use, how you spot it, how you determine what the areas where the error is and how to fix it.   So being able to start from the beginning with an approach that, that that can give you the possibility to test that all the branches,   that the logical branches you have in your code is really essential.   So you need to be sure that you define unit of testing per each one of the alternative sets that you have in your code.   In theory, you should define the testing strategy before doing the code, meaning you define the logical structure of your code.   That is pretty much a sort of a flowchart of the code.   And then you write the code. So you define the structure, you define how you are going to test it.   You develop the code, then you tested the and then you check.   Most of the time this is not what is happening.   Most of the time you do a sort of retrofitting.   You already have the code and you want to be sure that you will test all the possible alternatives, all the possible branches that are in your code.   Again, you want to be sure that testing is an essential part of your development.   When you test the you basically test the four carrots.   So you want to be sure that, one, there are no formal areas.   And secondly, that there are no logical areas.   So let me go briefly into. The types of that that you can get.   So there are three main types of area.   So one is a syntax error. So we know that in Python you have a.   Collins at the end of the conditional statements like if at the end you are call on a while through your column.   And you also know that after call on that you have an indentation.   If you know that if you if equal something, the equal something is not a single equal sign number is a double.   If you miss one of those, you will get a syntax ever.   Then there are execution errors.   Execution errors is when you do something that is not in line with what Python is allowing.   Like you want, you are adding a string and a number and you will get an error.   So you are breaking the the python rules.   So again, execution error is when you have something that is against the python rules.   Example, mixing data times. And then there are the more and is key difficult to detect the error out design add ons meaning the   program is not leaving any error or any transfer but is not generating the results that you expect.   So there is something wrong in the logic of the program.   So those types of error are more difficult to detect because I mean,   with the the previous to the interpreter Python will tell you there is a this particular type of Arora on the line whatever is the number of the line.   So you go there, you check it, you fix it. When is the sign the point?   Python is not telling you anything because from a python standpoint, everything is fine, but the program is not doing what it's supposed to do.   So at that point, a problem testing is what is really relevant.   You need to really understand that the logical structure of the program.   You need to understand that what are the alternatives that the program should do.   So let's go back here for a second. So in this case, for this program that you have, so you want to test the mean,   one of the four think if the program will exit when the user is typing done.   So the branch, if done, is something that you need to test to see if it's working and then if it's detecting the done,   if it's doing what it's supposed to do when the user is typing done that is axing exiting the program, then the second is testing.   If the program is testing for being numerical or the input.   And there's the second test that you need to do so.   And then if it's testing is generating the expected the message or result and there's the second test.   And then the loop.   So it's looping the proper way, meaning that once they have what they have, is it going back and asking me for another time pressure?   So those are the questions that you need to ask to create the testing strategy that   you need to have to be sure that you are testing all the branches in the code.   One of the ways that we normally do to be sure that we are checking all the boxes is to put some print statements in the key steps of your process.   So, like, I don't know.   If I mean in these cases or in painting something.   So in all the cases is printing something meaning is not necessary.   But if I didn't have. I mean that. If the requirement was only when it's done, just exit.   And adding a printer to say I'm passing through this branch is something that you want to happen.   Printing may not be just saying I was here, but can also be the type of variable or the value of the variable.   That is what I expected. So those kind of temporary pop up print statements are working for the debugging.   Once you fix it, you may want either to delete them or to make them comments because you are not totally sure.   Eventually you will change something and you may need to go back to the same point.   The meaning you are a pop up print will be useful again.   So at that point you comment the print, adding the number, sign the in front of it, and then once you complete the full debugging, you can remove it.   So again, three types of errors syntax error, execution error.   Those are easier wrote. They are easy because Python is telling you the type of error.   And when the error occurred, the designer roles are more tricky because the interpreter python is not telling anything,   but the program is not doing what is supposed to do.   At that point, you really need to do a through debugging.   Questions so far. All right.   So let's keep going. And let's go through some other python thinks that we may want to add.   So question. Yeah, go ahead. Sorry, I didn't realize it was on hold.   So I was looking through the exercise 33 to 36 or something like that from the reading assignment.   And I was trying to do the percent deal. But the percent d and then the percent I with our homework.   But it didn't work out. Is it because it's not an array by any chance?   Okay. Can you share them? Because I really don't remember.   I can tell you I can maybe share the book, so.   Sure. Because I deleted it because it didn't work.   Okay. Okay. Okay. Okay. Yeah. I just share what the questions were and then we will review them.   Okay. I think it's in the book exercise the the the hard way the prodding the hard way.   I believe it's an exercise 32. So if you have it and you you if you can shed the screen.   Otherwise, I will I will check it. I don't.   Okay, that's fine, I. If. All right.   If you can make it a little bit bigger would be great.   Okay. I think it is.   I'm trying to find it to believe it's here.   Yes. Is this. I got percent. Ah, and this is percent I.   Where is that integer or not integer. The, the value I guess.   But I tried to do that with my print in the at the very end this says Celsius.   I did the same thing. Let's say the answer is percent.   I put D and then I did the same exact syntax here, but percent temperature C.   But it didn't work for me and I wasn't sure why.   Yeah. Oh, okay. So there are a couple of issues.   There is one issue and one consideration on that print.   The first issue is because the code that you have is for Python two with Python to you have no parentheses.   So you need to have parentheses meaning is I mean the up to the the percentage sign and then I whatever it is just before that you need   to close the parentheses so the public will start before the quotation sign and then will end after the last the last quotation sign.   Gotcha. All right. I'll try to work on it when I have.   Yeah. And then. Yeah, yeah. And then the second point, we generally don't use this way to do the printing with there are two ways of doing it.   The one is print the parentheses F and then you have a the name of the variables in curly brackets.   And the second is using the percentage sign and the corresponding value after the parentheses in the print.   It's kind of confusing. I don't like it that much.   I really like more a plain way of printing.   If you stop sharing for a second, I go back to my code and yeah, and I will just use the code as an example.   But thank you for raising this point and that's a very good point.   So. If you consider this print right here.   So in this case, apart from the new line, I have a comment.   I have but one value. I have another comment or a stronger.   And then another volume. I mean, this way, in my opinion, is very clear and straightforward.   If I start using a variable, I mean, it is more parametric, but it's less readable.   So I strongly encourage you, I mean, again, that the course is not on the esthetic of coding but is more on maybe functionally work.   So I really don't care how you do the printing, but in my opinion that this way of doing the printing is more readable.   So in Python three, don't forget the parentheses at the beginning of the printing and the end,   and then inside that whatever you print as to be separated by comma.   So you have this first portion that is a string and the fix.   The string is not a variable. Then you have a comma, a variable you can or cannot have.   A space is up to you again to use this basis because it is more readable.   And then you can have as many variables and strings in your print you do not want   to do thinks of that too long because otherwise it will become not readable.   But I'm in my opinion that this way is more readable.   What do you think we are now? Yeah, I did. I did exactly what you had.   I just wasn't sure. Yeah, y why you didn't work, but.   Okay, that makes sense. Yeah. All right. Okay, so let me go back to sharing and let me go here.   So we are going to talk about some other Python related things.   We will go we will review the variable types and how to move across types.   So with Python, you have several types of variables.   So we mention strings numbers.   So strings are defined by quotation science and can be single or double.   For Python, that is just the same you want to say with either or, but mixing and matching doesn't work.   Recently I use single up, but I mean really the same numbers can be inside.   That can be number. It can be a.   Alphabetical characters or whatever you want.   Numbers can be integers or a floating point, but.   You can do operations between a body of both.   So on the left side, the you have strings.   On the right side, you have numbers.   You can add the strings, variables containing strings like in this case, when you add the they will combined attach the one to the other.   Obviously if they are numbers, you do the arithmetic operation with that same thing with multiplication you can apply the multiplication   to a strings and it will be the same string repeated the time each time you attach one to the other.   When you do with number two, you have the arithmetic operation.   Again, there is no mix and match.   Excuse me, because otherwise you will get an execution ever.   So, like, in this case, you have an error.   The good pointer is that python that will tell you the line that when the error record.   You can ask by doing the type of variable that you are working on that seems to be irrelevant.   But sometimes when you do debugging, when you when you are doing testing to a code that you do an operation, you get an error.   The program is pretty long. You don't know why you have a wrong combination of variables because you expect the one   variable to be numerical and you are adding a number to that variable and you got an error.   Why? This variable is not numerical.   You can go back. And add some print statements to the code, the printing, the type or the variable before you do the operation.   Just to be sure that you trace the evolution of the values of the variable while you are creating that.   So Typer is useful. So if in this case is a stringer,   you do type-A for the value for the I mean the variable and you will get the in a minor bit less than greater than the characters type.   And then the type of variable that that particular string is the particular.   But for this same thing, if it was an integer, you would get the list floating, things like that.   So again, type is something I spending more time than I should, but it is important.   We have integers, floating points. We know that you can convert one to the other if the conversion makes sense.   We know that if you try to do a conversion of a string that has not numbers in it, but into integer or floating it, you will get an error.   Those are other examples. So you have.   Operation said that out on the algebraic outside the operations that are.   So you are transforming content and then you doing an arithmetical operation.   In this case you try, as we know, to do a conversion of a string into a number,   but the content of the variable, it's not a number and you will get an error.   Again. That's another example. We know how to do the management of errors in this case,   keeping in mind that when you do try accept that you can specify one particular   type of error and have eventually different messages for different types of air.   If you check online in the pilot on the website or in StackOverflow, you will get the codes for the different type so that there are several.   User input. We know how to do it.   We know that you can convert it if it's convertible.   We mention that you can do operations with strings.   Now let's talk about lists. So lists are another formula, another type of variable, not that are a container, so they are defined by square brackets.   So there is a square back at the beginning, a one at the end, and then you have elements inside of those elements a.   Are separated by commas. And they can be anything they can.   They can be any type of variable. They can be numbers.   They can be strings. They can be other lists.   Meaning you can have a list that is nested into another list.   We will talk about that. Uh.   Lisa, our new double meaning.   You can change the content. You can change the order.   You can add elements. So let's say that you have this list here.   Like. Pretty much everything in Python.   We start from zero up to the end. So the first element is the element zero.   So if you want to recall the first element, then you need to point to that element.   So that's the list that we created. Again, the square brackets, commas, separating the elements and any type of element, any type of variable inside.   If you in that, I mean, that is sort of pseudo code that the brackets are missing here.   So example at zero, meaning you are pointing to one, two, three and you would get it.   The example is three.   You are pointing to the fourth element that is fish. You can change an element.   So in this case and saying example is that three equal tilapia.   So I changing from fish to tilapia. When I printed that, instead of getting a fish, I would get the new body.   And one more feature that is really useful when you deal with the ball with lists or strings is a slicing them so.   We mention that you can point to one particular element.   So if you are saying this leads to a2h3 meaning I'm pointing to the fourth element that is a the I can   do three column that meaning is from the fourth element on and you have all this piece of the list so.   I can do up to that point the meaning all.   But the fourth element that before the fourth amendment, then you will get A, B, C, you can slice a piece of it.   So you are saying from three to 3 to 5.   So you have from the fourth element to the fifth element.   I mean, from the fourth element to the sixth element, the excluded.   So you have the any. You can do backwards.   So you can do minus one, meaning the last one you can do minus to call on meaning the last two.   So this lighting is something that you will practice and is pretty useful when you are dealing with managing text in particular.   One thing that is relevant is the use of append and remove append.   That means that you can add the elements to a list.   So you have an existing list like what we used before, and you want to add the number 42 with append that will add the in the end.   You can remove elements. So you just call the element you want to remove and you do name on the list remove.   And in parentheses you have the value that you want to remove.   One thing that is sometimes is generating a little bit of confusion is the difference between concatenation and appending.   So you have the same lists. The first is a one, two, three, four, five, six.   You do a concatenation, so you do C plus a C equal to A plus B when you print in the you will get the combination of the two lists.   So you have a one, two, three, four, five, six. If you do append, you are upending an element to an existing list.   So the same A and B, you do a append b.   When you printed that you had the first three elements, and then another element that there is the entire list.   Append is what is normally used.   When you have a loop, you have a loop and you are creating you are generating a list, adding each iteration, an element.   So at that point you are appending, just like we did here with 42, you are appending fortitude to the list.   Think about a loop or instead of 42, you have an index and you have numbers from.   A to Z, whatever R, E and Z in values.   Each time you will add an element. At the end of the loop, you will have to create a list with all the elements.   So append. Is what we normally use that when we are creating lists and we want to add at each adoration and element.   Another useful thing to know is the existence of, in or not in.   So in this case you have at least the but could be a string and you are asking if 42 is in the list and you get through.   So in this case is not in the if. But if you have that in a if statement, then you can take different actions based on if it's true or if it's false.   So if you ask 55 in a, you will get false.   You can do not in that. And if you do 50 fi not in a then the result would be true.   Lists are ordered, meaning the way you create them is the way they will stay.   That seems to be obvious, but not all the data types in the variable types in python stay order.   You can change the order because again, lists are mutable.   So you can. Either add elements, move elements.   You can delete elements. You can sort the elements.   So all of those is doable. But if you do nothing of the three, the least that will keep the order that you used when you created it.   They can be sorted again. You can sort them in ascending or descending order.   If that is number, it is easy. If it is a string.   It would be alphabetical order. Pretty much the same is for a string.   So when you have three things you can do.   Slicing. Just like we saw for lists.   So five on meaning. I mean, instead of a I mean, from five on you skip the first five, but then you do the remaining.   You can pick the fourth element that could be a D.   You can. Do a loop within a string, and then at that point the loop will be on the single character in the string,   including space because space is a character for all intents and purposes.   There are some methods that can be used with the strings upper lower to capitalize either.   All of those are things that can be added to any string to make it a little bit different.   So if you have this combination of a lower and cubby the letter certainly in the string, if you do lower, you get low words and so on.   You can also do start with them. And that's something that sometimes may be useful.   So you want to put into all the courses started with em so you can have a list of courses,   you do a loop, and then in each loop you have one course and then you start that.   If you ask it and you have and if statement the if the name of the variable start with M, then do something, count it and so on.   Keeping in mind that again is more lettering.   Capital letters in Python are different.   So if you test in this case a capital H e, you will get through.   If you do a smaller H e, you will get folds.   There are about 33 methods, and you can check them online.   Uh. Again, that's another example that can be applied kind of a combination.   So you are checking if one string is in.   In the least. That's something that can be useful in many case.   You have a list that you imported.   You read from a file or whatever, and you want to see if a certain value is there or not.   And then you do different things based on the different options before the results.   Replacer again, you can replace elements to both lists and strings and mutable meaning.   You can change them the way you want.   You can do conversions with all the caveats that we mentioned many times when you convert a string into a list.   The resulting list will be composed by the individual characters that can be letters, spaces, or any other character in the string.   So in this case, when you are making a list out of the word cements, the resulting list would be, I mean, the individual letters in the name Stephens.   That's something I want to draw your attention to.   So this is a list where the first element is a number of the second is a string.   The third element is a list, and the fourth is a number.   So if you do, if you point to the third element, so it will be two.   So if you do my list two, you will get this list if you want to get the.   The second element of the NRA list.   Then you have you need to have two indexes.   So the first one will point to the inner list and then the second that will point to the element that within the list.   So lists of lists are very common, and I strongly encourage you to become familiar with those.   Conditional accepts. Again, we can have loops working on the lists.   So you have a list of names and then you do a loop and each time you will print one.   It is just an example of how to use lists in a.   Loop. You can apply all the comparison operators.   As we know, you can break out of loops using a break you can use continue for doing nothing.   Meaning in this case, if the and the first element of line that would be the number sign continue meaning you go back,   you do nothing if it's doesn't break. We know that in this case you are going out of the loop.   Other application of loops.   You have these values and you want to count to do the summation and then you want to bring the count to the summation and the specific part.   So that's another example on how to use loops.   And when you finish, when you are out of the loop, you can do a mean summation of the values divided by the number of values and you have the average.   And then you have links to the.   To the Python website when you can have more details on that.   All right. It's 718 at this point.   I would introduce if there is no question, I would introduce the in-class assignment and I will give you.   About half an hour or 25 minutes for the assignment.   Then we will, let's say 20 to 25 minutes.   We would discuss the assignment and I will introduce the next exercise, and that would be the end of the class.   And obviously, if you have questions, I will be happy to address it.   Okay. So. Let me share the screen again.   And let me. Go here.   So they think class. Exercise as a user for a number depending on whether the number is even or the.   You want to print an appropriate message. Meaning if the number is let's say two, then you will print.   The number is odd. The number I'd say is a five.   It's, it's, even if it is five, the, uh, the number is, uh, you want to print the,   you are asked to print a different message if the input number is at four.   So if it's for you, print something different.   If it's not the it just odd or even.   So let me stop sharing and let me create some breakout rooms.   So I am creating three breakout rooms with three participants.   Each room. I am opening it so I will knew them myself.   I will be here all the time anyway. And.   The rooms will stay open for about 20 minutes. Then I will close the rooms.   I will send you a message before closing.   And then that we will reconvene. We will discuss what we had.   And then I would talk about the next class.   The next assignment, see in about 20 minutes.   Resume the recording. Oh, right.   Welcome back. All right, so anyone wants to share what you did.   Again, there is no grading. There is no judgment.   It's just for sharing and sharing the comments.   Your areas may be useful to someone else.   So. You're strongly encouraged to share it.   I can share, professor, during the volunteers? Sure, please.   Okay. And so this one.   We had two exercises. Is that correct, Professor? Well, it's one exercise.   So, uh, let's see what you have.   Okay, maybe I read something different because I was still seeing, like, all the assignments from, like, last year.   Yeah. Yeah. So maybe I read the wrong one. But anyway, so again,   this program prompted us to enter a number one or two to understand exercise one or exercise to exercise one will basically ask the user for a number.   Write checks if it's hard, and then if it's not odd or check if it's even.   Or multiple four. Sorry. If it's if it's multiple four and it's not zero, then it will print.   You know, the this number is even but also multiple for otherwise in in the case the last case,   if the user enters zero, it would enter, uh, you know, zero is even, right?   So that's the case that's carried here. And in the second exercise with it prompts user for a name,   you would enter your H and it would compute basically the year that you will reach 100 years old,   including the days until that first day of that 100th year.   And so that's what Exercise two does. So I'll start with Exercise one, right and true one and turn number on like let's do 2.5.   And as you discussed, that's not going to work. So let's do 20.   So now you get 20 is even and also a multiple of four.   Right. Well, let's try to assess case this. We'll start over with this one and our number 200 is even and that's the last which case there.   So that's for exercise one. Exercise two is an attorney, Kevin, until your age 30.   And then it says in seven years you will be 100 years old and that's 25,530 days away.   So that's the last part and how I did.   The last part is I use StackOverflow, as you recommended, to obtain some information about how to obtain the current year.   Years remaining is a variable where I subtract the user provided age 100 minus that age and   then the year at 100 or the current year plus the years remaining and to calculate the days.   So the first day of that 100th year i u is the date time that now to get the current day and for day two,   I use that time of passing three arguments, including the year at that that year that I'm talking about a hundred years old.   And I subtract that right and then I assess the days using different days.   So that's how I print that result there. Sounds great.   I do have a good question, professor. Good. Is it bad practice to have the numbers like this in a variable and by the.   Well, I mean, the names or the variables can be pretty much anything.   Apart from the risks of words and keeping in mind that they should be mnemonic.   So here hundred it is mnemonic.   It's absolutely legitimate. And I mean, I don't know why I would have added an underscore here on this quote hundred, but that's me.   I mean, that is absolutely fine. The main goal is to have names that will remind you what the content could be.   So in this case, that is just fine. Okay.   Are there any obvious issues you see on these two exercises or.   It looks fine. No, they looks fine.   I mean, as a general consideration, I mean, we are doing exercises in a sort of a gradual way.   Yeah, but a more basic way of doing it would be not using functions, but just right.   What is the day today? What is the object in the code that would be less effective?   Less sophisticated? The more basic.   I'm not saying that is the way to do it.   That wouldn't be the way of doing it.   But it would be the way that I would expect at this stage of the class.   And that the only consideration, I mean, the way you are doing it is the way you're doing it that you don't want to have data in the code.   Generally speaking, you want to have the code, the doing only the functional part of the process and the data being outside.   Yeah, but I mean that at that sort of an early stage of the skills getting process to where you are. 1:   I don't care much if you okay in the script, but generally speaking, that's the way of doing it. 1:   Another approach that I generally follow is to have, but we didn't do yet. 1:   Handing files is to have a parameter that's like the data. 1:   I mean, they wouldn't want the effort, but with any form of parameter in a configuration file, 1:   meaning you read the file with all the data, the data will be incorporated in the code and becoming values for variables. 1:   Meaning if you want to change the values, you don't need to change the program, but you just go into the file with the values and change the values. 1:   So we will go to that in a few classes. 1:   So far, I think his next class when you will do read the right files. 1:   So for the time being, what you did was just good. Okay. 1:   Thank you for feedback for anything. Thank you. Any other. 1:   Yeah. I can share. Please. So we didn't know about the second son who was not there. 1:   Okay. So for the first assignment, we took a pretty simple approach. 1:   So I have this variable number and then we pass that number to an eight and divided by two to determine its half number, 1:   which is this other variable that we made. And we used this half number that there was there. 1:   That's why I did work anyway. The fourth thing worked before. 1:   But, uh, so we said the, the actual part of it is down here with the we use integer. 1:   And we said if half number is integer print that it's an even number. 1:   And then we said else it's an odd number. 1:   And then we have just some other cases up here where we rule out the zero case and the four case that I requested so I can go ahead and run it. 1:   But so it's not the, the prettiest, uh, code. 1:   Okay, but. We? 1:   She were. I remember we just said all the cases or whatever. 1:   Yeah. I mean, it's working. 1:   That's absolutely fine. A couple of considerations. 1:   The first one. You do a loop. 1:   You use a way through if there is a loop. 1:   So in this case, the loop is not use the match. 1:   It could have been used better. Adding it down when you want to end it instead of breaking the loop. 1:   Each time you reach a conclusion. 1:   You may want to have it continue in the testing if done at the very beginning, like we did in the previous assignment. 1:   Yeah. Yeah. I mean, it is working again. 1:   I'm not teaching. This time of coding is more a functional approach than teaching. 1:   But keep in mind that that that could be a good addition. 1:   Yeah, sure. The second point is divided by two is working. 1:   But you can use the function reminder, meaning if the remainder of the deviation by two zero, then it's equal to. 1:   Yeah. I saw that in the previous. Yeah. And that's another approach. 1:   But I mean both the options are working fine like divided by two because there's more narratively using the basic functions. 1:   And at the stage we are. If it's good to use the basic functions, then we will add a step by step, class by class, new elements. 1:   All right. Thank you. Okay. 1:   So you can stop sharing. 1:   And I will briefly introduce how we shared the screen and I would talk for a second about the qualities that. 1:   So in this case, certainly I'm doing the same way through. 1:   So I'm asking if. I mean, I ask him the number. 1:   Baking. If it's done. If it's done breaking, then I'm checking if it's numerical. 1:   If not, the meaning. If there will be an error here, will be intercepted by the accept the option will go back, 1:   ask for another one then and checking if is a multiple of four meaning the reminder from the division before it zero then is a multiple of four. 1:   And then if it's the remainder with the division of two, it zero is even. 1:   Otherwise is that all the and we go back. 1:   Abdul-Mahdi again, I could add that continue here, but it's redundant that we go back anyway. 1:   So if I ran it up. It will do? 1:   I don't know. 22. It's even 11. 1:   It's odd than that is out. 1:   That's very basic. So overall, I want to apologize for the kind of mess that is on campus. 1:   The main reason is because when I had the original content, there were a lot of parts missing. 1:   I uploaded the previous version the previous semester, but then instead of replacing, it kind of added to it. 1:   Meaning you may have multiple versions for the same thing and cleaning up. 1:   So be patient. My apologies from next class. 1:   This is not going to happen and doing all the cleanup. 1:   So let me go back to the assignments for next week. 1:   I strongly encourage you to pay attention to at least the beginning of the description of the assignment. 1:   So the assignment is two parts. One is on testing and one is on writing the code that they are related to the same problem. 1:   So the testing is not genetic testing, but is testing the code that you will write the in part two. 1:   So for the part one, you want to design testing. 1:   So to design the testing, you will use this template. 1:   So the template has the goal of the program. 1:   And the goal of the program is from the program as a loop and all the rest. 1:   So that's the goal of the program. Then you have the testing strategy, meaning what are the logical step so that the program will perform, 1:   for example, done to exit the program and printing the proper statement when the user enters, 1:   done checking if the price is a number, another string printing a proper statement when the user enter a price that is not multiple of five. 1:   So all the alternatives will be here in English. 1:   No, no code here, no python statements here. 1:   And then for each one of those you will write the data that the user would use to go in each one of the options. 1:   So if you are to print in the proper statement that when the user enters done, it will be input done. 1:   Output goodbye input at 12 output price not a multiple of $0.05 something that that. 1:   So again. Part one is reality two. 1:   Part two is not a theoretical question. So part two is basically write a program that will ask the user for a number and then a. 1:   The number has to be a multiple of five and that's the procedure. 1:   So you prom the user for the price in cents, meaning it has to be a multiple of $0.05. 1:   You need to check if the number if the input is numerical, if it's not negative, 1:   and if it's a multiple of five and you will provide different messages for the different roles, if the user will enter done, then goodbye and stop. 1:   If the user enter a price that is actually a multiple or a five, you will print the price and you will repeat the old think. 1:   You will submit the doc or PDF for the testing based on this document and in your programmer as a p y document in canvas. 1:   So I will review the submissions for the previous assignment that again, there was some confusion on your side, on our side. 1:   And I apologize for that. We will fix everything from next class and for the time being and just accept my apologies that. 1:   Okay. So. Questions. 1:   So, Thomas, I'm for a second. You have a question.
Hello. Hello, everybody. It's 630 and January 31st, the last day of January. 2:27 And we are here for the second class sell this year. 2:38 624 So let me just start asking you if you have any question that is something you want to discuss. 2:43 A There was something in the material worth reviewing somehow. 2:56 So. Oh, things are going. 3:03 All right. So. I mean that next time try to be more into questions. 3:13 So to make it more interactive. So the whole experience of online is what it is. 3:24 I mean, it's not like being in person, but let's try to make it more more personal somehow. 3:33 So I'm glad that you are switching. 3:44 I have a question. Yeah. Good. Really answer right now. 3:48 So for one of the like when I came to to change to one, 3:53 I once I finally got the software download and everything working when I came to change the name. 3:59 There's no way to change the name of like like if you see the way I submitted it, it says homework like p16789 something, something or something. 4:04 And I don't have any way to change that. But I was able to change it like I changed my homework name. 4:14 I just couldn't change the name of the actual folder. Yeah. 4:20 I mean, but I mean, the incumbents are own or in your own former. 4:23 No in my folder and byton. Okay. So I mean, generally speaking, what you have on the end is in is in pie chart, right? 4:30 Yes. I mean, in the pie chart. Let me share the screen and let me show you. 4:42 That's probably easier. Okay. 4:54 So that's. That's by Sharma. 4:57 You have? Yes. Yes. You see all the way to the left? Yep. You see what's on the left. 5:02 So instead of me being able to name the label the folder m600, 5:07 I was able to to label it where it says exercise 00. P lives and able to change that name. 5:12 But I don't know how to change the folder to 624. 5:18 Yeah. I mean the only way it's basically to go here and to go in the folder like in this case that folder is EMC X 24. 5:22 And is this folder here? Mm hmm. And just run the way. 5:38 Yeah. You need to change here. Uh, okay. 5:42 I was trying to change it from there, and it wasn't. Yeah, no, I mean, I'm not even sure if there is a rename here, but. 5:46 No, no, no. Yeah. I mean, the only way would be changing here. 5:53 And then when you will. And then I'm in the closet. 6:00 Change it closer the entire program, change the name and then open with the new name. 6:05 And that's. I think everything else is pretty straightforward. 6:11 Yeah. Yeah. I mean, that is not a real file management area. 6:16 I give you some things, but is more for displaying what is inside than for doing something active. 6:24 Okay. Thank you. Sure. All right. 6:34 Okay. So. Uh, let me keep sharing, actually. 6:38 Go back and watch it. 6:47 All right, so we are in module two, software engineering and some python coding. 6:52 So. We had some issues last week with this quiz and 185, Check Your Knowledge. 7:03 I republished it. Some of you already answered the question and know this as some of you already have 7:16 knowledge of coding in general and knowledge of coding in Python in particular, 7:28 but some other you do not. 7:35 So the course up to the midterm is going to be quite basic meaning. 7:40 I cannot leave people behind, meaning that I will start with the very basic elements of Python and then we will be up from after the midterm. 7:49 Things will be more engaging for pretty much everybody. 8:04 In the meantime, there will be other portions that are less strictly on Python and more on writing code, 8:09 managing software, development projects and things like that. 8:20 So. If you didn't do yet, please go back to the M1 B5, check your knowledge and do the quiz is very basic. 8:27 And I mean, the two most important questions are, are you familiar with coding offers? 8:41 Are you familiar with coding in Python disciple? All right. 8:48 So. During the lecture today, we will talk about software engineering. 8:53 A little bit of history. 9:02 Some example is going to be just an introduction because we are going to have a more detailed section on those aspects in a few weeks. 9:04 Then we will continue with the. By Donna, we will do an in-class exercise and then a will present. 9:16 We will discuss the solution of the in-class exercise, and I will introduce the next homework. 9:34 All right. So let me start the fourth. 9:47 With some slides. So. The first group of lights is on developing software. 9:54 So again, that is going to be relatively short. So software initially was just a part of our work, so that people were developing and installing the hardware was also the people doing the software. Then they realized that that there were competencies that should have been developed in different ways. And then that's the beginning of the. Jobs of the coders, programmers or whatever you want to call them. Initially that we are from the people who developed the hardware, then mathematicians and then develop the in its own discipline. When I already mentioned that when I graduated from the University of Rome. There was no information technology. It was a beginning of AIDS. There was no information technology. And I graduated as a master in math, and it was applied math. And part of the applied math was something close to information technology, but was not a separate discipline. And we are talking 1980. It is not 1880. So that's just to have a timeframe. When you develop software, that is a component that is all in terms, of course, that is the development course. And there is also a component that is on testing and testing. The software can take a big chunk of the overall costs. Generally speaking, you don't have the same people are doing the development and doing the testing because the goal of someone doing the development is basically to make it run and to make the code running. The goal for people doing the testing is just the opposite. To find the failure point, make the code fail. Two different mental attitudes to different skills. But we need the both of them in additional costs. It's cost of maintenance. So software is doing a certain job. The job can have all been time. There could be new routes. You are in marketing. There are new campaign. There is something changing the way the code was originally intended to. So cost of development, the costs of testing, and then once the product has been or the solution has been deployed. The costs of maintenance will stop. When you design software, there are different ways to do it. The. Typically when you develop something, you do your requirements, you do you design the solution, you implement the design, and then you pass the solution to the user and then the maintenance will stop. This approach is normally called the waterfall model was the base for most of the so-called legacy systems for quite a few time. Let's say for a good 15 years. Advantages and disadvantages. So advantages are quite obvious. You can define the requirements in depth. You can design the system based on the requirements in the most appropriate way. You have to allocate the resources in an optimal way. That's all in theory. But then the drawback is that there is no flexibility for change. Meaning all the beautiful plane that you did may not work because the condition changed, because you didn't really understand the flight well, what the requirements where for any reason the lack of flexibility is the main reason why the vast majority of the projects didn't went well. Using this approach 1995. That was probably the peak of this waterfall approach. Only 16% could be called the successful. It was obvious at that point that this approach was not the right approach. So that's why people started thinking in terms of software engineering, so how we can apply methods, approaches coming from engineering to the development of software. Again. We will talk with more details about that. But. Software engineering is a pretty much the system engineering applied to software development. So is the same holistic approach in the same thinking that there is no system living in a vacuum. But you need to consider all the other components that can be related to that. One of the point of reference in software engineering is the Software Engineering Research Center that is headquartered in the Software Engineering Institute that is headquartered in Carnegie Mellon University. And they develop the several stages of their methodologies that has been applied for quite long time in developing software. I was a partner member of the Software Engineering Institute for a few years. Some basic principles when you develop software. News. Open source. As much as possible, it is cost effective. There are drawbacks that there is no one really standing behind the product because it is open source. But on the other hand, it has the flexibility that you may need in many cases. Consider you have something that is not working the way you want on the software. If it is a commercial software, the only way to do it is submit a ticket and wait for the issue to be addressed. It can be in what it is to release or can never be incorporated in a future release because it is considered not strategic by the developer. So with open source, you can change the code yourself to make it like you want. Use the industry standard. There are two standards for many of the things that we are doing from interacting with the web, interacting with servers, exchanging file suites, but use that instead of reinventing the wheel. Third point make the graphical user interface. In general, the user interface separated from the backend system, the system with the logic. So they have different lifestyles. So the graphic, the user interface can change because you have a new platform, you have new system, you have a new user, but the backend may stay the same or vice versa. You can change the criteria where you develop, you provide the answers, but the format will be the same and you are not going to change the user interface. So generally speaking, keep those two aspects separated. So those are sort of common sense on developing software. Question so far. All right. So let's keep moving. And let's talk about Python. So some of the things are well known because we already introduce them last class. But I just want to be sure that they are just clearer and well fixed in your memories. So like many other languages, like all the languages, Biden has variable operators functions. We will talk about functions later on conditionals, cooperators, loops and all kind of operations. Um, there are some functions that are built in. There are some types of variables that are built in. So you have a, in terms of types of numeric integer floating, you have strings, lists to build. So we will go into the details of all of those. You may have dictionaries. So all of those are different types for, let's say, basic different types for Python. You can assign, as we know, values to variables. In this case is a I mean that is we are in the strengths area. So in this case is a list. You can have numbers, lists. We will talk about lists shortly, but a list is basically a collection of elements in a square brackets where each element is separated by comma. And the elements can be pretty much anything, can be numbers, can be strings, can be other lists. And you address the single element by calling them by number. And Python is starting from zero. So X is the element zero. Coffee is the element two. Strings can be both strings, and these can be changed, meaning you can't replace the values. We had some examples from the strings. Naming variables, we mention that there are some words that you cannot use to name variables. Those words are a result of the words. So you have words like and the F or from. So all all of those are or into are reserved words. You cannot call a variable a print. You can call a print with a capital P because up in in python has more letters and capital letters are considered a definite. Again, names are a case sensitive. So you can use any one of those with capital letters and there will be no conflict. But you want to avoid that because it could be confusing at the very end. You also want to have a name. So. That we will remind you what is the function that the body of Ebola is having in your program? Those are called the mnemonic variables, names, variable names, some meaning the names that are remaining. Again, the the role of that variable within the script. Comparison. You have the usual. There is not much difference from pretty much any other language, probably apart from two one that is that equal to the equal in the comparison is a two equal signs. So we use one equals sign to assign by the user to by their both. So we use a two equal signs for the comparison, all for the conditional statements. Another one that may be different, that is not equal to that is exclamation marker equal. In some other languages you has, you have a combination of less than a greater than one after the other. But I think in some versions of Python that works as well. But what is most commonly used is a exclamation mark equal. In computational theory, there is a theorem that is called Yeah, that is named after the creator of the theorem that is being been theorem saying that every logical operation or process can be represented with the combination of three elements sequences, conditionals and loops, meaning a language to be able to solve any logical problem needs to have those three components in it. So in Python, sequentially, it starts from the top, going to the bottom, from the left, going to the right. So in this case. If you have a program where X is equal to X is equal to x, you will get two. Then you do x equals x plus two, x will be four. So that's the sequence. Conditional. We mention that there are operators for comparison. Those operators are used, among other things, for the conditional. So conditional are with the if or if. So if today equal so double equal sign the October 30th pre into happy birthday John elif means else if. Meaning if this is not not true, then I am asking is today June 21st then that if yes, print Happy birthday, Loreen. And if not the meaning this is these fail that these fail that you are here else meaning for anything else you print. Good morning. Keep in mind that two things. One, you need to have a call on that at the end of the conditional statement. The second that you need to have. The true case, meaning when the conditional statement is through the condition, the condition, the statement, the meaning of the statement that will be executed when the condition is true, that has to be indented. So after a call on that, you always have an indentation. That's one of the most common error at the beginning of coding in Python. So on the left, you have a graphical representation, a flowchart for. An example of conditionals. So you have x equals five in the room by the you have the FS. So I'm asking if X is less than ten. If yes. And you're putting smaller then and you go here then I'm meaning this is the case for her. Yes. But if is no will go here anyway. So if is yes will print smaller if is greater than 20. And it would be bigger bigger if is not the either one will bring finish. So again, double equal call on the indentation. So those are three things that you need to keep in mind. When you do conditionals. We already mentioned that loops so loops meaning repaired repeating statements so that can be one single statement or a bunch of them. So in this case I initialize the variable n with the value five. Then I'm asking is and greater than zero. If yes, it will print an and will subtract one from n and we'll continue printing n fill. It will become negative and meaning not greater than zero. And then at that point that will go here and will print the finish. Couple of things. So one. We have several ways to represent loops in Pi Donut. So during the first assignment you experience why that was a loop. The other option that can be why conditioner through then you would exit the if is not true anymore order for for in this case you have a list of names. For anybody, any name or variable to say name in the list of names. Print the name. So the first round, the variable name will have the value of the first element in the list. That is frank. So the first round will print Frank. Then the second round it will point to. The second one again is a loop memory and so on till the end. What is really important when you do a loop? You want to be sure that there is no indefinite or infinite loop. So you need to have, if I remove this one and this statement, an equal. And minus one, he will stay in the loop forever, so it will burn your CPU. So be sure that you have a condition that will let the program to leave the. This is another example of a new person in this case that is using Whyalla. So while countdown at least a hundred. Bring the count down and then you decrease one element from that I that countdown. So initially, countdown is 100. That is not I mean, initially is at 25. So you are here. Yeah. Less than 100. Yes. We printed will be 24 and so on I think would be one. And then it will stop. So again, call on indentation. So the indentation is basically creating a sort of logical block. So you can have as many statements here. All right. So. You can use in the looper two statements that are particularly useful. You can use breaker. We use that in exercise zero and one. So in this case, what we are doing, we are generating a bunch of numbers and of numbers. So then we will print the numbers in a loop and if the number is five will break. So it will start with the first number, that would be zero. So range then is generating a list of numbers from 0 to 10. So when the number is equal five, it will break. Breaking mean means getting out of the loop. In this case. It's another statement that is continue. So I have a range of numbers from one to 1 to 1 with step one. I mean, you don't need to memorize all the all of those. You will use them. And you will remember then that you start the loop. Over here. I'm saying if the remainder of the number divided by ten is different from zero, then continue. Meaning don't do anything. That means that I'm looking for numbers that are multiple of ten. So because the number as a reminder at zero if is a multiple of that number. So in this case, it's ten when I have the first nine numbers. I will just continue doing nothing. When the number will become a ten, the remainder will be zero zero. And then I will print the number and then the string is a multiple and then we'll continue until the I mean, the value of 100. That example. So. In this case, a list of elements. And they're that names are the same names that we saw before the looper in this case and saying if a name start with M, then you will a printed meaning in the loop. It will be Mary and Mohammad, this one age 20, while age less than 66. You add one to age. Then when you go out of death, meaning when you are not less than 66, then you will bring the age and retirement. When we work with Python or similar languages, you can easily run all the code you want directly in the Python console. But then you don't have a trace of what happened. And when you closed your editorial, your computer, those variables, I mean, the next section, next session or Python, the previous ones would be deleted. So that's why we created we added the statements in one single file. In general, use the dot pie as an extension. Those files that we run executed by the Python interpreter. So what's wrong in this? So in this case, you don't have the indentation here. So that's what is missing in this other case. And you have. Numbers. You have a knife. But unfortunately the if there's one single equal sign, none too. So over here there was the missing the indentation over here, the error is the missing second equal sign. And then you have all the links for. I mean, more information on the on the Python website Internet. So let me stop sharing. And that is he. If you have questions, those links will have the commands of staff for reference. Yes. But I mean, generally speaking. Quite honestly, I rarely go directly into the python. Her website obviously is the only place with official information about what you are doing, but there are different ways to get the information that you may need. So let me share the screen in a second. For example, if I do. I don't know, transformer. Right on the list. Two strings. I mean, there are generally million different links you have on a campus building to StackOverflow. That's generally the way to go. So if you go to StackOverflow, you are going to have all the information you need to check the popularity of the question and the answer, just to have an idea on how reliable the answer can be. So in this case, I mean, that's a very basic but common question and that's why you have more than 1000 people upvoted the question and this answer with this in statement is super popular. So I mean, you can go to Python and check exactly what the manual is saying, but the StackOverflow is probably the easiest way to go. I personally I mean, I called for million reasons. So just to give you an idea. One of the things that they're doing a. Is managing the scheduling of courses and classes within the School of Systems and Enterprises. Now we hired someone who will be just on this one, but for about two years I did it by myself, but I'm still involved. One of the issues that we have in comments is, I mean, like all the companies that the data are not very well-structured. So when you need that information, that piece of information can be all over the places. So that's why I created this script. The. Just to give you an idea of how I mean, in daily life you can use Python. So in this case, a download from the system that we use as a backend in a commencé is called what they. The list of all the courses that are happening in a given semester are at sea events. And then. I basically clean the file because it's a giant file with all the courses for all the schools, but they need only the courses that are in my programs. Im IAC as is w and s way is. So I clean the data, I add then some features. What is the program? What is the level ground with undergraduate, corporate and information like like that. So when they do something like that, I mean, I'm familiar with Python. So I have I mean, I wrote I don't know how many probably by the thousand programs, but there is always something that you don't remember but you're not sure. And we tend to be lazy and I'm not proud of that. But these a matter of facts. And instead of going back in my memories and trying to find a solution, I Google it. So most of the time I end up in StackOverflow and I use it. So now I'm using also her chart, the GP. I'm pretty sure you're familiar with that. The. It is a very interesting tool. So it has been trained using pretty much everything that is available in open source, including the content on GitHub. So GitHub is a huge repository of code where you can have your own repository with your own code. Generally, if you have a space and inside this space you have your different projects. GitHub is owned by Microsoft, but Microsoft is one of the largest investors in open. And yeah, that is the company that created a chart to cut it. Besides using, I mean, everything that you can get from the Internet. I don't know if this is true, but this is what they said. It has also access to everything that is publicly available in GitHub, meaning it can really give you a way to do the coding. I'm not particularly happy with the result. So most of the time is missing. Something is using pseudocode instead of code. But you think it is called the you try to run. It is not working but can be helpful. I am writing a book on the societal implication of AA and machine learning and I'm actually using it as a sort of sidekick. So when I have an idea instead of google it, I use the bot and will give me a sort of a compiled version of what is available in open source. But in in education, we are all kind of scared of how students can use that because we all know that there is always a risk of plagiarism with that. These tools or tools like that, the the risk of plagiarism is even higher. We are still developing tools, able to detect what is coming from Jupiter and what is now her that again, you may use it to cheat her. I don't know if this is beneficial for your education. Most likely not much. So I encourage you to play with it because somehow we'll be part of our lives. But before copying and pasting things, switch your brain on and read what is inside and be sure that it is doing it in the proper way. We will run a chat group on some of the questions to compare the results, and we will apply the same concept that we apply for the copying in general. So if you are using chat, the GP and someone else is using the same. Probably your code will be similar, if not identical. At that point you will be penalized not for using GP, but because someone else did the same and you had the same code. So it is like doing after the fact because we cannot really intercept the cheating at the very beginning. So there is no way. In theory, we could create a classifier. So coding is very personal. So. We could create a classifier analyzing the code and detecting a sort of a footprint. So if there is the footprint of GPT three, then it's cheating, but the tool is not ready. Student University of Princeton created a sort of prototype that are the traditional tools 13 that is the most commonly used. The thing that is the most commonly used to detect plagiarism is developing a module on a chart. Gupta. But I mean that this tool has been released by the end of November. There is nothing that is ready yet. So but again, I really wanted to share with you very openly with what what is available if you need help. Apart from obviously myself and our work to StackOverflow and Google that are always good again, don't feel that you are doing something wrong again. I was coding since a while, but when I have even a trivial question so that I don't remember right away, instead of wasting my time, I googled it and they get the results. And then now we have a chart, the JPT, that can be an additional help. So that's the long story, probably longer than you want, but that's the full story. So is a seven, the 17? What we are going to do now will be an in-class exercise. So let me share the screen. And let me go. Be here. Okay. So I will publish it in a moment in in canvas. But I want to introduce it to you first. So you want to write a program that will you will call like changed the UI, that kind of a mimic, a conversion from U.S. dollars to another currency. So you will ask the user how many U.S. dollars you want to change. Let's say the input is hundred. Enter the name of the currency you want to convert the dollar into, let's say is yen. What is the exchange rate? Let's say the user is inputting 114 and the output will be you can exchange the number that you got from the user. It's 800 U.S. dollars for the multiplication are between hundred and ¥814. So you will use a loop that is pretty much similar to what you did the previous exercises. But to ask for the different input. And then you will check if the value that has been provided by the user is a number of now and you will use is digit as a. In a function that is associated to this to the string. So whatever is the name daughter is digit the open and close brackets Diawara that is digit the is pretty dumb and is considered number but only if there. Real numbers, meaning no minus sign, no decimal point, no comma. If you have one of those characters, it will not recognize it as a number. So in this exercise, that is obviously not. I mean that defeating the purpose of creating a currency conversion tool, you will consider only integers for the two numbers that you receive from the user. And then you will paint a blank line, whatever is the way you want to paint the blank liner that can be just printing nothing. Or you can use the special character a backslash and telling her new line The meaning is keeping a line. And then you will print her, you can exchange or whatever is the number of dollars for whatever is the result, whatever currency you are using. So let me make sure that. The. Okay. So the assignment has been published. The. What I will do now is to create some breakout rooms for. So I am creating for breakout rooms and you will be assigned automatically to those rooms. You will have about 20 minutes to do it. I will pause the recording for those 20 minutes and I will be here to whatever question you may have. And then after the 20 minutes, I will give you a warning and I will close the room and we will discuss the results. So you will have the opportunity to present what you did. If you want, I will present the possible solutions. And then we will talk about the assignment for next week. Okay. I open the room. I'm okay. So I am resuming recording. So we'll come back. Any volunteer just to share what you did. There is no judgment. There is no nothing just to share. Now I can share, Professor. So please. Yeah, sure. Okay. Let me know when you can see the screen. It's coming up. Yep. Here we go. Okay, let me just stay on somehow. Yeah. I don't know what the lines are for, but when we did is a while loop and we first asked the user for input in USD and we used the instruction from the assignment as to check if USD is digit. If it is a digit, then we would then proceed to ask the user for currency on the exchange rate and then we re proceed to cast both a USD, an exchange rate to a integer and then we have a variable call result. We did the calculation of exchange rate times USD and then we print the statement your exchange rate is USD US dollars for results. You can see the output below. We entered 100 for the currency name. We entered apples and for the exchange rate we entered 77. So now the output is then 100 USD 47700 apples. We also then proceeded to enter a random characters and continue the loop again and again with enter US dollars because it was not an integer. So good, good, good. I mean, the only thing that I can say is there is no way to be out of the loop. So if you remember except size zero, we had done with break. So in this case you don't have this option is that the assignment had the requirement to. No, no, no, no, no, but entirely. Oh, you're just making a statement. Okay. Yeah, I mean, it is working, but there is no way out. Yeah. So that's the only thing. It was not a requirement. Okay. But is generally something that you want to do because otherwise it will run forever. Yeah. Take it and call it out. Okay. Thank you. Anyone else who want to share? All right. So let me share the screen and we go to. Couple options. So does the basic one. No. That's the basic one. So the basic one is basic. I mean, just what you saw. So you have the input for the amount in U.S. dollars. If done, we break. Then checking if digit if not will print something and will continue. Then ask you for the currency conversion rate checking. Same thing if is digit and then calculating the value floating or integer in this case would be the same. And then printing. So if I run it. And I'd say 11. I have no idea what it is. And then done. When it's finished. I mean, it's all good. Nothing wrong with that. They only think, obviously, if we have a. Here. Whatever it is. If the exchange rate, let's say it's 1.4. I will give an error. So and that's basically the way is the Egypt works. So because the point was not consider a number meaning the old think is now the number. So another way that you have to intercept those airports is using tri except so the initial part is the same. So asking for the input, then the checking instead of using a digit and doing a tri except tri. It's basically a way to intercept errors. So meaning I'm asking by that to try to do this operation here. If there is no air ora will continue keeping the. Except if there is an error. Meaning. The amount is not dramatic other than that, except the statement that will kick in will bring to growing input and will continue going back and then. Yes, sorry, I have a question in Python version 3.9, the interpreter. Can you explain a difference between like accept with a colon versus accept with a explicit value like value error? Because it was asking for a specific exception, it was giving like a warning or something of that nature when you just leave accepted by itself without something more after the colon. Before the colon. Sure. I mean, I don't remember. 1: What is the Python version that I'm using? 1: Anyway. So I think I'm using nine. But that's not not the point. 1: Um, um. Well, the try. 1: Except that it can intercept the any kind of error. 1: Uh, you can either leave it open. 1: So the method that you received that is a warning is not something that is blocking the execution. 1: Um, you can, I mean, if you go online, you will see what are the code there for the different errors. 1: So you can probably you can get from the help here. 1: Yeah. Yeah. So you can say, okay, try this one, accept this particular error and then do something. 1: You can also have more than one exception, meaning you can really create a logic in there. 1: So in this case, you can say, I don't know. 1: I mean floating the idea or that you can get it if is not the number. 1: But when you have statements that are more complex, you can have more than one number. 1: I think you want to retrieve value from a database. 1: So at that point, the database may not be available, that the file may not exist in the written code. 1: It can be wrong. So each one will have a different the error code. 1: So you can have different messages for different efforts? 1: Yeah. Go ahead. Yes. Sorry. 1: Would you say that it's best practice to be explicit about the exception and provide a specific where you want to accept or. 1: No, no, no, no, no. I mean, it depends. If you don't have one particular reason for intercepting one particular errata, you don't need to do that. 1: Keeping in mind that you can have, I think a use that. 1: Here. So you can have a library. 1: I mean, we are not there yet, but with Python, you can add functions. 1: So in this case, I'm out doing libraries, doing special things. 1: This function here is ignoring the warnings because sometimes can be a mean in this case and 1: adding it because one library is not fully compatible with another library generating errors. 1: So because I know that there is an error, but there is nothing that I can do because it's just a matter of compatibility between libraries. 1: I just use the function that is bypassing the warning messages. 1: So if the warning method is annoying, you can remove it. 1: But generally speaking, I really don't see the need to specify the error in the except unless you have a good reason. 1: So in this case, quite honestly, the only error that could happen there. 1: I mean, it could also be I misspelled the name of the variable and there could be the variable and not been assigned. 1: So eventually those are two types of carrots and they can have two different accents. 1: But I mean that you don't need to do that. Thank you, Professor. 1: I hope it's not confusing, but that there is always more than one answer to most of the questions by the. 1: Thank you. So. All right. So, I mean, in this case is doing a pretty much the same thing. 1: So if I ran it that. I can have a, I don't know, $22. 1: I want to convert in euros and let's say 0.97. 1: So in this case, again, no error. So and then all the rest would be the same. 1: So again, I will post both solutions on canvas. 1: Generally speaking, there is the Gita, as are quite a lot of limitations. 1: I mean, if there is a floating point number, it will never pass. 1: So the try, except it's the way to go. 1: All right. Okay. So let me now introduce the next six essays. 1: So this exercise is, in a sense, easier to. 1: Then the one you did in class is another conversion. 1: So you want to convert a temperature in Celsius to a temperature in Fahrenheit. 1: As you can imagine, being born in Europe. 1: I am more used to censors than Fahrenheit and the conversion is something that either with a calculator or just mentally I do most of the time. 1: So that's why I'm starting with Celsius to Fahrenheit instead of ISO. 1: That's just to give you a background so the user will be prompt to input. 1: Temperature in Celsius, for example, at 15. 1: And then the system will do the calculation using the formula, the temperature in Celsius multiplied by 1.8 plus 32. 1: I'm still like, sense your sermon. 1: You may think why you are thinking in that sense. I like the idea of a freezing point being zero, so it's kind of easier to me. 1: But anyway, so you ask for this input, you apply the formula and you will put it into the equivalent of this case. 1: 15 degrees Celsius is at 60 Fahrenheit. 1: So this is the formula, and you will print the result with one blank line. 1: And that's basically it. So that's pretty much everything. 1: Let me make sure that you have everything you need to. 1: I am publishing the in-class. 1: Exercises the to the by. 1: Publishing the text of the assignment and publishing the slides that I use today. 1: All right. So that's basically it. 1: The questions. 1: All right. So if you have any question down the road, either myself or you, we will be happy to help. 1: If not. See you next week. Hey, Professor. 1: Yeah. Yeah. Good. Module A modules and canvas is still messed up. 1: Are you. Are you aware of that situation? Okay, which one? 1: Well, when we look at assignments and everyone chime in if you can. 1: It's showing with due dates is showing all of the assignments from. 1: It looks like a previous term of last year. It shows us everything. 1: Oh. Okay. I mean, in theory, they shouldn't be published. 1: Okay. I really thank you for letting me know. Yeah. Okay. 1: And also, in addition, I did send you two separate emails. 1: Is your email address r c la forza at stevens idea? 1: Yeah. Okay. I don't know if you had a chance to look at it or maybe you were getting the same types of emails, so you just corrected it. 1: I did see the correction to the quiz, which you mentioned earlier. 1: Oh, yeah, yeah, yeah, yeah, yeah. I mean, there was an error and a chunk. 1: Yeah, yeah, yeah. Okay. Yeah, that's. That's basically all I had. 1: All right. Yeah. So how could they? Okay. Thank you, you two. 1: And thank you for letting me know. Yeah. Right.
fs STEVENS
lw INSTITUTE of TECHNOLOGY

Intro to Data Science, Al/ML for Systems
and Software Engineering

A
|

a
eA “—-



clipizzi@stevens.edu

SSE

 

What is Artificial Intelligence? S

¢ Alis the study and design of intelligent agents
where an intelligent agent is a system that
perceives ifs environment and takes actions that
maximize its chances of success

¢ Al basically puts human intelligence info a machine
The problem

¢ The human mind ts currently not fully understood

¢ There is no method of determining when a machine
is actually intelligent

STEVENS INSTITUTE of TECHNOLOGY | 2

 

 

History of Al

    
  

  
 
  

  

     
  

STEVENS INSTITUTE of TECHNOLOGY | 3

 

Evaluating Al - the Turing test ie

Turing test

During the Turing test, the human questioner asks a series of questions to both respondents.
After the specified time, the questioner tries to decide which terminal is operated by the
human respondent and which terminal is operated by the computer.




The Turing test, developed by Alan Turing in 1950, is a test of a machine's
ability to exhibit intelligent behavior equivalent to, or indistinguishable from,
that of a human

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 4

 



Al in its context 
 

Data growth

The digital tools we are using every day are creating data from everything we do at an unprecedented rate: every day, 2.5
quintillion (1018) bytes of data are created and 90% of the data in the world today was created within the past two years

The bulk of this flood of data - the “Big Data” -is a
combination of Social Media (such as Twitter or
Facebook), blogs, news, websites

This kind of Big Data is generated by the “online
population”, that is a growing portion of the total:
50% worldwide, 74% of Europeans, 89% of North
Americans

Because so much of the population is generating it,
Big Data can provide potentially useful information
for our lives and businesses

Mining the Big Data requires a combination of tools,

ability to represent knowledge and domain-specific
expertise

* 1 Exabyte = 1 billion gigabytes



STEVENS INSTITUTE of TECHNOLOGY | 6

°

 

Machine Learning and data ie

“Machine learning is programming computers to optimize a
performance criterion using example data as past experience” Intro to
Machine Learning, Alpaydin, 2010

Machine Learning means learning from Data
Machine Learning Is just Data + Algorithms, but Data Is more important

Same steps as Data Science:

Start with an idea and create the data pipeline [Business Understanding]

Find the necessary data - Analyze and validate the data [Data
Understanding]

Prepare the data - Enrich and transform the data [Data Preparation]
Develop and optimize the ML model with an ML tool/engine [Modeling]
Evaluate the results [Testing]

Operationalize the entire process for reuse [Deployment]

STEVENS INSTITUTE of TECHNOLOGY |

 

P

Why Machine Learning e

¢ No human experts
— industrial/manufacturing control
— mass spectrometer analysis, drug design, astronomic discovery
¢ Black-box human expertise
— face/handwriting/sopeech recognition
— driving a car, flying a plane
¢ Rapidly changing phenomena
— credit scoring, financial modeling
— diagnosis, fraud detection
¢ Need for customization/personalization
— personalized news reader
— movie/book recommendation

STEVENS INSTITUTE of TECHNOLOGY |

 

8

 

Current Al research directions ie

¢ Thought processes modeling

¢ Knowledge representation and its application
¢ Natural language processing

¢ Robotics

¢ Machine creativity

¢ Machine learning

STEVENS INSTITUTE of TECHNOLOGY | 9






 

 
 
 
    
  Machine Learning in Software Development
• Software development itself will increasingly be automated
• Machine Learning as the next level of abstraction
• For tasks like face recognition, we use ML because we don’t know how to write the
software, and it has been difficult to collect the data. For other tasks, like billing, it’s easy to
write a program based on a few simple business rules
• Machine learning is already making code more efficient: Google’s Jeff Dean
has reported that 500 lines of TensorFlow code has replaced 500,000 lines of code in
Google Translate
• Neural networks can create new programs by combining existing modules. The system is
trained using execution traces from other programs
• Developer may become a teacher, a curator of training data, and an analyst of results
• Machine learning can be used to generate short programs from training data; to optimize
small parts of larger programs, but not the entire program
• ML is playing an increasing role in Data management and infrastructure
• It is difficult to imagine a future in which humans no longer need to create software. But it’s
easy to imagine that “human in the loop” software development will be a big part of the
future
   
  
  




Machine Learning in Systems Engineering

 

¢ This is a 2-way road: ML can benefit from SE approach and SE will use
approaches, methods and algorithms from ML to better design systems and to
add capabilities to existing systems

¢ Model-driven SE Is traditionally based on classic top down modeling, but data-
driven, ML-supported model are emerging and will eventually synergically use
the traditional top down ones

¢ The “datification” of our society is making available an unprecedented
quantity of data. While in data-scarce environments traditional top-down
modeling was the only choice, now huge data allows bottom-up, data-driven
approaches

¢ When data is huge, from different sources, all relevant, a data science/ML
approach ts the only way to go

¢ Design Machine Learning solution with a “system” view. This would impact the
growing hybrid (human/machine) systems segment

STEVENS INSTITUTE of TECHNOLOGY | 18

 

@

Complex/system Implementations of Al lw

Some form of AI in everyday life
Cars
• Self-parking
• Cruise control
• Speech recognition
Banks
• Monitoring for fraud

ee
Complex/system Implementations of Al

 

Military
¢ Miniature robot tank

¢ Unmanned underwater vehicles

 

¢ Unmanned aerial vehicles

 

STEVENS INSTITUTE of TECHNOLOGY | 20

Hello. Hello, everybody. Oh, there. 1:41 Oh, no. So it's 630 now and we are right on time. 1:49 All right. So, I mean, come on campus today and. 1:57 Let's start so let me start with the screen in a second, but. 2:03 Right. Okay. 2:15 So I'm sharing the screen. And. 2:28 All right. So we are in module five. 2:36 Uh, you did the. 2:43 The previous assignment. So for module module five, that's basically what we have in the agenda. 2:47 So we will discuss the previous assignment. 2:56 We will talk about data in a broad sense, then we will talk about Python libraries and in particular pandas. 3:01 Then we will do the usual in-class exercise and then I will introduce the next assignment, the some comments and that will be the end of the class. 3:09 All right. So the previous assignment that was this one was about working on. 3:22 Who files. And doing pretty much the same thing, comparing them. 3:33 So let's go to a pie chart and let's see an example of the. 3:39 Luciana quoted. So it was three parts. 3:50 The first part was on the CSB file. 3:54 We wanted to keep it clear this. 3:59 We want to skip the first line with the header. 4:03 There are different ways of doing it. So one way could be to read the and I mean the the first line outside of the loop. 4:08 The second is just initializing the counter to minus one meaning. 4:22 Then when you will count, you will automatically skip the first one. 4:29 So I'm initializing the counter at minus one again, not to count the first one to the header. 4:36 Then I initialize a list with the subscriber list. 4:46 I mean the counter of this. Could I borrow the counter for the customer and an empty list for the call that started the loop again. 4:52 Obviously it would have been probably easier using pandas, but you didn't know. 5:06 Officially pandas are up to today. 5:14 Meaning we are using a C as B files and regular this. 5:18 So I'm appending the ROE, meaning the line to the file to the lists. 5:26 Then I'm adding one to the overall counter, checking if the in that row there is a subscriber. 5:34 If yes, I will increase by one the counter subscribers. 5:45 Same thing for the customers. And then I will continue till the end of the file and then I will print it out of the loop. 5:50 The following are the last five lines of the file. 6:00 So I'm going from 1 to 6 and then going from the end line of content to minus. 6:06 And so that's the pointer. And then getting and printing the from the list the pointer, 6:17 I mean the elements with you pointed by the pointer that at the very beginning will be the last one. 6:27 And then because the. 6:36 I mean, the point it's 1264 thing will be one meaning in the end, minus one year, minus two and so on. 6:42 And then maybe add to the three to remove special characters and blank spaces at the beginning. 6:53 At the end I calculated the percentage. 7:01 I use it round with two decimal values. 7:07 You don't need to do that, but it would look nicer. And then same thing for the subscriber. 7:13 Print the results. Part two. 7:20 Pretty much same thing for the other five. 7:24 In this case, because I mean that in the first case, I was printing the last five lines here in print, 7:30 in the first five lines, meaning I want to skip the first one because again, 7:38 it's the one with the header and I don't want to print it and then I'm checking if it's less than five and then if so, 7:44 I will printed the same thing, adding one to the counter. 7:55 So for subscribers and customers and then increasing the overall count. 8:00 Same thing calculated a percentage printing and less to. 8:06 Part three. I'm comparing the two values. 8:12 I mean, the two find the values for the two files. So the overall number of elements. 8:16 And the percentage. If I run it. 8:26 I have the last five lines of the file. 8:33 There are 21 320 21,350 rows in the first file. 8:39 Of those. Those are a user type customer and per subscriber percentage. 8:48 Stay on file. First five lines. Total number distribution percentage. 9:01 Spring riders are more than winter riders. 9:10 During the spring. There are more customers to non-subscribers than winter. 9:14 That's kind of understandable. So during the winter, you have more of the regular clients instead of those that I mean, just go and take a bike. 9:22 It's winter is cold. All right. 9:40 So that's basically it. Questions? 9:43 Comments. Did you pretty much did the same. 9:48 I didn't check your submission yet. So just want to be sure that is pretty much in line with what you did. 9:56 I'm in in this assignment. The only thing. So let me share again. That we are a little bit more complex. We are defined. I mean, the fact that you need to keep one line so you need to initialize to either minus one or just read the first line out of the loop, then I mean that all the rest is pretty much straightforward. You could have probably done a function for dealing with the two sides, but we didn't introduce the functions yet that we will do today, meaning it wouldn't make sense. And then even with the function in the first case it was about printing. The last five lines in the segment was the first five lines, meaning the two were different anyway, but you could have a function to calculate the subscriber and the customer itself, but it wouldn't save much in terms of the processing. All right. So let me go now to the actual content and let me start just talking 37 slides, but we will go relatively fast on that. So Data, what we do in this course is pretty much extracting meaning insights from data. But what is data? I mean, it's kind of a generic term. So let's talk a little bit about which kind of data we can deal with. So it could be structured data like databases, data warehouses, things like that, or can be data that is totally unstructured, like the web, like other pieces of information, music videos, images that may not be in the structured. Databases or data structures. You may want to use it, but that's it. Most of the time when you have a. Structure, the systems for data, you deal with databases. So databases are a large integrated collection of data and they are handled by what is called a DB, a massive database management system that is a software that makes our lives easier to interact with the data. Both the ways. Writing data. Retrieving data. Updating data. When we work with data, there are several layers that we need to consider. So there is the physical layer, meaning a one in zero day an hour on your device, whatever. The device user could be a hard disk to be a remote storage. But at the very end that what you are going to have will be anyway one zero on a device. Then you have a how does one zero hour recorded on the physical device. So if you consider that you interact with that physical device more than once, you may not want to have all the data in one given place, but you need to have somehow a piece of software that will unlock the doors that one zero belonging to one particular file impositions that are optimizing the space you have on the disk. You have sort of cross-reference the role that for each file will point you or to the system to where physically the data are located. Then you have a conceptual schema of the data, what they are presenting because they are representing students. They are representing courses. They are representing clients. Representing whatever. So those are the logical view of the data that we can have. And then you have the views of the users. So if you are in a. Or the example. If you are in a university, you have the view of students, the view of administrative personnel, the view of the faculty. Each one will see only a portion of the entire data as a data base or dataset. So that again, you have a different linear from the very physical to the physical schema that is a layer provided by the operating system. Then you have the conceptual schema that is provided by the DB mass and then different views that again are provided by the DB mass, that order, that software. The advantages of having a DB a mass is that you basically do not need to worry too much about the data itself. So you just I mean, point to the logical. Entity and you will get the information. The master is also doing a bunch of other things, either letting you extract the data in the aggregation you want, or that this aggregation you want is also managing the integrity and the security of the dataset. But the integrity means you have multiple accesses. Again, concurrent access is an example, so you have multiple people accessing the same data, but you need to define a way to handle those situation. Typically there is a lock on the first one, and when the second is trying to access the same data cannot access it. But this lock is either handled by the DB crash recovery. What if in the middle of a transaction you have a power failure and you have no backup you in the power pick up unit. So at that point, typically DBM assets are doing all or nothing. Meaning either you have your transaction completed or you have nothing. So is not leaving in the middle. There are several of those. The. Those are some of the examples. Oracle is the one of the leaders that historically is one of the leaders, IBM, Microsoft, that are solutions that are open source. MySchool is what the general use, even if it is open source but is now owned by Oracle and the community is not very happy with keep using it. There are other options. I mean, the doubt is, yes, now is open. So it's at the certain point the Oracle can pull the plug and say, okay, it's not available anymore. Again, there are other options. Anyway, Dbmr is three main components. So a storage query process or transaction manager. So the storage manager is what is really dealing with the physical writing, updating. Deleting the data on your physical device, the query processor. What is it in charge for letting you talk to the data? Meaning creating queries, optimizing selections of data that you may have. Transaction manager is what is handling the transactions, meaning in error, either making sure that concurrent requests will be handled in the proper way. So those are the SC. The properties again stressing are pretty much the four characteristics that I was mentioning. Let's go now to the logic. I mean, it is a sort of a stack from the physical data to the very logical lot to the different views. So we are going up in this stock and we are now on the conceptual level and we are talking about a data model. So the model is primarily three main characteristics that are entity attributes and keys. The model has been created to meet the 1970s and by Peter Chen. And again, it's called entity relationship because entity relationships are the key ingredient of this representation. So in this chart you have the entities that are in the rectangular shape box. So there are two entities here, a shopper and item, and then you have relationships that are in the room, Boyds. And in this case, you have shopper buys item and then you have attributes so that are this kind of rounded shape boxes where the attributes are the characteristics of the entity. So shopper buys item item as three attributes, type price and sorts. So there's the basic example. In real life, things can be obviously way much more complicated. Thinks about large companies may have quite a lot of those entity and relationships making the old think a kind of complex. Do you really need to have that? In some cases, yes. So in some cases, they are required by law for the traceability of the data that you are handling. The Sarbanes Oxley that was created roughly 15, 20 years ago is saying that all the companies that are publicly traded, we really need to document processes and data that are related to money handling in a broad sense. So you have that you need to have a good representation of the processes and a presentation of the data has to be compliant to the requirements. One of the things that you may want to do with your data is normalization. So normalization, uh, instead of giving all the theory, let's use an example. So this table is from we don't needs to go, but the concept is still the same in this case is using Microsoft access but can be any read VMs and you have all that you have what is in the order you have with the supplier and with the customer. So you have three different things that are in the same table. If the certain point you have a customer who changed the address, you need to go through all the records, all the lines with that customer and change the address. It would be easier not to have three different tables, one for each one of those different objects, different entities. So you have one table for the order, one table for the supplier and one table for the customer. And then you have a way with the keys to jump from one to the other and eventually recreate what you have, what you had in the previous table. So that's a form of normalization that when you do when you work with the other basis, you want to have your data normalized. There are different stages of normalization, but what we need that in this case, in this course, that's for using this generic way of doing a normalization at the school or systems and enterprises. We don't have records on diplomats is and on as you have at the graduate level and I created an undergraduate course for. They the engineering that is rooted in as well. But it is the language that is used by relational databases. But it is again, undergraduate, I think is ISC to 25. But I mean, and since we do have courses on the computer science side of the of the Institute. When you have multiple of those databases each one Fogg used on one of the characteristics of the. Of your business, then you need to have something that will let you have an integrated view. So those a collection of databases are called data warehouses with different logical pieces of the data of your company in that you need then to have something, another piece of software that will help you put together the different pieces to have one single view. So this is the EDL that stands for extract transformer and loader that will take pieces from the different databases and provide an integrated view. EPL, I mean, is it a big business? There are companies that are thriving on it. Um. We mentioned that one of the features of the beams is their ability to interact with the data, to interact with the day that you need to have a language to deal with it. Edgar called the created the relational model and Escuela structured the query language and it is what we use to interact with the relational database. It's. Again that we don't have a course at the graduate level on actual ISC to 25 is the only court that is spending quite some time not on that but. As well is a language meaning like all the languages, like Python. It has all the structures that you expect in any language. Most of the time, you don't write entire programs in as well. You do only the portion that is directly related to the. Interaction with the database. But you could. What I normally do in my programs, when I have a database in board, I write the logic in Python and then I call that the query. Just asking one specific value that is the result of the processing that they that they did with Python. But again, it's basically up to you where you want to place the logic. If most on the python side, the moth on this other side or all in, you have all the possible degrees of where to place the logic. Um. I would keep those. Uh, again, there are several databases. My Askew and Postgres are two of the most commonly used as open source. Again, I using my ask you a lot, but they are pretty much the same. Keep in mind that the scale is sort of a standard, but it is not 100% standard. Meaning each one of the different VMs is may have a special instruction, special features that are working on Dave system and on Dave system. So this is intentional to lock the clients on that particular would be a mess, but you have a good number of common actual statements. So if you write your ask well in a basic format, it will run on all the DB masses. Now. Not everything is on relational databases because a relational database is a required, required structure. When you have data that is not structured, then relational database may not be what you really need. So the database that are not relational are called know as well, meaning a database as well. Assuming that all the relational are using as well, that is pretty much true. But the reality is that there are more and more unstructured data that would fit poorly on relational databases. So the main reason for people looking for non relational solutions is because the relational is they have limited flexibility because they have a rigid schema that you predefined. Think about the Excel you need to have the data that are in the format that is defined by your header to I mean upload new data. If you don't have it, then it is not going to fit. Um, some companies are using a sort of a combined solution, so you have something for the frontend and something for the backend. Facebook has a relational database for the frontend that's called Cassandra. They created it and then they made it open source. And then for the backend they have a relational database. I don't know exactly what they have, but is a relational one meaning information about, I don't know your characteristics. So the network of friends you have the about yourself and all the rest is on a relational database. But the content that you provide, the pictures, I don't know. Details, thanks. Then we go into Cassandra as a non structured and then there is software taking from the unstructured and populate or update the structure. There is no real definition of those known as well. Generally speaking, they are just non. As you well know, the election of I generally use MongoDB as a not relational database. I use MongoDB because the internal structure is based on JSON. That is a format for file. So that is pretty common in exchange of information between servers. Meaning if I have a if I upload, let's say tweets from Twitter, I will receive JSON files and I can just dump those JSON files to a MongoDB and they will fit right away. When you work with those relational databases, you don't have the same features that you have with the relational meaning. For example, you do not have a control of the integrity of the transaction, meaning those controls would be up to you. So you need to take care of the inconsistencies on the integrity of the other transactions and all the other good things that we are in. The list of the capabilities of a relational database is pretty much what you have on the web. So you get information based on best effort so they can be there or cannot be there. So it's up to your code doing it in the proper way. MongoDB Again, it's pretty much 1 to 1 with the relational databases making the transformation from the extraction of data from MongoDB database to a relational one. Pretty straightforward. And yet it is clear that another element that could be of interest is the MapReduce. So MapReduce is a technique for handling relational data databases, where you take the data, you map it with a B, it's a key value. And then when you want to retrieve, you reduce the. Pretty much you merge and sort the key values players and get the results as. I mean really using the the the differences that you get in this emergence of Hadoop is using the MapReduce approach in uni for a loop is not only a DMS like but is more a distributed resource manager that can be files but can be I mean processing resources so so we use it to put together a bunch of a mac mini that we had from a sponsor. So instead of running them individually, we can, we connected them with a layer. All of Hadoop, managing those resources for a sort of a parallel processing hub well was supposed to revolve, shoot up, revolutionize the industry. Um, but if you look at the numbers, the number of Hadoop, uh, systems didn't grow dramatically. The main reason is because when you need more storage capabilities, you go to the cloud. So you go to Amazon, you go to Google, you go to to Microsoft. They do have Hadoop for managing their. Storage system, but you don't see it. So you may use Hadoop without knowing it, but the number of systems is not so big as that. Initially, we don't. When you work with data, one essential step is the preparation. So you want to be sure that you prepare the data in the proper way. Data preparation in the entire process of extracting knowledge in a broad sense from the data can take up to 70 80% of the total time. That data preparation is essential because at the very end, the quality of the results of your analysis will be meaning the quality of the decisions that you will take based on the analysis. It's pretty much highly correlated with the quality of data that you provided. But if you have either a not enough data or a low quality, the analysis cannot be good. Generally speaking, there are three main steps in the pre-processing the data cleaning, meaning you need to deal with the missing values, you need to deal with the outliers. So those things or inconsistencies are all part of the cleaning. Then you need to do data integration. So the more sources you have, the more you know of the domain that you are analyzing. Markets today are so good in pinpointing our needs because they can put together a. Pieces of information about us from different sources so they can put together the beats that we visited through different tracking systems. Facebook is doing a lot of it. Sometimes they have pieces of software in your computer, and when you visit the page so that are in their network, they basically read those pieces. So they actually read the cache memory on your computer, downloading the entire history of your browsing. So that's the page that you visited. But then you have information from the credit cards on your spending patterns and then you have information on a, I don't know, easy pass on the, the, the traveling that you did. Then you have information from your phone provider and so on. I mean, different sources, different formats. But then when you do the integration, you could really go into the repeat in my own name with all the characteristics. And marketers can do that because there are privacy restrictions. They don't go up to that, but they could. So this is just to say, how important is data integration? Without data integration, you cannot really know enough of someone or some problem because one source will give you only a portion of the information doing the integration. It can be complex because the data may not have the same format. Format is not just the format, one is CSP and one less or something else, or one is from a database, a certain type and one another type. But sometimes that can be in the way the entities are named. So just a stupid, simple example. A street can be as Dot Street, the old world as the dot media in different ways. Stephen So can be Stephen's conceit of technology can be Steven's to a New Jersey and so on. I mean that you need to sort it out. I mean, the second part is more on the cleaning, but the first one, the three, is a good example. So that's an easy way. But sometimes you have even more complex than that. You may not have the names of the entity, then you need to extract the what the entities are from the context where you are. So those are issues with integration and transformation. We mentioned the normalization and we also want to aggregate different entities of different attributes that that you have. So if you have, I don't know, an event that is happening over time and you have the number of records and the duration, but then if you do the number of runs in that unit of time, you have more of an indication of what's going on. So that's an example of aggregation of data. So cleaning the integration transformation, those are the three elements that are really important when you do the pre-processing. So that's basically it. Let me stop for a second on this fourth part that is related to data. In a broad sense, questions so far. All right. So let's move on and let's go now to functions. Let me share the screen again and let's talk now about functions. So we mentioned briefly today about the possibility to kind of put together parts of the code, creating an entity that can be used multiple times. So those are functions. So functions are different types. So that are functions that are built in in Python functions that we can import in our code, functions that we can create. So examples of builtin functions are range some land. So those are building functions in Python. You can create your own if you create your own. You use death to define a name that is your own name, and then in parentheses the parameters that you are passing to the function. In this case, I'm passing nothing. And you have defined. Hello? Hello. I mean, like in the conditions. You had the colon and the indentation and then you have it in this case, print, high print. You might not do much more than that, though, when you call in your program. Hello. You will get the high gym. Then you bring something else. Then you call again. And we do again at the same thing. So basically it's a way to write once and use multiple times. So that's the basic reason why you want the functions. That's another example. So you have you define a function to calculate the area of a triangle and you pass the height and the with the two dimensions and you calculate the area and it is you apply the formula and you retard the value of the area. So if you pass those two values to the function like this one, then you will get the area of three triangle Iguala. I mean, you apply the formula, then you change the values, you use the same function and you will get a different value. So obviously if you use it just once, it would it may make much sense to use a function. If you use it multiple times, then yes, there is reason. That's another example. So square root. Same thing. This is a little bit more complex. And then when you want to use it, you just call it and pass as an argument parameter to the value that you want the function to use. Again. That's another example. This is a function with some ifs, so you will pass to the function, the language. If the language is yes, it will print. Hola. If else. If ops is f are not French. Bonjour. Otherwise is. Hello. So if you pass. If you call greet yen, you would get hello. If you do greet if our is. Bonjour. So he's not returning anything. In this case, this function is returning. Hello? I mean, that's a basic stupid function, but, I mean, explaining how it works. So if you. Hola, Greta. It will retain. Hello. And you will have the first one. Hello, Glen. Hello. Sending this ceremony one. You can do what we did here without return. With the return meaning you can pass up the language and then return either a whole bunch of hello and then that's what you have. So it's pretty much the same, but structured in a different way. Just as a recap. So you have in this case, the function is called Max and you pass a law. So the argument that whatever you want to call it is a award that will be passed to the function. The function will do whatever it's supposed to do and then will return W as a result. You can pass multiple parameters to a function. The parameter will be separated by comma. You can have a default value for the arguments. So in this case the function is make sandwich and you have meat cheese. That's the default value of cheddar bread. The default value. Right. If you. And then with those parameters that you print, I will have a sandwich. Me, cheese bread. So if you pass, we go instead of meat and you will have this outcome. Same thing. If you change the value, you will get the first one. I mean, then you will get whatever is the value. Like in the first example. If you pass two parameters, the second part of the first parameter will go into the argument that with no default and this tag on would go instead of the default if it was a third one, that will go to the third one. You can have a variable number of arguments. I generally don't use it because it can be a little bit confusing. But you could. Why and when you use functions, so you use functions for sure for a sort of an economy of scale. So you have the square root, you call the square root multiple times. You write it one once and then you call up multiple times. You can also use the function search to better organize your code, having sort of logical elements of what people use to code. With object oriented approach that's more familiar. There are other ways to doing it. We will say with the functions. Another important element is the difference between local and global variables. So local variables are variables that are, let's say, in a function and we in the function. When you go out of the function, they will have no value. So that's an example. So you have this function up with those values inside the function. I define that a as global. So outside of the function, not assign a, b, x, y, those values. Then I print before calling the function A, B, X and y, and then that's what I get. So I have one, two, three, four, that's what I have. Then I pass the 25 and 50 to the function and then I print after. So if I go I mean, when a calling I'm sorry, when I'm calling the function, I go into the function, I will assign those values. And then that's what I have. A is five, B is. Then I flip the X and Y, meaning instead of 25 and 50, I have 1525. Then I stepped out of the function print after a because a global the value that I set for a in the function that changed the from 1 to 5 meaning outside of the function. Now I have a stay at five, but b is not global. So when I'm out of the function would go back to the original value and same for the other things. So local, global sometimes can be used. Let's talk now about the imported functions. So you import function from libraries. So in Python, there are several thousand libraries that can really make your life easier. Pretty much you have libraries for anything that you may think about. No, quite so. But pretty much. Obviously is not solving your specific problem but is addressing the needs that they got. A million years ago when I was starting working in artificial intelligence, I was in what is called expert systems, so rule based systems. But we also did something that is pretty much in the real mode, machine learning. We did the algorithm some. There is one algorithm that is decision trees. We manually created the algorithm in our code. So that was either a Fortran was lisp was ops five and we created our own libraries. Now I import a library in my python code. I call the function for the decision tree passing the parameters and then they get the result. So writing a decision tree can be several lines of code now with two lines that import the library and call the function. And then so that's something that is making lives easier for coders, not solving the big problems of the world, but it's solving our small problems. So for example, you have a library that is called math, so you import the library and then you bring to the square root. So security is a function part of the library. So you call it the as the name of the function. And then you pass the argument and it's and you get the result. There are different ways for doing it. So the first one is what we. So you can. Import to only the function that they need. And then at that point, you don't need to write the math dope as Quixote, but just as are keeping in mind that when you import the function, you import the your the old function into your code. Some of the functions are big. When something is big, your code will become big. Meaning if you also deal with large data, you might run out of memory. So you may want to say importing only what you really need of big libraries. You can also use the nicknames. I mean, in this case doesn't make much sense because you are basically saving one character only. But if you have libraries with longer name than using a three character as nickname may be useful. But again, that's an application. And those are links. So let's talk about the I mean, we mentioned the building functions. Those are some of the libraries that are very common in Python. Amoeba is one that is kind of the most commonly used library for plotting for a basic. Graphs so you can have a. I mean, you imported with Matlock Lockley. But again, this case being longer P.A. is kind of useful. So in this case, it's sort of arch of a parable. You can do plotline. You can have parameters. Call auto market and style of line scatterplot, but shop by shot the histogram integrated view so you can have one single metadata. With subplots, you define the different subplots and you have one single view. Noon pie is another very popular. A common library is the root is the foundation of most of the scientific numerical libraries. One of the main characteristics is the use of another variable type that is array. So in canvas you have a linker kind of comparing arrays with lists. They are they look the same, but they are different. The primary arrays are used to deal with that exceeds so think an array as a mavericks and then can be one dimension multiple dimensions, but pretty much is what you really need if you want to use linear algebra pretty much. Those are examples. And thus it's probably the most common library in dealing with data, 1: not because it is doing a lot of hardcore data science, but because it's creating data structures. 1: Then you use the data structures for doing something else. 1: Keeping in mind that is based on a known pie, meaning embedded in pandas that are all the hearts that you can get from that. 1: From the nearby. So let's talk briefly on pandas and then you will do exercises on that and we will probably talk a little bit more next class. 1: So the name is not from the animal, but it's from Python Data Analysis Library. 1: Um, there are a lot of similarities between the general concept of table that can be as well Table Excel spreadsheet and the PANDAS data structures. 1: There are two types of data structure. 1: One series. One is a one dimensional data frames, two dimensional a most of the time we call data frame, not no matter what. 1: So the name series is less used even if one dimensional, but it will be more besides pandas. 1: We the library has been created for one specific company. 1: It was in finance. And then the order asked that after a few years of use, 1: I mean the exclusive use by this company to make it the open source and we are all happy that they agreed. 1: In doing so you can import into pandas pretty much any form of data. 1: So can be. This can be. 1: She has defiance. It can be fired from a database so all of those can be imported into pandas. 1: Dictionaries are a kind of 1 to 1. 1: But again, dictionaries are a form of data structure. 1: So I will not do much more than that on the pandas. 1: You will have the slides. You will do exercises. 1: I don't want to skip the the in-class exercise because it's already 730 and change. 1: And I want to make sure that you will have the opportunity to do it. 1: All right. Let me go for a second here and share this screen again. 1: And. So the in-class exercise has two parts. 1: So it starts with asking the user to input three numbers. 1: You want to perform an automatic check on the numbers, and you want to use a function that you will create to do the checking. 1: So no, each one is number or try except by the function doing for all of them. 1: Then with those three numbers, the first part is calculate the factorial as a function for the first number that is being it. 1: Then the second to you will use to calculate the floating value of the solution in the equation. 1: The linear equation a multiplied by x plus b equals zero, where A and B are the second and third numbers. 1: The second and third order of the three numbers, meaning X is equal to minus B divided by eight. 1: So you want to write your own function for both the factorial and the linear equation. 1: And you do not want to use it for this particular exercise except in a library. 1: So let me stop sharing and let me make sure that you have it. 1: Okay. I published the. The solution on the previous assignment. 1: And. Okay. 1: So. And publish the in-class exercise. 1: Let me create the breakout rooms. 1: Okay. There are four breakout rooms with two or three participants per room. 1: Creating and opening the rooms and give you 15 minutes to work on it between 15 and 20 minutes. 1: And then we will reconvene and we will discuss the solution and then I will introduce the next assignment. 1: It is a little bit tricky and so I need to explain what to do. 1: So opening the rooms, I will pause the recording and I will resume it after a 1520 minutes. 1: Meaning in the recording. So we'll come back. 1: Let's give people another 15 seconds. 1: And so comments. Get in there. All right. 1: So any volunteer. 1: Anyone want to share what you did? 1: Nope. All right, so let me share my solution. 1: Make sure to screen. All right. 1: So again, my solution is not the solution, but the solution. 1: So just to be clear, as usual, obviously, I had more time to refine it, to present in the proper way. 1: You had up 10 minutes or so. So that's not possible to be done in 10 minutes for most of us. 1: But that's the way it is. So comments over here again, comments can be on multiple quotation symbols or single astorga. 1: This is the function to check if the value is numeric. 1: Is basically doing is getting the value then is trying floating of the could be into same think 1: floating for the value if there is no error meaning is going here and or with no problem with it. 1: Why, if there is an error will go into the accept and with have down and. 1: This is the function for the factorial. 1: So taking the number, checking if it's zero, the factorial zero is one and is returning one if the number is zero. 1: Otherwise it will return recursively. 1: The value of the factorization until the end of the loop quoted. 1: Over here, I'm defining the function not to calculate the linear equation. 1: And that's the formula, passing a list of numbers. 1: So instead of individual numbers, they pass the entire. 1: So what is this top number? So I initialize as an endless list of numbers so that the user will provide this. 1: So we know that there will be three numbers. So I'm counting them up initializing to zero. 1: And then while counter is less or equal to two, 1: I will look for asking for a new numbers with just telling the user how many numbers he already entered. 1: For each one I'm calling the is numeric function getting the result and either going back to asking again or asking the new one, 1: then predicting the factorial. Just calling the function up, passing the first value, and then solving the linear equation. 1: Passing the entire list. Keeping in mind that the function would take the second and the third values only. 1: And then printing the solution and the process the mirror at the. 1: But number, let's say it's five, then 22, 66, whatever it is. 1: So I have a. I mean, just not is the fact that he's kind of a useful tool for the user to know where you are. 1: So it's one out of three, two out of three, three out of three factorial solution. 1: They are recreation and the liberals and that's it. 1: Questions. All right. 1: So let me go now to. 1: The next assignment. Admin closed this section. 1: Right. Share the screen again. 1: Okay. The assignment is again on the CD biker bike sharing program in New York City. 1: And in this case, we are going to compare two different finds, one from November 2021 and 1st November 2016. 1: That. When you compare data from different periods. 1: So even if they want to represent the same think or they may be different. 1: So and this is the case. So let me go to the two files. 1: So those are the two files that we have. 1: So this one is the. Newest one. 1: This is the oldest one. So in the older one, you have. 1: Repudiation that is not in the new one. 1: You have the user type that is either a subscriber and a customer where a subscriber is a subscriber, 1: customer is someone vocationally like this guy here occasionally getting the bike in the new one. 1: You don't have subscriber and customer, but you have a casual or member. 1: So member a subscriber. Casual is customer. 1: So again, semantically is the same thing but has been represented that in different ways. 1: So again, you have two different files collected from Jersey City, one in 2016, one in 2021. 1: They are pretty much doing the same job, but the format is slightly different. 1: So that's probably the single most complex part of the assignment. 1: So then, I mean, the things to do are kind of similar to what you did in the previous one. 1: Uh, the indication that and giving you here are not using pandas, but you can use pandas at this point. 1: If you don't use pandas, then you use lists. So each line meaning. 1: Each one of those. Let's say this one up would be. 1: One element of your list and with if you call the entire data set data then would be data want to do because you have 1: to file to read the data to would be the third line that you will read and it will look something like this one. 1: I mean, that is not from those files, but it will look similar to this one. 1: So if you want the data, you need to go into number two, in this case, zero. 1: If it is the fifth entry, it will be data for zero, meaning you are pointing to the first element or the fifth row. 1: So that list of lists, that is kind of a usual thing to do. 1: But again, if you use pandas, you can do it in different ways. 1: So you want to create a function called the print details that will loop into the list, 1: that will collect the the daily trip duration that, again, you have in one file. 1: The actual duration. And for the other one you don't meaning either you take for both the difference between and in the started, 1: or you take that for the second one and the trip duration for the first one is up to you. 1: So. You, Luper, you take that reiteration and then you will. 1: Create a blank line, you will print the average daily trip duration for the five most popular starting and ending stations. 1: The number of members and casual users. So again, that duration is different to five members and casual. 1: That's the way a user types are called in. 1: The new one for the old one is a different way, so you need to create the same language. 1: Compare that the results from the two files, the comparison that should create the elements to evaluate the ridership. 1: I mean, how the ridership changed over the two years. 1: And then you will print the end of the processing. 1: You will write one two page report based on the data hub over to explain how the ridership changed. 1: So that's pretty much it. 1: Again, the most there are two complexities. 1: One is the fact that the two fights are structured in different ways with the same informational content, but presented in two different ways. 1: And the second, if you usurp and assert that is the complexity of working with pandas for the first time. 1: Again, you can read into Pan thus. And if you don't use Pan thus working with the least obvious, that can be with some level of complexity. 1: Okay. So let me. Publish everything. 1: So you have the assignment. 1: Okay. I would publish it. Now I can publish right away. 1: I published the in class. 1: Exercise. And I'm publishing all the material and the recording will be available as soon as a zoom will allow us. 1: All right. So questions. 1: Any professor who's having trouble accessing the pre pre-work module for module five, the pre class assignment. 1: I think the ungraded email on there will be. 1: Yeah. I um. I made it available. 1: Can you check is available now? Yeah, I just do it. 1: But I have to do that. Yeah. Yeah. 1: I mean, that that's absolutely fine. I mean, also considering going into the zero four is not a problem at all. 1: I'm sorry if sometimes I don't get back to you by the million things, but I definitely care for your comments and that definitely. 1: That's fine. I assume I assume that you accidentally deleted it or anything.

Hello. Hello, everybody. On the weather less. 0:43   Hoboken less. So we had. 0:48   A broken main yesterday. 0:55   So it's more than 24 hours without water. 1:00   So it's a lot of class. All right. 1:04   So let's wait another minute or so, less than a minute, and then with start. 1:15   Okay. So we have a relatively busy schedule for today, so we have an assignment to review. 1:29   That was not an easy one. 1:42   I would say that there were several things that we are not exactly mainstream compared to the previous assignments at least. 1:47   And then we will talk about the. 2:01   So quit engineering in a broad sense. We will talk about the visual intelligence and we will do an in-class exercise. 2:07   One of you asked me to give more time for the in-class exercise just to have more options to practice. 2:20   So I will do my best to be as fast as possible in the initial part, meaning the lights and the lecture and all the rest. 2:30   So. Let me start sharing this screen. 2:44   Welcome, everybody. So I'm sharing the screen. 3:00   I would start close this. 3:05   I would start with this. 3:10   So in the previous assignment, there was a. 3:16   Now. It's the trauma of the previous assignment that was on. 3:23   The Sidbi files. So the main problem with those files was that we had two different files with two different formats. 3:32   So one had the duration to the other, one had not yet all. 3:51   They have us started and ended up the time. 3:57   And the other difference was the. 4:03   Use a type that in the old one was the name of the field user type and was a subscriber customer while in the new one. 4:09   Instead of that user type member, underscore casual and you have a member or a casual. 4:23   The other issue is that there were some characters that are a blank and I mean that. 4:30   When you work with the real data, things can be messy. 4:43   So when you work in a protected environment, all the records are true. 4:51   But there are no missing values unless there's an exit size or missing values. 4:57   And we think that there are no strange characters in in the content. 5:04   I mean, in this case, in the cells and everything is fine. 5:13   When you work with real world data, you get what you get. 5:19   So if you work with social media, for example, when people is posting something, they character, etc., that they may use may be different. 5:25   So if they are in one geographical region, you have a given character, etc. 5:36   Another geographical reason is that region is a different character, etc. 5:44   When you put them together and you use the US character, etc., some of those values, 5:49   some of those characters may be recognized for something that you would not want to consider as such like. 5:57   And then the line and the fine. 6:05   So those characters that need to be cleaned somehow. 6:08   So those are some of the issues that you may have when you deal with real world data. 6:14   So in this case, there were some missing values that somebody else that we are not numerical 6:23   know when they were supposed to be numerical or some other values that we are. 6:28   Data, but may have not been recognized as a date with when you read into pandas. 6:36   So those were the major challenges. 6:46   So let me go here. I develop the solutions in two different ways. 6:50   One with Lisa and one with pandas. 7:02   So we introduce pandas last class. 7:07   And some of you may be not that familiar with that approach and may want to use lists. 7:12   So with the lists, things are more kind of convoluted. 7:23   So just as a quick comparison, so there's the least version, any is a 129. 7:31   I mean, obviously there are comments that are blanks, but all in all is under and 29 the pandas version. 7:38   Same thing with all the comments and all the rest is at 79. 7:46   So 79 compared to 129. 7:51   So is a big difference. So. 7:57   And then, unfortunately, the way the lists are working, when you work with lists, be prepared to do loops going into the content. 8:02   When you work with pandas, you may not need the loops because you may have ways to do the calculation on the entire data frame. 8:15   So with the lists, the idea is to read them those files into two lists of lists and then deal with whatever the content is going to be. 8:29   Because we have been asked to use a function and to be named the present details. 8:47   We want to be sure that they input that this function is going to get there will be the same. 8:58   Even if the two files have a different structure and different content in terms of. 9:05   I mean, how to name specific things like member casual or a user type. 9:12   Meaning that before passing the values to the function print details if we need to create one single format for both of them. 9:20   The print details. Function. 9:33   It's relatively straightforward. So I initialize the all the counters that I will need. 9:39   I started a loop for into the content. 9:50   That is a list of lists with all the records that I read from the files with some changes that I will show in a minute. 9:56   So you do you start the list, you add the duration so that this function is taking a duration.   So just want to draw your attention to that is not end and start and end of the trip, but duration.   So meaning that I calculated the duration before passing the value to the function start sation and station and that   member or casual meaning I already changed from a customer and a user whatever was the name into member and casual.   So. I increase the duration by the value in duration for each line.   Then if is member one to the counter of members.   If casual one to the counter of casual.   Otherwise. Meaning if he's a blank or something like that, continue.   Meaning doing nothing. So it is not meant, but not casual meaning.   You have a blank. And we know that there are some blanks on that column to continue.   Meaning doing nothing then. I'm checking if the name of the station where the tweet started is already in the dictionary.   If yes, I will add one to the account. But we did something similar when we calculated the occurrence.   The frequency of names in one of the in-class exercises is the same approach.   I use a dictionary to count them. I mean, in pandas, you don't do that.   You just count using a function embedded in pandas.   That is a nonparty function. So.   I mean, over here. I knew just that we already did a few times.   I do not want to go back to the same then same thing for the end of the start, station and station.   Same thing when I finished the lupus.   I have all the information for that file and I can print that the average daily duration is and all the rest.   Now I define a structure for the date that is a year to month date.   Now what? Minutes and seconds. So you need this case.   You need to define the format for the date.   Opening the first file. Initializing a list that will contain the records will be, again, a list of police.   Then I started the loop. If subscriber or customer.   Start time is going to be. I will take the format that I define here to create a value with the start time.   Same thing for the end time. Calculate the difference and then I add the to list the time difference and the value for the.   For the state, stopping an interstate and ending station.   Else continue meaning there is a blank or something.   Uh, then, um. I'm asking you if subscriber.   So this is basically to eliminate the basis.   The blanks in subscribe for customer can be done in a different way.   If subscriber, I will append. Member So I'm doing the transformation of transcriber to member.   With this statement, that same thing, if casual would be is customary, would be casual, and then I will append at the end.   I will have a list of lists with all the records within the first file.   Pretty much the same with the second file,   but the single file is going to be easier because I don't have the this portion replacing the subscriber with member and the customer with casual.   I already has those elements, so that's why.   Is it shorter? Then I call the function for the first one, call the function for the second one clause in the file not required.   But I did it. And then I am printing and processing.   So let me run it.   So that's basically what you have a process in the first file average duration most populous start inflation and inflation casual users members.   And then same thing for the second file.   Again. That's with the lists.   Without pandas, things would be faster with the pandas shorter.   But there are some issues in particular.   Keep in mind that if you are using Max and if you have one of the latest generation of a max or the one with the Apple silicon microprocessor,   you may get errors. So I actually created a new directory.   You see, that's one I'm in the. This one is the old one and they add to the warnings because otherwise I would get notifications of errors that   were due to the non compatibility of that version of the version of Python that I was using on my old Mac before,   but with an intel processor on the new Mac with an Apple processor and it didn't work well in some cases blocking the execution of the program.   So I strongly encourage you, if you have this type of errors, go to the site on.   Website and download the version of Python that is not the internal one but the other one.   I don't remember how it's called, but you will check it if you have problems.   Obviously just send me an email and I would send you the other way to go.   So with pandas again it will be shorter.   If you look at the function. So the function is basically just doing a small calculation on the time and then printing.   So this case is taking a is taking a.   Taken of over a generation. Most popular staff station.   The most popular RN Station. A number of casual users and number of members.   So opening the files and reading the science into pandas data structure.   Initializing a list for the values for printing them.   The one that I will pass to the PM dictates function.   So what I'm doing here, I'm calculating it again.   That's not the only way to do it.   When I tried to do it on the all data structure, I got an error that most likely was related to the wrong version of Python.   But I decided to change it and work on separate data frames for the different parts that they would need so they'd start.   I calculated I transformed the value using the pandas to the time, specifying the format.   Again, I could have done a different way using a just like I did for the previous one and variable with the format instead of rewriting it.   So I did just two times. Doesn't change much.   Same thing at the end. They put one on the stop start time.   Stop time. Duration. It's just a difference.   I do. You need to use this type because otherwise you will get an error or I mean, it is not something where you can do a calculation upon.   And then I added the duration to that data for the search list that I will pass to the.   A function that most frequent station.   Super easy. So just from the dataframe that I created, I calculated the name that is according the most one line instead of a.   It's very powerful. And then I added this value to the list that I will search for, for printing, same thing and station user type.   Well, in this case, I don't need to do the conversion.   So for the first one, I can calculate the number of members as a user type.   So as I mean from the the call on user type and calculating subscriber.   Meaning I no really need to do any conversion I calculating as members the records   with the subscriber in the same thing customer and then passing it to the list.   Similar thing for the second file. So same thing.   On the first one, I'm not even considering the field duration.   I could have done in a different way, but just wanted to have the same approach for the two files.   So. Duration. Adding to the list the most frequent same thing that I did for the first one in this case cases number of members   is a name taking a different name for what call on it because in the second one I have a different for the name.   So the first one was user type, the second one is a member casual instead of looking for a subscriber and customer, in this case, a member and casual.   So that's basically it. At this point.   I have in data force all the elements on the first file, not the actual elements all defined, but all the values that they want to print.   So it's a slightly different approach from the first one.   The first one I was passing the entire file. So this way is more efficient and will generate the same results.   Then calling the function with the fourth with the second that I could have done in a different way.   Obviously, instead of having two separate. I could have a list of lists and doing a loop here on the two elements of the list.   But I mean, it is just two elements and so doesn't make much sense and not saving that much space with it.   So those was where the results from the non pandas version, the list version.   This is going to be for. The new one.   That is exactly the same. The only difference is that I have a different format for the duration because I wanted to have.   I wanted to have Howard's minutes and seconds just to have a nicer printing.   But I mean, it's basically the same value in a different format.   Okay. Questions? I have one.   Well, yeah. I mean, I would be surprised if no one had questions.   It was a complex. I have a real one and then a bit of a silly one.   Know. Is there going to be grading on this one?   Because this gave me this is a real run for my money and faster in terms of like difficulty jump from the last couple of homeworks.   Well, yes, there is there will be grading, but it will be kind of generous grading, considering that is more complex.   Okay. And then I had another question that was actually homework or is that one kind of in the works?   It was definitely a bit more hefty, I guess, than the other ones in terms of speed, but it'll take to grade.   But I was just wondering. It should be graded sooner.   I mean, our T.A. is grading all the assignments.   We also have a greater. But we have two sections.   4 a.m. 624. Okay. We have this section that is relatively small.   And then we have another section with the 55 C. So that is running on Wednesdays, meaning they may have a little bit of issues on their.   Okay. Thank you so much. And in particular, just to be very honest, we had several cases on the previous assignment of cheating in the other class.   And so that's why we are a little bit behind.   Just to let you know. So it took me almost a day to go through the entire process.   So we have a tool that has been developed by University of Stanford that is comparing.   I mean, you provide the to the tool, all the assignments,   and they do automatically a comparison 1 to 1 with all the combinations at the   very end that you have the number that will tell you the level of similarity.   We used that level of similarity as a sort of red flag to determine if there could be plagiarism or not.   Once we have the red flag, then we go 1 to 1 in that we manually analyze if there was a plagiarism or not because I mean,   that is an automatic to learn, may work or may not work.   And at the very end that is our responsibility to do something.   Then I have a spreadsheet for evaluation.   So with the grade for the assignment of the number of people doing the same assignment.   In this case, we had the C six classes.   So, I mean, it's a B class. We have the largest cluster with four people.   There's more. Laura There's more or less cluster with two people.   So one of the parameters in my model is the number of people involved in the project.   So you have the the grade that can be hundred or a can be not so good, can be 80, whatever the number of people in that particular case.   Then the evaluation from the tool and then the if the like.   In that case, if there are two parts of the assignment, what is the relative weight of the two?   I can probably go and show it to you.   That is probably easier. No, I don't.   Okay. So and at the very end, different things.   Okay. So let me share this. So that's what we have.   We have in this case, there were two parts.   One, that was the coda and the other one that was the interpretation.   So you have the score for each one of the two parts, the number of students that are involved.   The percentage of similarity from the tool to the revised two, because again,   we do 1 to 1 then is calculating what is the penalty and then the actual score for each one.   Then you have the relative weight. You have the average.   Because if. The average Joe dissimilarity is above the similarity that the students have.   Then there is no penalty. But if the similarity in their submissions is higher than this will kick in.   And then you have this or grade. So as you can imagine, this is taking a little bit of time.   So my apologies, but I really wanted to give you all the insights.   We want to be fair. So and we want to be clear in the process that we are doing so.   But you can be sure that within a couple of days you will be graded.   I appreciate the inside scoop. Thank you. This year, I mean, things can happen when you have a class that is 55 people coming from different cultures.   Sometimes they really think that sharing solutions is something that you are not supposed to do.   So I'm generally very clear at the very beginning of the class.   But I mean, some students may not even be so fortunate.   That's how. Other questions. Yeah.   So I got a quick question. Yeah. Good for the quiz for quiz five.   I know it's for you know, after I did it, it didn't tell you, like,   which ones you got, right, which one you got wrong, like it just gave you the score. Didn't show you that the solutions is the one.   Yeah. I don't know why that I will check it.   I mean, it's just a matter of the box that probably has not been checked.   The one of the questions that they had was on how you call, uh, the library number.   And was a you and is right. Who asked me that.   Yeah. I had mentioned that in my email yesterday. Yeah, yeah, yeah, yeah, yeah.   So, I mean when you call a function, there is not one single way to do it.   In that case, there was that one way.   That was great. That is one of the many ways to do it and all the other that were wrong.   Keep in mind that with by on capital letters sends more letters.   They are different. So if you have the import non pi with the capital m is wrong because there is no library.   No by with the with the capital n.   So. But again, I'm sorry if there was no published solution for that.   I will definitely check the box and make sure that it would be checked.   Thank you. Sure. Sorry about that. Other questions.   All right. So. Yeah.   Yes. But the for this assignment and Simon finds out we're able to resubmit.   Yeah. Yeah. Be sure that you are not using the solutions that I presented.   Okay. Because, I mean, it wouldn't be fair. Of course.   All right. Thank you. Okay. All right.   So let me share the screen.   And let me go now to the.   File for the lecture. So let's talk about data science and machine learning.   We do offer a course that is a machine learning for systems and enterprises is a course so that I created a couple   of years ago and it's running once a year in fall and as am6 24 as a prerequisite because it's python intensive.   I mean, m 624 or equivalent. So you need to be proficient in Python that would take the courts and is not one of your electives.   But if you are interested, we can find a way some of your colleagues in the same cohort.   The structure program took it and they liked it.   Um, I'm also writing a book on a suicidal impacts of a machine learning,   and some of the things that they would present today are somehow related to the work that I'm doing.   What is artificial intelligence? So, I mean, we can talk for for days, months that the computers do not have intelligence.   So no computer, no system that is being created.   It's intelligent in the way we normally use intelligence as an attribute.   But the point is just that we don't really know how to define national intelligence if we cannot define national intelligence.   So we cannot define artificial intelligence, the history.   But we have a mean test to evaluate the intelligence.   So we're talking to me not about that. So the history of artificial intelligence is a starting back several centuries ago,   just because there is no formal definition in the given the period of time when people realized that was an intelligent behavior   or a behavior by the machine that was similar to what a human was doing then that that was labeled as an artificial intelligence.   Generally speaking, artificial intelligence is considered to start with Alan Turing.   So in 1950, Alan Turing was a.   Working during World War Two and decrypting encrypted messages from the Germans.   And it was a brilliant mathematician.   And he created the Turing test. He created this decryptor a machine that was considered, for example, an intelligent one.   Then if you move forward in that, let's say in the second half, in the 20th century, in the 1900s,   artificial intelligence kind of took a little bit of momentum in the late eighties.   I was involved in artificial intelligence.   I was in a R&D company within an information technology group, and I was working on what we are called the time expert systems,   meaning systems based on rules that the rules that we are representing, the expertise of the experts in a given field.   So that was in 1986 when they started working on that.   Then we realized that no matter how many roots we can create, we will never really have the same level of expertise as a human being then.   Obviously there were other conditions, so we didn't have enough computing power.   So. We didn't even have a computer with a graphical user interface.   So the power of the computer were limited.   We didn't have a python with all the libraries, meaning all the algorithms.   So we had to write themselves ourselves by hand from scratch.   Meaning? I mean, it was a lot of effort.   Now you call a library, you pass the parameter, and you are good to go.   Not at the time. So the limitations of the hardware resources, the limitations of the.   Talk to the interpreters or copilot that we had at the time, the limitations of the data we had.   So now everything is digital at that time.   Very little was digital, although it was a lot of analog meaning how you can do machine learning or a data analysis if you don't have the data.   So anyway, so then because of those limitations, there was what was called the winter of a yeah.   So for a good 15 years, almost 20 years, nothing happened.   And then with more data, more power and better languages, we started again.   So this is just to give you a little bit of. The context and perspective.   So artificial intelligence is not something that was born a few years ago, but is something that is relatively old.   I mentioned the Turing test, so the Turing test is working pretty much like that.   You have a wall and you have on one side the human being on the other side.   We don't know. The human being is placing question to whatever it is on the other side of the wall.   If no matter what the questions are, the answers are such that the human being cannot define.   If on the other side there is a, uh, a human or an artificial entity.   Then they are artificial and it is an artificial entity.   Then the artificial entity passed the Turing test.   Um, you will know that we have since on the health of November.   Chuck GP That is a. On all over the news any is considering artificial intelligence.   So people is using it for a million different things.   It is a very powerful tool, but it's pretty much the equivalent of Google with the layer of, let's say, a conversational manager.   So what is doing is taking a mountain, an ocean of data,   and then discovering patterns in the data and matching the questions that you have with the partners.   And based on the proximity of your question with the past, answer is giving you an answer in a conversational way.   So it's a sort of compile the Google search answer presented in a conversational way.   Would it pass the Turing test? Well, I asked Chuck Gupta, would you pass the Turing test?   And the answer was absolutely right.   It depends on who is asking the questions.   Because if I am asking questions that are pretty basic, I may be bold.   So like that they are called the lines, the larger language models.   So what to do? The NLM may pass it.   But then if the question questioner, the human questioner is more advanced, it wouldn't pass.   So you may already have grabbed that in different outlets.   But if you search, there are interesting examples of failures of charter.   Some are kind of ironic. Some are funny.   So it's kind of interesting. Anyway, a what is a a generally speaking, a part of automation.   Not everything that is automated has intelligent.   But if. If. You have something with artificial intelligence is part of the broader approach to automation.   Robots can be intelligent or not.   If you have a robot piling up a bulldozer and doing all that, we cannot really define it as intelligent.   Within artificial intelligence, you may have autonomy or not.   So if you have a group that has nothing to do with autonomy.   If you have a so-called self-driving car that's never been there, that not happening.   As we know from recent news that is on the line of autonomy.   Drilling down artificial intelligence, machine learning, data science.   So those are three components that are kind of interconnected.   Not all like the machine learning or better not all the artificial intelligence is machine learning.   Generally speaking, there are two schools, one that is called symbolic, that is based on symbols.   Symbols can be a taxonomy.   So taxonomy is a structured approach to knowledge.   Consider the taxonomy of the human kingdom of the human or the animal kingdom.   So you have animals. You have either no mammals, no mammals, and then you have all the different animals.   So that's an example of a taxonomy of a classification of a domain.   Each domain has its own classification taxonomy, and you can use the taxonomy as a way to navigate into the semantic meaning,   the meaning of the documents or the images or whatever elements with the informational content that you want to analyze.   So that's an example that I mentioned before, the expert system with rules.   That's another example of a symbolic approach. So that's one way to interpret artificial intelligence.   If you go back in time, this approach is based on that.   So in the representation of knowledge, there were two different approaches.   One that is based on the concept.   We start with a blank slate and then we learn a.   With experience. So those are the empty seats.   And this is what the machine learning is doing. This is what it is doing from the data.   All the knowledge is there. Then there are algorithms that are optimized to get to discover patterns in the data   and present the pattern and associate the most appropriate patterns to your request.   Is this a representation of a knowledge?   Yes. Is this a representation of knowledge that we use?   No. Meaning you cannot really define intelligent a system that is based only on a given data because it is not able to analyze new data,   is not able to grow. It is just doing a matching though it discovering pattern and doing a matching.   There are many other things that I would be happy to discuss with you, but I don't want to drive the entire class on that.   So one is the empiricist machine learning.   The other one is based on a concept that knowledge is something that is embedded in our brain.   So we have a pattern. So we have a predefined model that we were already born with it.   So and that's the symbolic approach. So one,   but I mean that those two schools of thought where I mean that they started 2000 years ago or the traditional   artificial intelligence or symbolic and the more modern machine learning approach are mapping 1 to 1,   to schools of representational knowledge that are 2000 years old, obviously.   We can do machine learning today because we have a lot of data.   You couldn't do machine learning. When I started working in Artificial Intelligence in 1986,   because there were not enough data to do battery recognition or to do any inference on the data.   Data science is working for both. So when you have data, you need to process the data.   What we did, for example, in the last assignment there was a cleaning the data, processing the data present in the data.   So that's the real data science.   Not everything in data science as an intersection with machine learning or artificial intelligence, but when you do in particular machine learning,   it's kind of difficult to do machine learning without using elements that are typically originally from data science.   In data science, you have all the data preparation, you have all the algorithms that you normally use in machine learning.   So the intersection is pretty tight. Again, one of the key elements is the availability of data.   Without data, you cannot do any machine learning.   So machine learning. A system that is based on machine learning is to make main components, one that is the algorithm, one that is the data.   Algorithms with no data will generate a model of no user data with no model.   So you can do something, but it's not going to be a machine learning system.   So again, that machine learning system is as good as the combination of algorithm plus data.   What is more critical is the data.   Keep in mind that on a chart GPT, they use an algorithm that is a I mean several are good, but the core algorithm is what is called transform.   That is an algorithm created by Google and.   It was used this transformative approach as a base for what is called GPP.   That is the first instance of challenged jeep.   So it is based on GP2, GP2.   I mean, all of those are based on neural networks that are a sort of representation of the human brain.   In the brain we have synapses in the neural networks, in the artificial neural networks.   So you have the weights that you provide in the different multiplications that you do in the representation in GPT.   Again, the GP2 is used and is using a half a billion of those weights of those parameters.   We are in that GPT 3.5 with GPT four.   That is due shortly with I don't remember how many billion parameters, probably 2.4 billion parameters.   To train those systems to train a child GPU that is based again in on GP2.   The course was a $1.5 million and as much energy as the energy required to run 1200.   That's the number that they had in mind. But it's more than 1000 anyway.   Medium sized cities in the US.   Where I'm in. The energy is poor running.   The computer's doing the calculation. So that's basically the difference between a GP and the GP.   So they can only see GPT. They then building by the way, will have a tiny fraction of the data,   meaning is not going to be as smart as the GP, GPT, even if the algorithms will be the same.   So I mean the core algorithm is 300 lines, so binary code.   So it's that much. But then you have several billion words collected from all over the Web.   So those are some of the directions.   In a robotics is definitely big.   There are several flavors robotics,   robotics and autonomous robotics or robotics to enhance human characteristics that can be physical or can be mental.   Natural language processing definitely one of that creativity.   I don't know about that. So Gardner is one of the leaders in.   Market analysis and technology is doing those hype cycles where her technologies,   where you start with the innovation, then pretty much everybody is talking about it.   Think about. So they will reach the peak of expectations.   And then either the technology will die or will become mainstream.   So that's basically the. Life cycle of technology.   And you would see quite a lot of things that we were discussing.   You would see autonomous vehicles, cognitive systems and natural language processing.   And then you have different calls for when Gartner believes that doing things is going to happen.   Obviously, when you have elements that are more than ten years, then who knows?   I mean, in in technology, something that is so far that they need we don't know what is going to happen.   So if you look at the bottom left, the artificial leg in general, intelligence,   meaning something like a system passing the Turing test with the examiner that is quite knowledgeable.   It's far, far away. So we say more that then we have said, but we have no idea when it's going to happen.   Same thing for autonomous vehicles.   We know that Tesla has recalled most of the vehicles with autonomous I mean, the self-driving component.   I mean, there are several complexity issues on there.   But that's another view. And I would keep on that.   Just driving the attention to the chart on the right.   The number of public companies mentioning AA and machine learning in their earnings calls.   So we went from zero to a huge number.   Pretty much every one. We want to be in the AA machine.   So it's events. Each school is, of course is on that a machine learning because each school wants more students.   And when you have machine learning, students would come, oh, is this something good for the schools?   Yes, it is something good for students. I don't know what mean is generating more confusion than ever.   But that's another. A set of charts.   Look at the bottom. Right. So the sentiment on the articles referencing a yeah.   So from that skeptical period we are going to the enthusiastic period. 1:   So a lot of people is thinking that a could do. 1:   I'm in. I don't believe in a controlling what you do. 1:   I do believe that there will be high expectations, probably a use for what he cannot do. 1:   Generating results that could be disappointing. 1:   Hopefully will not be harmful somehow. 1:   Amazon is using elements of machine learning in software development. 1:   Machine learning can be used both ways, can be used in helping develop and developing software that can write code for you. 1:   I'm in. Feel free to use it. I use it for a few things, but you really need to know how to code to get a real benefit. 1:   From what Chad duped he can give you. 1:   By the way, not taking clients, meaning you cannot say okay, redefined, created a program to redefine and do something based on what is in the field. 1:   You cannot do that because cannot read clients. So in a sense it can be helpful in that way. 1:   But also software development can be helpful to develop better machine learning systems. 1:   You may know DevOps as term, meaning all the set of rules and methodologies to better develop software. 1:   When you develop software is not just writing the code but is having the code running in the system. 1:   In the larger system, when your piece of code will do his part, it's pretty much the same thing with machine learning. 1:   You have some code that will run within an environment. 1:   So this set of placing your code, the real world is a managing the operations of the code and has the equivalent. 1:   So a lot is on DevOps for machine learning. 1:   System engineering. Same thing, keeping in mind that when you develop something, 1:   even if a brilliant piece of technology based on machine learning is still a portion or something that can be bigger. 1:   So you need to keep the system view of what you are doing with that human so that somehow as to be in the loop. 1:   Just a brief example when you when they develop the chart the GPT. 1:   They had not one algorithm but was a method. 1:   So you have it. The first step is collecting this ocean of data. 1:   The second is to have human beings going into these additional data and doing two things. 1:   One, eliminating content that is inappropriate. 1:   And unfortunately, in the Web, there is a lot of content that is inappropriate. 1:   So those humans are doing a job that is one of the worst jobs that a human can do. 1:   Meaning going into the inappropriate content and remove it in five days, a week or more, 8 hours a day or more, just reading inappropriate content. 1:   So it is not a great job. And the second thing that they are doing up with tagging the. 1:   The area is machine learning with the highest number of humans employed is on net tagging data, 1:   meaning you have her text images eventually sound a new target for what they are 1:   that those will be to train the system when the system will read something similar. 1:   So the humans are cleaning, tagging a portion of the data, not the entire mountain. 1:   Then the system will use this first training portion to train a larger portion, and you humans will give scores to that. 1:   And then you have another element that is based on an algorithm that is called regarding neural network. 1:   That is basically using as a score to the proximity to the evaluation of the humans to evaluate the additional data that you may have at that point. 1:   You have a relatively large set of data as a training subset, and then you use that to classify all the rest and target in the proper way. 1:   So that's how the pattern of creation has been done. 1:   Once you have that, then you can apply systems for classification as the transform that was mentioning before. 1:   So again, it seems to be you have an algorithm with data, you apply, you're good to go. 1:   No is a system and you need to design the system in the appropriate way. 1:   Some applications are so. Uh self-parking. 1:   Cruise control or speech recognition, banks monitoring fraud. 1:   Those are examples of either machine learning or a, um, military just to share with you. 1:   Uh, an example I was attending, um, I'm working with the D.O.D. through circulatory system Engineering Research Center. 1:   That we have at the School of Systems and Enterprises. 1:   And we had a meeting presenting the research that we, as principal investigators, did for the day of the. 1:   And there was an invited guests who was talking about the some examples that he collected in a book that is called The War None The Army of Man. 1:   And he was he was serving in Afghanistan. 1:   And at the certain point, he was a close with the ledger, with the platoon, where, 1:   I mean, he was part of a and the certain point that a girl approached them. 1:   It was a nice build. 1:   They offer her some food. She stayed for a bit. 1:   Then she left. And then from the village, they started attacking them. 1:   So the question he had was if instead of us, 1:   there was an artificial intelligence evaluating the risk that that girl could have been sent to spy on us and then decide to kill the girl. 1:   Well, an artificial intelligence that would have done that. 1:   We didn't. So that's an example of human judgment. 1:   So there are autonomous vehicles. 1:   So some of the drones are both aerial drones or land drones. 1:   They are fully automated, but there are quite a lot of questions. 1:   So in the US we tend to have more ethical questions, even in our warfare. 1:   Some other countries may not. So we know that there are unmanned vehicles that are on the territory just acting as elements, active elements of war. 1:   So I would stop here and I would briefly go into something completely different. 1:   I will go very fast on this. So I will leave you this. 1:   Lights. So, software development. 1:   We know how it was created. One of the main issues when you develop software is that the specs are being lost in translation. 1:   So you have what the client needs, what the contractor understood, what the other of the subcontractor. 1:   And then at the very end, the delivery may be not exactly either what the client wanted or what the client asked them. 1:   So between what the client is asking, what the client it will want, and what the contractor is perceiving as a need, 1:   and what the I mean, all the parts are moving parts and there is quite a lot of lost in translation. 1:   So when you develop software, there is a portion of that is on development, the actual development. 1:   Fortunately, there is in testing and a portion of it is in maintenance. 1:   The sooner you find the harasser, the lower the cost of fixing is going to be. 1:   So one of the elements that we generally consider when we develop software is the concept of the software lifecycle. 1:   So the software as a. A beginning and an end. 1:   Some software apps thought of zombies. They never die. 1:   So we still have a COBOL software that is running since a few years. 1:   And it is running. It is running in public administration is running in some banks, but generally speaking you have a lifecycle and the retirement. 1:   So not always but most of the time that I tribally as a standard for that. 1:   So one of the traditional ways of developing software is a waterfall. 1:   So the waterfall meaning that you start with the requirements based on the requirements you do the design based on the design do decoding. 1:   When you complete the coding, you build testing and then you release the software to. 1:   The client, the annual start, the maintenance. 1:   So unfortunately, this approach, even if is great, for defining the requirements as a lack of flexibility for changes, 1:   making it not very successful, actually only 16% in the peak. 1:   This approach was called successful. 1:   So you can add a little bit of flexibility with a little bit of prototyping and later parameterization, but the problem is deeper than that. 1:   So that's basically when software engineering started the. 1:   With the idea of applying elements of engineering to engineering, 1:   design and development to software and computer science is essential for software 1:   engineering that physics is essential for electrical or mechanical engineering. 1:   I generally describe software engineering as a system. 1:   Engineering applied to computer science is basically considering the complexity of the software development 1:   that it is not just developing the components but having the components working in the proper environment. 1:   The The Software Engineering Institute that is currently based within Carnegie Mellon University in Pittsburgh. 1:   I was a partner member of a software engineer initiative to develop the capability maturity model. 1:   That is a way to define levels, to measure where you are and each level and help you moving up in the levels. 1:   It is very rare that you have a software at level four. 1:   Very few, if not none either. Higher than that. 1:   So those are a definition of a level. But in order to move from level C, you need to measure everything. 1:   Meaning you need to have a sort of big brother, big sister meaning quiet. 1:   Although all that adds to capture a score, analyze the information when you have a very large projector, 1:   the cost that the overhead may be justified by the size of the project. 1:   When you have smaller projects, not so much Agile. 1:   Agile was introduced kind of to keep a methodology, but with more degrees of flexibility. 1:   The GI Manifesto. So that's the main characteristics. 1:   We do not offer courses on Agile in a broad sense, so we do offer courses on Agile for software development. 1:   And we talk about agile development or agile management in our project management course. 1:   Principles. So pretty much you interact a lot. 1:   You have a lot of prototyping. So those are some of the methods. 1:   Scrum is one of the most common. And. 1:   I would stop here because it's, uh, it's later than 745 and you have the lights and you can definitely go through them. 1:   I strongly encourage you also to go through there's lights and I mean, they are relatively old, 1:   but is a description of one of the largest failures in software development. 1:   The in this case was by the FBI. 1:   The ritual case file that court said asks that it be a huge amount of money generating not much. 1:   So spoiler alert the real reason is the lack of communication and lack of clear responsibilities. 1:   So again, it's not just a matter of how you develop software, but how you manage your bank. 1:   And so from the client to the developer, the environment, the real needs. 1:   So it's kind of interesting. 1:   I use the original slides that were published in 2005 but didn't change anything because I just wanted to give you. 1:   An example of what's going on. All right. 1:   So in class exercise. So the in class exercise, it's about analyzing a password. 1:   So the user will impose a set of passwords separated by a comma. 1:   And you need to check if all those conditions are verified, meaning the password is at least one letter between those, 1:   one number between those, some of the capital letters, some special characters, minimum length of maximum lengths. 1:   And you will take from the user the. 1:   Passwords. Most likely, at least. 1:   And then you look into the list. You would write a function of. 1:   That will take each one of the passwords and do the checks. 1:   And then returning is good. Is not good. 1:   All right, so I'm stopping the sharing that I'm publishing. 1:   The in-class exercise. 1:   SHUBERT Yes. Well. And. 1:   I will create the breakout rooms. Four breakout rooms to three participants, creating opening 15 minutes. 1:   All right. The only 2 minutes. Then 15 minutes. 1:   Hi. Sorry, my laptop died. Can you put me back in the room? 1:   One. Okay. Hold on a sec. 1:   I sprinted to get my charger so I can go back there. 1:   Okay. Thank you. 1:   Sure. Resuming the recording. 1:   Okay. So we are all back. Any one of you want to share either the solution or just the frustration of not having enough time to do it? 1:   All right. Okay, so let me. 1:   Share the screen and let me do. 1:   Here. So. 1:   I use the the library string. 1:   Uh, you can do without. 1:   So let's see both the options. So in the comment, I added all the requirements. 1:   Then I imported this string. You would see how this would be used. 1:   And then this is the function validating the password. 1:   So it's taking a single password then is checking if is either less than six characters or more than 12. 1:   If this is the case, it will return an. 1:   Dan is transforming the world into a place when you transform a stranger into a least the elements on the list, 1:   that will be the characters, the single characters. 1:   So one element, one character from the origins thing. 1:   Then I created a list that I will use for getting the okay for each one of the tests. 1:   So there are four tests to do as more letters, numbers, capital letters, special characters and order each one. 1:   I will eventually change the RN into White. 1:   So the first one I am checking if the element has a I mean, I need to go a little bit down. 1:   So I created a list for small letters, capital letters, member numbers. 1:   This can be done using the function three anchor or you can manually create. 1:   So in comments I use the manual creation on those lists. 1:   Out of the comments I use the function string that it was mentioning before. 1:   So those are the values. So. And checking if it's more letters. 1:   If a is a number is numbers, if capital letters, special characters, each one is an okay. 1:   Then we'll change that and into y and then. 1:   If it's and then we're done the list. 1:   I mean, if no one of those or those if we generate it. 1:   Yes. You will get no other otherwise. 1:   Yes. So when you go down. 1:   You pass the elements. 1:   So it's taking the password. Then there is a passenger, each one of their passwords into the function, and then if it is returning, why then is okay. 1:   Otherwise we'll continue. Who let me save the cops. 1:   This list here, and I will use it as an example. 1:   So let me run it. Back. 1:   And as a result. All right. 1:   So you will find the two more. 1:   Filed by those creeps. Just play with them. 1:   But they will get back the next class to explain a little bit more what they are. 1:   So they are basically to generate reports or to analyze the files for what is called data, exploratory analysis. 1:   So one is doing the full exploration. 1:   The other one is a focus on the coordination on. 1:   All right. So. Next assignment, the. 1:   It's going to be. On COVID 19. 1:   So the file that you will use for your analysis will be, What is it? 1:   Will be like this one. So you have. 1:   Condition groups specific condition. 1:   A code that you would not use the age group. 1:   The number of deaths and the number of mentions. 1:   So this is the file you have. So based on that, the. 1:   You will do. 1:   I mean, let's keep but one of is just questions. 1:   So you will basically calculate the number of people in the different categories. 1:   So where are categories it would be? 1:   So you will not read the file. You will create a dictionary, or you want to use pandas and feel free to do it. 1:   Population less than 35. Then you will. 1:   Plot the. For two elements. 1:   So one bar plot and a pie plot similar to this one. 1:   But this one is from a different example. So not the actual graph. 1:   And we'll show the distribution of deaths by age group, meaning you want to calculate. 1:   So create a dictionary with the number of deaths for each one of the categories. 1:   You would use a function. So. 1:   You will then generate the chart. 1:   Bar by. Ukraine, their co-morbidity with the highest number of deaths with a population of less than 35. 1:   Run correlation analysis to determine the relationship between variables. 1:   You will have your script. 1:   And then there will be part one. That would be just questions. 1:   So what is a giant software? What is a why? 1:   Waterfall is not a great solution to define machine learning. 1:   And some examples. So that's basically it. 1:   If you have time, adjust and eventually you will have the recording, considering you will use the correlation. 1:   I just want to be sure that you will see it running at least once. 1:   So correlation. In this case, I'm using a basic pandas functions and my plot labor to. 1:   I have the graphical representation of the keyboard and is another graphical library on top of my plot. 1:   So I'm reading the NFL Census CSB file. 1:   Calculating the correlation and representing it as a graph and eventually save it has a point. 1:   If I run it. The fine is relatively big. 1:   You have the correlation. So you will see that the salary, for example. 1:   So the correlation is a negative correlation, meaning one is decreased is increasing, the other is decreasing, and vice versa. 1:   Positive one is increasing and the other is decreasing. So the more blue is, the more positive. 1:   So obviously along the main diagonal there is a one. 1:   So you have the salary. 1:   That will increase with experience and if the Pro Bowler will be higher. 1:   That's an example. And you will have the file that would be generated. 1:   The other one is a little bit more powerful, even if a few lines. 1:   So. Is generating. 1:   A reporter. So it's creating it. 1:   Summarizing the dataset. Generating the report. 1:   Generating an edge male end is basically something that. 1:   Not sure where this one comes. 1:   Go here for a moment. But. 1:   So there's the excitement that is being generated. 1:   It's I mean, it's amazing that in Q line to line, basically you can generate the entire report with overview description of variables. 1:   Distributional numbers. Analysis for each one of the variables, including the distribution. 1:   To evaluate the skewness. Then you have. 1:   Interaction correlation pretty much as the other one. 1:   Either as a heat map or a stable. Missing values and a sample. 1:   So all of this. With pretty much one library. 1:   And one. I mean, there's three things. And one line. 1:   Two lines. And then if you want the five, three lines. 1:   Okay. So I will post all the content again. 1:   I already send an email to the DEA and the grader to make sure that the evaluation will be less strict as the previous one for text size for. 1:   And you will have another couple of days for your submission. 1:   Not that you need to do a late submission, but you have the option.
 

aft, STEVENS

lw INSTITUTE of TECHNOLOGY

Text mining in an evolving
society



clipizzi@stevens.edu

SSE

 

Language: How we acquire it

   

B.F. Skinner

Behaviorist theory

infants learn language from
other human role models
through a process involving
imitation, rewards, and
practice

   

Tt ot

J. Piaget

Constructivist theory

Language is acquired within the
context of the child's broader
intellectual development.
Language is not an
independent system, but part
of our general cognitive
makeup

 

 

N. Chomsky

Nativist theory

Children are born equipped
with an innate template for
language, and this blueprint
aids the child in the task of
constructing a grammar for
their language

STEVENS INSTITUTE of TECHNOLOGY | 2

ne | A
i

Language as part of the human evolution 

 



STEVENS INSTITUTE of TECHNOLOGY | 3

The relevance of the Medium: 
The Medium is the message?

"..Technological media are staples or
natural resources, exactly as are coal
and cotton and oil”

 

Marshall McLuhan (1911 — 1980)

"... Tis the medium that shapes and controls the scale and form
of human association and action. The content or uses of such
media are as diverse as they are ineffectual in shaping the form
of human association”

STEVENS INSTITUTE of TECHNOLOGY | 4

 

The “Printing” society

Focused on
Religion
Education
Industry
Thought
Conflict
Ideas
Community
Organization
Truth

STEVENS INSTITUTE of TECHNOLOGY | 5

 

 

The “Mass Media” society le

Focused on

Leisure time
Education
Knowledge of the other
Politics
Global Connections

Speed of Life

Mean World Effect “At an accelerating pace throughout the
century, the electronic transmission of news and

Minimizing Empathy
entertainment changed virtually all features of
American Life” (Robert Putnam, Bowling Alone).

 

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 6

 

The “Always-on Internet” society

Focused on
Sharing
Cooperation
Collective Actions
Reduced Privacy
Source uncertainty
Information overload
“Living in the cloud”

 

ee
STEVENS INSTITUTE of TECHNOLOGY | 7

 

“Living in the cloud” effects: the lw
overstimulation

¢ The average attention span in 2015 - 8 seconds
¢« The average attention span in 2000 - 12 seconds
« The average attention span of a gold fish - 9 seconds

¢ Percent of teens who forget major details of close friends and
relatives - 25 %

¢ Percent of people who forget their own birthdays from time to time -
7 %

« Average number of times per hour an office worker checks their
email inbox - 30

¢ Average length watched of a single internet video - 2.7 minutes

STEVENS INSTITUTE of TECHNOLOGY | 8

 

How we communicate in “living in the lw
cloud” times

 

¢ Percent of page views that last less than 4 seconds: 17%

¢ Percent of page views that lasted more than 10 minutes: 4%

« Percent of words read on web pages with 111 words or less: 49 %

¢ Percent of words read on an average (593 words) web page: 28 %
¢« Users soend only 4.4 seconds more for each additional 100 words

Source: Harald Weinreich, Hartmut Obendorf, Eelco Herder, and Matthias Mayer: “Not Quite the Average:
An Empirical Study of Web Use,” in the ACM Transactions on the Web, vol. 2, no. 1 (February 2008).

STEVENS INSTITUTE of TECHNOLOGY | 9

Understanding what people say — Text lw
mining today

¢ Communications today are becoming shorter, less structured, more
“chopped”

¢ Some of the media have intrinsic limitations in terms of length but even
those with no such limitations tend to be used for short, fragmented, multi
threads or even single message Communications. This implies less
syntactic structure, less rhetorical elements

¢ Traditional methods to extract knowledge from text are based on the
existence of a predefined structure, that can be in the syntax, in writing
patterns, in preset taxonomies/ontologies

¢ The lack of structure makes the traditional media either less or not
effective anymore

STEVENS INSTITUTE of TECHNOLOGY | 10

 

   

‘) STEVENS

ae INSTITUTE of TECHNOLOGY
Jy INNOVATION 1 RSIT\

Mining Social media

Using the social structure as a proxy for
the lacking conversational structure

 

Mining Social media

 

 

When communications have a
social component, the social
structure may act as a proxy for
the conversational structure

 

STEVENS INSTITUTE of TECHNOLOGY | 12

cf
Extracting “meanings” 

• Using a combination of semantic and topological
analyses we can extract dynamic concept maps
from social media streams
• Conversations are characterized by structural
patterns whose properties can be assessed
through quantitative metrics
• A tool has based on this approach can be used
as a “backchannel” for real life events




° Conversational patterns can provide dynamic insights on what people say about
the event

° Conversational metrics potentially contain early signals to predict users’
oreferences and choices

STEVENS INSTITUTE of TECHNOLOGY | 13

 

ani ii

Focus on Twitter 

   
We used message streams from Twitter micro-
blogs because of

    
 • Twitter popularity
• Micro-blog is the main social
medium to share & broadcast
opinions
• Wide availability for downloads
• Intrinsic reduced semantic complexity
due to the limited number of characters
     

   
  

STEVENS INSTITUTE of TECHNOLOGY | 14

 

[~~ Sl

Perspective: Conversational analysis ©&

 

° We use a conversational metaphor and
assume that in backchanneling applications
micro-blogs stream exhibit some properties
of conversations

° According to the Common Ground theory a
conversation is a form of collective action
requiring participants to coordinate on
content and on process (Brennan & Clark,
1991)

 

STEVENS INSTITUTE of TECHNOLOGY | 15




 



 


Mining open texts
Extracting the structure

 

 

- cf.
Mining open texts lw

¢ When communications have no social component, there is no
social structure that can act as a proxy of the conversational one.
This is the case for novels or larger corpora

¢ The distributional semantic approach, quantifying and categorizing
semantic similarities between linguistic items based on their
distributional properties in text, can be the basis to induce the
missing structure

¢ One of the increasingly popular methods to extract topics from text
is word2vec, that is a group of models that are used to produce
word embeddings (Mikolov, Chen, Corrado, & Dean, 2013a)

¢ Word embedding is the collective name for a set of language
modeling and feature learning techniques in natural language
processing where words or phrases from the source are mapped to
vectors of real numbers (Mikolov, Sutskever, Chen, Corrado, & Dean,
2013b)

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 49

Word2Vec lw

Published by Google in 2013
Python implementation in 2014 (gensim library)
Generates distributed vector representations of
words (“word to vec”) using a neural net
e In those distributed vector representations of words

o each word Is encoded as a vector of floats

0 V@Cgueen= (0.2, -0.3, .7, 0, ..., .3)

0 V@Cwoman = (0.1, -0.2, .6, 0.1, ..., .2)

o length of the vectors = dimension of the word
representation

o key concept of word2vec: words with similar vectors
have a similar meaning (context)

STEVENS INSTITUTE of TECHNOLOGY | 50

 

Why this approach is relevant & how is structured lw

¢ Zellig Harris (1954):
— “oculist and eye-doctor ... occur in almost the same
environments”

— “If Aand B have almost identical environments we say that
they are synonyms.”

¢ Firth (1957):

— “You shall know a word by the company it keeps!”
¢ Intuition for algorithm:

— Two words are similar if they have similar word contexts
¢ The meaning of a word Is a vector of numbers

— Vector models are also called “embeddings”

STEVENS INSTITUTE of TECHNOLOGY | 54

 

 

ots

From words co-occurrence to embeddings lw

¢« Word co-occurrence measure how often a word occurs with
another, within a given number of words of separation

¢ Using co-occurrence we can create a word-word co-
occurrence matrix, where rows and columns are the unique
words in the source text

¢ Each word is represented this way by a vector, that Is soarse and
with most of the values being zero

¢ The whole approach Is based only on the actual words proximity

STEVENS INSTITUTE of TECHNOLOGY | 52

From words co-occurrence to embeddings - lw
Word2Vec

¢ Instead of entire documents, Word2vec uses words a few
positions away from each center word. The pairs of center
word/context word are called “skip-grams”

“It was a bright cold day in April, and the clocks were striking”
Center word: red

Context words: blue

¢e Word2vec considers all words as center words, and all their
context words

STEVENS INSTITUTE of TECHNOLOGY | 53

 

 

From words co-occurrence to embeddings - lw


*: predicting the context given a word
**> Continuous Bag Of Words - predicting the word
given its context


Distributed Representations of Words and Phrases and their Compositionality
Tomas Mikolov, Ilva Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, NIPS 2013

STEVENS INSTITUTE of TECHNOLOGY | 54

 

From words co-occurrence to embeddings — lw
Word2Vec
Word2vec optimizes a softmax* loss for each output word:
7 exp(u/ v;)
pl) =

k=1 €XP(U; Vi)

Where j is the output word, i is the input word. j ranges over a context of +
3-5 positions around the Inout word

u IS Gn Output embedding vector
v Is AN input embedding vector
Word2vec can be implemented with standard ANN toolkits, by

backpropagating to optimize u and v

*Softmax or normalized exponential function is normally used to highlight the largest values

SS LULU
STEVENS INSTITUTE of TECHNOLOGY | 55

ots

Using Word2Vec to create a network structure lw

¢ Data collection and Preprocessing. Word2vec is a 2 steps
process: training the algorithm and using it

¢ Creation of the word tables. Using Python/Gensim, we created 2
tables:

— Annbyn matrix, with all fhe n unique words on both the
headers and the values from the word2vec embeddings

— Areduced dimension matrix n by m where the n words are
represented by a vector composed by m dimensions. We
used m = 300 and we cut out words occurring less than a
given number of times, depending on the size of the source
text

STEVENS INSTITUTE of TECHNOLOGY | 5,

 

 

ots

Using Word2Vec to create a network structure lw

¢ Network elements definition
— Nodes. The nodes of the network are the words from the
above matrix. Their attributes are their m dimensions
— Relationships. Two nodes are related if their cosine similarity is
above a given threshold, calculated starting from the
average
¢ Network representation. We used a combination of Networx,
pyvis and eventually Gephi, a GUl-based network analysis tool,
for fast proof of concepts

STEVENS INSTITUTE of TECHNOLOGY | 57

Using the network structure lw

¢ Just as in the social network-based case, from the network we
extracted clusters representing words aggregations/topics. As in the
social network based case, for the clustering, we used a
combination of k-core and Lauvain community detection methods.
In order to reduce the noise, we focused on larger clusters only

i |
\ 7 4 . ©
F ~ © 4
ks =

es ©

     

 

STEVENS INSTITUTE of TECHNOLOGY | 58

 

